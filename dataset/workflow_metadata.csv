repo,workflow_file,content,loc,num_jobs,num_triggers,trigger_types,num_reused_actions
google/truth,ci.yml,"name: CI

on:
  push:
    branches:
      - master
  pull_request:
    branches:
      - master

jobs:
  test:
    name: ""JDK ${{ matrix.java }}""
    strategy:
      matrix:
        java: [ 8, 11 ]
    runs-on: ubuntu-latest
    steps:
      # Cancel any previous runs for the same branch that are still running.
      - name: 'Cancel previous runs'
        uses: styfle/cancel-workflow-action@85880fa0301c86cca9da44039ee3bb12d3bedbfa
        with:
          access_token: ${{ github.token }}
      - name: 'Check out repository'
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      - name: 'Set up JDK ${{ matrix.java }}'
        uses: actions/setup-java@c5195efecf7bdfc987ee8bae7a71cb8b11521c00
        with:
          java-version: ${{ matrix.java }}
          distribution: 'zulu'
          cache: 'maven'
      - name: 'Install'
        shell: bash
        run: mvn -B -P!standard-with-extra-repos install -U -DskipTests=true
      - name: 'Test'
        shell: bash
        run: mvn -B -P!standard-with-extra-repos verify -U -Dmaven.javadoc.skip=true
      - name: 'Javadoc Test Run'
        shell: bash
        run: mvn -B -P!standard-with-extra-repos javadoc:aggregate -U

  publish_snapshot:
    name: 'Publish snapshot'
    needs: test
    if: github.event_name == 'push' && github.repository == 'google/truth'
    runs-on: ubuntu-latest
    steps:
      - name: 'Check out repository'
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      - name: 'Set up JDK 11'
        uses: actions/setup-java@c5195efecf7bdfc987ee8bae7a71cb8b11521c00
        with:
          java-version: 11
          distribution: 'zulu'
          cache: 'maven'
          server-id: sonatype-nexus-snapshots
          server-username: CI_DEPLOY_USERNAME
          server-password: CI_DEPLOY_PASSWORD
      - name: 'Publish'
        env:
          CI_DEPLOY_USERNAME: ${{ secrets.CI_DEPLOY_USERNAME }}
          CI_DEPLOY_PASSWORD: ${{ secrets.CI_DEPLOY_PASSWORD }}
        run: mvn -B clean source:jar javadoc:jar deploy -DskipTests=true

  generate_docs:
    name: 'Generate latest docs'
    needs: test
    if: github.event_name == 'push' && github.repository == 'google/truth'
    runs-on: ubuntu-latest
    steps:
      - name: 'Check out repository'
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      - name: 'Set up JDK 11'
        uses: actions/setup-java@c5195efecf7bdfc987ee8bae7a71cb8b11521c00
        with:
          java-version: 11
          distribution: 'zulu'
          cache: 'maven'
      - name: 'Generate latest docs'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: ./util/generate-latest-docs.sh
",82,3,2,"push, pull_request",7
google/dagger,ci.yml,"name: CI

on:
  push:
    branches:
      - master
  pull_request:
    branches:
      - master

env:
  USE_JAVA_DISTRIBUTION: 'zulu'
  USE_JAVA_VERSION: '11'
  # This is required by Gradle 8.0+.
  USE_JAVA_VERSION_FOR_GRADLE_PLUGIN: '17'
  # Required by JDK Toolchain Configuration
  USE_JAVA_VERSION_FOR_GRADLE: '18'
  # The default Maven 3.9.0 has a regression so we manually install 3.8.7.
  # https://issues.apache.org/jira/browse/MNG-7679
  USE_MAVEN_VERSION: '3.8.7'

jobs:
  validate-latest-dagger-version:
    name: 'Validate Dagger version'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/prechecks
  bazel-build:
    name: 'Bazel build'
    needs: validate-latest-dagger-version
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/bazel-build
  bazel-test:
    name: 'Bazel tests'
    needs: validate-latest-dagger-version
    runs-on:
      group: large-runner-group
      labels: ubuntu-22.04-16core
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/bazel-test
  gradle-build:
    name: 'Gradle build'
    runs-on:
      group: large-runner-group
      labels: ubuntu-22.04-16core
    steps:
    - uses: actions/checkout@v4
    - uses: ./.github/actions/gradle-build
  artifact-verification-tests:
    name: 'Artifact verification tests'
    needs: bazel-build
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/artifact-verification-tests
  artifact-java-local-tests:
    name: 'Artifact Java local tests'
    needs: bazel-build
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/artifact-java-local-tests
  test-gradle-plugin:
    name: 'Test Hilt Gradle plugin'
    needs: bazel-build
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/test-gradle-plugin
  artifact-android-local-tests:
    name: 'Artifact Android local tests (AGP ${{ matrix.agp }})'
    needs: bazel-build
    runs-on: ubuntu-latest
    strategy:
      matrix:
        include:
          - agp: '8.1.1'
            jdk: '17'
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/artifact-android-local-tests
        with:
          agp: '${{ matrix.agp }}'
          jdk: '${{ matrix.jdk }}'
  artifact-android-emulator-legacy-api-tests:
    name: 'Artifact Android emulator tests (API ${{ matrix.api-level }})'
    # We only run this on master push (essentially a postsubmit) since these
    # can take a while to run
    if: github.event_name == 'push' && github.repository == 'google/dagger' && github.ref == 'refs/heads/master'
    needs: bazel-build
    # It's recommended to run emulator tests on macOS
    # See https://github.com/marketplace/actions/android-emulator-runner
    runs-on: macos-latest
    strategy:
      matrix: # Run on 16 (PreL), 21 (L), and 26 (O).
        api-level: [16, 21, 26, 30]
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/artifact-android-emulator-tests
        timeout-minutes: 35
        with:
          api-level: '${{ matrix.api-level }}'
  publish-snapshot:
    name: 'Publish snapshot'
    # TODO(bcorso): Consider also waiting on artifact-android-emulator-tests
    # and artifact-android-emulator-legacy-api-tests after checking flakiness.
    needs: [
      bazel-test,
      artifact-verification-tests,
      artifact-java-local-tests,
      artifact-android-local-tests,
      test-gradle-plugin
    ]
    if: github.event_name == 'push' && github.repository == 'google/dagger' && github.ref == 'refs/heads/master'
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: 'Install Java ${{ env.USE_JAVA_VERSION }}'
        uses: actions/setup-java@v4
        with:
          distribution: '${{ env.USE_JAVA_DISTRIBUTION }}'
          java-version: '${{ env.USE_JAVA_VERSION }}'
          server-id: sonatype-nexus-snapshots
          server-username: CI_DEPLOY_USERNAME
          server-password: CI_DEPLOY_PASSWORD
      - name: 'Check out repository'
        uses: actions/checkout@v4
      - name: 'Cache local Maven repository'
        uses: actions/cache@v4
        with:
          path: |
            ~/.m2/repository
            !~/.m2/repository/com/google/dagger
          key: ${{ runner.os }}-maven-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            ${{ runner.os }}-maven-
      - name: 'Cache Bazel files'
        uses: actions/cache@v4
        with:
          path: ~/.cache/bazel_github # The bazel cache path set in .bazelrc.
          key: ${{ runner.os }}-bazel-build-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-bazel-build-
      - name: 'Cache Gradle files'
        uses: actions/cache@v4
        with:
          path: |
            ~/.gradle/caches
            ~/.gradle/wrapper
          key: ${{ runner.os }}-gradle-${{ hashFiles('**/*.gradle*', '**/gradle-wrapper.properties') }}
          restore-keys: |
            ${{ runner.os }}-gradle-
      - name: 'Publish latest docs'
        run: ./util/generate-latest-docs.sh
        shell: bash
        env:
          GH_TOKEN: ${{ github.token }}
      - name: 'Publish latest snapshot'
        run: |
          util/deploy-all.sh \
            ""deploy:deploy-file"" \
            ""HEAD-SNAPSHOT"" \
            ""-DrepositoryId=sonatype-nexus-snapshots"" \
            ""-Durl=https://oss.sonatype.org/content/repositories/snapshots""
        shell: bash
        env:
          CI_DEPLOY_USERNAME: ${{ secrets.CI_DEPLOY_USERNAME }}
          CI_DEPLOY_PASSWORD: ${{ secrets.CI_DEPLOY_PASSWORD }}
      - name: 'Clean bazel cache'
        run: rm -rf $(bazel info repository_cache)
        shell: bash
  build-gradle-plugin-latest-agp:
    name: 'Build Hilt Gradle plugin against latest AGP version'
    # We only run this on master push (essentially a postsubmit) since we
    # don't want this job to prevent merges
    if: github.event_name == 'push' && github.repository == 'google/dagger' && github.ref == 'refs/heads/master'
    needs: bazel-build
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/build-gradle-plugin
        with:
          agp: '+'
  cleanup-caches:
    name: 'Clean up GitHub Action caches'
    # TODO(bcorso): Consider also waiting on artifact-android-emulator-tests
    # and artifact-android-emulator-legacy-api-tests after checking flakiness.
    needs: [
      bazel-test,
      artifact-verification-tests,
      artifact-java-local-tests,
      artifact-android-local-tests,
      test-gradle-plugin
    ]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/cleanup-caches
",203,12,2,"push, pull_request",27
google/dagger,release.yml,"name: Dagger Release

on:
  workflow_dispatch:
    inputs:
      dagger_release_version:
        description: 'The Dagger version to use in this release.'
        required: true

env:
  USE_JAVA_DISTRIBUTION: 'zulu'
  USE_JAVA_VERSION: '11'
  # This is required by Gradle 8.0+.
  USE_JAVA_VERSION_FOR_GRADLE_PLUGIN: '17'
  # Required by JDK Toolchain Configuration
  USE_JAVA_VERSION_FOR_GRADLE: '18'
  DAGGER_RELEASE_VERSION: ""${{ github.event.inputs.dagger_release_version }}""
  # The default Maven 3.9.0 has a regression so we manually install 3.8.7.
  # https://issues.apache.org/jira/browse/MNG-7679
  USE_MAVEN_VERSION: '3.8.7'

# TODO(bcorso):Convert these jobs into local composite actions to share with the
# continuous integration workflow.
jobs:
  validate-latest-dagger-version:
    name: 'Validate Dagger version'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/prechecks
  bazel-build:
    name: 'Bazel build'
    needs: validate-latest-dagger-version
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/bazel-build
  bazel-test:
    name: 'Bazel tests'
    needs: validate-latest-dagger-version
    runs-on:
      group: large-runner-group
      labels: ubuntu-22.04-16core
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/bazel-test
  artifact-verification-tests:
    name: 'Artifact verification tests'
    needs: bazel-build
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/artifact-verification-tests
  artifact-java-local-tests:
    name: 'Artifact Java local tests'
    needs: bazel-build
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/artifact-java-local-tests
  test-gradle-plugin:
    name: 'Test Hilt Gradle plugin'
    needs: bazel-build
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/test-gradle-plugin
  artifact-android-local-tests:
    name: 'Artifact Android local tests (AGP ${{ matrix.agp }})'
    needs: bazel-build
    runs-on: ubuntu-latest
    strategy:
      matrix:
        include:
          - agp: '8.1.1'
            jdk: '17'
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/artifact-android-local-tests
        with:
          agp: '${{ matrix.agp }}'
          jdk: '${{ matrix.jdk }}'
  publish-artifacts:
    name: 'Publish Artifact'
    needs: [
      bazel-test,
      artifact-verification-tests,
      artifact-java-local-tests,
      artifact-android-local-tests,
      test-gradle-plugin
    ]
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: 'Install Java ${{ env.USE_JAVA_VERSION }}'
        uses: actions/setup-java@v4
        with:
          distribution: '${{ env.USE_JAVA_DISTRIBUTION }}'
          java-version: '${{ env.USE_JAVA_VERSION }}'
          server-id: sonatype-nexus-staging
          server-username: CI_DEPLOY_USERNAME
          server-password: CI_DEPLOY_PASSWORD
          gpg-private-key: ${{ secrets.CI_GPG_PRIVATE_KEY }}
          gpg-passphrase: CI_GPG_PASSPHRASE
      - name: 'Check out repository'
        uses: actions/checkout@v4
      - name: 'Cache local Maven repository'
        uses: actions/cache@v4
        with:
          path: |
            ~/.m2/repository
            !~/.m2/repository/com/google/dagger
          key: ${{ runner.os }}-maven-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            ${{ runner.os }}-maven-
      - name: 'Cache Bazel files'
        uses: actions/cache@v4
        with:
          path: ~/.cache/bazel_github # The bazel cache path set in .bazelrc.
          key: ${{ runner.os }}-bazel-build-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-bazel-build-
      - name: 'Cache Gradle files'
        uses: actions/cache@v4
        with:
          path: |
            ~/.gradle/caches
            ~/.gradle/wrapper
          key: ${{ runner.os }}-gradle-${{ hashFiles('**/*.gradle*', '**/gradle-wrapper.properties') }}
          restore-keys: |
            ${{ runner.os }}-gradle-
      - name: Publish artifacts
        run: |
          util/deploy-all.sh \
            ""gpg:sign-and-deploy-file"" \
            ""${{ env.DAGGER_RELEASE_VERSION }}"" \
            ""-DrepositoryId=sonatype-nexus-staging"" \
            ""-Durl=https://oss.sonatype.org/service/local/staging/deploy/maven2/""
        shell: bash
        env:
          CI_DEPLOY_USERNAME: ${{ secrets.CI_DEPLOY_USERNAME }}
          CI_DEPLOY_PASSWORD: ${{ secrets.CI_DEPLOY_PASSWORD }}
          CI_GPG_PASSPHRASE: ${{ secrets.CI_GPG_PASSPHRASE }}
      - name: 'Set git credentials'
        run: |
            git config --global user.email ""dagger-dev+github@google.com""
            git config --global user.name ""Dagger Team""
        shell: bash
      - name: 'Publish tagged release'
        run: util/publish-tagged-release.sh ${{ env.DAGGER_RELEASE_VERSION }}
        shell: bash
      - name: 'Publish tagged docs'
        run: util/publish-tagged-docs.sh ${{ env.DAGGER_RELEASE_VERSION }}
        shell: bash
        env:
          GH_TOKEN: ${{ github.token }}
      - name: 'Clean bazel cache'
        run: rm -rf $(bazel info repository_cache)
        shell: bash
",160,8,1,workflow_dispatch,19
google/auto,ci.yml,"name: CI

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  test:
    name: ""JDK ${{ matrix.java }}""
    strategy:
      matrix:
        java: [ 8, 11, 17 ]
    runs-on: ubuntu-latest
    steps:
      # Cancel any previous runs for the same branch that are still running.
      - name: 'Cancel previous runs'
        uses: styfle/cancel-workflow-action@85880fa0301c86cca9da44039ee3bb12d3bedbfa
        with:
          access_token: ${{ github.token }}
      - name: 'Check out repository'
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      - name: 'Cache local Maven repository'
        uses: actions/cache@5a3ec84eff668545956fd18022155c47e93e2684
        with:
          path: ~/.m2/repository
          key: maven-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            maven-
      - name: 'Set up JDK ${{ matrix.java }}'
        uses: actions/setup-java@c5195efecf7bdfc987ee8bae7a71cb8b11521c00
        with:
          java-version: ${{ matrix.java }}
          distribution: 'zulu'
      - name: 'Install'
        shell: bash
        run: mvn -B dependency:go-offline test clean -U --quiet --fail-never -DskipTests=true -f build-pom.xml
      - name: 'Test'
        shell: bash
        run: mvn -B verify -U --fail-at-end -Dsource.skip=true -Dmaven.javadoc.skip=true -f build-pom.xml

  publish_snapshot:
    name: 'Publish snapshot'
    needs: test
    if: github.event_name == 'push' && github.repository == 'google/auto'
    runs-on: ubuntu-latest
    steps:
      - name: 'Check out repository'
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      - name: 'Cache local Maven repository'
        uses: actions/cache@5a3ec84eff668545956fd18022155c47e93e2684
        with:
          path: ~/.m2/repository
          key: maven-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            maven-
      - name: 'Set up JDK 11'
        uses: actions/setup-java@c5195efecf7bdfc987ee8bae7a71cb8b11521c00
        with:
          java-version: 11
          distribution: 'zulu'
          server-id: sonatype-nexus-snapshots
          server-username: CI_DEPLOY_USERNAME
          server-password: CI_DEPLOY_PASSWORD
      - name: 'Publish'
        env:
          CI_DEPLOY_USERNAME: ${{ secrets.CI_DEPLOY_USERNAME }}
          CI_DEPLOY_PASSWORD: ${{ secrets.CI_DEPLOY_PASSWORD }}
        run: ./util/publish-snapshot-on-commit.sh

  generate_docs:
    name: 'Generate latest docs'
    needs: test
    if: github.event_name == 'push' && github.repository == 'google/auto'
    runs-on: ubuntu-latest
    steps:
      - name: 'Check out repository'
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      - name: 'Cache local Maven repository'
        uses: actions/cache@5a3ec84eff668545956fd18022155c47e93e2684
        with:
          path: ~/.m2/repository
          key: maven-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            maven-
      - name: 'Set up JDK 17' # need 15+ to avoid https://bugs.openjdk.org/browse/JDK-8241780
        uses: actions/setup-java@c5195efecf7bdfc987ee8bae7a71cb8b11521c00
        with:
          java-version: 17
          distribution: 'zulu'
      - name: 'Generate latest docs'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: ./util/generate-latest-docs.sh
",97,3,2,"push, pull_request",10
google/go-github,linter.yml,"on: [push, pull_request]
name: linter

permissions:
  contents: read

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
    - uses: actions/setup-go@d35c59abb061a4a6fb18e82ac0862c26744d6ab5 # v5.5.0
      with:
        go-version: 1.x
        cache-dependency-path: ""**/go.sum""
    - run: script/lint.sh
      env:
        CHECK_GITHUB_OPENAPI: 1
        GITHUB_TOKEN: ${{ github.token }}
",19,1,2,"push, pull_request",2
google/go-github,tests.yml,"concurrency:
  group: ${{ github.repository }}-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

on:
  push:
    branches:
      - master
  pull_request:
    branches:
      - master

name: tests
env:
  GO111MODULE: on

permissions:
  contents: read

jobs:
  test:
    permissions:
      contents: read
      id-token: write
    defaults:
      run:
        shell: bash
    strategy:
      matrix:
        go-version: [1.x, 1.23.0] # test with N and the .0 release of N-1
        platform: [ubuntu-latest]
        include:
          # include windows, but only with the latest Go version, since there
          # is very little in the library that is platform specific
          - go-version: 1.x
            platform: windows-latest

          # only update test coverage stats with the most recent go version on linux
          - go-version: 1.x
            platform: ubuntu-latest
            update-coverage: true
    runs-on: ${{ matrix.platform }}

    steps:
      - uses: actions/setup-go@d35c59abb061a4a6fb18e82ac0862c26744d6ab5 # v5.5.0
        with:
          go-version: ${{ matrix.go-version }}
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      # Get values for cache paths to be used in later steps
      - id: cache-paths
        run: |
          echo ""go-cache=$(go env GOCACHE)"" >> $GITHUB_OUTPUT
          echo ""go-mod-cache=$(go env GOMODCACHE)"" >> $GITHUB_OUTPUT

      - name: Cache go modules
        uses: actions/cache@5a3ec84eff668545956fd18022155c47e93e2684 # v4.2.3
        with:
          path: |
            ${{ steps.cache-paths.outputs.go-cache }}
            ${{ steps.cache-paths.outputs.go-mod-cache }}
          key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}
          restore-keys: ${{ runner.os }}-go-

      - name: Run go test
        run: |
          if [ -n ""${{ matrix.update-coverage }}"" ]; then
            script/test.sh -race -covermode atomic -coverprofile coverage.txt ./...
            exit
          fi
          script/test.sh -race -covermode atomic ./...

      - name: Ensure integration tests build
        # don't actually run tests since they hit live GitHub API
        run: go test -v -tags=integration -run=^$ ./test/integration

      - name: Upload coverage to Codecov
        if: ${{ matrix.update-coverage }}
        uses: codecov/codecov-action@18283e04ce6e62d37312384ff67231eb8fd56d24 # v5.4.3
        with:
          use_oidc: true
",81,1,2,"push, pull_request",4
google/go-querystring,linter.yml,"name: linter
on: [push, pull_request]

concurrency:
  group: ""${{ github.workflow }} @ ${{ github.event.pull_request.head.label || github.head_ref || github.ref }}""
  cancel-in-progress: true

jobs:
  lint:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - name: golangci-lint
        uses: golangci/golangci-lint-action@1481404843c368bc19ca9406f87d6e0fc97bdcfd # v7.0.0
        with:
          version: v2.1.2
",17,1,2,"push, pull_request",2
google/go-querystring,tests.yml,"name: tests

on:
  push:
    branches:
      - master
  pull_request:
    branches:
      - master

concurrency:
  group: ""${{ github.workflow }} @ ${{ github.event.pull_request.head.label || github.head_ref || github.ref }}""
  cancel-in-progress: true

jobs:
  test:
    strategy:
      matrix:
        go-version: [stable, oldstable, ""1.13""]
        platform: [ubuntu-latest]
        include:
          # only update test coverage stats with most recent go version on linux
          - go-version: stable
            platform: ubuntu-latest
            update-coverage: true
    runs-on: ${{ matrix.platform }}

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: actions/setup-go@0aaccfd150d50ccaeb58ebd88d36e91967a5f35b # v5.4.0
        with:
          go-version: ${{ matrix.go-version }}

      - name: Run go test
        run: go test -v -race -coverprofile coverage.txt -covermode atomic ./...

      - name: Upload coverage to Codecov
        if: ${{ matrix.update-coverage }}
        uses: codecov/codecov-action@ad3126e916f78f00edff4ed0317cf185271ccc2d #v5.4.2
",39,1,2,"push, pull_request",3
google/jimfs,ci.yml,"name: CI

on:
  push:
    branches:
      - master
  pull_request:
    branches:
      - master

jobs:
  test:
    name: ""JDK ${{ matrix.java }} on ${{ matrix.os }}""
    strategy:
      matrix:
        os: [ ubuntu-latest ]
        java: [ 21, 11, 8 ]
        # Only test on macos and windows with a single recent JDK to avoid a
        # combinatorial explosion of test configurations.
        # Most OS-specific issues are not specific to a particular JDK version.
        include:
          - os: macos-latest
            java: 21
          - os: windows-latest
            java: 21
    runs-on: ${{ matrix.os }}
    steps:
      # Cancel any previous runs for the same branch that are still running.
      - name: 'Cancel previous runs'
        uses: styfle/cancel-workflow-action@85880fa0301c86cca9da44039ee3bb12d3bedbfa
        with:
          access_token: ${{ github.token }}
      - name: 'Check out repository'
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      - name: 'Cache local Maven repository'
        uses: actions/cache@5a3ec84eff668545956fd18022155c47e93e2684
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-maven-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            ${{ runner.os }}-maven-
      - name: 'Set up JDK ${{ matrix.java }}'
        uses: actions/setup-java@c5195efecf7bdfc987ee8bae7a71cb8b11521c00
        with:
          java-version: ${{ matrix.java }}
          distribution: 'zulu'
      - name: 'Install'
        shell: bash
        run: mvn -B install -U -DskipTests=true
      - name: 'Test'
        shell: bash
        run: mvn -B verify -U -Dmaven.javadoc.skip=true

  publish_snapshot:
    name: 'Publish snapshot'
    needs: test
    if: github.event_name == 'push' && github.repository == 'google/jimfs'
    runs-on: ubuntu-latest
    steps:
      - name: 'Check out repository'
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      - name: 'Cache local Maven repository'
        uses: actions/cache@5a3ec84eff668545956fd18022155c47e93e2684
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-maven-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            ${{ runner.os }}-maven-
      - name: 'Set up JDK 8'
        uses: actions/setup-java@c5195efecf7bdfc987ee8bae7a71cb8b11521c00
        with:
          java-version: 8
          distribution: 'zulu'
          server-id: sonatype-nexus-snapshots
          server-username: CI_DEPLOY_USERNAME
          server-password: CI_DEPLOY_PASSWORD
      - name: 'Publish'
        env:
          CI_DEPLOY_USERNAME: ${{ secrets.CI_DEPLOY_USERNAME }}
          CI_DEPLOY_PASSWORD: ${{ secrets.CI_DEPLOY_PASSWORD }}
        run: mvn -B clean source:jar javadoc:jar deploy -DskipTests=true

  generate_docs:
    name: 'Generate latest docs'
    needs: test
    if: github.event_name == 'push' && github.repository == 'google/jimfs'
    runs-on: ubuntu-latest
    steps:
      - name: 'Check out repository'
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      - name: 'Cache local Maven repository'
        uses: actions/cache@5a3ec84eff668545956fd18022155c47e93e2684
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-maven-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            ${{ runner.os }}-maven-
      - name: 'Set up JDK 11'
        uses: actions/setup-java@c5195efecf7bdfc987ee8bae7a71cb8b11521c00
        with:
          java-version: 11
          distribution: 'zulu'
      - name: 'Generate latest docs'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: ./util/update_snapshot_docs.sh
",106,3,2,"push, pull_request",10
google/blockly,appengine_deploy.yml,"# Workflow that prepares files and deploys to appengine

name: Deploy to App Engine

# Controls when the workflow will run
on:
  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

jobs:
  prepare:
    name: Prepare
    runs-on: ubuntu-latest

    steps:
      # Checks-out the repository under $GITHUB_WORKSPACE.
      # When running manually this checks out the master branch.
      - uses: actions/checkout@v4

      - name: Prepare demo files
        # Install all dependencies, then copy all the files needed for demos.
        run: |
          npm install
          npm run prepareDemos

      - name: Upload
        uses: actions/upload-artifact@v4
        with:
          name: appengine_files
          path: _deploy/

  deploy:
    name: Deploy
    runs-on: ubuntu-latest
    # The prepare step must succeed for this step to run.
    needs: prepare
    steps:
      - name: Download prepared files
        uses: actions/download-artifact@v4
        with:
          name: appengine_files
          path: _deploy/

      - name: Deploy to App Engine
        uses: google-github-actions/deploy-appengine@v2.1.5
        # For parameters see:
        # https://github.com/google-github-actions/deploy-appengine#inputs
        with:
          working_directory: _deploy/
          deliverables: app.yaml
          project_id: ${{ secrets.GCP_PROJECT }}
          credentials: ${{ secrets.GCP_SA_KEY }}
          promote: false
          version: vtest
",54,2,1,workflow_dispatch,4
google/blockly,assign_reviewers.yml,"name: Assign requested reviewers

# This workflow adds requested reviewers as assignees. If you remove a
# requested reviewer, it will not remove them as an assignee.
#
# See https://github.com/google/blockly/issues/5643 for more
# information on why this was added.
#
# N.B.: Runs with a read-write repo token.  Do not check out the
# submitted branch!
on:
  pull_request_target:
    types: [review_requested]

jobs:
  requested-reviewer:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
    steps:
      - name: Assign requested reviewer
        uses: actions/github-script@v7
        with:
          script: |
            try {
              if (context.payload.pull_request === undefined) {
              throw new Error(""Can't get pull_request payload. "" +
                              'Check a request reviewer event was triggered.');
              }
              const reviewers = context.payload.pull_request.requested_reviewers;
              // Assignees takes in a list of logins rather than the
              // reviewer object.
              const reviewerNames = reviewers.map(reviewer => reviewer.login);
              const {number:issue_number} = context.payload.pull_request;
              github.rest.issues.addAssignees({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issue_number,
                assignees: reviewerNames
              });
            } catch (error) {
              core.setFailed(error.message);
            }
",43,1,1,pull_request_target,1
google/blockly,browser_test.yml,"# This workflow will do a clean install, start the selenium server, and run
# all of our browser based tests

name: Run browser manually

on:
  workflow_dispatch:

permissions:
  contents: read

jobs:
  build:
    timeout-minutes: 10
    runs-on: ${{ matrix.os }}

    strategy:
      matrix:
        # TODO (#2114): re-enable osx build.
        # os: [ubuntu-latest, macos-latest]
        os: [macos-latest]
        node-version: [18.x, 20.x]
        # See supported Node.js release schedule at
        # https://nodejs.org/en/about/releases/

    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Reconfigure git to use HTTP authentication
        run: >
          git config --global url.""https://github.com/"".insteadOf
          ssh://git@github.com/

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}

      - name: Npm Install
        run: npm install

      - name: Linux Test Setup
        if: runner.os == 'Linux'
        run: source ./tests/scripts/setup_linux_env.sh

      - name: Run Build
        run: npm run build

      - name: Run Test
        run: npm run test:browser

        env:
          CI: true
",55,1,1,workflow_dispatch,2
google/blockly,build.yml,"# This workflow will do a clean install, start the selenium server, and run
# all of our tests.

name: Node.js CI

on: [pull_request]

permissions:
  contents: read

jobs:
  build:
    timeout-minutes: 10
    runs-on: ${{ matrix.os }}

    strategy:
      matrix:
        # TODO (#2114): re-enable osx build.
        # os: [ubuntu-latest, macos-latest]
        os: [ubuntu-latest]
        node-version: [18.x, 20.x, 22.x]
        # See supported Node.js release schedule at
        # https://nodejs.org/en/about/releases/

    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Reconfigure git to use HTTP authentication
        run: >
          git config --global url.""https://github.com/"".insteadOf
          ssh://git@github.com/

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}

      - name: Npm Clean Install
        run: npm ci

      - name: Linux Test Setup
        if: runner.os == 'Linux'
        run: source ./tests/scripts/setup_linux_env.sh

      - name: Run
        run: npm run test

        env:
          CI: true

  lint:
    timeout-minutes: 5
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Use Node.js 20.x
        uses: actions/setup-node@v4
        with:
          node-version: 20.x

      - name: Npm Install
        run: npm install

      - name: Lint
        run: npm run lint

  format:
    timeout-minutes: 5
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Use Node.js 20.x
        uses: actions/setup-node@v4
        with:
          node-version: 20.x

      - name: Npm Install
        run: npm install

      - name: Check Format
        run: npm run format:check
",85,3,1,pull_request,6
google/blockly,conventional-label.yml,"on:
  pull_request_target:
    types:
      - opened
      - edited
name: conventional-release-labels
jobs:
  label:
    runs-on: ubuntu-latest
    permissions:
      issues: write
      pull-requests: write
    steps:
      - uses: bcoe/conventional-release-labels@v1
        with:
          type_labels:
            '{""feat"": ""PR: feature"", ""fix"": ""PR: fix"", ""breaking"": ""breaking
            change"", ""chore"": ""PR: chore"", ""docs"": ""PR: docs"", ""refactor"": ""PR:
            refactor"", ""revert"": ""PR: revert"", ""deprecate"": ""deprecation""}'
          ignored_types: '[]'
",20,1,1,pull_request_target,1
google/blockly,develop_freeze.yml,"# This workflow will comment on pull requests that are submitted while develop
# is frozen during the week of release. Skips any pull requests that have the
# label 'ignore-freeze'.
# This workflow should be enabled only while develop is frozen.

name: Develop Freeze PR Comment

on:
  # Trigger the workflow on pull request on develop branch
  pull_request:
    types:
      - opened
      - reopened
    branches:
      - develop

jobs:
  freeze-comment:
    if: ${{ !contains(github.event.pull_request.labels.*.name, 'ignore-freeze') }}
    runs-on: ubuntu-latest
    steps:
      - name: PR Comment
        uses: github-actions-up-and-running/pr-comment@f1f8ab2bf00dce6880a369ce08758a60c61d6c0b
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          message: 'Thanks for the PR! The develop branch is currently frozen in preparation for the release so it may not be addressed until after release week.'
",26,1,1,pull_request,1
google/blockly,keyboard_plugin_test.yml,"# Workflow for running the keyboard navigation plugin's automated tests.

name: Keyboard Navigation Automated Tests

on:
  workflow_dispatch:
  pull_request:
  push:
    branches:
      - develop

permissions:
  contents: read

jobs:
  webdriverio_tests:
    name: WebdriverIO tests
    timeout-minutes: 10
    runs-on: ${{ matrix.os }}

    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest]

    steps:
      - name: Checkout core Blockly
        uses: actions/checkout@v4
        with:
          path: core-blockly

      - name: Checkout keyboard navigation plugin
        uses: actions/checkout@v4
        with:
          repository: 'google/blockly-keyboard-experimentation'
          ref: 'main'
          path: blockly-keyboard-experimentation

      - name: Use Node.js 20.x
        uses: actions/setup-node@v4
        with:
          node-version: 20.x

      - name: NPM install
        run: |
          cd core-blockly
          npm install
          cd ..
          cd blockly-keyboard-experimentation
          npm install
          cd ..

      - name: Link latest core develop with plugin
        run: |
          cd core-blockly
          npm run package
          cd dist
          npm link
          cd ../../blockly-keyboard-experimentation
          npm link blockly
          cd ..

      - name: Run keyboard navigation plugin tests
        run: |
          cd blockly-keyboard-experimentation
          npm run test
",66,1,3,"workflow_dispatch, pull_request, push",3
google/blockly,tag_module_cleanup.yml,"# For new pull requests against the goog_module branch, adds the 'type: cleanup'
# label and sets the milestone to q3 2021 release.

name: Tag module cleanup

# Trigger on pull requests against goog_module branch only
# Uses pull_request_target to get write permissions so that it can write labels.
on:
  pull_request_target:
    branches:
      - goog_module

jobs:
  tag-module-cleanup:
    # Add the type: cleanup label
    runs-on: ubuntu-latest
    steps:
      - uses: actions/github-script@v7
        with:
          script: |
            // Note that pull requests are considered issues and ""shared""
            // actions for both features, like manipulating labels and
            // milestones are provided within the issues API.
            await github.issues.update({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              // 2021 q3 release milestone.
              // https://github.com/google/blockly/milestone/18
              milestone: 18
            })
            await github.issues.addLabels({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              labels: ['type: cleanup']
            })
",37,1,1,pull_request_target,1
google/blockly,welcome_new_contributors.yml,"on:
  pull_request_target:
    types:
      - opened
name: Welcome new contributors
jobs:
  welcome:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
    steps:
      - uses: actions/first-interaction@v1
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          pr-message: >
            Welcome! It looks like this is your first pull request in Blockly,
            so here are a couple of tips:

            - You can find tips about contributing to Blockly and how to
            validate your changes on our
            [developer site](https://developers.google.com/blockly/guides/contribute/core#making_and_verifying_a_change).

            - All contributors must sign the Google Contributor License
            Agreement (CLA). If the google-cla bot leaves a comment on this
            PR, make sure you follow the instructions.

            - We use conventional commits to make versioning the package easier. Make sure your commit
            message is in the [proper format](https://developers.google.com/blockly/guides/contribute/get-started/commits)
            or [learn how to fix it](https://developers.google.com/blockly/guides/contribute/get-started/commits#fixing_non-conventional_commits).

            - If any of the other checks on this PR fail, you can click on
            them to learn why. It might be that your change caused a test
            failure, or that you need to double-check the
            [style guide](https://developers.google.com/blockly/guides/contribute/core/style_guide).

            Thank you for opening this PR! A member of the Blockly team will review it soon.
",36,1,1,pull_request_target,1
google/grr,build.yml,"name: Build
on: [push, pull_request]
env:
  GCS_BUCKET: autobuilds.grr-response.com
  GCS_BUCKET_OPENAPI: autobuilds-grr-openapi
  DOCKER_REPOSITORY: ghcr.io/google/grr
jobs:
  test-devenv:
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/checkout@v4
      - name: check_deps
        run: |
          sudo apt-get update && sudo apt-get install -y podman curl jq
          sudo usermod --add-subuids 500000-565535 --add-subgids 500000-565535 ""${USER}""
          devenv/devenv.sh check_deps
      - name: start
        run: |
          devenv/devenv.sh start
          OK=false
          for attempt in $(seq 20); do
            curl -su admin:admin http://localhost:4280/api/clients |
              sed 1d |
              jq -r "".items[].value.client_id.value"" |
              egrep -e ""^C[.][0-9a-f]{16}\$"" && { OK=true; break; }
            echo ""attempt ${attempt}: devenv not ready""
            sleep 10
          done
          [[ $OK = true ]]

  test-ubuntu:
    runs-on: ubuntu-22.04
    env:
      CHROME_DEB: google-chrome-stable_current_amd64.deb
    steps:
      - uses: actions/checkout@v4
      - name: Install
        run: |
          free -hmw
          lscpu
          sudo apt install -y python3-dev python3-pip python3-venv python3-mysqldb
          if [[ -z ""$(type google-chrome 2>/dev/null)"" ]]; then
            wget ""https://dl.google.com/linux/direct/${CHROME_DEB}"" && sudo apt install -y ""./${CHROME_DEB}"";
          fi
          python3 -m venv --system-site-packages ""${HOME}/INSTALL""
          travis/install.sh
      - name: Test
        run: |
          source ""${HOME}/INSTALL/bin/activate""
          pip install pytest-xdist==2.2.1 pytest==6.2.5
          # We have 4 vCPUs available, but only use 3 here to avoid timeouts like
          # https://ci.appveyor.com/project/grr/grr-ia94e/builds/20483467/messages ,
          # which happen when tests stall.
          pytest --verbose -n 3 grr/ --ignore grr/server/grr_response_server/gui/selenium_tests/ --ignore grr/client/grr_response_client/client_actions/windows/
          # jsTree tests seem to fail on Chrome 71 headless due to https://github.com/GoogleChrome/puppeteer/issues/3463
          if [ $(google-chrome --version | grep -Eo "" [0-9]{1,3}"") != ""71"" ]; then (cd grr/server/grr_response_server/gui/static/ && npm run gulp test); fi

  build-openapi:
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/checkout@v4
      - name: Install
        run: |
          sudo apt install -y python3-dev python3-pip python3-venv python3-mysqldb
          python3 -m venv --system-site-packages ""${HOME}/INSTALL""
          travis/install.sh
      - name: Build
        run: |
          source ""${HOME}/INSTALL/bin/activate""
          mkdir -p _openapi_artifacts/openapi_description
          mkdir -p _openapi_artifacts/documentation
          travis/build_api_documentation.sh ""_openapi_artifacts/openapi_description/openapi_description.json"" ""_openapi_artifacts/documentation/openapi_documentation.html""
          ls -la _openapi_artifacts/*
      - name: Upload OpenAPI to GitHub artifacts
        uses: actions/upload-artifact@v4
        with:
          name: openapi
          path: _openapi_artifacts/
          retention-days: 1

  build-ubuntu:
    runs-on: ubuntu-22.04
    env:
      GCS_TAG: ubuntu_64bit
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.9'
      - name: Set up
        run: |
          sudo apt install fakeroot debhelper libffi-dev libssl-dev
          pip install virtualenv
          virtualenv ""${HOME}/INSTALL""
      - name: Build
        run: |
          travis/install_client_builder.sh
          travis/build_templates.sh
          ls -la gcs_upload_dir
      - name: Upload installers to GitHub artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ubuntu-installers
          path: gcs_upload_dir/
          retention-days: 1

  build-osx:
    runs-on: macos-13
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.9'
      - name: Set up
        run: |
          pip install --upgrade setuptools virtualenv
          virtualenv ""${HOME}/INSTALL""
      - name: Build installers
        run: |
          travis/install_client_builder.sh
          travis/build_templates.sh
          ls -la gcs_upload_dir
      - name: Upload installers to GitHub artifacts
        uses: actions/upload-artifact@v4
        with:
          name: osx-installers
          path: gcs_upload_dir/
          retention-days: 1

  build-centos:
    runs-on: ubuntu-22.04
    env:
      GCS_TAG: centos_64bit
      DOCKER_IMG: grrdocker/centos7-python39
      DOCKER_CONTAINER: centos_64bit_container
      DOCKER_USER: grrbot
    steps:
      - uses: actions/checkout@v4
      - name: Build installers
        run: |
          docker run -dit \
            --volume ""${PWD}:/mnt/grr"" \
            --workdir /mnt/grr \
            --env DOCKER_USER=""${DOCKER_USER}"" \
            --env TRAVIS_OS_NAME=""linux"" \
            --name ""${DOCKER_CONTAINER}"" \
            ""${DOCKER_IMG}""
          # Using `bash -l` here and below to make sure devtools
          # (including the C++ 14 compatible compiler) are properly
          # registered in the environment variables.
          docker exec ""${DOCKER_CONTAINER}"" bash -l travis/set_up_test_user.sh
          docker exec --user ""${DOCKER_USER}"" ""${DOCKER_CONTAINER}"" bash -l -c '/usr/local/bin/python3.9 -m venv ""/home/${DOCKER_USER}/INSTALL""'
          docker exec --user ""${DOCKER_USER}"" ""${DOCKER_CONTAINER}"" bash -l travis/install_client_builder.sh
          docker exec --user ""${DOCKER_USER}"" ""${DOCKER_CONTAINER}"" bash -l travis/build_templates.sh
          docker exec ""${DOCKER_CONTAINER}"" rpm -vih gcs_upload_dir/*.rpm
          ls -la gcs_upload_dir
      - name: Upload installers to GitHub artifacts
        uses: actions/upload-artifact@v4
        with:
          name: centos-installers
          path: gcs_upload_dir/
          retention-days: 1

  build-windows:
    runs-on: windows-2022
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.9'
      - name: Build installers
        shell: bash
        run: |
          set -ex
          pip install virtualenv wheel
          python -u appveyor/windows_templates/build_windows_templates.py --grr_src=$GITHUB_WORKSPACE --output_dir=$GITHUB_WORKSPACE/output --test_repack_install
          mkdir -p gcs_upload_dir
          mv -v output*/* gcs_upload_dir
          ls -la gcs_upload_dir
      - name: Upload installers to GitHub artifacts
        uses: actions/upload-artifact@v4
        with:
          name: windows-installers
          path: gcs_upload_dir/
          retention-days: 1

  build-docker-image:
    runs-on: ubuntu-22.04
    permissions:
      contents: 'read'
    needs:
      - build-centos
      - build-ubuntu
      - build-osx
      - build-windows
    steps:
      - uses: actions/checkout@v4
      - name: Download installers from GitHub artifacts
        id: download
        uses: actions/download-artifact@v4
        with:
          pattern: '*installer*'
          path: _installers
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Extract metadata (tags, labels) for Docker
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.DOCKER_REPOSITORY }}
      - name: Build and export
        uses: docker/build-push-action@v5
        with:
          context: .
          file: Dockerfile
          # Temporarily add a `testing`-tag to identify this image
          # for testing, tag is removed again before uploading to
          # github container registry.
          tags: |
            ${{ env.DOCKER_REPOSITORY }}:testing
            ${{ steps.meta.outputs.tags }}
          outputs: type=docker,dest=/tmp/grr_base_image.tar
      - name: Upload docker image
        uses: actions/upload-artifact@v4
        with:
          name: grr_base_image
          path: /tmp/grr_base_image.tar
          retention-days: 3

  docker-compose-e2e-test:
    permissions:
      contents: 'read'
    runs-on: ubuntu-22.04
    needs:
      - build-docker-image
    steps:
      - uses: actions/checkout@v4
      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: grr_base_image
          path: /tmp
      - name: Load image
        run: |
          docker load --input /tmp/grr_base_image.tar
      - name: Start docker compose stack
        shell: bash
        run: |
          docker_config_files/init_certs.sh
          docker compose \
            -f compose.yaml \
            -f compose.testing.yaml \
            up -d --wait
      - name: Test
        shell: bash
        run: |
          docker compose exec grr-client bash /configs/client/create_fake_user.sh
          docker run \
            --add-host=host.docker.internal:host-gateway \
            -v $(pwd):/github_workspace \
            -v ./docker_config_files:/configs \
            -w /github_workspace \
            --entrypoint appveyor/e2e_tests/run_docker_compose_e2e_test.sh \
            ${{ env.DOCKER_REPOSITORY }}:testing \
            $(docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' grr-client)
      - name: Dump docker compose log output
        if: always()
        shell: bash
        run: |
          docker compose logs > /tmp/docker_compose_test.log
      - name: Upload docker compose logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: docker_commpose_test_logs
          path: /tmp/docker_compose_test.log
          retention-days: 3
      - name: Stop the docker compose stack
        if: always()
        shell: bash
        run: |
          docker compose down --volumes

  push-docker-image:
    if: ${{ github.event_name == 'push' }}
    env:
      REGISTRY: ghcr.io
    permissions:
      packages: write
      contents: write
    runs-on: ubuntu-22.04
    needs:
      - test-ubuntu
      - docker-compose-e2e-test
    steps:
      - uses: actions/checkout@v4
      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: grr_base_image
          path: /tmp
      - name: Load image
        run: |
          docker load --input /tmp/grr_base_image.tar
      - name: Login to GitHub Container registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Push Docker image
        run: |
          # Remove the tag used to identify the image for testing again.
          docker rmi ${{ env.DOCKER_REPOSITORY }}:testing
          docker push --all-tags ${{ env.DOCKER_REPOSITORY }}

  upload-artifacts:
    if: ${{ github.event_name == 'push' }}
    permissions:
      contents: 'read'
      id-token: 'write'
    runs-on: ubuntu-22.04
    needs:
      - docker-compose-e2e-test
      - test-ubuntu
      - build-centos
      - build-ubuntu
      - build-osx
      - build-windows
      - build-openapi
    steps:
      - uses: actions/checkout@v4
      - name: Download installers from GitHub artifacts
        id: download
        uses: actions/download-artifact@v4
        with:
          path: _artifacts
      - run: |
          ls -la _artifacts/*/
          COMMIT_TIME=$(git show -s --date='format-local:%Y-%m-%dT%H:%M:%SZ' --format=""%cd"" $GITHUB_SHA)
          OUTPUT_DIR=gcs_upload_dir/${COMMIT_TIME}_${GITHUB_SHA}/
          echo ""OUTPUT_DIR=$OUTPUT_DIR"" >> $GITHUB_ENV
          mkdir -p $OUTPUT_DIR/centos/
          mv -v _artifacts/centos-installers/* $OUTPUT_DIR/centos
          mkdir -p $OUTPUT_DIR/ubuntu/
          mv -v _artifacts/ubuntu-installers/* $OUTPUT_DIR/ubuntu
          mkdir -p $OUTPUT_DIR/osx/
          mv -v _artifacts/osx-installers/* $OUTPUT_DIR/osx
          mkdir -p $OUTPUT_DIR/windows/
          mv -v _artifacts/windows-installers/* $OUTPUT_DIR/windows
      - name: Authenticate
        uses: 'google-github-actions/auth@v1'
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
          export_environment_variables: true
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v1.1.0
      - name: Upload installers to GCS
        uses: google-github-actions/upload-cloud-storage@v1.0.0
        with:
          path: gcs_upload_dir/
          destination: ${{ env.GCS_BUCKET }}
          # Omit `path` (e.g. /home/runner/deploy/) in final GCS path.
          parent: false
      - name: Upload OpenAPI to GCS
        uses: google-github-actions/upload-cloud-storage@v1.0.0
        with:
          path: _artifacts/openapi/
          destination: ${{ env.GCS_BUCKET_OPENAPI }}
          # Omit `path` (e.g. /home/runner/deploy/) in final GCS path.
          parent: false
",371,11,2,"push, pull_request",33
google/grr,publish-pypi.yml,"name: Publish to PyPi
on:
  release:
    types: [published]

jobs:
  build-pypi-packages:
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.9'
      - uses: actions/setup-node@v4
        with:
          node-version: 16.13.0
      - name: Build
        run: |
          set -ex
          mkdir /tmp/sdists
          python grr/proto/setup.py --quiet sdist --formats=zip --dist-dir=""/tmp/sdists""
          python grr/core/setup.py --quiet sdist --formats=zip --dist-dir=""/tmp/sdists""
          python grr/client/setup.py --quiet sdist --formats=zip --dist-dir=""/tmp/sdists""
          python grr/client_builder/setup.py --quiet sdist --formats=zip --dist-dir=""/tmp/sdists""
          python grr/server/setup.py --quiet sdist --formats=zip --dist-dir=""/tmp/sdists""
          python grr/test/setup.py --quiet sdist --formats=zip --dist-dir=""/tmp/sdists""
          python colab/setup.py --quiet sdist --formats=zip --dist-dir=""/tmp/sdists""
          python api_client/python/setup.py --quiet sdist --formats=zip --dist-dir=""/tmp/sdists""
      - name: Upload grr-response-proto
        uses: actions/upload-artifact@v4
        with:
          name: grr-response-proto
          path: /tmp/sdists/grr-response-proto-[0-9]*.zip
          retention-days: 3          
      - name: Upload grr-response-core
        uses: actions/upload-artifact@v4
        with:
          name: grr-response-core
          path: /tmp/sdists/grr-response-core-[0-9]*.zip
          retention-days: 3          
      - name: Upload grr-response-client
        uses: actions/upload-artifact@v4
        with:
          name: grr-response-client
          path: /tmp/sdists/grr-response-client-[0-9]*.zip
          retention-days: 3          
      - name: Upload grr-response-client-builder
        uses: actions/upload-artifact@v4
        with:
          name: grr-response-client-builder
          path: /tmp/sdists/grr-response-client-builder-[0-9]*.zip
          retention-days: 3          
      - name: Upload grr-response-server
        uses: actions/upload-artifact@v4
        with:
          name: grr-response-server
          path: /tmp/sdists/grr-response-server-[0-9]*.zip
          retention-days: 3              
      - name: Upload grr-response-test
        uses: actions/upload-artifact@v4
        with:
          name: grr-response-test
          path: /tmp/sdists/grr-response-test-[0-9]*.zip
          retention-days: 3
      - name: Upload grr-colab
        uses: actions/upload-artifact@v4
        with:
          name: grr-colab
          path: /tmp/sdists/grr-colab-[0-9]*.zip
          retention-days: 3          
      - name: Upload grr-api-client
        uses: actions/upload-artifact@v4
        with:
          name: grr-api-client
          path: /tmp/sdists/grr-api-client-[0-9]*.zip
          retention-days: 3          
  
  publish-to-pypi-grr-response-proto:
    name: Publish grr-response-proto to PyPI
    needs:
    - build-pypi-packages
    runs-on: ubuntu-22.04
    environment:
      name: pypi
      url: https://pypi.org/p/grr-response-proto
    permissions:
      id-token: write  # IMPORTANT: mandatory for trusted publishing
    steps:
    - name: Download the artifact
      uses: actions/download-artifact@v4
      with:
        name: grr-response-proto
        path: dist/
    - name: Publish to PyPI
      uses: pypa/gh-action-pypi-publish@release/v1

  publish-to-pypi-grr-response-core:
    name: Publish grr-response-core to PyPI
    needs:
    - build-pypi-packages
    runs-on: ubuntu-22.04
    environment:
      name: pypi
      url: https://pypi.org/p/grr-response-core
    permissions:
      id-token: write  # IMPORTANT: mandatory for trusted publishing
    steps:
    - name: Download the artifact
      uses: actions/download-artifact@v4
      with:
        name: grr-response-core
        path: dist/
    - name: Publish to PyPI
      uses: pypa/gh-action-pypi-publish@release/v1

  publish-to-pypi-grr-response-client:
    name: Publish grr-response-client to PyPI
    needs:
    - build-pypi-packages
    runs-on: ubuntu-22.04
    environment:
      name: pypi
      url: https://pypi.org/p/grr-response-client
    permissions:
      id-token: write  # IMPORTANT: mandatory for trusted publishing
    steps:
    - name: Download the artifact
      uses: actions/download-artifact@v4
      with:
        name: grr-response-client
        path: dist/
    - name: Publish to PyPI
      uses: pypa/gh-action-pypi-publish@release/v1

  publish-to-pypi-grr-response-client-builder:
    name: Publish grr-response-client-builder to PyPI
    needs:
    - build-pypi-packages
    runs-on: ubuntu-22.04
    environment:
      name: pypi
      url: https://pypi.org/p/grr-response-client-builder
    permissions:
      id-token: write  # IMPORTANT: mandatory for trusted publishing
    steps:
    - name: Download the artifact
      uses: actions/download-artifact@v4
      with:
        name: grr-response-client-builder
        path: dist/
    - name: Publish to PyPI
      uses: pypa/gh-action-pypi-publish@release/v1

  publish-to-pypi-grr-response-server:
    name: Publish grr-response-server to PyPI
    needs:
    - build-pypi-packages
    runs-on: ubuntu-22.04
    environment:
      name: pypi
      url: https://pypi.org/p/grr-response-server
    permissions:
      id-token: write  # IMPORTANT: mandatory for trusted publishing
    steps:
    - name: Download the artifact
      uses: actions/download-artifact@v4
      with:
        name: grr-response-server
        path: dist/
    - name: Publish to PyPI
      uses: pypa/gh-action-pypi-publish@release/v1

  publish-to-pypi-grr-response-test:
    name: Publish grr-response-test to PyPI
    needs:
    - build-pypi-packages
    runs-on: ubuntu-22.04
    environment:
      name: pypi
      url: https://pypi.org/p/grr-response-test
    permissions:
      id-token: write  # IMPORTANT: mandatory for trusted publishing
    steps:
    - name: Download the artifact
      uses: actions/download-artifact@v4
      with:
        name: grr-response-test
        path: dist/
    - name: Publish to PyPI
      uses: pypa/gh-action-pypi-publish@release/v1

  publish-to-pypi-grr-colab:
    name: Publish grr-colab to PyPI
    needs:
    - build-pypi-packages
    runs-on: ubuntu-22.04
    environment:
      name: pypi
      url: https://pypi.org/p/grr-colab
    permissions:
      id-token: write  # IMPORTANT: mandatory for trusted publishing
    steps:
    - name: Download the artifact
      uses: actions/download-artifact@v4
      with:
        name: grr-colab
        path: dist/
    - name: Publish to PyPI
      uses: pypa/gh-action-pypi-publish@release/v1

  publish-to-pypi-grr-api-client:
    name: Publish grr-api-client to PyPI
    needs:
    - build-pypi-packages
    runs-on: ubuntu-22.04
    environment:
      name: pypi
      url: https://pypi.org/p/grr-api-client
    permissions:
      id-token: write  # IMPORTANT: mandatory for trusted publishing
    steps:
    - name: Download the artifact
      uses: actions/download-artifact@v4
      with:
        name: grr-api-client
        path: dist/
    - name: Publish to PyPI
      uses: pypa/gh-action-pypi-publish@release/v1
",228,9,1,release,27
microsoft/TypeScript,accept-baselines-fix-lints.yaml,"name: Accept Baselines and Fix Lints

on:
  workflow_dispatch: {}

permissions:
  contents: read

# Ensure scripts are run with pipefail. See:
# https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#exit-codes-and-error-action-preference
defaults:
  run:
    shell: bash

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          token: ${{ secrets.TS_BOT_GITHUB_TOKEN }}
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'

      - name: Configure Git, Run Tests, Update Baselines, Apply Fixes
        run: |
          git config user.email ""typescriptbot@microsoft.com""
          git config user.name ""TypeScript Bot""
          npm ci
          git rm -r --quiet tests/baselines/reference
          npx hereby runtests-parallel --ci --fix || true
          npx hereby baseline-accept
          git add ./src
          git add ./tests/baselines/reference
          git diff --cached
          git commit -m ""Update Baselines and/or Applied Lint Fixes""
          git push
",39,1,1,workflow_dispatch,2
microsoft/TypeScript,ci.yml,"name: CI

on:
  push:
    branches:
      - main
      - release-*
  pull_request:
    branches:
      - main
      - release-*

permissions:
  contents: read

# Ensure scripts are run with pipefail. See:
# https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#exit-codes-and-error-action-preference
defaults:
  run:
    shell: bash

jobs:
  test:
    strategy:
      fail-fast: false
      matrix:
        os:
          - ubuntu-latest
          - windows-latest
          - macos-latest
        node-version:
          - '22'
          - '20'
          - '18'
          - '16'
          - '14'
        bundle:
          - 'true'
        include:
          - node-version: 'lts/*'
            bundle: false
            os: ubuntu-latest
        exclude:
          # No Node 14 on ARM macOS
          - node-version: '14'
            os: macos-latest

    runs-on: ${{ matrix.os }}
    name: Test Node ${{ matrix.node-version }} on ${{ matrix.os }}${{ (!matrix.bundle && ' with --no-bundle') || '' }}

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - name: Use node version ${{ matrix.node-version }}
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: ${{ matrix.node-version }}
          check-latest: true
      - run: npm ci

      - name: Tests
        id: test
        # run tests, but lint separately
        run: npm run test -- --no-lint --bundle=${{ matrix.bundle }}

      - name: Print baseline diff on failure
        if: ${{ failure() && steps.test.conclusion == 'failure' }}
        run: |
          npx hereby baseline-accept
          git add tests/baselines/reference
          git diff --staged --exit-code

  coverage:
    runs-on:
      - 'self-hosted'
      - '1ES.Pool=TypeScript-1ES-GitHub-Large'
      - '1ES.ImageOverride=mariner-2.0'

    permissions:
      id-token: write
      contents: read

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
      - run: npm ci

      - name: Run tests with coverage
        run: npm test -- --no-lint --coverage

      - name: Upload coverage artifact
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: coverage
          path: coverage

      - uses: codecov/codecov-action@ad3126e916f78f00edff4ed0317cf185271ccc2d # v5.4.2
        with:
          use_oidc: ${{ !(github.event_name == 'pull_request' && github.event.pull_request.head.repo.fork) }}
          disable_search: true
          files: ./coverage/codecov.json

  lint:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
      - run: npm ci

      - name: Linter
        run: npm run lint

  knip:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
      - run: npm ci

      - name: Unused exports
        run: npm run knip

  format:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
      - run: npm ci

      - uses: actions/cache@5a3ec84eff668545956fd18022155c47e93e2684 # v4.2.3
        with:
          path: ~/.cache/dprint
          key: ${{ runner.os }}-dprint-${{ hashFiles('package-lock.json', '.dprint.jsonc') }}
          restore-keys: |
            ${{ runner.os }}-dprint-

      - name: Check formatting
        run: npx dprint check

  browser-integration:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
      - run: npm ci

      - name: Installing browsers
        run: npx playwright install --with-deps

      - name: Validate the browser can import TypeScript
        run: npx hereby test-browser-integration

  typecheck:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
      - run: npm ci

      - name: Build src
        run: npx hereby build-src

  smoke:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
      - run: |
          npm --version
          # corepack enable npm
          npm install -g $(jq -r '.packageManager' < package.json)
          npm --version

      - run: npm ci

      - run: npx hereby lkg
      - run: |
          node ./scripts/addPackageJsonGitHead.mjs package.json
          npm pack
          mv typescript*.tgz typescript.tgz
          echo ""package=$PWD/typescript.tgz"" >> ""$GITHUB_OUTPUT""
        id: pack

      - name: Smoke test
        run: |
          cd ""$(mktemp -d)""
          npm init --yes
          npm install ${{ steps.pack.outputs.package }}

          echo ""Testing tsc...""
          npx tsc --version

          echo ""Testing tsserver...""
          echo '{""seq"": 1, ""command"": ""status""}' | npx tsserver

          node $GITHUB_WORKSPACE/scripts/checkModuleFormat.mjs typescript
          node $GITHUB_WORKSPACE/scripts/checkModuleFormat.mjs typescript/lib/tsserverlibrary

  package-size:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          path: pr

      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          path: base
          ref: ${{ github.base_ref }}

      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
      - run: |
          npm --version
          # corepack enable npm

      - run: |
          npm install -g $(jq -r '.packageManager' < package.json)
          npm --version
        working-directory: ./pr

      - run: npm ci
        working-directory: ./pr

      - run: npm ci
        working-directory: ./base

      - run: npx hereby lkg
        working-directory: ./pr

      - run: npx hereby lkg
        working-directory: ./base

      - run: |
          echo ""See $GITHUB_SERVER_URL/$GITHUB_REPOSITORY/actions/runs/$GITHUB_RUN_ID for more info.""
          node ./pr/scripts/checkPackageSize.mjs ./base ./pr >> $GITHUB_STEP_SUMMARY

  misc:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
      - run: npm ci

      - name: Build scripts
        run: npx hereby scripts

      - name: ESLint tests
        run: npx hereby run-eslint-rules-tests

  self-check:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
      - run: npm ci

      - name: Build tsc
        run: npx hereby tsc

      - name: Clean
        run: npx hereby clean-src

      - name: Self build
        run: npx hereby build-src --built

  baselines:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
      - run: npm ci

      - name: Remove all baselines
        run: rm -rf tests/baselines/reference

      - name: Run tests
        run: npm test &> /dev/null || exit 0

      - name: Accept baselines
        run: |
          npx hereby baseline-accept
          git add tests/baselines/reference

      - name: Check baselines
        id: check-baselines
        run: |
          function print_diff() {
            if ! git diff --staged --exit-code --quiet --diff-filter=$1; then
              echo ""$2:""
              git diff --staged --name-only --diff-filter=$1
            fi
          }

          if ! git diff --staged --exit-code --quiet; then
            print_diff ACR ""Missing baselines""
            print_diff MTUXB ""Modified baselines""
            print_diff D ""Unused baselines""
            git diff --staged > fix_baselines.patch
            exit 1
          fi

      - name: Upload baseline diff artifact
        if: ${{ failure() && steps.check-baselines.conclusion == 'failure' }}
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: fix_baselines.patch
          path: fix_baselines.patch
",340,12,2,"push, pull_request",29
microsoft/TypeScript,close-issues.yml,"name: Close issues

on:
  schedule:
    - cron: '0 1 * * *'
  workflow_dispatch:

permissions:
  contents: read

# Ensure scripts are run with pipefail. See:
# https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#exit-codes-and-error-action-preference
defaults:
  run:
    shell: bash

jobs:
  close-issues:
    runs-on: ubuntu-latest
    if: github.repository == 'microsoft/TypeScript'
    permissions:
      contents: read # Apparently required to create issues
      issues: write

    steps:
      - name: Close issues
        env:
          GH_TOKEN: ${{ secrets.TS_BOT_GITHUB_TOKEN }}
        run: |
          DATE=$(date --date='2 days ago' --iso-8601)

          close_issues() {
            echo ""Closing issues marked as '$1'.""
            for issue in $(gh issue list --limit 100 --label ""$1"" --repo ${{ github.repository }} --state open --search ""updated:<$DATE"" --json number --jq '.[].number'); do
              echo ""Closing https://github.com/${{ github.repository }}/issues/$issue""
              gh issue close $issue --repo ${{ github.repository }} --reason ""not planned"" --comment ""This issue has been marked as \""$1\"" and has seen no recent activity. It has been automatically closed for house-keeping purposes.""
            done
          }

          close_issues ""Duplicate""
          close_issues ""Unactionable""
          close_issues ""Not a Defect""
          close_issues ""External""
          close_issues ""Working as Intended""
          close_issues ""Question""
          close_issues ""Out of Scope""
          close_issues ""Declined""
          close_issues ""Won't Fix""
          close_issues ""Too Complex""
          close_issues ""Design Limitation""
",50,1,2,"schedule, workflow_dispatch",0
microsoft/TypeScript,codeql.yml,"name: 'Code Scanning - Action'

on:
  push:
    branches:
      - main
      - release-*
  pull_request:
    branches:
      - main
      - release-*
  schedule:
    #        ┌───────────── minute (0 - 59)
    #        │  ┌───────────── hour (0 - 23)
    #        │  │ ┌───────────── day of the month (1 - 31)
    #        │  │ │ ┌───────────── month (1 - 12 or JAN-DEC)
    #        │  │ │ │ ┌───────────── day of the week (0 - 6 or SUN-SAT)
    #        │  │ │ │ │
    #        │  │ │ │ │
    #        │  │ │ │ │
    #        *  * * * *
    - cron: '30 1 * * 0'

permissions:
  contents: read

# Ensure scripts are run with pipefail. See:
# https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#exit-codes-and-error-action-preference
defaults:
  run:
    shell: bash

jobs:
  CodeQL-Build:
    # CodeQL runs on ubuntu-latest, windows-latest, and macos-latest
    runs-on: ubuntu-latest
    if: github.repository == 'microsoft/TypeScript'

    permissions:
      # required for all workflows
      security-events: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      # Initializes the CodeQL tools for scanning.
      - name: Initialize CodeQL
        uses: github/codeql-action/init@45775bd8235c68ba998cffa5171334d58593da47 # v3.28.15
        with:
          config-file: ./.github/codeql/codeql-configuration.yml
        # Override language selection by uncommenting this and choosing your languages
        # with:
        #   languages: go, javascript, csharp, python, cpp, java

      # Autobuild attempts to build any compiled languages (C/C++, C#, or Java).
      # If this step fails, then you should remove it and run the build manually (see below).
      - name: Autobuild
        uses: github/codeql-action/autobuild@45775bd8235c68ba998cffa5171334d58593da47 # v3.28.15

      # ℹ️ Command-line programs to run using the OS shell.
      # 📚 See https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#jobsjob_idstepsrun

      # ✏️ If the Autobuild fails above, remove it and uncomment the following
      #    three lines and modify them (or add more) to build your code if your
      #    project uses a compiled language

      #- run: |
      #     make bootstrap
      #     make release

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@45775bd8235c68ba998cffa5171334d58593da47 # v3.28.15
",73,1,3,"push, pull_request, schedule",4
microsoft/TypeScript,copilot-setup-steps.yml,"name: 'Copilot Setup Steps'
on: workflow_dispatch

jobs:
  # The job MUST be called `copilot-setup-steps` or it will not be picked up by Copilot.
  copilot-setup-steps:
    runs-on: ubuntu-latest

    # Set the permissions to the lowest permissions possible needed for your steps.
    # Copilot will be given its own token for its operations.
    permissions:
      # If you want to clone the repository as part of your setup steps, for example to install dependencies, you'll need the `contents: read` permission. If you don't clone the repository in your setup steps, Copilot will do this for you automatically after the steps complete.
      contents: read

    # You can define any steps you want, and they will run before the agent starts.
    # If you do not check out your code, Copilot will do this for you.
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
      - run: npm ci
      # pull dprint caches before network access is blocked
      - run: npx hereby check-format || true
",22,1,1,workflow_dispatch,2
microsoft/TypeScript,create-cherry-pick-pr.yml,"name: Create cherry pick PR

on:
  workflow_dispatch:
    inputs:
      pr:
        description: PR number to cherry-pick
        required: true
        type: number
      target_branch:
        description: Target branch to cherry-pick to
        required: true
        type: string

      # Inputs provided by the bot
      distinct_id:
        description: '(bot) A distinct ID'
        required: false
        default: ''
      source_issue:
        description: '(bot) The issue that triggered this workflow'
        required: false
        default: ''
      requesting_user:
        description: '(bot) The user who requested this workflow'
        required: false
        default: ''
      status_comment:
        description: '(bot) The comment to update with the status of this workflow'
        required: false
        default: ''

run-name: ${{ github.workflow }}${{ inputs.distinct_id && format(' (bot run {0})', inputs.distinct_id) || '' }}

permissions:
  contents: read

# Ensure scripts are run with pipefail. See:
# https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#exit-codes-and-error-action-preference
defaults:
  run:
    shell: bash

jobs:
  open-pr:
    runs-on: ubuntu-latest
    if: github.repository == 'microsoft/TypeScript'

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          filter: blob:none # https://github.blog/2020-12-21-get-up-to-speed-with-partial-clone-and-shallow-clone/
          fetch-depth: 0 # Default is 1; need to set to 0 to get the benefits of blob:none.
          token: ${{ secrets.TS_BOT_GITHUB_TOKEN }}

      - uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea # v7.0.1
        id: open-pr
        env:
          PR: ${{ inputs.pr }}
          TARGET_BRANCH: ${{ inputs.target_branch }}
          DISTINCT_ID: ${{ inputs.distinct_id }}
          SOURCE_ISSUE: ${{ inputs.source_issue }}
          REQUESTING_USER: ${{ inputs.requesting_user }}
          STATUS_COMMENT: ${{ inputs.status_comment }}
        with:
          retries: 3
          github-token: ${{ secrets.TS_BOT_GITHUB_TOKEN }}
          result-encoding: string
          script: |
            const {
              PR,
              TARGET_BRANCH,
              DISTINCT_ID,
              SOURCE_ISSUE,
              REQUESTING_USER,
              STATUS_COMMENT,
            } = process.env;

            const pr = await github.rest.pulls.get({
              owner: context.repo.owner,
              repo: context.repo.repo,
              pull_number: +PR,
            });

            if (!pr.data.merge_commit_sha) throw new Error(""No merge commit sha found"");

            const pickBranch = `cherry-pick/${PR}/${TARGET_BRANCH}`;

            const title = `🤖 Pick PR #${PR} (${pr.data.title.substring(0, 35)}${pr.data.title.length > 35 ? ""..."" : """"}) into ${TARGET_BRANCH}`;

            await exec.exec(""git"", [""config"", ""user.email"", ""typescriptbot@microsoft.com""]);
            await exec.exec(""git"", [""config"", ""user.name"", ""TypeScript Bot""]);
            await exec.exec(""git"", [""switch"", ""--detach"", `origin/${TARGET_BRANCH}`]);
            await exec.exec(""git"", [""switch"", ""-c"", pickBranch]);

            let updatedBaselinesMessage = """";
            try {
              await exec.exec(""git"", [""cherry-pick"", ""-m"", ""1"", pr.data.merge_commit_sha]);
            } catch (e) {
              console.log(e);

              // The cherry-pick failed. If all of the conflicts are in tests/baselines,
              // try to run the tests and accept the new baselines.

              await exec.exec(""git"", [""add"", ""tests/baselines""]);
              // This will fail if any other files were modified.
              await exec.exec(""git"", [""-c"", ""core.editor=true"", ""cherry-pick"", ""--continue""]);

              await exec.exec(""npm"", [""ci""]);

              try {
                await exec.exec(""npm"", [""test"", ""--"", ""--no-lint""]);
              } catch {
                // Expected to fail.
              }

              await exec.exec(""npx"", [""hereby"", ""baseline-accept""]);
              await exec.exec(""git"", [""add"", ""tests/baselines""]);
              await exec.exec(""git"", [""commit"", ""-m"", ""Update baselines""]);

              updatedBaselinesMessage = "" This involved updating baselines; please check the diff."";
            }

            await exec.exec(""git"", [""push"", ""--force"", ""--set-upstream"", ""origin"", pickBranch]);

            const existingPulls = await github.rest.pulls.list({
              owner: context.repo.owner,
              repo: context.repo.repo,
              head: `${context.repo.owner}:${pickBranch}`,
            });

            let commentBody;

            if (existingPulls.data.length === 0) {
              console.log(`No existing PRs found for ${pickBranch}`);

              const body = `This cherry-pick was triggered by a request on #${PR}.\n\nPlease review the diff and merge if no changes are unexpected.${updatedBaselinesMessage}`;

              const newPr = await github.rest.pulls.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                base: TARGET_BRANCH,
                head: pickBranch,
                title,
                body,
              });

              await github.rest.issues.addAssignees({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: newPr.data.number,
                assignees: [""DanielRosenwasser""],
              });

              await github.rest.pulls.requestReviewers({
                owner: context.repo.owner,
                repo: context.repo.repo,
                pull_number: newPr.data.number,
                reviewers: [""DanielRosenwasser"", REQUESTING_USER],
              });

              commentBody = `I've created #${newPr.data.number} for you.${updatedBaselinesMessage}`;
            }
            else {
              const existing = existingPulls.data[0];
              console.log(`Found existing PR #${existing.number} for ${pickBranch}`);

              await github.rest.pulls.update({
                owner: context.repo.owner,
                repo: context.repo.repo,
                pull_number: existing.number,
                title,
              });

              commentBody = `I've updated #${existing.number} for you.${updatedBaselinesMessage}`;
            }

            return commentBody;

      - uses: microsoft/typescript-bot-test-triggerer/.github/actions/post-workflow-result@master
        if: ${{ !cancelled() && inputs.distinct_id }}
        with:
          success_comment: ${{ steps.open-pr.outputs.result }}
          failure_comment: 'I was unable to cherry-pick this PR.'
          github_token: ${{ secrets.TS_BOT_GITHUB_TOKEN }}
          distinct_id: ${{ inputs.distinct_id }}
          source_issue: ${{ inputs.source_issue }}
          requesting_user: ${{ inputs.requesting_user }}
          status_comment: ${{ inputs.status_comment }}
",189,1,1,workflow_dispatch,3
microsoft/TypeScript,error-deltas-watchdog.yaml,"name: 'typescript-error-deltas Watchdog'

on:
  workflow_dispatch:
  schedule:
    - cron: '0 0 * * 3' # Every Wednesday

permissions:
  contents: read

# Ensure scripts are run with pipefail. See:
# https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#exit-codes-and-error-action-preference
defaults:
  run:
    shell: bash

jobs:
  check-for-recent:
    runs-on: ubuntu-latest
    if: github.repository == 'microsoft/TypeScript'
    permissions:
      contents: read # Apparently required to create issues
      issues: write
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      TAGS: '@navya9singh @RyanCavanaugh @DanielRosenwasser'
    steps:
      - name: NewErrors
        run: | # --json and --jq prints exactly one issue number per line of output
          DATE=$(date --date=""7 days ago"" --iso-8601)
          gh issue list --repo microsoft/typescript --search ""[NewErrors] created:>=$DATE"" --state all --json number --jq "".[].number"" \
          | grep -qe ""[0-9]"" \
          || gh issue create --repo ${{ github.repository }} --title ""No NewErrors issue since $DATE"" --body ""$TAGS Please check the [pipeline](https://typescript.visualstudio.com/TypeScript/_build?definitionId=48).""
      - name: ServerErrors TS
        run: |
          DATE=$(date --date=""7 days ago"" --iso-8601)
          gh issue list --repo microsoft/typescript --search ""[ServerErrors][TypeScript] created:>=$DATE"" --state all --json number --jq "".[].number"" \
          | grep -qe ""[0-9]"" \
          || gh issue create --repo ${{ github.repository }} --title ""No TypeScript ServerErrors issue since $DATE"" --body ""$TAGS Please check the [pipeline](https://typescript.visualstudio.com/TypeScript/_build?definitionId=59).""
      - name: ServerErrors JS
        run: |
          DATE=$(date --date=""7 days ago"" --iso-8601)
          gh issue list --repo microsoft/typescript --search ""[ServerErrors][JavaScript] created:>=$DATE"" --state all --json number --jq "".[].number"" \
          | grep -qe ""[0-9]"" \
          || gh issue create --repo ${{ github.repository }} --title ""No JavaScript ServerErrors issue since $DATE"" --body ""$TAGS Please check the [pipeline](https://typescript.visualstudio.com/TypeScript/_build?definitionId=58).""
",45,1,2,"workflow_dispatch, schedule",0
microsoft/TypeScript,insiders.yaml,"name: Publish Insiders

on:
  workflow_dispatch: {}
  repository_dispatch:
    types: [publish-insiders]

permissions:
  contents: read

# Ensure scripts are run with pipefail. See:
# https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#exit-codes-and-error-action-preference
defaults:
  run:
    shell: bash

jobs:
  test:
    runs-on: ubuntu-latest
    if: github.repository == 'microsoft/TypeScript'

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
      - run: |
          npm --version
          # corepack enable npm
          npm install -g $(jq -r '.packageManager' < package.json)
          npm --version
      - name: Test insiders
        run: |
          npm ci
          npx hereby configure-insiders
          npm test

  publish:
    needs: test

    runs-on: ubuntu-latest
    if: github.repository == 'microsoft/TypeScript'

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
          # Use NODE_AUTH_TOKEN environment variable to authenticate to this registry.
          registry-url: https://registry.npmjs.org/
      - run: |
          npm --version
          # corepack enable npm
          npm install -g $(jq -r '.packageManager' < package.json)
          npm --version
      - name: Setup and publish insiders
        run: |
          npm whoami
          npm ci
          npx hereby configure-insiders
          npx hereby LKG
          node ./scripts/addPackageJsonGitHead.mjs package.json
          npm publish --tag insiders
        env:
          NODE_AUTH_TOKEN: ${{secrets.npm_token}}
",65,2,2,"workflow_dispatch, repository_dispatch",4
microsoft/TypeScript,lkg.yml,"name: Update LKG

on:
  workflow_dispatch:
    inputs:
      branch_name:
        description: Release branch name to LKG
        required: true
        type: string

permissions:
  contents: read

# Ensure scripts are run with pipefail. See:
# https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#exit-codes-and-error-action-preference
defaults:
  run:
    shell: bash

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - run: |
          if [[ ! ""${{ inputs.branch_name }}"" =~ ^release- ]]; then
            echo ""Branch name must start with 'release-'""
            exit 1
          fi

      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          ref: ${{ inputs.branch_name }}
          token: ${{ secrets.TS_BOT_GITHUB_TOKEN }}
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
      - run: |
          npm --version
          # corepack enable npm
          npm install -g $(jq -r '.packageManager' < package.json)
          npm --version
      - run: |
          npm ci
          npx hereby LKG
          git add --force ./lib
          git config user.email ""typescriptbot@microsoft.com""
          git config user.name ""TypeScript Bot""
          git commit -m 'Update LKG'
          git push
",49,1,1,workflow_dispatch,2
microsoft/TypeScript,new-release-branch.yaml,"name: New Release Branch

on:
  workflow_dispatch:
    inputs:
      branch_name:
        description: Release branch name to create
        required: true
        type: string
      package_version:
        description: Release package version
        required: true
        type: string
      core_major_minor:
        description: Release core major.minor version
        required: true
        type: string

      # Inputs provided by the bot
      distinct_id:
        description: '(bot) A distinct ID'
        required: false
        default: ''
      source_issue:
        description: '(bot) The issue that triggered this workflow'
        required: false
        default: ''
      requesting_user:
        description: '(bot) The user who requested this workflow'
        required: false
        default: ''
      status_comment:
        description: '(bot) The comment to update with the status of this workflow'
        required: false
        default: ''

run-name: ${{ github.workflow }}${{ inputs.distinct_id && format(' (bot run {0})', inputs.distinct_id) || '' }}

permissions:
  contents: read

# Ensure scripts are run with pipefail. See:
# https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#exit-codes-and-error-action-preference
defaults:
  run:
    shell: bash

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          filter: blob:none # https://github.blog/2020-12-21-get-up-to-speed-with-partial-clone-and-shallow-clone/
          fetch-depth: 0 # Default is 1; need to set to 0 to get the benefits of blob:none.
          token: ${{ secrets.TS_BOT_GITHUB_TOKEN }}
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
      - run: |
          npm --version
          # corepack enable npm
          npm install -g $(jq -r '.packageManager' < package.json)
          npm --version
      - run: |
          git checkout -b ${{ inputs.branch_name }}
          sed -i -e 's/""version"": "".*""/""version"": ""${{ inputs.package_version }}""/g' package.json
          sed -i -e 's/const versionMajorMinor = "".*""/const versionMajorMinor = ""${{ inputs.core_major_minor }}""/g' src/compiler/corePublic.ts
          sed -i -e 's/const versionMajorMinor = "".*""/const versionMajorMinor = ""${{ inputs.core_major_minor }}""/g' tests/baselines/reference/api/typescript.d.ts
          sed -i -e 's/const version\(: string\)\{0,1\} = .*;/const version = ""${{ inputs.package_version }}"" as string;/g' src/compiler/corePublic.ts
          npm ci
          npm install # update package-lock.json to ensure the version bump is included
          npx hereby LKG
          npm test
          git diff
          git add package.json package-lock.json
          git add src/compiler/corePublic.ts
          git add tests/baselines/reference/api/typescript.d.ts
          git add --force ./lib
          git config user.email ""typescriptbot@microsoft.com""
          git config user.name ""TypeScript Bot""
          git commit -m 'Bump version to ${{ inputs.package_version }} and LKG'
          git push --set-upstream origin ${{ inputs.branch_name }}

      - uses: microsoft/typescript-bot-test-triggerer/.github/actions/post-workflow-result@master
        if: ${{ !cancelled() && inputs.distinct_id }}
        with:
          success_comment: ""I've created ${{ inputs.branch_name }} with version ${{ inputs.package_version }} for you.""
          failure_comment: 'I was unable to create the new release branch.'
          github_token: ${{ secrets.TS_BOT_GITHUB_TOKEN }}
          distinct_id: ${{ inputs.distinct_id }}
          source_issue: ${{ inputs.source_issue }}
          requesting_user: ${{ inputs.requesting_user }}
          status_comment: ${{ inputs.status_comment }}
",95,1,1,workflow_dispatch,3
microsoft/TypeScript,nightly.yaml,"name: Publish Nightly

on:
  schedule:
    - cron: '0 7 * * *'
  # enable users to manually trigger with workflow_dispatch
  workflow_dispatch: {}

permissions:
  contents: read

# Ensure scripts are run with pipefail. See:
# https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#exit-codes-and-error-action-preference
defaults:
  run:
    shell: bash

jobs:
  test:
    runs-on: ubuntu-latest
    if: github.repository == 'microsoft/TypeScript'

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
      - run: |
          npm --version
          # corepack enable npm
          npm install -g $(jq -r '.packageManager' < package.json)
          npm --version
      - name: Setup and publish nightly
        run: |
          npm ci
          npx hereby configure-nightly
          npm test

  publish:
    needs: [test]
    runs-on: ubuntu-latest
    if: github.repository == 'microsoft/TypeScript'

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
          # Use NODE_AUTH_TOKEN environment variable to authenticate to this registry.
          registry-url: https://registry.npmjs.org/
      - run: |
          npm --version
          # corepack enable npm
          npm install -g $(jq -r '.packageManager' < package.json)
          npm --version
      - name: Setup and publish nightly
        run: |
          npm whoami
          npm ci
          npx hereby configure-nightly
          npx hereby LKG
          node ./scripts/addPackageJsonGitHead.mjs package.json
          npm publish --tag next
        env:
          NODE_AUTH_TOKEN: ${{secrets.npm_token}}
",65,2,2,"schedule, workflow_dispatch",4
microsoft/TypeScript,pr-modified-files.yml,"name: Check modified files
on:
  # For security reasons, we have to use pull_request_target here.
  # This differs from pull_request in that it runs at the _base_ of the PR,
  # e.g. main. This allows us to access secrets. In this workflow, we should
  # never actually clone the PR, as it may contain malicious code.
  # https://securitylab.github.com/research/github-actions-preventing-pwn-requests/
  pull_request_target:
    branches:
      - main

# We only ever need one of these running on a single PR.
# Just let the newest one complete if there are multiple running.
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

permissions:
  contents: read

# Ensure scripts are run with pipefail. See:
# https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#exit-codes-and-error-action-preference
defaults:
  run:
    shell: bash

jobs:
  manage-prs:
    runs-on: ubuntu-latest
    if: github.repository == 'microsoft/TypeScript'

    # No need to set explicit permissions; we are using typescript-bot's token, not github-actions' token.

    env:
      GH_TOKEN: ${{ secrets.TS_BOT_GITHUB_TOKEN }}

    steps:
      - name: Check if PR author is in pr_owners.txt
        id: pr_owner
        run: |
          curl -s https://raw.githubusercontent.com/microsoft/TypeScript/main/.github/pr_owners.txt > pr_owners.txt
          if grep -Fxq -m1 ""${{ github.event.pull_request.user.login }}"" pr_owners.txt; then
            echo ""pr_owner=true"" >> ""$GITHUB_OUTPUT""
          else
            echo ""pr_owner=false"" >> ""$GITHUB_OUTPUT""
          fi

      - name: Create scripts
        run: |
          cat > is_changed.sh <<'EOF'
            #!/bin/bash
            FILENAME=changed_files.txt
            if [ ! -f $FILENAME ]; then
              # The gh command only returns info for the first 100 files. To get
              # the rest, we have to use the graphql API. See:
              # https://github.com/cli/cli/issues/5368#issuecomment-1344253654
              gh api graphql -f query='
                query($endCursor: String) {
                  repository(owner: ""microsoft"", name: ""TypeScript"") {
                    pullRequest(number: ${{ github.event.pull_request.number }}) {
                      files(first: 100, after: $endCursor) {
                        pageInfo{ hasNextPage, endCursor }
                        nodes {
                          path
                        }
                      }
                    }
                  }
                }' --paginate --jq '.data.repository.pullRequest.files.nodes.[].path' > $FILENAME
            fi
            for file in ""$@""; do
              grep -Fxq -m1 ""$file"" $FILENAME && exit 0
            done
            exit 1
          EOF
          chmod +x is_changed.sh

          cat > already_commented.sh <<'EOF'
            #!/bin/bash
            FILENAME=bot_comments.txt
            if [ ! -f $FILENAME ]; then
              gh pr view ${{ github.event.pull_request.number }} --repo ${{ github.repository }} \
                --json 'comments' --jq '.comments[] | select(.author.login == ""typescript-bot"") | .body' > $FILENAME
            fi
            exec grep -Fq -m1 ""$1"" $FILENAME
          EOF
          chmod +x already_commented.sh

      - name: Generated DOM files
        if: steps.pr_owner.outputs.pr_owner == 'false'
        run: |
          if ./is_changed.sh ""src/lib/dom.generated.d.ts"" \
              ""src/lib/dom.iterable.generated.d.ts"" \
              ""src/lib/webworker.generated.d.ts"" \
              ""src/lib/webworker.iterable.generated.d.ts""; then
            MESSAGE=""It looks like you've sent a pull request to update some generated declaration files related to the DOM.""
            MESSAGE+="" These files aren't meant to be edited by hand, as they are synchronized with files in""
            MESSAGE+="" [the TypeScript-DOM-lib-generator repository](https://github.com/microsoft/TypeScript-DOM-lib-generator).""
            MESSAGE+="" You can [read more here](https://github.com/microsoft/TypeScript/blob/main/CONTRIBUTING.md#contributing-libdts-fixes).""
            MESSAGE+="" For house-keeping purposes, this pull request will be closed.""

            gh pr close ${{ github.event.pull_request.number }} --repo ${{ github.repository }} --comment ""$MESSAGE""
            exit 1 # Stop the pipeline; we just closed the PR.
          fi

      - name: Check if PR modifies protocol.ts
        run: |
          if ./is_changed.sh ""src/server/protocol.ts""; then
            MESSAGE=""Thanks for the PR! It looks like you've changed the TSServer protocol in some way.""
            MESSAGE+="" Please ensure that any changes here don't break consumers of the current TSServer API.""
            MESSAGE+="" For some extra review, we'll ping @sheetalkamat, @mjbvz, @zkat, and @joj for you.""
            MESSAGE+="" Feel free to loop in other consumers/maintainers if necessary.""

            if ./already_commented.sh ""It looks like you've changed the TSServer protocol in some way.""; then
              echo ""Already commented.""
            else
              gh pr comment ${{ github.event.pull_request.number }} --repo ${{ github.repository }} --body ""$MESSAGE""
            fi
          fi

      - name: Check for breaking changes
        run: |
          if ./is_changed.sh ""tests/baselines/reference/api/typescript.d.ts""; then
            MESSAGE=""Looks like you're introducing a change to the public API surface area.""
            MESSAGE+="" If this includes breaking changes, please document them""
            MESSAGE+="" [on our wiki's API Breaking Changes page](https://github.com/microsoft/TypeScript/wiki/API-Breaking-Changes).""
            MESSAGE+=$'\n\n'
            MESSAGE+=""Also, please make sure @DanielRosenwasser and @RyanCavanaugh are aware of the changes, just as a heads up.""

            if ./already_commented.sh ""Looks like you're introducing a change to the public API surface area.""; then
              echo ""Already commented.""
            else
              gh pr comment ${{ github.event.pull_request.number }} --repo ${{ github.repository }} --body ""$MESSAGE""
            fi
          fi
",135,1,1,pull_request_target,0
microsoft/TypeScript,release-branch-artifact.yaml,"name: Create Releasable Package Drop

on:
  push:
    branches:
      - release-*

permissions:
  contents: read

# Ensure scripts are run with pipefail. See:
# https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#exit-codes-and-error-action-preference
defaults:
  run:
    shell: bash

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
      - run: |
          npm --version
          # corepack enable npm
          npm install -g $(jq -r '.packageManager' < package.json)
          npm --version
      - name: npm install and test
        run: |
          npm ci
          npm test
      - name: Installing browsers
        run: npx playwright install --with-deps
      - name: Validate the browser can import TypeScript
        run: npx hereby test-browser-integration
      - name: LKG, clean, and pack
        run: |
          npx hereby LKG
          npx hereby clean
          node ./scripts/addPackageJsonGitHead.mjs package.json
          npm pack ./
          mv typescript-*.tgz typescript.tgz
      - name: Upload built tarfile
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: tgz
          path: typescript.tgz
",50,1,1,push,3
microsoft/TypeScript,scorecard.yml,"# This workflow uses actions that are not certified by GitHub. They are provided
# by a third-party and are governed by separate terms of service, privacy
# policy, and support documentation.

name: Scorecard supply-chain security
on:
  # For Branch-Protection check. Only the default branch is supported. See
  # https://github.com/ossf/scorecard/blob/main/docs/checks.md#branch-protection
  branch_protection_rule:
  # To guarantee Maintained check is occasionally updated. See
  # https://github.com/ossf/scorecard/blob/main/docs/checks.md#maintained
  schedule:
    - cron: '19 15 * * 4'
  push:
    branches: ['main']

# Declare default permissions as read only.
permissions: read-all

jobs:
  analysis:
    name: Scorecard analysis
    runs-on: ubuntu-latest
    permissions:
      # Needed to upload the results to code-scanning dashboard.
      security-events: write
      # Needed to publish results and get a badge (see publish_results below).
      id-token: write

    steps:
      - name: 'Checkout code'
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          persist-credentials: false

      - name: 'Run analysis'
        uses: ossf/scorecard-action@f49aabe0b5af0936a0987cfb85d86b75731b0186 # v2.4.1
        with:
          results_file: results.sarif
          results_format: sarif

          # Publish results to OpenSSF REST API for easy access by consumers
          # Allows the repository to include the Scorecard badge.
          # See https://github.com/ossf/scorecard-action#publishing-results.
          publish_results: true

      # Upload the results as artifacts (optional). Commenting out will disable uploads of run results in SARIF
      # format to the repository Actions tab.
      - name: 'Upload artifact'
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: SARIF file
          path: results.sarif
          retention-days: 5

      # Upload the results to GitHub's code scanning dashboard.
      - name: 'Upload to code-scanning'
        uses: github/codeql-action/upload-sarif@45775bd8235c68ba998cffa5171334d58593da47 # v3.28.15
        with:
          sarif_file: results.sarif
",60,1,3,"branch_protection_rule, schedule, push",4
microsoft/TypeScript,set-version.yaml,"name: Set branch version

on:
  workflow_dispatch:
    inputs:
      branch_name:
        description: Release branch name to create
        required: true
        type: string
      package_version:
        description: Release package version
        required: true
        type: string
      core_major_minor:
        description: Release core major.minor version
        required: true
        type: string

      # Inputs provided by the bot
      distinct_id:
        description: '(bot) A distinct ID'
        required: false
        default: ''
      source_issue:
        description: '(bot) The issue that triggered this workflow'
        required: false
        default: ''
      requesting_user:
        description: '(bot) The user who requested this workflow'
        required: false
        default: ''
      status_comment:
        description: '(bot) The comment to update with the status of this workflow'
        required: false
        default: ''

run-name: ${{ github.workflow }}${{ inputs.distinct_id && format(' (bot run {0})', inputs.distinct_id) || '' }}

permissions:
  contents: read

# Ensure scripts are run with pipefail. See:
# https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#exit-codes-and-error-action-preference
defaults:
  run:
    shell: bash

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          ref: ${{ inputs.branch_name }}
          token: ${{ secrets.TS_BOT_GITHUB_TOKEN }}
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
      - run: |
          npm --version
          # corepack enable npm
          npm install -g $(jq -r '.packageManager' < package.json)
          npm --version
      # notably, this is essentially the same script as `new-release-branch.yaml` (with fewer inputs), but it assumes the branch already exists
      # do note that executing the transform below will prevent the `configurePrerelease` script from running on the source, as it makes the
      # `version` identifier no longer match the regex it uses
      # required client_payload members:
      # branch_name - the target branch
      # package_version - the full version string (eg, `3.9.1-rc` or `3.9.2`)
      # core_major_minor - the major.minor pair associated with the desired package_version (eg, `3.9` for `3.9.3`)
      - run: |
          sed -i -e 's/""version"": "".*""/""version"": ""${{ inputs.package_version }}""/g' package.json
          sed -i -e 's/const versionMajorMinor = "".*""/const versionMajorMinor = ""${{ inputs.core_major_minor }}""/g' src/compiler/corePublic.ts
          sed -i -e 's/const versionMajorMinor = "".*""/const versionMajorMinor = ""${{ inputs.core_major_minor }}""/g' tests/baselines/reference/api/typescript.d.ts
          sed -i -e 's/const version\(: string\)\{0,1\} = .*;/const version = ""${{ inputs.package_version }}"" as string;/g' src/compiler/corePublic.ts
          npm ci
          npm install # update package-lock.json to ensure the version bump is included
          npx hereby LKG
          npm test
          git diff
          git add package.json package-lock.json
          git add src/compiler/corePublic.ts
          git add tests/baselines/reference/api/typescript.d.ts
          git add --force ./lib
          git config user.email ""typescriptbot@microsoft.com""
          git config user.name ""TypeScript Bot""
          git commit -m 'Bump version to ${{ inputs.package_version }} and LKG'
          git push

      - uses: microsoft/typescript-bot-test-triggerer/.github/actions/post-workflow-result@master
        if: ${{ !cancelled() && inputs.distinct_id }}
        with:
          success_comment: ""I've set the version of ${{ inputs.branch_name }} to ${{ inputs.package_version }} for you.""
          failure_comment: 'I was unable set the version.'
          github_token: ${{ secrets.TS_BOT_GITHUB_TOKEN }}
          distinct_id: ${{ inputs.distinct_id }}
          source_issue: ${{ inputs.source_issue }}
          requesting_user: ${{ inputs.requesting_user }}
          status_comment: ${{ inputs.status_comment }}
",99,1,1,workflow_dispatch,3
microsoft/TypeScript,sync-branch.yaml,"name: Sync branch with master

on:
  workflow_dispatch:
    inputs:
      branch_name:
        description: Release branch name to create
        required: true
        type: string

      # Inputs provided by the bot
      distinct_id:
        description: '(bot) A distinct ID'
        required: false
        default: ''
      source_issue:
        description: '(bot) The issue that triggered this workflow'
        required: false
        default: ''
      requesting_user:
        description: '(bot) The user who requested this workflow'
        required: false
        default: ''
      status_comment:
        description: '(bot) The comment to update with the status of this workflow'
        required: false
        default: ''

run-name: ${{ github.workflow }}${{ inputs.distinct_id && format(' (bot run {0})', inputs.distinct_id) || '' }}

permissions:
  contents: read

# Ensure scripts are run with pipefail. See:
# https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#exit-codes-and-error-action-preference
defaults:
  run:
    shell: bash

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          ref: ${{ inputs.branch_name }}
          filter: blob:none # https://github.blog/2020-12-21-get-up-to-speed-with-partial-clone-and-shallow-clone/
          fetch-depth: 0 # Default is 1; need to set to 0 to get the benefits of blob:none.
          token: ${{ secrets.TS_BOT_GITHUB_TOKEN }}
      # required client_payload members:
      # branch_name - the target branch
      - run: |
          git config user.email ""typescriptbot@microsoft.com""
          git config user.name ""TypeScript Bot""
          git fetch origin main
          git merge origin/main --no-ff
          npm ci
          npx hereby LKG
          git add --force ./lib
          git commit -m 'Update LKG'
          git push

      - uses: microsoft/typescript-bot-test-triggerer/.github/actions/post-workflow-result@master
        if: ${{ !cancelled() && inputs.distinct_id }}
        with:
          success_comment: ""I've pulled main into ${{ inputs.branch_name }} for you.""
          failure_comment: 'I was unable merge main into ${{ inputs.branch_name }}.'
          github_token: ${{ secrets.TS_BOT_GITHUB_TOKEN }}
          distinct_id: ${{ inputs.distinct_id }}
          source_issue: ${{ inputs.source_issue }}
          requesting_user: ${{ inputs.requesting_user }}
          status_comment: ${{ inputs.status_comment }}
",76,1,1,workflow_dispatch,3
microsoft/TypeScript,sync-wiki.yml,"name: Sync Two Wiki Repos

on: [gollum]

permissions:
  contents: read

# Ensure scripts are run with pipefail. See:
# https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#exit-codes-and-error-action-preference
defaults:
  run:
    shell: bash

jobs:
  sync:
    runs-on: ubuntu-latest
    steps:
      - name: Get repo name
        run: R=${GITHUB_REPOSITORY%?wiki}; echo ""BASENAME=${R##*/}"" >> $GITHUB_ENV
      - name: Checkout ${{ env.BASENAME }}-wiki
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          repository: '${{ GITHUB.repository_owner }}/${{ env.BASENAME }}-wiki'
          token: ${{ secrets.TS_BOT_GITHUB_TOKEN }}
          fetch-depth: 0
      - name: Run sync
        run: ./.github/workflows/sync
        env:
          PUSHER: typescript-bot <bot@typescriptlang.org>
          AUTH: ${{ secrets.TS_BOT_GITHUB_TOKEN }}
",30,1,1,gollum,1
microsoft/TypeScript,twoslash-repros.yaml,"name: Twoslash Code Sample Repros

on:
  schedule:
    - cron: '0 8 * * *'
  workflow_dispatch:
    inputs:
      issue:
        description: Limits run to a single issue.
        required: false
        type: string
      bisect:
        description: If set, runs a git bisect on an existing repro. Requires 'issue' to be set. Value can be revision labels (e.g. `good v4.7.3 bad main`) or `true` to infer bisect range.
        required: false
        type: string

      # Inputs provided by the bot
      distinct_id:
        description: '(bot) A distinct ID'
        required: false
        default: ''
      source_issue:
        description: '(bot) The issue that triggered this workflow'
        required: false
        default: ''
      requesting_user:
        description: '(bot) The user who requested this workflow'
        required: false
        default: ''
      status_comment:
        description: '(bot) The comment to update with the status of this workflow'
        required: false
        default: ''

run-name: ${{ github.workflow }}${{ inputs.distinct_id && format(' (bot run {0})', inputs.distinct_id) || '' }}

permissions:
  contents: read

# Ensure scripts are run with pipefail. See:
# https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#exit-codes-and-error-action-preference
defaults:
  run:
    shell: bash

jobs:
  run:
    if: ${{ github.repository == 'microsoft/TypeScript' }}
    runs-on: ubuntu-latest
    steps:
      - if: ${{ github.event.inputs.bisect }}
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          filter: blob:none # https://github.blog/2020-12-21-get-up-to-speed-with-partial-clone-and-shallow-clone/
          fetch-depth: 0 # Default is 1; need to set to 0 to get the benefits of blob:none.
      - if: ${{ !github.event.inputs.bisect }}
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
      - uses: microsoft/TypeScript-Twoslash-Repro-Action@master
        with:
          github-token: ${{ secrets.TS_BOT_GITHUB_TOKEN }}
          issue: ${{ github.event.inputs.issue }}
          bisect: ${{ github.event.inputs.bisect }}
",65,1,2,"schedule, workflow_dispatch",4
microsoft/TypeScript,update-package-lock.yaml,"name: Update package-lock.json

on:
  schedule:
    # This is probably 6am UTC, which is 10pm PST or 11pm PDT
    # Alternatively, 6am local is also fine
    - cron: '0 6 * * *'
  workflow_dispatch: {}

permissions:
  contents: read

# Ensure scripts are run with pipefail. See:
# https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#exit-codes-and-error-action-preference
defaults:
  run:
    shell: bash

jobs:
  build:
    runs-on: ubuntu-latest
    if: github.repository == 'microsoft/TypeScript'

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          token: ${{ secrets.TS_BOT_GITHUB_TOKEN }}
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 'lts/*'
      - run: |
          npm --version
          # corepack enable npm
          npm install -g $(jq -r '.packageManager' < package.json)
          npm --version

      - name: Update package-lock.json and push
        run: |
          rm package-lock.json
          npm install

          if git diff --exit-code --name-only package-lock.json; then
            echo ""No change.""
          else
            npm test
            npx hereby LKG
            git config user.email ""typescriptbot@microsoft.com""
            git config user.name ""TypeScript Bot""
            git add -f package-lock.json
            git commit -m ""Update package-lock.json""
            git push
          fi
",52,1,2,"schedule, workflow_dispatch",2
facebook/hhvm,ubuntu.yml,"name: Ubuntu CI

on:
  push:
    branches-ignore:
      # Exclude the push event for exported diffs, because the CI for export
      # should have been covered by GitHub Actions triggered by pull requests.
      - 'export-D+'
  pull_request:

concurrency:
  # If the workflow is triggered by a pull request, then cancel previous runs
  # for the same pull request, which share the same `github.ref`, otherwise the
  # run ID is used to identify the concurrency group, which is a no-op because
  # the run ID is always unique for each trigged event.
  group: ubuntu-ci-${{ github.event_name == 'pull_request' && github.ref || github.run_id }}
  cancel-in-progress: true

# OUT needs to be global. Ideally it would be local to the job so we could
# store it in ${{ runner.temp }} instead of the checkout directory.
# Unfortunately this is hitting some GitHub Action inconsistency with `env`:
# https://github.com/actions/runner/issues/480
env:
  OUT: ${{ format('{0}/out', github.workspace) }}
  DEBIAN_FRONTEND: ""noninteractive""

jobs:
  build_ubuntu_focal_nightly:
    runs-on: 16-core
    container:
      image: ubuntu:focal
      env:
        DISTRO: ubuntu-20.04-focal
        IS_NIGHTLY: 1
        CLANG_VERSION: 12
    steps:
    - name: Installing dependencies to bootstrap env
      run: apt update -y && apt install -y git wget lsb-release software-properties-common gpg
    - name: Installing llvm
      run: |
        wget https://apt.llvm.org/llvm.sh
        chmod +x llvm.sh
        # Note: Keep this version in sync with the one in the Debian control file.
        ./llvm.sh ${CLANG_VERSION}
    - name: Making LLVM the default compiler
      run: |
        if [ -f /etc/alternatives/cc ]
        then
          update-alternatives --remove-all cc
        fi

        if [ -f /etc/alternatives/c++ ]
        then
          update-alternatives --remove-all c++
        fi
        
        update-alternatives --install /usr/bin/cc cc /usr/bin/clang++-${CLANG_VERSION} 500
        update-alternatives --set cc /usr/bin/clang++-${CLANG_VERSION}
        update-alternatives --install /usr/bin/c++ c++ /usr/bin/clang++-${CLANG_VERSION} 500
        update-alternatives --set c++ /usr/bin/clang++-${CLANG_VERSION}
    - name: Fetching HHVM and its submodules
      uses: actions/checkout@v3
      with:
        submodules: 'recursive'
    - name: Installing HHVM deps and building HHVM
      run: ci/bin/make-debianish-package
    - name: Uploading artifacts
      uses: actions/upload-artifact@v3
      with:
        name: out-directory
        path: ${{ env.OUT }}
",71,1,2,"push, pull_request",2
facebook/facebook-android-sdk,needs-attention.yml,"name: Issue Needs Attention
# This workflow is triggered on issue comments.
on:
  issue_comment:
    types: created

jobs:
  applyNeedsAttentionLabel:
    name: Apply Needs Attention Label
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Apply Needs Attention Label
        uses: hramos/needs-attention@v1
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          response-required-label: 'waiting-for-response'
          needs-attention-label: 'needs-triage'
",18,1,1,issue_comment,2
facebook/facebook-android-sdk,stale.yml,"name: 'Close stale issues and PRs'
on:
  schedule:
    - cron: '30 5 * * *'

jobs:
  stale:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/stale@v4
        with:
          close-issue-message: 'Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please feel free to create a new issue with up-to-date information.'
          stale-issue-message: 'Hey there, it looks like there has been no activity on this issue recently. Has the issue been fixed, or does it still require the community&#39;s attention? This issue may be closed if no further activity occurs. Thank you for your contributions.'
          days-before-stale: 90
          days-before-close: 7
          enable-statistics: true
          operations-per-run: 60
          exempt-issue-labels: 'acknowledged,needs-triage'
",18,1,1,schedule,1
facebook/facebook-ios-sdk,needs-attention.yml,"name: Issue Needs Attention
# This workflow is triggered on issue comments.
on:
  issue_comment:
    types: created

jobs:
  applyNeedsAttentionLabel:
    name: Apply Needs Attention Label
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Apply Needs Attention Label
        uses: hramos/needs-attention@v1
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          response-required-label: 'waiting-for-response'
          needs-attention-label: 'needs-triage'
",18,1,1,issue_comment,2
facebook/facebook-ios-sdk,stale.yml,"name: 'Close stale issues and PRs'
on:
  schedule:
    - cron: '30 5 * * *'

permissions:
  contents: read

jobs:
  stale:
    permissions:
      issues: write  # for actions/stale to close stale issues
      pull-requests: write  # for actions/stale to close stale PRs
    runs-on: ubuntu-latest
    steps:
      - uses: actions/stale@v4
        with:
          close-issue-message: 'Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please feel free to create a new issue with up-to-date information.'
          stale-issue-message: 'Hey there, it looks like there has been no activity on this issue recently. Has the issue been fixed, or does it still require the community&#39;s attention? This issue may be closed if no further activity occurs. Thank you for your contributions.'
          days-before-stale: 90
          days-before-close: 7
          enable-statistics: true
          operations-per-run: 60
          exempt-issue-labels: 'acknowledged,needs-triage'
",24,1,1,schedule,1
facebook/folly,TagIt.yml,"on:
  push:
    tags:
      # Only match TagIt tags, which always start with this prefix
      - 'v20*'

name: TagIt

permissions:
  contents: read

jobs:
  build:
    permissions:
      contents: write  # for actions/create-release to create a release
    name: Release
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Archive project
        id: archive_project
        run: |
          FILE_NAME=${GITHUB_REPOSITORY#*/}-${GITHUB_REF##*/}
          git archive ${{ github.ref }} -o ${FILE_NAME}.zip
          git archive ${{ github.ref }} -o ${FILE_NAME}.tar.gz
          echo ""file_name=${FILE_NAME}"" >> $GITHUB_OUTPUT
      - name: Compute digests
        id: compute_digests
        run: |
          echo ""tgz_256=$(openssl dgst -sha256 ${{ steps.archive_project.outputs.file_name }}.tar.gz)"" >> $GITHUB_OUTPUT
          echo ""tgz_512=$(openssl dgst -sha512 ${{ steps.archive_project.outputs.file_name }}.tar.gz)"" >> $GITHUB_OUTPUT
          echo ""zip_256=$(openssl dgst -sha256 ${{ steps.archive_project.outputs.file_name }}.zip)"" >> $GITHUB_OUTPUT
          echo ""zip_512=$(openssl dgst -sha512 ${{ steps.archive_project.outputs.file_name }}.zip)"" >> $GITHUB_OUTPUT
      - name: Create Release
        id: create_release
        uses: actions/create-release@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tag_name: ${{ github.ref }}
          release_name: ${{ github.ref }}
          body: |
            Automated release from TagIt
            <details>
              <summary>File Hashes</summary>
              <ul>
                <li>${{ steps.compute_digests.outputs.zip_256 }}</li>
                <li>${{ steps.compute_digests.outputs.zip_512 }}</li>
                <li>${{ steps.compute_digests.outputs.tgz_256 }}</li>
                <li>${{ steps.compute_digests.outputs.tgz_512 }}</li>
              </ul>
            </details>
          draft: false
          prerelease: false
      - name: Upload zip
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ steps.create_release.outputs.upload_url }}
          asset_path: ./${{ steps.archive_project.outputs.file_name }}.zip
          asset_name: ${{ steps.archive_project.outputs.file_name }}.zip
          asset_content_type: application/zip
      - name: Upload tar.gz
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ steps.create_release.outputs.upload_url }}
          asset_path: ./${{ steps.archive_project.outputs.file_name }}.tar.gz
          asset_name: ${{ steps.archive_project.outputs.file_name }}.tar.gz
          asset_content_type: application/gzip
",73,1,1,push,4
facebook/folly,getdeps_linux.yml,"# This file was @generated by getdeps.py

name: linux

on:
  push:
    branches:
    - main
  pull_request:
    branches:
    - main

permissions:
  contents: read  #  to fetch code (actions/checkout)

jobs:
  build:
    runs-on: ubuntu-22.04
    steps:
    - uses: actions/checkout@v4
    - name: Show disk space at start
      run: df -h
    - name: Free up disk space
      run: sudo rm -rf /usr/local/lib/android
    - name: Show disk space after freeing up
      run: df -h
    - name: Update system package info
      run: sudo --preserve-env=http_proxy apt-get update
    - name: Install system deps
      run: sudo --preserve-env=http_proxy python3 build/fbcode_builder/getdeps.py --allow-system-packages install-system-deps --recursive folly && sudo --preserve-env=http_proxy python3 build/fbcode_builder/getdeps.py --allow-system-packages install-system-deps --recursive patchelf
    - id: paths
      name: Query paths
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages query-paths --recursive --src-dir=. folly  >> ""$GITHUB_OUTPUT""
    - name: Fetch boost
      if: ${{ steps.paths.outputs.boost_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests boost
    - name: Fetch ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests ninja
    - name: Fetch cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests cmake
    - name: Fetch double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests double-conversion
    - name: Fetch fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests fast_float
    - name: Fetch fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests fmt
    - name: Fetch gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests gflags
    - name: Fetch glog
      if: ${{ steps.paths.outputs.glog_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests glog
    - name: Fetch googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests googletest
    - name: Fetch libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libdwarf
    - name: Fetch libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libevent
    - name: Fetch zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests zlib
    - name: Fetch lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests lz4
    - name: Fetch snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests snappy
    - name: Fetch zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests zstd
    - name: Fetch autoconf
      if: ${{ steps.paths.outputs.autoconf_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests autoconf
    - name: Fetch automake
      if: ${{ steps.paths.outputs.automake_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests automake
    - name: Fetch libtool
      if: ${{ steps.paths.outputs.libtool_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libtool
    - name: Fetch libiberty
      if: ${{ steps.paths.outputs.libiberty_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libiberty
    - name: Fetch libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libsodium
    - name: Fetch libunwind
      if: ${{ steps.paths.outputs.libunwind_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libunwind
    - name: Fetch xz
      if: ${{ steps.paths.outputs.xz_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests xz
    - name: Restore boost from cache
      id: restore_boost
      if: ${{ steps.paths.outputs.boost_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.boost_INSTALL }}
       key: ${{ steps.paths.outputs.boost_CACHE_KEY }}-install
    - name: Build boost
      if: ${{ steps.paths.outputs.boost_SOURCE && ! steps.restore_boost.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests boost
    - name: Save boost to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.boost_SOURCE && ! steps.restore_boost.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.boost_INSTALL }}
       key: ${{ steps.paths.outputs.boost_CACHE_KEY }}-install
    - name: Restore ninja from cache
      id: restore_ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.ninja_INSTALL }}
       key: ${{ steps.paths.outputs.ninja_CACHE_KEY }}-install
    - name: Build ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE && ! steps.restore_ninja.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests ninja
    - name: Save ninja to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.ninja_SOURCE && ! steps.restore_ninja.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.ninja_INSTALL }}
       key: ${{ steps.paths.outputs.ninja_CACHE_KEY }}-install
    - name: Restore cmake from cache
      id: restore_cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.cmake_INSTALL }}
       key: ${{ steps.paths.outputs.cmake_CACHE_KEY }}-install
    - name: Build cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE && ! steps.restore_cmake.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests cmake
    - name: Save cmake to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.cmake_SOURCE && ! steps.restore_cmake.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.cmake_INSTALL }}
       key: ${{ steps.paths.outputs.cmake_CACHE_KEY }}-install
    - name: Restore double-conversion from cache
      id: restore_double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.double-conversion_INSTALL }}
       key: ${{ steps.paths.outputs.double-conversion_CACHE_KEY }}-install
    - name: Build double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE && ! steps.restore_double-conversion.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests double-conversion
    - name: Save double-conversion to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.double-conversion_SOURCE && ! steps.restore_double-conversion.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.double-conversion_INSTALL }}
       key: ${{ steps.paths.outputs.double-conversion_CACHE_KEY }}-install
    - name: Restore fast_float from cache
      id: restore_fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fast_float_INSTALL }}
       key: ${{ steps.paths.outputs.fast_float_CACHE_KEY }}-install
    - name: Build fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE && ! steps.restore_fast_float.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests fast_float
    - name: Save fast_float to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fast_float_SOURCE && ! steps.restore_fast_float.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fast_float_INSTALL }}
       key: ${{ steps.paths.outputs.fast_float_CACHE_KEY }}-install
    - name: Restore fmt from cache
      id: restore_fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fmt_INSTALL }}
       key: ${{ steps.paths.outputs.fmt_CACHE_KEY }}-install
    - name: Build fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE && ! steps.restore_fmt.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests fmt
    - name: Save fmt to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fmt_SOURCE && ! steps.restore_fmt.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fmt_INSTALL }}
       key: ${{ steps.paths.outputs.fmt_CACHE_KEY }}-install
    - name: Restore gflags from cache
      id: restore_gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.gflags_INSTALL }}
       key: ${{ steps.paths.outputs.gflags_CACHE_KEY }}-install
    - name: Build gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE && ! steps.restore_gflags.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests gflags
    - name: Save gflags to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.gflags_SOURCE && ! steps.restore_gflags.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.gflags_INSTALL }}
       key: ${{ steps.paths.outputs.gflags_CACHE_KEY }}-install
    - name: Restore glog from cache
      id: restore_glog
      if: ${{ steps.paths.outputs.glog_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.glog_INSTALL }}
       key: ${{ steps.paths.outputs.glog_CACHE_KEY }}-install
    - name: Build glog
      if: ${{ steps.paths.outputs.glog_SOURCE && ! steps.restore_glog.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests glog
    - name: Save glog to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.glog_SOURCE && ! steps.restore_glog.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.glog_INSTALL }}
       key: ${{ steps.paths.outputs.glog_CACHE_KEY }}-install
    - name: Restore googletest from cache
      id: restore_googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.googletest_INSTALL }}
       key: ${{ steps.paths.outputs.googletest_CACHE_KEY }}-install
    - name: Build googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE && ! steps.restore_googletest.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests googletest
    - name: Save googletest to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.googletest_SOURCE && ! steps.restore_googletest.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.googletest_INSTALL }}
       key: ${{ steps.paths.outputs.googletest_CACHE_KEY }}-install
    - name: Restore libdwarf from cache
      id: restore_libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libdwarf_INSTALL }}
       key: ${{ steps.paths.outputs.libdwarf_CACHE_KEY }}-install
    - name: Build libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE && ! steps.restore_libdwarf.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests libdwarf
    - name: Save libdwarf to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libdwarf_SOURCE && ! steps.restore_libdwarf.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libdwarf_INSTALL }}
       key: ${{ steps.paths.outputs.libdwarf_CACHE_KEY }}-install
    - name: Restore libevent from cache
      id: restore_libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libevent_INSTALL }}
       key: ${{ steps.paths.outputs.libevent_CACHE_KEY }}-install
    - name: Build libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE && ! steps.restore_libevent.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests libevent
    - name: Save libevent to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libevent_SOURCE && ! steps.restore_libevent.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libevent_INSTALL }}
       key: ${{ steps.paths.outputs.libevent_CACHE_KEY }}-install
    - name: Restore zlib from cache
      id: restore_zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.zlib_INSTALL }}
       key: ${{ steps.paths.outputs.zlib_CACHE_KEY }}-install
    - name: Build zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE && ! steps.restore_zlib.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests zlib
    - name: Save zlib to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.zlib_SOURCE && ! steps.restore_zlib.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.zlib_INSTALL }}
       key: ${{ steps.paths.outputs.zlib_CACHE_KEY }}-install
    - name: Restore lz4 from cache
      id: restore_lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.lz4_INSTALL }}
       key: ${{ steps.paths.outputs.lz4_CACHE_KEY }}-install
    - name: Build lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE && ! steps.restore_lz4.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests lz4
    - name: Save lz4 to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.lz4_SOURCE && ! steps.restore_lz4.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.lz4_INSTALL }}
       key: ${{ steps.paths.outputs.lz4_CACHE_KEY }}-install
    - name: Restore snappy from cache
      id: restore_snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.snappy_INSTALL }}
       key: ${{ steps.paths.outputs.snappy_CACHE_KEY }}-install
    - name: Build snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE && ! steps.restore_snappy.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests snappy
    - name: Save snappy to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.snappy_SOURCE && ! steps.restore_snappy.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.snappy_INSTALL }}
       key: ${{ steps.paths.outputs.snappy_CACHE_KEY }}-install
    - name: Restore zstd from cache
      id: restore_zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.zstd_INSTALL }}
       key: ${{ steps.paths.outputs.zstd_CACHE_KEY }}-install
    - name: Build zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE && ! steps.restore_zstd.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests zstd
    - name: Save zstd to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.zstd_SOURCE && ! steps.restore_zstd.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.zstd_INSTALL }}
       key: ${{ steps.paths.outputs.zstd_CACHE_KEY }}-install
    - name: Restore autoconf from cache
      id: restore_autoconf
      if: ${{ steps.paths.outputs.autoconf_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.autoconf_INSTALL }}
       key: ${{ steps.paths.outputs.autoconf_CACHE_KEY }}-install
    - name: Build autoconf
      if: ${{ steps.paths.outputs.autoconf_SOURCE && ! steps.restore_autoconf.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests autoconf
    - name: Save autoconf to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.autoconf_SOURCE && ! steps.restore_autoconf.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.autoconf_INSTALL }}
       key: ${{ steps.paths.outputs.autoconf_CACHE_KEY }}-install
    - name: Restore automake from cache
      id: restore_automake
      if: ${{ steps.paths.outputs.automake_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.automake_INSTALL }}
       key: ${{ steps.paths.outputs.automake_CACHE_KEY }}-install
    - name: Build automake
      if: ${{ steps.paths.outputs.automake_SOURCE && ! steps.restore_automake.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests automake
    - name: Save automake to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.automake_SOURCE && ! steps.restore_automake.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.automake_INSTALL }}
       key: ${{ steps.paths.outputs.automake_CACHE_KEY }}-install
    - name: Restore libtool from cache
      id: restore_libtool
      if: ${{ steps.paths.outputs.libtool_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libtool_INSTALL }}
       key: ${{ steps.paths.outputs.libtool_CACHE_KEY }}-install
    - name: Build libtool
      if: ${{ steps.paths.outputs.libtool_SOURCE && ! steps.restore_libtool.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests libtool
    - name: Save libtool to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libtool_SOURCE && ! steps.restore_libtool.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libtool_INSTALL }}
       key: ${{ steps.paths.outputs.libtool_CACHE_KEY }}-install
    - name: Restore libiberty from cache
      id: restore_libiberty
      if: ${{ steps.paths.outputs.libiberty_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libiberty_INSTALL }}
       key: ${{ steps.paths.outputs.libiberty_CACHE_KEY }}-install
    - name: Build libiberty
      if: ${{ steps.paths.outputs.libiberty_SOURCE && ! steps.restore_libiberty.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests libiberty
    - name: Save libiberty to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libiberty_SOURCE && ! steps.restore_libiberty.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libiberty_INSTALL }}
       key: ${{ steps.paths.outputs.libiberty_CACHE_KEY }}-install
    - name: Restore libsodium from cache
      id: restore_libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libsodium_INSTALL }}
       key: ${{ steps.paths.outputs.libsodium_CACHE_KEY }}-install
    - name: Build libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE && ! steps.restore_libsodium.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests libsodium
    - name: Save libsodium to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libsodium_SOURCE && ! steps.restore_libsodium.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libsodium_INSTALL }}
       key: ${{ steps.paths.outputs.libsodium_CACHE_KEY }}-install
    - name: Restore libunwind from cache
      id: restore_libunwind
      if: ${{ steps.paths.outputs.libunwind_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libunwind_INSTALL }}
       key: ${{ steps.paths.outputs.libunwind_CACHE_KEY }}-install
    - name: Build libunwind
      if: ${{ steps.paths.outputs.libunwind_SOURCE && ! steps.restore_libunwind.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests libunwind
    - name: Save libunwind to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libunwind_SOURCE && ! steps.restore_libunwind.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libunwind_INSTALL }}
       key: ${{ steps.paths.outputs.libunwind_CACHE_KEY }}-install
    - name: Restore xz from cache
      id: restore_xz
      if: ${{ steps.paths.outputs.xz_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.xz_INSTALL }}
       key: ${{ steps.paths.outputs.xz_CACHE_KEY }}-install
    - name: Build xz
      if: ${{ steps.paths.outputs.xz_SOURCE && ! steps.restore_xz.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests xz
    - name: Save xz to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.xz_SOURCE && ! steps.restore_xz.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.xz_INSTALL }}
       key: ${{ steps.paths.outputs.xz_CACHE_KEY }}-install
    - name: Build folly
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --src-dir=. folly  --project-install-prefix folly:/usr/local
    - name: Copy artifacts
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fixup-dyn-deps --strip --src-dir=. folly _artifacts/linux  --project-install-prefix folly:/usr/local --final-install-prefix /usr/local
    - uses: actions/upload-artifact@v4
      with:
        name: folly
        path: _artifacts
    - name: Test folly
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages test --src-dir=. folly  --project-install-prefix folly:/usr/local
    - name: Show disk space at end
      if: always()
      run: df -h
",464,1,2,"push, pull_request",46
facebook/folly,getdeps_mac.yml,"# This file was @generated by getdeps.py

name: mac

on:
  push:
    branches:
    - main
  pull_request:
    branches:
    - main

permissions:
  contents: read  #  to fetch code (actions/checkout)

jobs:
  build:
    runs-on: macOS-latest
    steps:
    - uses: actions/checkout@v4
    - name: Show disk space at start
      run: df -h
    - name: Free up disk space
      run: sudo rm -rf /usr/local/lib/android
    - name: Show disk space after freeing up
      run: df -h
    - name: Install system deps
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages install-system-deps --recursive folly
    - id: paths
      name: Query paths
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages query-paths --recursive --src-dir=. folly  >> ""$GITHUB_OUTPUT""
    - name: Fetch boost
      if: ${{ steps.paths.outputs.boost_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests boost
    - name: Fetch openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests openssl
    - name: Fetch ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests ninja
    - name: Fetch cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests cmake
    - name: Fetch double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests double-conversion
    - name: Fetch fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests fast_float
    - name: Fetch fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests fmt
    - name: Fetch gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests gflags
    - name: Fetch glog
      if: ${{ steps.paths.outputs.glog_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests glog
    - name: Fetch googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests googletest
    - name: Fetch libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libdwarf
    - name: Fetch libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libevent
    - name: Fetch lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests lz4
    - name: Fetch snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests snappy
    - name: Fetch zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests zstd
    - name: Fetch autoconf
      if: ${{ steps.paths.outputs.autoconf_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests autoconf
    - name: Fetch automake
      if: ${{ steps.paths.outputs.automake_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests automake
    - name: Fetch libtool
      if: ${{ steps.paths.outputs.libtool_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libtool
    - name: Fetch libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libsodium
    - name: Fetch xz
      if: ${{ steps.paths.outputs.xz_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests xz
    - name: Restore boost from cache
      id: restore_boost
      if: ${{ steps.paths.outputs.boost_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.boost_INSTALL }}
       key: ${{ steps.paths.outputs.boost_CACHE_KEY }}-install
    - name: Build boost
      if: ${{ steps.paths.outputs.boost_SOURCE && ! steps.restore_boost.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests boost
    - name: Save boost to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.boost_SOURCE && ! steps.restore_boost.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.boost_INSTALL }}
       key: ${{ steps.paths.outputs.boost_CACHE_KEY }}-install
    - name: Restore openssl from cache
      id: restore_openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.openssl_INSTALL }}
       key: ${{ steps.paths.outputs.openssl_CACHE_KEY }}-install
    - name: Build openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE && ! steps.restore_openssl.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests openssl
    - name: Save openssl to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.openssl_SOURCE && ! steps.restore_openssl.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.openssl_INSTALL }}
       key: ${{ steps.paths.outputs.openssl_CACHE_KEY }}-install
    - name: Restore ninja from cache
      id: restore_ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.ninja_INSTALL }}
       key: ${{ steps.paths.outputs.ninja_CACHE_KEY }}-install
    - name: Build ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE && ! steps.restore_ninja.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests ninja
    - name: Save ninja to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.ninja_SOURCE && ! steps.restore_ninja.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.ninja_INSTALL }}
       key: ${{ steps.paths.outputs.ninja_CACHE_KEY }}-install
    - name: Restore cmake from cache
      id: restore_cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.cmake_INSTALL }}
       key: ${{ steps.paths.outputs.cmake_CACHE_KEY }}-install
    - name: Build cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE && ! steps.restore_cmake.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests cmake
    - name: Save cmake to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.cmake_SOURCE && ! steps.restore_cmake.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.cmake_INSTALL }}
       key: ${{ steps.paths.outputs.cmake_CACHE_KEY }}-install
    - name: Restore double-conversion from cache
      id: restore_double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.double-conversion_INSTALL }}
       key: ${{ steps.paths.outputs.double-conversion_CACHE_KEY }}-install
    - name: Build double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE && ! steps.restore_double-conversion.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests double-conversion
    - name: Save double-conversion to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.double-conversion_SOURCE && ! steps.restore_double-conversion.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.double-conversion_INSTALL }}
       key: ${{ steps.paths.outputs.double-conversion_CACHE_KEY }}-install
    - name: Restore fast_float from cache
      id: restore_fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fast_float_INSTALL }}
       key: ${{ steps.paths.outputs.fast_float_CACHE_KEY }}-install
    - name: Build fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE && ! steps.restore_fast_float.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests fast_float
    - name: Save fast_float to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fast_float_SOURCE && ! steps.restore_fast_float.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fast_float_INSTALL }}
       key: ${{ steps.paths.outputs.fast_float_CACHE_KEY }}-install
    - name: Restore fmt from cache
      id: restore_fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fmt_INSTALL }}
       key: ${{ steps.paths.outputs.fmt_CACHE_KEY }}-install
    - name: Build fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE && ! steps.restore_fmt.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests fmt
    - name: Save fmt to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fmt_SOURCE && ! steps.restore_fmt.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fmt_INSTALL }}
       key: ${{ steps.paths.outputs.fmt_CACHE_KEY }}-install
    - name: Restore gflags from cache
      id: restore_gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.gflags_INSTALL }}
       key: ${{ steps.paths.outputs.gflags_CACHE_KEY }}-install
    - name: Build gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE && ! steps.restore_gflags.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests gflags
    - name: Save gflags to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.gflags_SOURCE && ! steps.restore_gflags.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.gflags_INSTALL }}
       key: ${{ steps.paths.outputs.gflags_CACHE_KEY }}-install
    - name: Restore glog from cache
      id: restore_glog
      if: ${{ steps.paths.outputs.glog_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.glog_INSTALL }}
       key: ${{ steps.paths.outputs.glog_CACHE_KEY }}-install
    - name: Build glog
      if: ${{ steps.paths.outputs.glog_SOURCE && ! steps.restore_glog.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests glog
    - name: Save glog to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.glog_SOURCE && ! steps.restore_glog.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.glog_INSTALL }}
       key: ${{ steps.paths.outputs.glog_CACHE_KEY }}-install
    - name: Restore googletest from cache
      id: restore_googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.googletest_INSTALL }}
       key: ${{ steps.paths.outputs.googletest_CACHE_KEY }}-install
    - name: Build googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE && ! steps.restore_googletest.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests googletest
    - name: Save googletest to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.googletest_SOURCE && ! steps.restore_googletest.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.googletest_INSTALL }}
       key: ${{ steps.paths.outputs.googletest_CACHE_KEY }}-install
    - name: Restore libdwarf from cache
      id: restore_libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libdwarf_INSTALL }}
       key: ${{ steps.paths.outputs.libdwarf_CACHE_KEY }}-install
    - name: Build libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE && ! steps.restore_libdwarf.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests libdwarf
    - name: Save libdwarf to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libdwarf_SOURCE && ! steps.restore_libdwarf.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libdwarf_INSTALL }}
       key: ${{ steps.paths.outputs.libdwarf_CACHE_KEY }}-install
    - name: Restore libevent from cache
      id: restore_libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libevent_INSTALL }}
       key: ${{ steps.paths.outputs.libevent_CACHE_KEY }}-install
    - name: Build libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE && ! steps.restore_libevent.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests libevent
    - name: Save libevent to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libevent_SOURCE && ! steps.restore_libevent.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libevent_INSTALL }}
       key: ${{ steps.paths.outputs.libevent_CACHE_KEY }}-install
    - name: Restore lz4 from cache
      id: restore_lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.lz4_INSTALL }}
       key: ${{ steps.paths.outputs.lz4_CACHE_KEY }}-install
    - name: Build lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE && ! steps.restore_lz4.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests lz4
    - name: Save lz4 to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.lz4_SOURCE && ! steps.restore_lz4.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.lz4_INSTALL }}
       key: ${{ steps.paths.outputs.lz4_CACHE_KEY }}-install
    - name: Restore snappy from cache
      id: restore_snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.snappy_INSTALL }}
       key: ${{ steps.paths.outputs.snappy_CACHE_KEY }}-install
    - name: Build snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE && ! steps.restore_snappy.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests snappy
    - name: Save snappy to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.snappy_SOURCE && ! steps.restore_snappy.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.snappy_INSTALL }}
       key: ${{ steps.paths.outputs.snappy_CACHE_KEY }}-install
    - name: Restore zstd from cache
      id: restore_zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.zstd_INSTALL }}
       key: ${{ steps.paths.outputs.zstd_CACHE_KEY }}-install
    - name: Build zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE && ! steps.restore_zstd.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests zstd
    - name: Save zstd to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.zstd_SOURCE && ! steps.restore_zstd.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.zstd_INSTALL }}
       key: ${{ steps.paths.outputs.zstd_CACHE_KEY }}-install
    - name: Restore autoconf from cache
      id: restore_autoconf
      if: ${{ steps.paths.outputs.autoconf_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.autoconf_INSTALL }}
       key: ${{ steps.paths.outputs.autoconf_CACHE_KEY }}-install
    - name: Build autoconf
      if: ${{ steps.paths.outputs.autoconf_SOURCE && ! steps.restore_autoconf.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests autoconf
    - name: Save autoconf to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.autoconf_SOURCE && ! steps.restore_autoconf.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.autoconf_INSTALL }}
       key: ${{ steps.paths.outputs.autoconf_CACHE_KEY }}-install
    - name: Restore automake from cache
      id: restore_automake
      if: ${{ steps.paths.outputs.automake_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.automake_INSTALL }}
       key: ${{ steps.paths.outputs.automake_CACHE_KEY }}-install
    - name: Build automake
      if: ${{ steps.paths.outputs.automake_SOURCE && ! steps.restore_automake.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests automake
    - name: Save automake to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.automake_SOURCE && ! steps.restore_automake.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.automake_INSTALL }}
       key: ${{ steps.paths.outputs.automake_CACHE_KEY }}-install
    - name: Restore libtool from cache
      id: restore_libtool
      if: ${{ steps.paths.outputs.libtool_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libtool_INSTALL }}
       key: ${{ steps.paths.outputs.libtool_CACHE_KEY }}-install
    - name: Build libtool
      if: ${{ steps.paths.outputs.libtool_SOURCE && ! steps.restore_libtool.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests libtool
    - name: Save libtool to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libtool_SOURCE && ! steps.restore_libtool.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libtool_INSTALL }}
       key: ${{ steps.paths.outputs.libtool_CACHE_KEY }}-install
    - name: Restore libsodium from cache
      id: restore_libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libsodium_INSTALL }}
       key: ${{ steps.paths.outputs.libsodium_CACHE_KEY }}-install
    - name: Build libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE && ! steps.restore_libsodium.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests libsodium
    - name: Save libsodium to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libsodium_SOURCE && ! steps.restore_libsodium.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libsodium_INSTALL }}
       key: ${{ steps.paths.outputs.libsodium_CACHE_KEY }}-install
    - name: Restore xz from cache
      id: restore_xz
      if: ${{ steps.paths.outputs.xz_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.xz_INSTALL }}
       key: ${{ steps.paths.outputs.xz_CACHE_KEY }}-install
    - name: Build xz
      if: ${{ steps.paths.outputs.xz_SOURCE && ! steps.restore_xz.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests xz
    - name: Save xz to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.xz_SOURCE && ! steps.restore_xz.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.xz_INSTALL }}
       key: ${{ steps.paths.outputs.xz_CACHE_KEY }}-install
    - name: Build folly
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --src-dir=. folly  --project-install-prefix folly:/usr/local
    - name: Copy artifacts
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fixup-dyn-deps --src-dir=. folly _artifacts/mac  --project-install-prefix folly:/usr/local --final-install-prefix /usr/local
    - uses: actions/upload-artifact@v4
      with:
        name: folly
        path: _artifacts
    - name: Test folly
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages test --src-dir=. folly  --project-install-prefix folly:/usr/local
    - name: Show disk space at end
      if: always()
      run: df -h
",424,1,2,"push, pull_request",42
facebook/folly,getdeps_windows.yml,"# This file was @generated by getdeps.py

name: windows

on:
  push:
    branches:
    - main
  pull_request:
    branches:
    - main

permissions:
  contents: read  #  to fetch code (actions/checkout)

jobs:
  build:
    runs-on: windows-2019
    steps:
    - name: Export boost environment
      run: ""echo BOOST_ROOT=%BOOST_ROOT_1_83_0% >> %GITHUB_ENV%""
      shell: cmd
    - name: Fix Git config
      run: >
        git config --system core.longpaths true &&
        git config --system core.autocrlf false &&
        git config --system core.symlinks true
      shell: cmd
    - uses: actions/checkout@v4
    - id: paths
      name: Query paths
      run: python build/fbcode_builder/getdeps.py query-paths --recursive --src-dir=. folly  >> $env:GITHUB_OUTPUT
      shell: pwsh
    - name: Fetch boost
      if: ${{ steps.paths.outputs.boost_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests boost
    - name: Fetch libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests libsodium
    - name: Fetch ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests ninja
    - name: Fetch cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests cmake
    - name: Fetch double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests double-conversion
    - name: Fetch fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests fast_float
    - name: Fetch fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests fmt
    - name: Fetch gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests gflags
    - name: Fetch glog
      if: ${{ steps.paths.outputs.glog_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests glog
    - name: Fetch googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests googletest
    - name: Fetch libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests libdwarf
    - name: Fetch lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests lz4
    - name: Fetch jom
      if: ${{ steps.paths.outputs.jom_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests jom
    - name: Fetch perl
      if: ${{ steps.paths.outputs.perl_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests perl
    - name: Fetch openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests openssl
    - name: Fetch snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests snappy
    - name: Fetch zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests zlib
    - name: Fetch zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests zstd
    - name: Fetch libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests libevent
    - name: Restore boost from cache
      id: restore_boost
      if: ${{ steps.paths.outputs.boost_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.boost_INSTALL }}
       key: ${{ steps.paths.outputs.boost_CACHE_KEY }}-install
    - name: Build boost
      if: ${{ steps.paths.outputs.boost_SOURCE && ! steps.restore_boost.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests boost
    - name: Save boost to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.boost_SOURCE && ! steps.restore_boost.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.boost_INSTALL }}
       key: ${{ steps.paths.outputs.boost_CACHE_KEY }}-install
    - name: Restore libsodium from cache
      id: restore_libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libsodium_INSTALL }}
       key: ${{ steps.paths.outputs.libsodium_CACHE_KEY }}-install
    - name: Build libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE && ! steps.restore_libsodium.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests libsodium
    - name: Save libsodium to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libsodium_SOURCE && ! steps.restore_libsodium.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libsodium_INSTALL }}
       key: ${{ steps.paths.outputs.libsodium_CACHE_KEY }}-install
    - name: Restore ninja from cache
      id: restore_ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.ninja_INSTALL }}
       key: ${{ steps.paths.outputs.ninja_CACHE_KEY }}-install
    - name: Build ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE && ! steps.restore_ninja.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests ninja
    - name: Save ninja to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.ninja_SOURCE && ! steps.restore_ninja.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.ninja_INSTALL }}
       key: ${{ steps.paths.outputs.ninja_CACHE_KEY }}-install
    - name: Restore cmake from cache
      id: restore_cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.cmake_INSTALL }}
       key: ${{ steps.paths.outputs.cmake_CACHE_KEY }}-install
    - name: Build cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE && ! steps.restore_cmake.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests cmake
    - name: Save cmake to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.cmake_SOURCE && ! steps.restore_cmake.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.cmake_INSTALL }}
       key: ${{ steps.paths.outputs.cmake_CACHE_KEY }}-install
    - name: Restore double-conversion from cache
      id: restore_double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.double-conversion_INSTALL }}
       key: ${{ steps.paths.outputs.double-conversion_CACHE_KEY }}-install
    - name: Build double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE && ! steps.restore_double-conversion.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests double-conversion
    - name: Save double-conversion to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.double-conversion_SOURCE && ! steps.restore_double-conversion.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.double-conversion_INSTALL }}
       key: ${{ steps.paths.outputs.double-conversion_CACHE_KEY }}-install
    - name: Restore fast_float from cache
      id: restore_fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fast_float_INSTALL }}
       key: ${{ steps.paths.outputs.fast_float_CACHE_KEY }}-install
    - name: Build fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE && ! steps.restore_fast_float.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests fast_float
    - name: Save fast_float to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fast_float_SOURCE && ! steps.restore_fast_float.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fast_float_INSTALL }}
       key: ${{ steps.paths.outputs.fast_float_CACHE_KEY }}-install
    - name: Restore fmt from cache
      id: restore_fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fmt_INSTALL }}
       key: ${{ steps.paths.outputs.fmt_CACHE_KEY }}-install
    - name: Build fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE && ! steps.restore_fmt.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests fmt
    - name: Save fmt to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fmt_SOURCE && ! steps.restore_fmt.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fmt_INSTALL }}
       key: ${{ steps.paths.outputs.fmt_CACHE_KEY }}-install
    - name: Restore gflags from cache
      id: restore_gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.gflags_INSTALL }}
       key: ${{ steps.paths.outputs.gflags_CACHE_KEY }}-install
    - name: Build gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE && ! steps.restore_gflags.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests gflags
    - name: Save gflags to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.gflags_SOURCE && ! steps.restore_gflags.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.gflags_INSTALL }}
       key: ${{ steps.paths.outputs.gflags_CACHE_KEY }}-install
    - name: Restore glog from cache
      id: restore_glog
      if: ${{ steps.paths.outputs.glog_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.glog_INSTALL }}
       key: ${{ steps.paths.outputs.glog_CACHE_KEY }}-install
    - name: Build glog
      if: ${{ steps.paths.outputs.glog_SOURCE && ! steps.restore_glog.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests glog
    - name: Save glog to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.glog_SOURCE && ! steps.restore_glog.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.glog_INSTALL }}
       key: ${{ steps.paths.outputs.glog_CACHE_KEY }}-install
    - name: Restore googletest from cache
      id: restore_googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.googletest_INSTALL }}
       key: ${{ steps.paths.outputs.googletest_CACHE_KEY }}-install
    - name: Build googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE && ! steps.restore_googletest.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests googletest
    - name: Save googletest to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.googletest_SOURCE && ! steps.restore_googletest.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.googletest_INSTALL }}
       key: ${{ steps.paths.outputs.googletest_CACHE_KEY }}-install
    - name: Restore libdwarf from cache
      id: restore_libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libdwarf_INSTALL }}
       key: ${{ steps.paths.outputs.libdwarf_CACHE_KEY }}-install
    - name: Build libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE && ! steps.restore_libdwarf.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests libdwarf
    - name: Save libdwarf to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libdwarf_SOURCE && ! steps.restore_libdwarf.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libdwarf_INSTALL }}
       key: ${{ steps.paths.outputs.libdwarf_CACHE_KEY }}-install
    - name: Restore lz4 from cache
      id: restore_lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.lz4_INSTALL }}
       key: ${{ steps.paths.outputs.lz4_CACHE_KEY }}-install
    - name: Build lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE && ! steps.restore_lz4.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests lz4
    - name: Save lz4 to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.lz4_SOURCE && ! steps.restore_lz4.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.lz4_INSTALL }}
       key: ${{ steps.paths.outputs.lz4_CACHE_KEY }}-install
    - name: Restore jom from cache
      id: restore_jom
      if: ${{ steps.paths.outputs.jom_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.jom_INSTALL }}
       key: ${{ steps.paths.outputs.jom_CACHE_KEY }}-install
    - name: Build jom
      if: ${{ steps.paths.outputs.jom_SOURCE && ! steps.restore_jom.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests jom
    - name: Save jom to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.jom_SOURCE && ! steps.restore_jom.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.jom_INSTALL }}
       key: ${{ steps.paths.outputs.jom_CACHE_KEY }}-install
    - name: Restore perl from cache
      id: restore_perl
      if: ${{ steps.paths.outputs.perl_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.perl_INSTALL }}
       key: ${{ steps.paths.outputs.perl_CACHE_KEY }}-install
    - name: Build perl
      if: ${{ steps.paths.outputs.perl_SOURCE && ! steps.restore_perl.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests perl
    - name: Save perl to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.perl_SOURCE && ! steps.restore_perl.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.perl_INSTALL }}
       key: ${{ steps.paths.outputs.perl_CACHE_KEY }}-install
    - name: Restore openssl from cache
      id: restore_openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.openssl_INSTALL }}
       key: ${{ steps.paths.outputs.openssl_CACHE_KEY }}-install
    - name: Build openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE && ! steps.restore_openssl.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests openssl
    - name: Save openssl to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.openssl_SOURCE && ! steps.restore_openssl.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.openssl_INSTALL }}
       key: ${{ steps.paths.outputs.openssl_CACHE_KEY }}-install
    - name: Restore snappy from cache
      id: restore_snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.snappy_INSTALL }}
       key: ${{ steps.paths.outputs.snappy_CACHE_KEY }}-install
    - name: Build snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE && ! steps.restore_snappy.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests snappy
    - name: Save snappy to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.snappy_SOURCE && ! steps.restore_snappy.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.snappy_INSTALL }}
       key: ${{ steps.paths.outputs.snappy_CACHE_KEY }}-install
    - name: Restore zlib from cache
      id: restore_zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.zlib_INSTALL }}
       key: ${{ steps.paths.outputs.zlib_CACHE_KEY }}-install
    - name: Build zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE && ! steps.restore_zlib.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests zlib
    - name: Save zlib to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.zlib_SOURCE && ! steps.restore_zlib.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.zlib_INSTALL }}
       key: ${{ steps.paths.outputs.zlib_CACHE_KEY }}-install
    - name: Restore zstd from cache
      id: restore_zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.zstd_INSTALL }}
       key: ${{ steps.paths.outputs.zstd_CACHE_KEY }}-install
    - name: Build zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE && ! steps.restore_zstd.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests zstd
    - name: Save zstd to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.zstd_SOURCE && ! steps.restore_zstd.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.zstd_INSTALL }}
       key: ${{ steps.paths.outputs.zstd_CACHE_KEY }}-install
    - name: Restore libevent from cache
      id: restore_libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libevent_INSTALL }}
       key: ${{ steps.paths.outputs.libevent_CACHE_KEY }}-install
    - name: Build libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE && ! steps.restore_libevent.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests libevent
    - name: Save libevent to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libevent_SOURCE && ! steps.restore_libevent.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libevent_INSTALL }}
       key: ${{ steps.paths.outputs.libevent_CACHE_KEY }}-install
    - name: Build folly
      run: python build/fbcode_builder/getdeps.py build --src-dir=. folly 
    - name: Copy artifacts
      run: python build/fbcode_builder/getdeps.py fixup-dyn-deps --src-dir=. folly _artifacts/windows  --final-install-prefix /usr/local
    - uses: actions/upload-artifact@v4
      with:
        name: folly
        path: _artifacts
    - name: Test folly
      run: python build/fbcode_builder/getdeps.py test --src-dir=. folly 
",404,1,2,"push, pull_request",40
facebook/folly,oss-build-and-test.yml,"name: Buck build and test
on: [push, pull_request, workflow_dispatch]
jobs:
  get-toolchains-to-install:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'true'
      - uses: facebook/install-dotslash@latest
      - name: get_buck_graph
        run: |
          BUCK_GRAPH=$(./buck2 cquery ... --output-attribute '^buck.type$|^name$')
          echo ""$BUCK_GRAPH"" > buck_graph_results.json
        shell: bash
      - name: Check if rust_binary
        id: check_rust
        run: |
          OUTPUT=$(cat buck_graph_results.json)
          if [[ ""$OUTPUT"" == *""rust_binary""* ]]; then
            echo ""uses_rust=true"" >> $GITHUB_ENV
          fi
        shell: bash
      - name: Check if cxx_binary
        id: check_cxx
        run: |
          OUTPUT=$(cat buck_graph_results.json)
          if [[ ""$OUTPUT"" == *""cxx_binary""* ]]; then
            echo ""uses_cxx=true"" >> $GITHUB_ENV
          fi
        shell: bash
      - name: Check if ocaml_binary
        id: check_ocaml
        run: |
          OUTPUT=$(cat buck_graph_results.json)
          if [[ ""$OUTPUT"" == *""ocaml_binary""* ]]; then
            echo ""uses_ocaml=true"" >> $GITHUB_ENV
          fi
        shell: bash
      - name: Check if python_binary
        id: check_python
        run: |
          OUTPUT=$(cat buck_graph_results.json)
          if [[ ""$OUTPUT"" == *""python_binary""* ]]; then
            echo ""uses_python=true"" >> $GITHUB_ENV
          fi
        shell: bash
    outputs:
      uses_rust: ${{ env.uses_rust }}
      uses_cxx: ${{ env.uses_cxx }}
      uses_ocaml: ${{env.uses_ocaml}}
      uses_python: ${{env.uses_python}}

  ubuntu-os-buck-build-and-test:
      needs: get-toolchains-to-install
      runs-on: ubuntu-latest
      steps:
        - uses: actions/checkout@v4
          with:
            submodules: 'true'
        - run: sudo apt-get update
          shell: bash
        - uses: facebook/install-dotslash@latest
        - name: Install Rust toolchain
          if: needs.get-toolchains-to-install.outputs.uses_rust == 'true'
          uses: dtolnay/rust-toolchain@stable
        - name: Install C++ toolchain
          if: needs.get-toolchains-to-install.outputs.uses_cxx == 'true'
          run: |
            sudo apt-get install cmake llvm cppcheck python3-pip
            sudo pip3 install conan==1.*
          shell: bash
        - name: Install OCaml toolchain
          if: needs.get-toolchains-to-install.outputs.uses_ocaml == 'true'
          uses: ocaml/setup-ocaml@v2
          with:
            ocaml-compiler: ""5.1""
        - name: Install Python toolchain
          if: needs.get-toolchains-to-install.outputs.uses_python == 'true'
          uses: actions/setup-python@v5
          with:
            python-version: '3.10'
        - name: buck2 build and test
          run: bash ./.github/scripts/buck_build_and_test.sh
  windows-os-buck-build-and-test:
      needs: get-toolchains-to-install
      runs-on: windows-latest
      steps:
        - uses: actions/checkout@v4
          with:
            submodules: 'true'
        - uses: facebook/install-dotslash@latest
        - name: Install Rust toolchain
          if: needs.get-toolchains-to-install.outputs.uses_rust == 'true'
          uses: dtolnay/rust-toolchain@stable
        - name: Install C++ toolchain
          if: needs.get-toolchains-to-install.outputs.uses_cxx == 'true'
          run: |
            choco install llvm cmake conan cppcheck -y
            if ($LASTEXITCODE -eq 3010) { $LASTEXITCODE = 0 }
          shell: pwsh
        - name: Install OCaml toolchain
          if: needs.get-toolchains-to-install.outputs.uses_ocaml == 'true'
          uses: ocaml/setup-ocaml@v2
          with:
            ocaml-compiler: ""4.12.0""
        - name: Install Python toolchain
          if: needs.get-toolchains-to-install.outputs.uses_python == 'true'
          uses: actions/setup-python@v5
          with:
            python-version: '3.10'
        - name: buck2 build and test
          run: bash ./.github/scripts/buck_build_and_test.sh
  mac-os-buck-build-and-test:
      needs: get-toolchains-to-install
      runs-on: macos-latest
      steps:
        - uses: actions/checkout@v4
          with:
            submodules: 'true'
        - uses: facebook/install-dotslash@latest
        - name: Install Rust toolchain
          if: needs.get-toolchains-to-install.outputs.uses_rust == 'true'
          uses: dtolnay/rust-toolchain@stable
        - name: Install C++ toolchain
          if: needs.get-toolchains-to-install.outputs.uses_cxx == 'true'
          run: |
            brew install cmake llvm cppcheck python3 conan@1
          shell: bash
        - name: Install OCaml toolchain
          if: needs.get-toolchains-to-install.outputs.uses_ocaml == 'true'
          uses: ocaml/setup-ocaml@v2
          with:
            ocaml-compiler: ""5.1""
        - name: Install Python toolchain
          if: needs.get-toolchains-to-install.outputs.uses_python == 'true'
          uses: actions/setup-python@v5
          with:
            python-version: '3.10'
        - name: Install homebrew deps
          run: |
            BUCK_GRAPH=$(./buck2 cquery ""attrregexfilter(labels, 'third-party:homebrew:', deps(//...))"" --json --output-attribute=labels)
            HOMEBREW_PACKAGES=$(echo $BUCK_GRAPH | jq '[.[] | .labels] | flatten | unique | map(select(contains(""third-party:homebrew:"")) | sub(""third-party:homebrew:""; """")) | .[] | @text')
            echo $HOMEBREW_PACKAGES
            echo $HOMEBREW_PACKAGES | xargs brew install pkg-config
        - name: buck2 build and test
          run: bash ./.github/scripts/buck_build_and_test.sh
",147,4,3,"push, pull_request, workflow_dispatch",17
facebook/watchman,getdeps_linux.yml,"# This file was @generated by getdeps.py

name: linux

on:
  push:
    branches:
    - main
  pull_request:
    branches:
    - main

permissions:
  contents: read  #  to fetch code (actions/checkout)

jobs:
  build:
    runs-on: ubuntu-22.04
    steps:
    - uses: actions/checkout@v4
    - name: Update system package info
      run: sudo --preserve-env=http_proxy apt-get update
    - name: Install system deps
      run: sudo --preserve-env=http_proxy python3 build/fbcode_builder/getdeps.py --allow-system-packages install-system-deps --recursive watchman && sudo --preserve-env=http_proxy python3 build/fbcode_builder/getdeps.py --allow-system-packages install-system-deps --recursive patchelf
    - id: paths
      name: Query paths
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages query-paths --recursive --src-dir=. watchman  >> ""$GITHUB_OUTPUT""
    - name: Install Rust Stable
      uses: dtolnay/rust-toolchain@stable
    - name: Fetch boost
      if: ${{ steps.paths.outputs.boost_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests boost
    - name: Fetch ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests ninja
    - name: Fetch cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests cmake
    - name: Fetch cpptoml
      if: ${{ steps.paths.outputs.cpptoml_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests cpptoml
    - name: Fetch fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests fmt
    - name: Fetch gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests gflags
    - name: Fetch glog
      if: ${{ steps.paths.outputs.glog_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests glog
    - name: Fetch googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests googletest
    - name: Fetch xxhash
      if: ${{ steps.paths.outputs.xxhash_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests xxhash
    - name: Fetch zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests zstd
    - name: Fetch double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests double-conversion
    - name: Fetch fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests fast_float
    - name: Fetch libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libdwarf
    - name: Fetch libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libevent
    - name: Fetch lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests lz4
    - name: Fetch snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests snappy
    - name: Fetch pcre2
      if: ${{ steps.paths.outputs.pcre2_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests pcre2
    - name: Fetch python-setuptools
      if: ${{ steps.paths.outputs.python-setuptools_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests python-setuptools
    - name: Fetch zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests zlib
    - name: Fetch openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests openssl
    - name: Fetch liboqs
      if: ${{ steps.paths.outputs.liboqs_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests liboqs
    - name: Fetch autoconf
      if: ${{ steps.paths.outputs.autoconf_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests autoconf
    - name: Fetch automake
      if: ${{ steps.paths.outputs.automake_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests automake
    - name: Fetch libtool
      if: ${{ steps.paths.outputs.libtool_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libtool
    - name: Fetch libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libsodium
    - name: Fetch libiberty
      if: ${{ steps.paths.outputs.libiberty_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libiberty
    - name: Fetch libunwind
      if: ${{ steps.paths.outputs.libunwind_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libunwind
    - name: Fetch xz
      if: ${{ steps.paths.outputs.xz_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests xz
    - name: Fetch folly
      if: ${{ steps.paths.outputs.folly_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests folly
    - name: Fetch fizz
      if: ${{ steps.paths.outputs.fizz_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests fizz
    - name: Fetch mvfst
      if: ${{ steps.paths.outputs.mvfst_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests mvfst
    - name: Fetch wangle
      if: ${{ steps.paths.outputs.wangle_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests wangle
    - name: Fetch fbthrift
      if: ${{ steps.paths.outputs.fbthrift_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests fbthrift
    - name: Fetch fb303
      if: ${{ steps.paths.outputs.fb303_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests fb303
    - name: Fetch edencommon
      if: ${{ steps.paths.outputs.edencommon_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests edencommon
    - name: Restore boost from cache
      id: restore_boost
      if: ${{ steps.paths.outputs.boost_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.boost_INSTALL }}
       key: ${{ steps.paths.outputs.boost_CACHE_KEY }}-install
    - name: Build boost
      if: ${{ steps.paths.outputs.boost_SOURCE && ! steps.restore_boost.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests boost
    - name: Save boost to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.boost_SOURCE && ! steps.restore_boost.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.boost_INSTALL }}
       key: ${{ steps.paths.outputs.boost_CACHE_KEY }}-install
    - name: Restore ninja from cache
      id: restore_ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.ninja_INSTALL }}
       key: ${{ steps.paths.outputs.ninja_CACHE_KEY }}-install
    - name: Build ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE && ! steps.restore_ninja.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests ninja
    - name: Save ninja to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.ninja_SOURCE && ! steps.restore_ninja.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.ninja_INSTALL }}
       key: ${{ steps.paths.outputs.ninja_CACHE_KEY }}-install
    - name: Restore cmake from cache
      id: restore_cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.cmake_INSTALL }}
       key: ${{ steps.paths.outputs.cmake_CACHE_KEY }}-install
    - name: Build cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE && ! steps.restore_cmake.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests cmake
    - name: Save cmake to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.cmake_SOURCE && ! steps.restore_cmake.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.cmake_INSTALL }}
       key: ${{ steps.paths.outputs.cmake_CACHE_KEY }}-install
    - name: Restore cpptoml from cache
      id: restore_cpptoml
      if: ${{ steps.paths.outputs.cpptoml_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.cpptoml_INSTALL }}
       key: ${{ steps.paths.outputs.cpptoml_CACHE_KEY }}-install
    - name: Build cpptoml
      if: ${{ steps.paths.outputs.cpptoml_SOURCE && ! steps.restore_cpptoml.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests cpptoml
    - name: Save cpptoml to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.cpptoml_SOURCE && ! steps.restore_cpptoml.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.cpptoml_INSTALL }}
       key: ${{ steps.paths.outputs.cpptoml_CACHE_KEY }}-install
    - name: Restore fmt from cache
      id: restore_fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fmt_INSTALL }}
       key: ${{ steps.paths.outputs.fmt_CACHE_KEY }}-install
    - name: Build fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE && ! steps.restore_fmt.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests fmt
    - name: Save fmt to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fmt_SOURCE && ! steps.restore_fmt.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fmt_INSTALL }}
       key: ${{ steps.paths.outputs.fmt_CACHE_KEY }}-install
    - name: Restore gflags from cache
      id: restore_gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.gflags_INSTALL }}
       key: ${{ steps.paths.outputs.gflags_CACHE_KEY }}-install
    - name: Build gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE && ! steps.restore_gflags.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests gflags
    - name: Save gflags to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.gflags_SOURCE && ! steps.restore_gflags.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.gflags_INSTALL }}
       key: ${{ steps.paths.outputs.gflags_CACHE_KEY }}-install
    - name: Restore glog from cache
      id: restore_glog
      if: ${{ steps.paths.outputs.glog_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.glog_INSTALL }}
       key: ${{ steps.paths.outputs.glog_CACHE_KEY }}-install
    - name: Build glog
      if: ${{ steps.paths.outputs.glog_SOURCE && ! steps.restore_glog.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests glog
    - name: Save glog to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.glog_SOURCE && ! steps.restore_glog.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.glog_INSTALL }}
       key: ${{ steps.paths.outputs.glog_CACHE_KEY }}-install
    - name: Restore googletest from cache
      id: restore_googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.googletest_INSTALL }}
       key: ${{ steps.paths.outputs.googletest_CACHE_KEY }}-install
    - name: Build googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE && ! steps.restore_googletest.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests googletest
    - name: Save googletest to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.googletest_SOURCE && ! steps.restore_googletest.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.googletest_INSTALL }}
       key: ${{ steps.paths.outputs.googletest_CACHE_KEY }}-install
    - name: Restore xxhash from cache
      id: restore_xxhash
      if: ${{ steps.paths.outputs.xxhash_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.xxhash_INSTALL }}
       key: ${{ steps.paths.outputs.xxhash_CACHE_KEY }}-install
    - name: Build xxhash
      if: ${{ steps.paths.outputs.xxhash_SOURCE && ! steps.restore_xxhash.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests xxhash
    - name: Save xxhash to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.xxhash_SOURCE && ! steps.restore_xxhash.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.xxhash_INSTALL }}
       key: ${{ steps.paths.outputs.xxhash_CACHE_KEY }}-install
    - name: Restore zstd from cache
      id: restore_zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.zstd_INSTALL }}
       key: ${{ steps.paths.outputs.zstd_CACHE_KEY }}-install
    - name: Build zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE && ! steps.restore_zstd.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests zstd
    - name: Save zstd to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.zstd_SOURCE && ! steps.restore_zstd.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.zstd_INSTALL }}
       key: ${{ steps.paths.outputs.zstd_CACHE_KEY }}-install
    - name: Restore double-conversion from cache
      id: restore_double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.double-conversion_INSTALL }}
       key: ${{ steps.paths.outputs.double-conversion_CACHE_KEY }}-install
    - name: Build double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE && ! steps.restore_double-conversion.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests double-conversion
    - name: Save double-conversion to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.double-conversion_SOURCE && ! steps.restore_double-conversion.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.double-conversion_INSTALL }}
       key: ${{ steps.paths.outputs.double-conversion_CACHE_KEY }}-install
    - name: Restore fast_float from cache
      id: restore_fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fast_float_INSTALL }}
       key: ${{ steps.paths.outputs.fast_float_CACHE_KEY }}-install
    - name: Build fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE && ! steps.restore_fast_float.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests fast_float
    - name: Save fast_float to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fast_float_SOURCE && ! steps.restore_fast_float.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fast_float_INSTALL }}
       key: ${{ steps.paths.outputs.fast_float_CACHE_KEY }}-install
    - name: Restore libdwarf from cache
      id: restore_libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libdwarf_INSTALL }}
       key: ${{ steps.paths.outputs.libdwarf_CACHE_KEY }}-install
    - name: Build libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE && ! steps.restore_libdwarf.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests libdwarf
    - name: Save libdwarf to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libdwarf_SOURCE && ! steps.restore_libdwarf.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libdwarf_INSTALL }}
       key: ${{ steps.paths.outputs.libdwarf_CACHE_KEY }}-install
    - name: Restore libevent from cache
      id: restore_libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libevent_INSTALL }}
       key: ${{ steps.paths.outputs.libevent_CACHE_KEY }}-install
    - name: Build libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE && ! steps.restore_libevent.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests libevent
    - name: Save libevent to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libevent_SOURCE && ! steps.restore_libevent.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libevent_INSTALL }}
       key: ${{ steps.paths.outputs.libevent_CACHE_KEY }}-install
    - name: Restore lz4 from cache
      id: restore_lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.lz4_INSTALL }}
       key: ${{ steps.paths.outputs.lz4_CACHE_KEY }}-install
    - name: Build lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE && ! steps.restore_lz4.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests lz4
    - name: Save lz4 to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.lz4_SOURCE && ! steps.restore_lz4.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.lz4_INSTALL }}
       key: ${{ steps.paths.outputs.lz4_CACHE_KEY }}-install
    - name: Restore snappy from cache
      id: restore_snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.snappy_INSTALL }}
       key: ${{ steps.paths.outputs.snappy_CACHE_KEY }}-install
    - name: Build snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE && ! steps.restore_snappy.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests snappy
    - name: Save snappy to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.snappy_SOURCE && ! steps.restore_snappy.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.snappy_INSTALL }}
       key: ${{ steps.paths.outputs.snappy_CACHE_KEY }}-install
    - name: Restore pcre2 from cache
      id: restore_pcre2
      if: ${{ steps.paths.outputs.pcre2_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.pcre2_INSTALL }}
       key: ${{ steps.paths.outputs.pcre2_CACHE_KEY }}-install
    - name: Build pcre2
      if: ${{ steps.paths.outputs.pcre2_SOURCE && ! steps.restore_pcre2.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests pcre2
    - name: Save pcre2 to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.pcre2_SOURCE && ! steps.restore_pcre2.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.pcre2_INSTALL }}
       key: ${{ steps.paths.outputs.pcre2_CACHE_KEY }}-install
    - name: Restore python-setuptools from cache
      id: restore_python-setuptools
      if: ${{ steps.paths.outputs.python-setuptools_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.python-setuptools_INSTALL }}
       key: ${{ steps.paths.outputs.python-setuptools_CACHE_KEY }}-install
    - name: Build python-setuptools
      if: ${{ steps.paths.outputs.python-setuptools_SOURCE && ! steps.restore_python-setuptools.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests python-setuptools
    - name: Save python-setuptools to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.python-setuptools_SOURCE && ! steps.restore_python-setuptools.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.python-setuptools_INSTALL }}
       key: ${{ steps.paths.outputs.python-setuptools_CACHE_KEY }}-install
    - name: Restore zlib from cache
      id: restore_zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.zlib_INSTALL }}
       key: ${{ steps.paths.outputs.zlib_CACHE_KEY }}-install
    - name: Build zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE && ! steps.restore_zlib.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests zlib
    - name: Save zlib to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.zlib_SOURCE && ! steps.restore_zlib.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.zlib_INSTALL }}
       key: ${{ steps.paths.outputs.zlib_CACHE_KEY }}-install
    - name: Restore openssl from cache
      id: restore_openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.openssl_INSTALL }}
       key: ${{ steps.paths.outputs.openssl_CACHE_KEY }}-install
    - name: Build openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE && ! steps.restore_openssl.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests openssl
    - name: Save openssl to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.openssl_SOURCE && ! steps.restore_openssl.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.openssl_INSTALL }}
       key: ${{ steps.paths.outputs.openssl_CACHE_KEY }}-install
    - name: Restore liboqs from cache
      id: restore_liboqs
      if: ${{ steps.paths.outputs.liboqs_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.liboqs_INSTALL }}
       key: ${{ steps.paths.outputs.liboqs_CACHE_KEY }}-install
    - name: Build liboqs
      if: ${{ steps.paths.outputs.liboqs_SOURCE && ! steps.restore_liboqs.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests liboqs
    - name: Save liboqs to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.liboqs_SOURCE && ! steps.restore_liboqs.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.liboqs_INSTALL }}
       key: ${{ steps.paths.outputs.liboqs_CACHE_KEY }}-install
    - name: Restore autoconf from cache
      id: restore_autoconf
      if: ${{ steps.paths.outputs.autoconf_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.autoconf_INSTALL }}
       key: ${{ steps.paths.outputs.autoconf_CACHE_KEY }}-install
    - name: Build autoconf
      if: ${{ steps.paths.outputs.autoconf_SOURCE && ! steps.restore_autoconf.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests autoconf
    - name: Save autoconf to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.autoconf_SOURCE && ! steps.restore_autoconf.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.autoconf_INSTALL }}
       key: ${{ steps.paths.outputs.autoconf_CACHE_KEY }}-install
    - name: Restore automake from cache
      id: restore_automake
      if: ${{ steps.paths.outputs.automake_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.automake_INSTALL }}
       key: ${{ steps.paths.outputs.automake_CACHE_KEY }}-install
    - name: Build automake
      if: ${{ steps.paths.outputs.automake_SOURCE && ! steps.restore_automake.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests automake
    - name: Save automake to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.automake_SOURCE && ! steps.restore_automake.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.automake_INSTALL }}
       key: ${{ steps.paths.outputs.automake_CACHE_KEY }}-install
    - name: Restore libtool from cache
      id: restore_libtool
      if: ${{ steps.paths.outputs.libtool_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libtool_INSTALL }}
       key: ${{ steps.paths.outputs.libtool_CACHE_KEY }}-install
    - name: Build libtool
      if: ${{ steps.paths.outputs.libtool_SOURCE && ! steps.restore_libtool.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests libtool
    - name: Save libtool to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libtool_SOURCE && ! steps.restore_libtool.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libtool_INSTALL }}
       key: ${{ steps.paths.outputs.libtool_CACHE_KEY }}-install
    - name: Restore libsodium from cache
      id: restore_libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libsodium_INSTALL }}
       key: ${{ steps.paths.outputs.libsodium_CACHE_KEY }}-install
    - name: Build libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE && ! steps.restore_libsodium.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests libsodium
    - name: Save libsodium to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libsodium_SOURCE && ! steps.restore_libsodium.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libsodium_INSTALL }}
       key: ${{ steps.paths.outputs.libsodium_CACHE_KEY }}-install
    - name: Restore libiberty from cache
      id: restore_libiberty
      if: ${{ steps.paths.outputs.libiberty_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libiberty_INSTALL }}
       key: ${{ steps.paths.outputs.libiberty_CACHE_KEY }}-install
    - name: Build libiberty
      if: ${{ steps.paths.outputs.libiberty_SOURCE && ! steps.restore_libiberty.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests libiberty
    - name: Save libiberty to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libiberty_SOURCE && ! steps.restore_libiberty.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libiberty_INSTALL }}
       key: ${{ steps.paths.outputs.libiberty_CACHE_KEY }}-install
    - name: Restore libunwind from cache
      id: restore_libunwind
      if: ${{ steps.paths.outputs.libunwind_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libunwind_INSTALL }}
       key: ${{ steps.paths.outputs.libunwind_CACHE_KEY }}-install
    - name: Build libunwind
      if: ${{ steps.paths.outputs.libunwind_SOURCE && ! steps.restore_libunwind.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests libunwind
    - name: Save libunwind to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libunwind_SOURCE && ! steps.restore_libunwind.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libunwind_INSTALL }}
       key: ${{ steps.paths.outputs.libunwind_CACHE_KEY }}-install
    - name: Restore xz from cache
      id: restore_xz
      if: ${{ steps.paths.outputs.xz_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.xz_INSTALL }}
       key: ${{ steps.paths.outputs.xz_CACHE_KEY }}-install
    - name: Build xz
      if: ${{ steps.paths.outputs.xz_SOURCE && ! steps.restore_xz.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests xz
    - name: Save xz to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.xz_SOURCE && ! steps.restore_xz.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.xz_INSTALL }}
       key: ${{ steps.paths.outputs.xz_CACHE_KEY }}-install
    - name: Restore folly from cache
      id: restore_folly
      if: ${{ steps.paths.outputs.folly_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.folly_INSTALL }}
       key: ${{ steps.paths.outputs.folly_CACHE_KEY }}-install
    - name: Build folly
      if: ${{ steps.paths.outputs.folly_SOURCE && ! steps.restore_folly.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests folly
    - name: Save folly to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.folly_SOURCE && ! steps.restore_folly.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.folly_INSTALL }}
       key: ${{ steps.paths.outputs.folly_CACHE_KEY }}-install
    - name: Restore fizz from cache
      id: restore_fizz
      if: ${{ steps.paths.outputs.fizz_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fizz_INSTALL }}
       key: ${{ steps.paths.outputs.fizz_CACHE_KEY }}-install
    - name: Build fizz
      if: ${{ steps.paths.outputs.fizz_SOURCE && ! steps.restore_fizz.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests fizz
    - name: Save fizz to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fizz_SOURCE && ! steps.restore_fizz.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fizz_INSTALL }}
       key: ${{ steps.paths.outputs.fizz_CACHE_KEY }}-install
    - name: Restore mvfst from cache
      id: restore_mvfst
      if: ${{ steps.paths.outputs.mvfst_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.mvfst_INSTALL }}
       key: ${{ steps.paths.outputs.mvfst_CACHE_KEY }}-install
    - name: Build mvfst
      if: ${{ steps.paths.outputs.mvfst_SOURCE && ! steps.restore_mvfst.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests mvfst
    - name: Save mvfst to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.mvfst_SOURCE && ! steps.restore_mvfst.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.mvfst_INSTALL }}
       key: ${{ steps.paths.outputs.mvfst_CACHE_KEY }}-install
    - name: Restore wangle from cache
      id: restore_wangle
      if: ${{ steps.paths.outputs.wangle_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.wangle_INSTALL }}
       key: ${{ steps.paths.outputs.wangle_CACHE_KEY }}-install
    - name: Build wangle
      if: ${{ steps.paths.outputs.wangle_SOURCE && ! steps.restore_wangle.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests wangle
    - name: Save wangle to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.wangle_SOURCE && ! steps.restore_wangle.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.wangle_INSTALL }}
       key: ${{ steps.paths.outputs.wangle_CACHE_KEY }}-install
    - name: Restore fbthrift from cache
      id: restore_fbthrift
      if: ${{ steps.paths.outputs.fbthrift_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fbthrift_INSTALL }}
       key: ${{ steps.paths.outputs.fbthrift_CACHE_KEY }}-install
    - name: Build fbthrift
      if: ${{ steps.paths.outputs.fbthrift_SOURCE && ! steps.restore_fbthrift.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests fbthrift
    - name: Save fbthrift to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fbthrift_SOURCE && ! steps.restore_fbthrift.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fbthrift_INSTALL }}
       key: ${{ steps.paths.outputs.fbthrift_CACHE_KEY }}-install
    - name: Restore fb303 from cache
      id: restore_fb303
      if: ${{ steps.paths.outputs.fb303_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fb303_INSTALL }}
       key: ${{ steps.paths.outputs.fb303_CACHE_KEY }}-install
    - name: Build fb303
      if: ${{ steps.paths.outputs.fb303_SOURCE && ! steps.restore_fb303.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests fb303
    - name: Save fb303 to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fb303_SOURCE && ! steps.restore_fb303.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fb303_INSTALL }}
       key: ${{ steps.paths.outputs.fb303_CACHE_KEY }}-install
    - name: Restore edencommon from cache
      id: restore_edencommon
      if: ${{ steps.paths.outputs.edencommon_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.edencommon_INSTALL }}
       key: ${{ steps.paths.outputs.edencommon_CACHE_KEY }}-install
    - name: Build edencommon
      if: ${{ steps.paths.outputs.edencommon_SOURCE && ! steps.restore_edencommon.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests edencommon
    - name: Save edencommon to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.edencommon_SOURCE && ! steps.restore_edencommon.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.edencommon_INSTALL }}
       key: ${{ steps.paths.outputs.edencommon_CACHE_KEY }}-install
    - name: Build watchman
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --src-dir=. watchman  --project-install-prefix watchman:/usr/local
    - name: Copy artifacts
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fixup-dyn-deps --strip --src-dir=. watchman _artifacts/linux  --project-install-prefix watchman:/usr/local --final-install-prefix /usr/local
    - uses: actions/upload-artifact@v4
      with:
        name: watchman
        path: _artifacts
    - name: Test watchman
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages test --src-dir=. watchman  --project-install-prefix watchman:/usr/local
",704,1,2,"push, pull_request",73
facebook/watchman,getdeps_mac.yml,"# This file was @generated by getdeps.py

name: mac

on:
  push:
    branches:
    - main
  pull_request:
    branches:
    - main

permissions:
  contents: read  #  to fetch code (actions/checkout)

jobs:
  build:
    runs-on: macOS-latest
    steps:
    - uses: actions/checkout@v4
    - name: Install system deps
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages install-system-deps --recursive watchman
    - id: paths
      name: Query paths
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages query-paths --recursive --src-dir=. watchman  >> ""$GITHUB_OUTPUT""
    - name: Install Rust Stable
      uses: dtolnay/rust-toolchain@stable
    - name: Fetch boost
      if: ${{ steps.paths.outputs.boost_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests boost
    - name: Fetch ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests ninja
    - name: Fetch cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests cmake
    - name: Fetch cpptoml
      if: ${{ steps.paths.outputs.cpptoml_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests cpptoml
    - name: Fetch fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests fmt
    - name: Fetch gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests gflags
    - name: Fetch glog
      if: ${{ steps.paths.outputs.glog_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests glog
    - name: Fetch googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests googletest
    - name: Fetch xxhash
      if: ${{ steps.paths.outputs.xxhash_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests xxhash
    - name: Fetch zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests zstd
    - name: Fetch double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests double-conversion
    - name: Fetch fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests fast_float
    - name: Fetch libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libdwarf
    - name: Fetch lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests lz4
    - name: Fetch openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests openssl
    - name: Fetch snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests snappy
    - name: Fetch pcre2
      if: ${{ steps.paths.outputs.pcre2_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests pcre2
    - name: Fetch python-setuptools
      if: ${{ steps.paths.outputs.python-setuptools_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests python-setuptools
    - name: Fetch libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libevent
    - name: Fetch liboqs
      if: ${{ steps.paths.outputs.liboqs_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests liboqs
    - name: Fetch zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests zlib
    - name: Fetch autoconf
      if: ${{ steps.paths.outputs.autoconf_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests autoconf
    - name: Fetch automake
      if: ${{ steps.paths.outputs.automake_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests automake
    - name: Fetch libtool
      if: ${{ steps.paths.outputs.libtool_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libtool
    - name: Fetch libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libsodium
    - name: Fetch xz
      if: ${{ steps.paths.outputs.xz_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests xz
    - name: Fetch folly
      if: ${{ steps.paths.outputs.folly_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests folly
    - name: Fetch fizz
      if: ${{ steps.paths.outputs.fizz_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests fizz
    - name: Fetch mvfst
      if: ${{ steps.paths.outputs.mvfst_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests mvfst
    - name: Fetch wangle
      if: ${{ steps.paths.outputs.wangle_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests wangle
    - name: Fetch fbthrift
      if: ${{ steps.paths.outputs.fbthrift_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests fbthrift
    - name: Fetch fb303
      if: ${{ steps.paths.outputs.fb303_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests fb303
    - name: Fetch edencommon
      if: ${{ steps.paths.outputs.edencommon_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests edencommon
    - name: Restore boost from cache
      id: restore_boost
      if: ${{ steps.paths.outputs.boost_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.boost_INSTALL }}
       key: ${{ steps.paths.outputs.boost_CACHE_KEY }}-install
    - name: Build boost
      if: ${{ steps.paths.outputs.boost_SOURCE && ! steps.restore_boost.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests boost
    - name: Save boost to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.boost_SOURCE && ! steps.restore_boost.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.boost_INSTALL }}
       key: ${{ steps.paths.outputs.boost_CACHE_KEY }}-install
    - name: Restore ninja from cache
      id: restore_ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.ninja_INSTALL }}
       key: ${{ steps.paths.outputs.ninja_CACHE_KEY }}-install
    - name: Build ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE && ! steps.restore_ninja.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests ninja
    - name: Save ninja to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.ninja_SOURCE && ! steps.restore_ninja.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.ninja_INSTALL }}
       key: ${{ steps.paths.outputs.ninja_CACHE_KEY }}-install
    - name: Restore cmake from cache
      id: restore_cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.cmake_INSTALL }}
       key: ${{ steps.paths.outputs.cmake_CACHE_KEY }}-install
    - name: Build cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE && ! steps.restore_cmake.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests cmake
    - name: Save cmake to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.cmake_SOURCE && ! steps.restore_cmake.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.cmake_INSTALL }}
       key: ${{ steps.paths.outputs.cmake_CACHE_KEY }}-install
    - name: Restore cpptoml from cache
      id: restore_cpptoml
      if: ${{ steps.paths.outputs.cpptoml_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.cpptoml_INSTALL }}
       key: ${{ steps.paths.outputs.cpptoml_CACHE_KEY }}-install
    - name: Build cpptoml
      if: ${{ steps.paths.outputs.cpptoml_SOURCE && ! steps.restore_cpptoml.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests cpptoml
    - name: Save cpptoml to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.cpptoml_SOURCE && ! steps.restore_cpptoml.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.cpptoml_INSTALL }}
       key: ${{ steps.paths.outputs.cpptoml_CACHE_KEY }}-install
    - name: Restore fmt from cache
      id: restore_fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fmt_INSTALL }}
       key: ${{ steps.paths.outputs.fmt_CACHE_KEY }}-install
    - name: Build fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE && ! steps.restore_fmt.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests fmt
    - name: Save fmt to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fmt_SOURCE && ! steps.restore_fmt.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fmt_INSTALL }}
       key: ${{ steps.paths.outputs.fmt_CACHE_KEY }}-install
    - name: Restore gflags from cache
      id: restore_gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.gflags_INSTALL }}
       key: ${{ steps.paths.outputs.gflags_CACHE_KEY }}-install
    - name: Build gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE && ! steps.restore_gflags.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests gflags
    - name: Save gflags to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.gflags_SOURCE && ! steps.restore_gflags.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.gflags_INSTALL }}
       key: ${{ steps.paths.outputs.gflags_CACHE_KEY }}-install
    - name: Restore glog from cache
      id: restore_glog
      if: ${{ steps.paths.outputs.glog_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.glog_INSTALL }}
       key: ${{ steps.paths.outputs.glog_CACHE_KEY }}-install
    - name: Build glog
      if: ${{ steps.paths.outputs.glog_SOURCE && ! steps.restore_glog.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests glog
    - name: Save glog to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.glog_SOURCE && ! steps.restore_glog.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.glog_INSTALL }}
       key: ${{ steps.paths.outputs.glog_CACHE_KEY }}-install
    - name: Restore googletest from cache
      id: restore_googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.googletest_INSTALL }}
       key: ${{ steps.paths.outputs.googletest_CACHE_KEY }}-install
    - name: Build googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE && ! steps.restore_googletest.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests googletest
    - name: Save googletest to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.googletest_SOURCE && ! steps.restore_googletest.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.googletest_INSTALL }}
       key: ${{ steps.paths.outputs.googletest_CACHE_KEY }}-install
    - name: Restore xxhash from cache
      id: restore_xxhash
      if: ${{ steps.paths.outputs.xxhash_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.xxhash_INSTALL }}
       key: ${{ steps.paths.outputs.xxhash_CACHE_KEY }}-install
    - name: Build xxhash
      if: ${{ steps.paths.outputs.xxhash_SOURCE && ! steps.restore_xxhash.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests xxhash
    - name: Save xxhash to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.xxhash_SOURCE && ! steps.restore_xxhash.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.xxhash_INSTALL }}
       key: ${{ steps.paths.outputs.xxhash_CACHE_KEY }}-install
    - name: Restore zstd from cache
      id: restore_zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.zstd_INSTALL }}
       key: ${{ steps.paths.outputs.zstd_CACHE_KEY }}-install
    - name: Build zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE && ! steps.restore_zstd.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests zstd
    - name: Save zstd to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.zstd_SOURCE && ! steps.restore_zstd.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.zstd_INSTALL }}
       key: ${{ steps.paths.outputs.zstd_CACHE_KEY }}-install
    - name: Restore double-conversion from cache
      id: restore_double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.double-conversion_INSTALL }}
       key: ${{ steps.paths.outputs.double-conversion_CACHE_KEY }}-install
    - name: Build double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE && ! steps.restore_double-conversion.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests double-conversion
    - name: Save double-conversion to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.double-conversion_SOURCE && ! steps.restore_double-conversion.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.double-conversion_INSTALL }}
       key: ${{ steps.paths.outputs.double-conversion_CACHE_KEY }}-install
    - name: Restore fast_float from cache
      id: restore_fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fast_float_INSTALL }}
       key: ${{ steps.paths.outputs.fast_float_CACHE_KEY }}-install
    - name: Build fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE && ! steps.restore_fast_float.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests fast_float
    - name: Save fast_float to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fast_float_SOURCE && ! steps.restore_fast_float.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fast_float_INSTALL }}
       key: ${{ steps.paths.outputs.fast_float_CACHE_KEY }}-install
    - name: Restore libdwarf from cache
      id: restore_libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libdwarf_INSTALL }}
       key: ${{ steps.paths.outputs.libdwarf_CACHE_KEY }}-install
    - name: Build libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE && ! steps.restore_libdwarf.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests libdwarf
    - name: Save libdwarf to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libdwarf_SOURCE && ! steps.restore_libdwarf.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libdwarf_INSTALL }}
       key: ${{ steps.paths.outputs.libdwarf_CACHE_KEY }}-install
    - name: Restore lz4 from cache
      id: restore_lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.lz4_INSTALL }}
       key: ${{ steps.paths.outputs.lz4_CACHE_KEY }}-install
    - name: Build lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE && ! steps.restore_lz4.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests lz4
    - name: Save lz4 to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.lz4_SOURCE && ! steps.restore_lz4.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.lz4_INSTALL }}
       key: ${{ steps.paths.outputs.lz4_CACHE_KEY }}-install
    - name: Restore openssl from cache
      id: restore_openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.openssl_INSTALL }}
       key: ${{ steps.paths.outputs.openssl_CACHE_KEY }}-install
    - name: Build openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE && ! steps.restore_openssl.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests openssl
    - name: Save openssl to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.openssl_SOURCE && ! steps.restore_openssl.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.openssl_INSTALL }}
       key: ${{ steps.paths.outputs.openssl_CACHE_KEY }}-install
    - name: Restore snappy from cache
      id: restore_snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.snappy_INSTALL }}
       key: ${{ steps.paths.outputs.snappy_CACHE_KEY }}-install
    - name: Build snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE && ! steps.restore_snappy.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests snappy
    - name: Save snappy to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.snappy_SOURCE && ! steps.restore_snappy.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.snappy_INSTALL }}
       key: ${{ steps.paths.outputs.snappy_CACHE_KEY }}-install
    - name: Restore pcre2 from cache
      id: restore_pcre2
      if: ${{ steps.paths.outputs.pcre2_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.pcre2_INSTALL }}
       key: ${{ steps.paths.outputs.pcre2_CACHE_KEY }}-install
    - name: Build pcre2
      if: ${{ steps.paths.outputs.pcre2_SOURCE && ! steps.restore_pcre2.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests pcre2
    - name: Save pcre2 to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.pcre2_SOURCE && ! steps.restore_pcre2.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.pcre2_INSTALL }}
       key: ${{ steps.paths.outputs.pcre2_CACHE_KEY }}-install
    - name: Restore python-setuptools from cache
      id: restore_python-setuptools
      if: ${{ steps.paths.outputs.python-setuptools_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.python-setuptools_INSTALL }}
       key: ${{ steps.paths.outputs.python-setuptools_CACHE_KEY }}-install
    - name: Build python-setuptools
      if: ${{ steps.paths.outputs.python-setuptools_SOURCE && ! steps.restore_python-setuptools.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests python-setuptools
    - name: Save python-setuptools to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.python-setuptools_SOURCE && ! steps.restore_python-setuptools.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.python-setuptools_INSTALL }}
       key: ${{ steps.paths.outputs.python-setuptools_CACHE_KEY }}-install
    - name: Restore libevent from cache
      id: restore_libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libevent_INSTALL }}
       key: ${{ steps.paths.outputs.libevent_CACHE_KEY }}-install
    - name: Build libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE && ! steps.restore_libevent.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests libevent
    - name: Save libevent to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libevent_SOURCE && ! steps.restore_libevent.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libevent_INSTALL }}
       key: ${{ steps.paths.outputs.libevent_CACHE_KEY }}-install
    - name: Restore liboqs from cache
      id: restore_liboqs
      if: ${{ steps.paths.outputs.liboqs_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.liboqs_INSTALL }}
       key: ${{ steps.paths.outputs.liboqs_CACHE_KEY }}-install
    - name: Build liboqs
      if: ${{ steps.paths.outputs.liboqs_SOURCE && ! steps.restore_liboqs.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests liboqs
    - name: Save liboqs to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.liboqs_SOURCE && ! steps.restore_liboqs.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.liboqs_INSTALL }}
       key: ${{ steps.paths.outputs.liboqs_CACHE_KEY }}-install
    - name: Restore zlib from cache
      id: restore_zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.zlib_INSTALL }}
       key: ${{ steps.paths.outputs.zlib_CACHE_KEY }}-install
    - name: Build zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE && ! steps.restore_zlib.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests zlib
    - name: Save zlib to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.zlib_SOURCE && ! steps.restore_zlib.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.zlib_INSTALL }}
       key: ${{ steps.paths.outputs.zlib_CACHE_KEY }}-install
    - name: Restore autoconf from cache
      id: restore_autoconf
      if: ${{ steps.paths.outputs.autoconf_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.autoconf_INSTALL }}
       key: ${{ steps.paths.outputs.autoconf_CACHE_KEY }}-install
    - name: Build autoconf
      if: ${{ steps.paths.outputs.autoconf_SOURCE && ! steps.restore_autoconf.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests autoconf
    - name: Save autoconf to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.autoconf_SOURCE && ! steps.restore_autoconf.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.autoconf_INSTALL }}
       key: ${{ steps.paths.outputs.autoconf_CACHE_KEY }}-install
    - name: Restore automake from cache
      id: restore_automake
      if: ${{ steps.paths.outputs.automake_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.automake_INSTALL }}
       key: ${{ steps.paths.outputs.automake_CACHE_KEY }}-install
    - name: Build automake
      if: ${{ steps.paths.outputs.automake_SOURCE && ! steps.restore_automake.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests automake
    - name: Save automake to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.automake_SOURCE && ! steps.restore_automake.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.automake_INSTALL }}
       key: ${{ steps.paths.outputs.automake_CACHE_KEY }}-install
    - name: Restore libtool from cache
      id: restore_libtool
      if: ${{ steps.paths.outputs.libtool_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libtool_INSTALL }}
       key: ${{ steps.paths.outputs.libtool_CACHE_KEY }}-install
    - name: Build libtool
      if: ${{ steps.paths.outputs.libtool_SOURCE && ! steps.restore_libtool.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests libtool
    - name: Save libtool to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libtool_SOURCE && ! steps.restore_libtool.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libtool_INSTALL }}
       key: ${{ steps.paths.outputs.libtool_CACHE_KEY }}-install
    - name: Restore libsodium from cache
      id: restore_libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libsodium_INSTALL }}
       key: ${{ steps.paths.outputs.libsodium_CACHE_KEY }}-install
    - name: Build libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE && ! steps.restore_libsodium.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests libsodium
    - name: Save libsodium to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libsodium_SOURCE && ! steps.restore_libsodium.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libsodium_INSTALL }}
       key: ${{ steps.paths.outputs.libsodium_CACHE_KEY }}-install
    - name: Restore xz from cache
      id: restore_xz
      if: ${{ steps.paths.outputs.xz_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.xz_INSTALL }}
       key: ${{ steps.paths.outputs.xz_CACHE_KEY }}-install
    - name: Build xz
      if: ${{ steps.paths.outputs.xz_SOURCE && ! steps.restore_xz.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests xz
    - name: Save xz to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.xz_SOURCE && ! steps.restore_xz.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.xz_INSTALL }}
       key: ${{ steps.paths.outputs.xz_CACHE_KEY }}-install
    - name: Restore folly from cache
      id: restore_folly
      if: ${{ steps.paths.outputs.folly_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.folly_INSTALL }}
       key: ${{ steps.paths.outputs.folly_CACHE_KEY }}-install
    - name: Build folly
      if: ${{ steps.paths.outputs.folly_SOURCE && ! steps.restore_folly.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests folly
    - name: Save folly to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.folly_SOURCE && ! steps.restore_folly.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.folly_INSTALL }}
       key: ${{ steps.paths.outputs.folly_CACHE_KEY }}-install
    - name: Restore fizz from cache
      id: restore_fizz
      if: ${{ steps.paths.outputs.fizz_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fizz_INSTALL }}
       key: ${{ steps.paths.outputs.fizz_CACHE_KEY }}-install
    - name: Build fizz
      if: ${{ steps.paths.outputs.fizz_SOURCE && ! steps.restore_fizz.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests fizz
    - name: Save fizz to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fizz_SOURCE && ! steps.restore_fizz.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fizz_INSTALL }}
       key: ${{ steps.paths.outputs.fizz_CACHE_KEY }}-install
    - name: Restore mvfst from cache
      id: restore_mvfst
      if: ${{ steps.paths.outputs.mvfst_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.mvfst_INSTALL }}
       key: ${{ steps.paths.outputs.mvfst_CACHE_KEY }}-install
    - name: Build mvfst
      if: ${{ steps.paths.outputs.mvfst_SOURCE && ! steps.restore_mvfst.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests mvfst
    - name: Save mvfst to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.mvfst_SOURCE && ! steps.restore_mvfst.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.mvfst_INSTALL }}
       key: ${{ steps.paths.outputs.mvfst_CACHE_KEY }}-install
    - name: Restore wangle from cache
      id: restore_wangle
      if: ${{ steps.paths.outputs.wangle_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.wangle_INSTALL }}
       key: ${{ steps.paths.outputs.wangle_CACHE_KEY }}-install
    - name: Build wangle
      if: ${{ steps.paths.outputs.wangle_SOURCE && ! steps.restore_wangle.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests wangle
    - name: Save wangle to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.wangle_SOURCE && ! steps.restore_wangle.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.wangle_INSTALL }}
       key: ${{ steps.paths.outputs.wangle_CACHE_KEY }}-install
    - name: Restore fbthrift from cache
      id: restore_fbthrift
      if: ${{ steps.paths.outputs.fbthrift_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fbthrift_INSTALL }}
       key: ${{ steps.paths.outputs.fbthrift_CACHE_KEY }}-install
    - name: Build fbthrift
      if: ${{ steps.paths.outputs.fbthrift_SOURCE && ! steps.restore_fbthrift.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests fbthrift
    - name: Save fbthrift to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fbthrift_SOURCE && ! steps.restore_fbthrift.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fbthrift_INSTALL }}
       key: ${{ steps.paths.outputs.fbthrift_CACHE_KEY }}-install
    - name: Restore fb303 from cache
      id: restore_fb303
      if: ${{ steps.paths.outputs.fb303_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fb303_INSTALL }}
       key: ${{ steps.paths.outputs.fb303_CACHE_KEY }}-install
    - name: Build fb303
      if: ${{ steps.paths.outputs.fb303_SOURCE && ! steps.restore_fb303.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests fb303
    - name: Save fb303 to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fb303_SOURCE && ! steps.restore_fb303.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fb303_INSTALL }}
       key: ${{ steps.paths.outputs.fb303_CACHE_KEY }}-install
    - name: Restore edencommon from cache
      id: restore_edencommon
      if: ${{ steps.paths.outputs.edencommon_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.edencommon_INSTALL }}
       key: ${{ steps.paths.outputs.edencommon_CACHE_KEY }}-install
    - name: Build edencommon
      if: ${{ steps.paths.outputs.edencommon_SOURCE && ! steps.restore_edencommon.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --no-tests edencommon
    - name: Save edencommon to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.edencommon_SOURCE && ! steps.restore_edencommon.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.edencommon_INSTALL }}
       key: ${{ steps.paths.outputs.edencommon_CACHE_KEY }}-install
    - name: Build watchman
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --src-dir=. watchman  --project-install-prefix watchman:/usr/local
    - name: Copy artifacts
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fixup-dyn-deps --src-dir=. watchman _artifacts/mac  --project-install-prefix watchman:/usr/local --final-install-prefix /usr/local
    - uses: actions/upload-artifact@v4
      with:
        name: watchman
        path: _artifacts
    - name: Test watchman
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages test --src-dir=. watchman  --project-install-prefix watchman:/usr/local
",664,1,2,"push, pull_request",69
facebook/watchman,getdeps_windows.yml,"# This file was @generated by getdeps.py

name: windows

on:
  push:
    branches:
    - main
  pull_request:
    branches:
    - main

permissions:
  contents: read  #  to fetch code (actions/checkout)

jobs:
  build:
    runs-on: windows-2019
    steps:
    - name: Export boost environment
      run: ""echo BOOST_ROOT=%BOOST_ROOT_1_83_0% >> %GITHUB_ENV%""
      shell: cmd
    - name: Fix Git config
      run: >
        git config --system core.longpaths true &&
        git config --system core.autocrlf false &&
        git config --system core.symlinks true
      shell: cmd
    - uses: actions/checkout@v4
    - id: paths
      name: Query paths
      run: python build/fbcode_builder/getdeps.py query-paths --recursive --src-dir=. watchman  >> $env:GITHUB_OUTPUT
      shell: pwsh
    - name: Install Rust Stable
      uses: dtolnay/rust-toolchain@stable
    - name: Fetch boost
      if: ${{ steps.paths.outputs.boost_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests boost
    - name: Fetch ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests ninja
    - name: Fetch cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests cmake
    - name: Fetch cpptoml
      if: ${{ steps.paths.outputs.cpptoml_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests cpptoml
    - name: Fetch fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests fmt
    - name: Fetch gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests gflags
    - name: Fetch glog
      if: ${{ steps.paths.outputs.glog_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests glog
    - name: Fetch googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests googletest
    - name: Fetch libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests libsodium
    - name: Fetch xxhash
      if: ${{ steps.paths.outputs.xxhash_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests xxhash
    - name: Fetch zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests zstd
    - name: Fetch double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests double-conversion
    - name: Fetch fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests fast_float
    - name: Fetch libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests libdwarf
    - name: Fetch lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests lz4
    - name: Fetch snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests snappy
    - name: Fetch zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests zlib
    - name: Fetch pcre2
      if: ${{ steps.paths.outputs.pcre2_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests pcre2
    - name: Fetch python-setuptools
      if: ${{ steps.paths.outputs.python-setuptools_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests python-setuptools
    - name: Fetch jom
      if: ${{ steps.paths.outputs.jom_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests jom
    - name: Fetch perl
      if: ${{ steps.paths.outputs.perl_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests perl
    - name: Fetch openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests openssl
    - name: Fetch libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests libevent
    - name: Fetch folly
      if: ${{ steps.paths.outputs.folly_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests folly
    - name: Fetch liboqs
      if: ${{ steps.paths.outputs.liboqs_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests liboqs
    - name: Fetch fizz
      if: ${{ steps.paths.outputs.fizz_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests fizz
    - name: Fetch mvfst
      if: ${{ steps.paths.outputs.mvfst_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests mvfst
    - name: Fetch wangle
      if: ${{ steps.paths.outputs.wangle_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests wangle
    - name: Fetch fbthrift
      if: ${{ steps.paths.outputs.fbthrift_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests fbthrift
    - name: Fetch fb303
      if: ${{ steps.paths.outputs.fb303_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests fb303
    - name: Fetch edencommon
      if: ${{ steps.paths.outputs.edencommon_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests edencommon
    - name: Restore boost from cache
      id: restore_boost
      if: ${{ steps.paths.outputs.boost_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.boost_INSTALL }}
       key: ${{ steps.paths.outputs.boost_CACHE_KEY }}-install
    - name: Build boost
      if: ${{ steps.paths.outputs.boost_SOURCE && ! steps.restore_boost.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests boost
    - name: Save boost to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.boost_SOURCE && ! steps.restore_boost.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.boost_INSTALL }}
       key: ${{ steps.paths.outputs.boost_CACHE_KEY }}-install
    - name: Restore ninja from cache
      id: restore_ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.ninja_INSTALL }}
       key: ${{ steps.paths.outputs.ninja_CACHE_KEY }}-install
    - name: Build ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE && ! steps.restore_ninja.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests ninja
    - name: Save ninja to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.ninja_SOURCE && ! steps.restore_ninja.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.ninja_INSTALL }}
       key: ${{ steps.paths.outputs.ninja_CACHE_KEY }}-install
    - name: Restore cmake from cache
      id: restore_cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.cmake_INSTALL }}
       key: ${{ steps.paths.outputs.cmake_CACHE_KEY }}-install
    - name: Build cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE && ! steps.restore_cmake.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests cmake
    - name: Save cmake to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.cmake_SOURCE && ! steps.restore_cmake.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.cmake_INSTALL }}
       key: ${{ steps.paths.outputs.cmake_CACHE_KEY }}-install
    - name: Restore cpptoml from cache
      id: restore_cpptoml
      if: ${{ steps.paths.outputs.cpptoml_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.cpptoml_INSTALL }}
       key: ${{ steps.paths.outputs.cpptoml_CACHE_KEY }}-install
    - name: Build cpptoml
      if: ${{ steps.paths.outputs.cpptoml_SOURCE && ! steps.restore_cpptoml.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests cpptoml
    - name: Save cpptoml to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.cpptoml_SOURCE && ! steps.restore_cpptoml.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.cpptoml_INSTALL }}
       key: ${{ steps.paths.outputs.cpptoml_CACHE_KEY }}-install
    - name: Restore fmt from cache
      id: restore_fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fmt_INSTALL }}
       key: ${{ steps.paths.outputs.fmt_CACHE_KEY }}-install
    - name: Build fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE && ! steps.restore_fmt.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests fmt
    - name: Save fmt to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fmt_SOURCE && ! steps.restore_fmt.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fmt_INSTALL }}
       key: ${{ steps.paths.outputs.fmt_CACHE_KEY }}-install
    - name: Restore gflags from cache
      id: restore_gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.gflags_INSTALL }}
       key: ${{ steps.paths.outputs.gflags_CACHE_KEY }}-install
    - name: Build gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE && ! steps.restore_gflags.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests gflags
    - name: Save gflags to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.gflags_SOURCE && ! steps.restore_gflags.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.gflags_INSTALL }}
       key: ${{ steps.paths.outputs.gflags_CACHE_KEY }}-install
    - name: Restore glog from cache
      id: restore_glog
      if: ${{ steps.paths.outputs.glog_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.glog_INSTALL }}
       key: ${{ steps.paths.outputs.glog_CACHE_KEY }}-install
    - name: Build glog
      if: ${{ steps.paths.outputs.glog_SOURCE && ! steps.restore_glog.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests glog
    - name: Save glog to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.glog_SOURCE && ! steps.restore_glog.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.glog_INSTALL }}
       key: ${{ steps.paths.outputs.glog_CACHE_KEY }}-install
    - name: Restore googletest from cache
      id: restore_googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.googletest_INSTALL }}
       key: ${{ steps.paths.outputs.googletest_CACHE_KEY }}-install
    - name: Build googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE && ! steps.restore_googletest.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests googletest
    - name: Save googletest to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.googletest_SOURCE && ! steps.restore_googletest.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.googletest_INSTALL }}
       key: ${{ steps.paths.outputs.googletest_CACHE_KEY }}-install
    - name: Restore libsodium from cache
      id: restore_libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libsodium_INSTALL }}
       key: ${{ steps.paths.outputs.libsodium_CACHE_KEY }}-install
    - name: Build libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE && ! steps.restore_libsodium.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests libsodium
    - name: Save libsodium to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libsodium_SOURCE && ! steps.restore_libsodium.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libsodium_INSTALL }}
       key: ${{ steps.paths.outputs.libsodium_CACHE_KEY }}-install
    - name: Restore xxhash from cache
      id: restore_xxhash
      if: ${{ steps.paths.outputs.xxhash_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.xxhash_INSTALL }}
       key: ${{ steps.paths.outputs.xxhash_CACHE_KEY }}-install
    - name: Build xxhash
      if: ${{ steps.paths.outputs.xxhash_SOURCE && ! steps.restore_xxhash.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests xxhash
    - name: Save xxhash to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.xxhash_SOURCE && ! steps.restore_xxhash.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.xxhash_INSTALL }}
       key: ${{ steps.paths.outputs.xxhash_CACHE_KEY }}-install
    - name: Restore zstd from cache
      id: restore_zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.zstd_INSTALL }}
       key: ${{ steps.paths.outputs.zstd_CACHE_KEY }}-install
    - name: Build zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE && ! steps.restore_zstd.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests zstd
    - name: Save zstd to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.zstd_SOURCE && ! steps.restore_zstd.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.zstd_INSTALL }}
       key: ${{ steps.paths.outputs.zstd_CACHE_KEY }}-install
    - name: Restore double-conversion from cache
      id: restore_double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.double-conversion_INSTALL }}
       key: ${{ steps.paths.outputs.double-conversion_CACHE_KEY }}-install
    - name: Build double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE && ! steps.restore_double-conversion.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests double-conversion
    - name: Save double-conversion to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.double-conversion_SOURCE && ! steps.restore_double-conversion.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.double-conversion_INSTALL }}
       key: ${{ steps.paths.outputs.double-conversion_CACHE_KEY }}-install
    - name: Restore fast_float from cache
      id: restore_fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fast_float_INSTALL }}
       key: ${{ steps.paths.outputs.fast_float_CACHE_KEY }}-install
    - name: Build fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE && ! steps.restore_fast_float.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests fast_float
    - name: Save fast_float to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fast_float_SOURCE && ! steps.restore_fast_float.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fast_float_INSTALL }}
       key: ${{ steps.paths.outputs.fast_float_CACHE_KEY }}-install
    - name: Restore libdwarf from cache
      id: restore_libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libdwarf_INSTALL }}
       key: ${{ steps.paths.outputs.libdwarf_CACHE_KEY }}-install
    - name: Build libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE && ! steps.restore_libdwarf.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests libdwarf
    - name: Save libdwarf to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libdwarf_SOURCE && ! steps.restore_libdwarf.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libdwarf_INSTALL }}
       key: ${{ steps.paths.outputs.libdwarf_CACHE_KEY }}-install
    - name: Restore lz4 from cache
      id: restore_lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.lz4_INSTALL }}
       key: ${{ steps.paths.outputs.lz4_CACHE_KEY }}-install
    - name: Build lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE && ! steps.restore_lz4.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests lz4
    - name: Save lz4 to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.lz4_SOURCE && ! steps.restore_lz4.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.lz4_INSTALL }}
       key: ${{ steps.paths.outputs.lz4_CACHE_KEY }}-install
    - name: Restore snappy from cache
      id: restore_snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.snappy_INSTALL }}
       key: ${{ steps.paths.outputs.snappy_CACHE_KEY }}-install
    - name: Build snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE && ! steps.restore_snappy.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests snappy
    - name: Save snappy to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.snappy_SOURCE && ! steps.restore_snappy.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.snappy_INSTALL }}
       key: ${{ steps.paths.outputs.snappy_CACHE_KEY }}-install
    - name: Restore zlib from cache
      id: restore_zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.zlib_INSTALL }}
       key: ${{ steps.paths.outputs.zlib_CACHE_KEY }}-install
    - name: Build zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE && ! steps.restore_zlib.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests zlib
    - name: Save zlib to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.zlib_SOURCE && ! steps.restore_zlib.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.zlib_INSTALL }}
       key: ${{ steps.paths.outputs.zlib_CACHE_KEY }}-install
    - name: Restore pcre2 from cache
      id: restore_pcre2
      if: ${{ steps.paths.outputs.pcre2_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.pcre2_INSTALL }}
       key: ${{ steps.paths.outputs.pcre2_CACHE_KEY }}-install
    - name: Build pcre2
      if: ${{ steps.paths.outputs.pcre2_SOURCE && ! steps.restore_pcre2.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests pcre2
    - name: Save pcre2 to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.pcre2_SOURCE && ! steps.restore_pcre2.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.pcre2_INSTALL }}
       key: ${{ steps.paths.outputs.pcre2_CACHE_KEY }}-install
    - name: Restore python-setuptools from cache
      id: restore_python-setuptools
      if: ${{ steps.paths.outputs.python-setuptools_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.python-setuptools_INSTALL }}
       key: ${{ steps.paths.outputs.python-setuptools_CACHE_KEY }}-install
    - name: Build python-setuptools
      if: ${{ steps.paths.outputs.python-setuptools_SOURCE && ! steps.restore_python-setuptools.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests python-setuptools
    - name: Save python-setuptools to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.python-setuptools_SOURCE && ! steps.restore_python-setuptools.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.python-setuptools_INSTALL }}
       key: ${{ steps.paths.outputs.python-setuptools_CACHE_KEY }}-install
    - name: Restore jom from cache
      id: restore_jom
      if: ${{ steps.paths.outputs.jom_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.jom_INSTALL }}
       key: ${{ steps.paths.outputs.jom_CACHE_KEY }}-install
    - name: Build jom
      if: ${{ steps.paths.outputs.jom_SOURCE && ! steps.restore_jom.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests jom
    - name: Save jom to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.jom_SOURCE && ! steps.restore_jom.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.jom_INSTALL }}
       key: ${{ steps.paths.outputs.jom_CACHE_KEY }}-install
    - name: Restore perl from cache
      id: restore_perl
      if: ${{ steps.paths.outputs.perl_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.perl_INSTALL }}
       key: ${{ steps.paths.outputs.perl_CACHE_KEY }}-install
    - name: Build perl
      if: ${{ steps.paths.outputs.perl_SOURCE && ! steps.restore_perl.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests perl
    - name: Save perl to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.perl_SOURCE && ! steps.restore_perl.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.perl_INSTALL }}
       key: ${{ steps.paths.outputs.perl_CACHE_KEY }}-install
    - name: Restore openssl from cache
      id: restore_openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.openssl_INSTALL }}
       key: ${{ steps.paths.outputs.openssl_CACHE_KEY }}-install
    - name: Build openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE && ! steps.restore_openssl.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests openssl
    - name: Save openssl to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.openssl_SOURCE && ! steps.restore_openssl.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.openssl_INSTALL }}
       key: ${{ steps.paths.outputs.openssl_CACHE_KEY }}-install
    - name: Restore libevent from cache
      id: restore_libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libevent_INSTALL }}
       key: ${{ steps.paths.outputs.libevent_CACHE_KEY }}-install
    - name: Build libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE && ! steps.restore_libevent.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests libevent
    - name: Save libevent to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libevent_SOURCE && ! steps.restore_libevent.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libevent_INSTALL }}
       key: ${{ steps.paths.outputs.libevent_CACHE_KEY }}-install
    - name: Restore folly from cache
      id: restore_folly
      if: ${{ steps.paths.outputs.folly_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.folly_INSTALL }}
       key: ${{ steps.paths.outputs.folly_CACHE_KEY }}-install
    - name: Build folly
      if: ${{ steps.paths.outputs.folly_SOURCE && ! steps.restore_folly.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests folly
    - name: Save folly to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.folly_SOURCE && ! steps.restore_folly.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.folly_INSTALL }}
       key: ${{ steps.paths.outputs.folly_CACHE_KEY }}-install
    - name: Restore liboqs from cache
      id: restore_liboqs
      if: ${{ steps.paths.outputs.liboqs_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.liboqs_INSTALL }}
       key: ${{ steps.paths.outputs.liboqs_CACHE_KEY }}-install
    - name: Build liboqs
      if: ${{ steps.paths.outputs.liboqs_SOURCE && ! steps.restore_liboqs.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests liboqs
    - name: Save liboqs to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.liboqs_SOURCE && ! steps.restore_liboqs.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.liboqs_INSTALL }}
       key: ${{ steps.paths.outputs.liboqs_CACHE_KEY }}-install
    - name: Restore fizz from cache
      id: restore_fizz
      if: ${{ steps.paths.outputs.fizz_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fizz_INSTALL }}
       key: ${{ steps.paths.outputs.fizz_CACHE_KEY }}-install
    - name: Build fizz
      if: ${{ steps.paths.outputs.fizz_SOURCE && ! steps.restore_fizz.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests fizz
    - name: Save fizz to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fizz_SOURCE && ! steps.restore_fizz.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fizz_INSTALL }}
       key: ${{ steps.paths.outputs.fizz_CACHE_KEY }}-install
    - name: Restore mvfst from cache
      id: restore_mvfst
      if: ${{ steps.paths.outputs.mvfst_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.mvfst_INSTALL }}
       key: ${{ steps.paths.outputs.mvfst_CACHE_KEY }}-install
    - name: Build mvfst
      if: ${{ steps.paths.outputs.mvfst_SOURCE && ! steps.restore_mvfst.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests mvfst
    - name: Save mvfst to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.mvfst_SOURCE && ! steps.restore_mvfst.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.mvfst_INSTALL }}
       key: ${{ steps.paths.outputs.mvfst_CACHE_KEY }}-install
    - name: Restore wangle from cache
      id: restore_wangle
      if: ${{ steps.paths.outputs.wangle_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.wangle_INSTALL }}
       key: ${{ steps.paths.outputs.wangle_CACHE_KEY }}-install
    - name: Build wangle
      if: ${{ steps.paths.outputs.wangle_SOURCE && ! steps.restore_wangle.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests wangle
    - name: Save wangle to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.wangle_SOURCE && ! steps.restore_wangle.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.wangle_INSTALL }}
       key: ${{ steps.paths.outputs.wangle_CACHE_KEY }}-install
    - name: Restore fbthrift from cache
      id: restore_fbthrift
      if: ${{ steps.paths.outputs.fbthrift_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fbthrift_INSTALL }}
       key: ${{ steps.paths.outputs.fbthrift_CACHE_KEY }}-install
    - name: Build fbthrift
      if: ${{ steps.paths.outputs.fbthrift_SOURCE && ! steps.restore_fbthrift.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests fbthrift
    - name: Save fbthrift to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fbthrift_SOURCE && ! steps.restore_fbthrift.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fbthrift_INSTALL }}
       key: ${{ steps.paths.outputs.fbthrift_CACHE_KEY }}-install
    - name: Restore fb303 from cache
      id: restore_fb303
      if: ${{ steps.paths.outputs.fb303_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fb303_INSTALL }}
       key: ${{ steps.paths.outputs.fb303_CACHE_KEY }}-install
    - name: Build fb303
      if: ${{ steps.paths.outputs.fb303_SOURCE && ! steps.restore_fb303.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests fb303
    - name: Save fb303 to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fb303_SOURCE && ! steps.restore_fb303.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fb303_INSTALL }}
       key: ${{ steps.paths.outputs.fb303_CACHE_KEY }}-install
    - name: Restore edencommon from cache
      id: restore_edencommon
      if: ${{ steps.paths.outputs.edencommon_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.edencommon_INSTALL }}
       key: ${{ steps.paths.outputs.edencommon_CACHE_KEY }}-install
    - name: Build edencommon
      if: ${{ steps.paths.outputs.edencommon_SOURCE && ! steps.restore_edencommon.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --no-tests edencommon
    - name: Save edencommon to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.edencommon_SOURCE && ! steps.restore_edencommon.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.edencommon_INSTALL }}
       key: ${{ steps.paths.outputs.edencommon_CACHE_KEY }}-install
    - name: Build watchman
      run: python build/fbcode_builder/getdeps.py build --src-dir=. watchman 
    - name: Copy artifacts
      run: python build/fbcode_builder/getdeps.py fixup-dyn-deps --src-dir=. watchman _artifacts/windows  --final-install-prefix /usr/local
    - uses: actions/upload-artifact@v4
      with:
        name: watchman
        path: _artifacts
    - name: Test watchman
      run: python build/fbcode_builder/getdeps.py test --src-dir=. watchman 
",634,1,2,"push, pull_request",65
facebook/watchman,package.yml,"name: package

on: push

jobs:
  docker-ubuntu:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push Docker image
        uses: docker/build-push-action@v6
        with:
          context: .
          build-args: ""UBUNTU_VERSION=24.04""
          file: watchman/build/package/ubuntu-env/Dockerfile
          push: true
          tags: ${{ format('ghcr.io/{0}/watchman-build-env:latest', github.repository) }}

  clone-and-build-and-package-ubuntu:
    needs: docker-ubuntu
    runs-on: ubuntu-latest
    container:
      image: ${{ format('ghcr.io/{0}/watchman-build-env:latest', github.repository) }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: rustup default stable
        run: rustup default stable

      - name: Install system dependencies
        run: ./install-system-packages.sh

      - name: Test cargo
        run: cargo --help

      - name: Fix dubious ownership
        run: git config --global --add safe.directory /__w/watchman/watchman

      - name: Build Watchman binaries
        run: ./autogen.sh

      - name: Make .deb
        run: ./watchman/build/package/make-deb.sh
",56,2,1,push,5
facebook/watchman,release.yml,"# @generated by generate-release-yml.rs
---
name: release
""on"":
  push:
    tags:
      - v*
permissions:
  contents: write
  packages: write
jobs:
  prepare:
    runs-on: ubuntu-latest
    outputs:
      release: ""${{ steps.info.outputs.name }}""
      upload_url: ""${{ steps.create_release.outputs.upload_url }}""
    steps:
      - name: Prepare release info
        id: info
        env:
          TAG: ""${{ github.ref }}""
        run: ""python -c \""print('::set-output name=name::' + '$TAG'.lstrip('refs/tags/'))\""""
      - name: Create release
        id: create_release
        uses: actions/create-release@v1
        env:
          GITHUB_TOKEN: ""${{ secrets.GITHUB_TOKEN }}""
        with:
          tag_name: ""${{ github.ref }}""
          release_name: ""${{ github.ref }}""
  docker-ubuntu-22:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ""${{ github.repository_owner }}""
          password: ""${{ secrets.GITHUB_TOKEN }}""
      - name: Build and push Docker image
        uses: docker/build-push-action@v6
        with:
          context: "".""
          build-args: UBUNTU_VERSION=22.04
          file: watchman/build/package/ubuntu-env/Dockerfile
          push: true
          tags: ""${{ format('ghcr.io/{0}/watchman-build-env:ubuntu-22-latest', github.repository) }}""
  docker-ubuntu-24:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ""${{ github.repository_owner }}""
          password: ""${{ secrets.GITHUB_TOKEN }}""
      - name: Build and push Docker image
        uses: docker/build-push-action@v6
        with:
          context: "".""
          build-args: UBUNTU_VERSION=24.04
          file: watchman/build/package/ubuntu-env/Dockerfile
          push: true
          tags: ""${{ format('ghcr.io/{0}/watchman-build-env:ubuntu-24-latest', github.repository) }}""
  docker-fedora-40:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ""${{ github.repository_owner }}""
          password: ""${{ secrets.GITHUB_TOKEN }}""
      - name: Build and push Docker image
        uses: docker/build-push-action@v6
        with:
          context: "".""
          build-args: FEDORA_VERSION=40
          file: watchman/build/package/fedora-env/Dockerfile
          push: true
          tags: ""${{ format('ghcr.io/{0}/watchman-build-env:fedora-40-latest', github.repository) }}""
  docker-fedora-41:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ""${{ github.repository_owner }}""
          password: ""${{ secrets.GITHUB_TOKEN }}""
      - name: Build and push Docker image
        uses: docker/build-push-action@v6
        with:
          context: "".""
          build-args: FEDORA_VERSION=41
          file: watchman/build/package/fedora-env/Dockerfile
          push: true
          tags: ""${{ format('ghcr.io/{0}/watchman-build-env:fedora-41-latest', github.repository) }}""
  docker-fedora-42:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ""${{ github.repository_owner }}""
          password: ""${{ secrets.GITHUB_TOKEN }}""
      - name: Build and push Docker image
        uses: docker/build-push-action@v6
        with:
          context: "".""
          build-args: FEDORA_VERSION=42
          file: watchman/build/package/fedora-env/Dockerfile
          push: true
          tags: ""${{ format('ghcr.io/{0}/watchman-build-env:fedora-42-latest', github.repository) }}""
  clone-build-package-ubuntu-22:
    needs:
      - prepare
      - docker-ubuntu-22
    runs-on: ubuntu-latest
    container:
      image: ""${{ format('ghcr.io/{0}/watchman-build-env:ubuntu-22-latest', github.repository) }}""
    steps:
      - name: Fix HOME
        run: echo HOME=/root >> $GITHUB_ENV
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Install system dependencies
        run: ""./install-system-packages.sh""
      - name: Fix dubious ownership
        run: git config --global --add safe.directory /__w/watchman/watchman
      - name: Build Watchman binaries
        run: ""./autogen.sh""
      - name: Make .deb
        env:
          UBUNTU_VERSION: ""22.04""
        run: ""./watchman/build/package/make-deb.sh""
      - name: Upload .deb
        env:
          GITHUB_TOKEN: ""${{ secrets.GITHUB_TOKEN }}""
        uses: actions/upload-release-asset@v1
        with:
          upload_url: ""${{ needs.prepare.outputs.upload_url }}""
          asset_path: /_debs/watchman.deb
          asset_name: ""watchman_ubuntu22.04_${{ needs.prepare.outputs.release }}.deb""
          asset_content_type: application/x-deb
  clone-build-package-ubuntu-24:
    needs:
      - prepare
      - docker-ubuntu-24
    runs-on: ubuntu-latest
    container:
      image: ""${{ format('ghcr.io/{0}/watchman-build-env:ubuntu-24-latest', github.repository) }}""
    steps:
      - name: Fix HOME
        run: echo HOME=/root >> $GITHUB_ENV
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Install system dependencies
        run: ""./install-system-packages.sh""
      - name: Fix dubious ownership
        run: git config --global --add safe.directory /__w/watchman/watchman
      - name: Build Watchman binaries
        run: ""./autogen.sh""
      - name: Make .deb
        env:
          UBUNTU_VERSION: ""24.04""
        run: ""./watchman/build/package/make-deb.sh""
      - name: Upload .deb
        env:
          GITHUB_TOKEN: ""${{ secrets.GITHUB_TOKEN }}""
        uses: actions/upload-release-asset@v1
        with:
          upload_url: ""${{ needs.prepare.outputs.upload_url }}""
          asset_path: /_debs/watchman.deb
          asset_name: ""watchman_ubuntu24.04_${{ needs.prepare.outputs.release }}.deb""
          asset_content_type: application/x-deb
  clone-build-package-fedora-40:
    needs:
      - prepare
      - docker-fedora-40
    runs-on: ubuntu-latest
    container:
      image: ""${{ format('ghcr.io/{0}/watchman-build-env:fedora-40-latest', github.repository) }}""
    steps:
      - name: Fix HOME
        run: echo HOME=/root >> $GITHUB_ENV
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Install system dependencies
        run: ""./install-system-packages.sh""
      - name: Fix dubious ownership
        run: git config --global --add safe.directory /__w/watchman/watchman
      - name: Build Watchman binaries
        run: ""./autogen.sh""
      - name: Make .rpm
        id: make_rpm
        env:
          FEDORA_VERSION: ""40""
        run: ""./watchman/build/package/make-rpm.sh""
      - name: Upload .rpm
        env:
          GITHUB_TOKEN: ""${{ secrets.GITHUB_TOKEN }}""
        uses: actions/upload-release-asset@v1
        with:
          upload_url: ""${{ needs.prepare.outputs.upload_url }}""
          asset_path: ""${{ steps.make_rpm.outputs.rpm_path }}""
          asset_name: ""${{ steps.make_rpm.outputs.rpm_name }}""
          asset_content_type: application/x-rpm
  clone-build-package-fedora-41:
    needs:
      - prepare
      - docker-fedora-41
    runs-on: ubuntu-latest
    container:
      image: ""${{ format('ghcr.io/{0}/watchman-build-env:fedora-41-latest', github.repository) }}""
    steps:
      - name: Fix HOME
        run: echo HOME=/root >> $GITHUB_ENV
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Install system dependencies
        run: ""./install-system-packages.sh""
      - name: Fix dubious ownership
        run: git config --global --add safe.directory /__w/watchman/watchman
      - name: Build Watchman binaries
        run: ""./autogen.sh""
      - name: Make .rpm
        id: make_rpm
        env:
          FEDORA_VERSION: ""41""
        run: ""./watchman/build/package/make-rpm.sh""
      - name: Upload .rpm
        env:
          GITHUB_TOKEN: ""${{ secrets.GITHUB_TOKEN }}""
        uses: actions/upload-release-asset@v1
        with:
          upload_url: ""${{ needs.prepare.outputs.upload_url }}""
          asset_path: ""${{ steps.make_rpm.outputs.rpm_path }}""
          asset_name: ""${{ steps.make_rpm.outputs.rpm_name }}""
          asset_content_type: application/x-rpm
  clone-build-package-fedora-42:
    needs:
      - prepare
      - docker-fedora-42
    runs-on: ubuntu-latest
    container:
      image: ""${{ format('ghcr.io/{0}/watchman-build-env:fedora-42-latest', github.repository) }}""
    steps:
      - name: Fix HOME
        run: echo HOME=/root >> $GITHUB_ENV
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Install system dependencies
        run: ""./install-system-packages.sh""
      - name: Fix dubious ownership
        run: git config --global --add safe.directory /__w/watchman/watchman
      - name: Build Watchman binaries
        run: ""./autogen.sh""
      - name: Make .rpm
        id: make_rpm
        env:
          FEDORA_VERSION: ""42""
        run: ""./watchman/build/package/make-rpm.sh""
      - name: Upload .rpm
        env:
          GITHUB_TOKEN: ""${{ secrets.GITHUB_TOKEN }}""
        uses: actions/upload-release-asset@v1
        with:
          upload_url: ""${{ needs.prepare.outputs.upload_url }}""
          asset_path: ""${{ steps.make_rpm.outputs.rpm_path }}""
          asset_name: ""${{ steps.make_rpm.outputs.rpm_name }}""
          asset_content_type: application/x-rpm
  linux-build:
    continue-on-error: true
    needs: prepare
    runs-on: ubuntu-24.04
    steps:
      - uses: actions/checkout@v4
      - name: Build watchman
        run: ""python3 build/fbcode_builder/getdeps.py build --src-dir=. watchman  --project-install-prefix watchman:/usr/local""
      - name: Copy artifacts
        run: ""python3 build/fbcode_builder/getdeps.py fixup-dyn-deps --strip --src-dir=. watchman _artifacts/linux  --project-install-prefix watchman:/usr/local --final-install-prefix /usr/local""
      - name: Test watchman
        run: ""python3 build/fbcode_builder/getdeps.py test --src-dir=. watchman  --project-install-prefix watchman:/usr/local""
      - name: Package watchman
        run: ""mv _artifacts/linux \""watchman-${{ needs.prepare.outputs.release }}-linux\"" && zip -r watchman-${{ needs.prepare.outputs.release }}-linux.zip \""watchman-${{ needs.prepare.outputs.release }}-linux/\""""
      - name: Upload Linux release
        env:
          GITHUB_TOKEN: ""${{ secrets.GITHUB_TOKEN }}""
        uses: actions/upload-release-asset@v1
        with:
          upload_url: ""${{ needs.prepare.outputs.upload_url }}""
          asset_path: ""./watchman-${{ needs.prepare.outputs.release }}-linux.zip""
          asset_name: ""watchman-${{ needs.prepare.outputs.release }}-linux.zip""
          asset_content_type: application/zip
  mac-build:
    continue-on-error: true
    needs: prepare
    runs-on: macOS-10.15
    steps:
      - uses: actions/checkout@v4
      - name: Build watchman
        run: ""SDKROOT=$(xcrun --show-sdk-path --sdk macosx11.1) python3 build/fbcode_builder/getdeps.py --allow-system-packages build --src-dir=. watchman  --project-install-prefix watchman:/usr/local""
      - name: Copy artifacts
        run: ""python3 build/fbcode_builder/getdeps.py --allow-system-packages fixup-dyn-deps --src-dir=. watchman _artifacts/mac  --project-install-prefix watchman:/usr/local --final-install-prefix /usr/local""
      - name: Test watchman
        run: ""python3 build/fbcode_builder/getdeps.py --allow-system-packages test --src-dir=. watchman  --project-install-prefix watchman:/usr/local""
      - name: Package watchman
        run: ""mv _artifacts/mac \""watchman-${{ needs.prepare.outputs.release }}-macos\"" && zip -r watchman-${{ needs.prepare.outputs.release }}-macos.zip \""watchman-${{ needs.prepare.outputs.release }}-macos/\""""
      - name: Upload macOS release
        env:
          GITHUB_TOKEN: ""${{ secrets.GITHUB_TOKEN }}""
        uses: actions/upload-release-asset@v1
        with:
          upload_url: ""${{ needs.prepare.outputs.upload_url }}""
          asset_path: ""./watchman-${{ needs.prepare.outputs.release }}-macos.zip""
          asset_name: ""watchman-${{ needs.prepare.outputs.release }}-macos.zip""
          asset_content_type: application/zip
  windows-build:
    continue-on-error: true
    needs: prepare
    runs-on: windows-2019
    steps:
      - uses: actions/checkout@v4
      - name: Export boost environment
        run: echo BOOST_ROOT=%BOOST_ROOT_1_69_0% >> %GITHUB_ENV%
        shell: cmd
      - name: Fix Git config
        run: git config --system core.longpaths true
      - name: Build watchman
        run: python build/fbcode_builder/getdeps.py --allow-system-packages build --src-dir=. watchman
      - name: Copy artifacts
        run: python build/fbcode_builder/getdeps.py --allow-system-packages fixup-dyn-deps --src-dir=. watchman _artifacts/windows  --final-install-prefix /usr/local
      - name: Test watchman
        run: python build/fbcode_builder/getdeps.py --allow-system-packages test --src-dir=. watchman
      - name: Package watchman
        run: ""mv _artifacts/windows \""watchman-${{ needs.prepare.outputs.release }}-windows\"" && Compress-Archive -DestinationPath \""watchman-${{ needs.prepare.outputs.release }}-windows.zip\"" -Path \""watchman-${{ needs.prepare.outputs.release }}-windows/\""""
      - name: Upload Windows release
        env:
          GITHUB_TOKEN: ""${{ secrets.GITHUB_TOKEN }}""
        uses: actions/upload-release-asset@v1
        with:
          upload_url: ""${{ needs.prepare.outputs.upload_url }}""
          asset_path: ""./watchman-${{ needs.prepare.outputs.release }}-windows.zip""
          asset_name: ""watchman-${{ needs.prepare.outputs.release }}-windows.zip""
          asset_content_type: application/zip",367,14,1,push,37
facebook/rocksdb,benchmark-linux.yml,"name: facebook/rocksdb/benchmark-linux
on: workflow_dispatch
permissions: {}
  # FIXME: Disabled temporarily
  # schedule:
  # - cron: 7 */2 * * *  # At minute 7 past every 2nd hour
jobs:
  benchmark-linux:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on: ubuntu-latest # FIXME: change this back to self-hosted when ready
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/build-for-benchmarks""
    - uses: ""./.github/actions/perform-benchmarks""
    - uses: ""./.github/actions/post-benchmarks""
",15,1,1,workflow_dispatch,4
facebook/rocksdb,nightly-candidate.yml,"name: facebook/rocksdb/nightly
on: workflow_dispatch
permissions: {}
jobs:
  # These jobs would be in nightly but are failing or otherwise broken for
  # some reason.
  build-linux-arm-test-full:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: arm64large
    container:
      image: ubuntu-2004:202111-02
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - uses: ""./.github/actions/install-gflags""
    - run: make V=1 J=4 -j4 check
    - uses: ""./.github/actions/post-steps""
",19,1,1,workflow_dispatch,4
facebook/rocksdb,nightly.yml,"name: facebook/rocksdb/nightly
on:
  schedule:
  - cron: 0 9 * * *
  workflow_dispatch:
permissions: {}
jobs:
  build-format-compatible:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 16-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
      with:
        fetch-depth: 0 # Need full repo history
        fetch-tags: true
    - uses: ""./.github/actions/setup-upstream""
    - uses: ""./.github/actions/pre-steps""
    - name: test
      run: |-
        export TEST_TMPDIR=/dev/shm/rocksdb
        rm -rf /dev/shm/rocksdb
        mkdir /dev/shm/rocksdb
        git config --global --add safe.directory /__w/rocksdb/rocksdb
        tools/check_format_compatible.sh
    - uses: ""./.github/actions/post-steps""
  build-linux-non-shm:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 16-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    env:
      TEST_TMPDIR: ""/tmp/rocksdb_test_tmp""
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - run: make V=1 -j32 check
    - uses: ""./.github/actions/post-steps""
  build-linux-clang-13-asan-ubsan-with-folly:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 16-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    env:
      CC: clang-13
      CXX: clang++-13
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - uses: ""./.github/actions/setup-folly""
    - uses: ""./.github/actions/build-folly""
    - run: LIB_MODE=static USE_CLANG=1 USE_FOLLY=1 COMPILE_WITH_UBSAN=1 COMPILE_WITH_ASAN=1 make -j32 check
    - uses: ""./.github/actions/post-steps""
  build-linux-valgrind:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 16-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - run: make V=1 -j32 valgrind_test
    - uses: ""./.github/actions/post-steps""
  build-windows-vs2022-avx2:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on: windows-2022
    env:
      CMAKE_GENERATOR: Visual Studio 17 2022
      CMAKE_PORTABLE: AVX2
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/windows-build-steps""
  build-linux-arm-test-full:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 4-core-ubuntu-arm
    steps:
      - uses: actions/checkout@v4.1.0
      - uses: ""./.github/actions/pre-steps""
      - run: sudo apt-get update && sudo apt-get install -y build-essential libgflags-dev
      - run: make V=1 J=4 -j4 check
      - uses: ""./.github/actions/post-steps""
  build-examples:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 4-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - name: Build examples
      run: make V=1 -j4 static_lib && cd examples && make V=1 -j4
    - uses: ""./.github/actions/post-steps""
  build-fuzzers:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 4-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - name: Build rocksdb lib
      run: CC=clang-13 CXX=clang++-13 USE_CLANG=1 make -j4 static_lib
    - name: Build fuzzers
      run: cd fuzz && make sst_file_writer_fuzzer db_fuzzer db_map_fuzzer
    - uses: ""./.github/actions/post-steps""
  build-linux-gcc-11-no_test_run:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 16-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - run: LIB_MODE=static CC=gcc-11 CXX=g++-11 V=1 make -j32 all microbench
    - uses: ""./.github/actions/post-steps""
  build-linux-cmake-with-folly-lite-no-test:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 16-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    env:
      CC: gcc-10
      CXX: g++-10
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - uses: ""./.github/actions/setup-folly""
    - run: ""(mkdir build && cd build && cmake -DUSE_FOLLY_LITE=1 -DWITH_GFLAGS=1 .. && make V=1 -j20)""
    - uses: ""./.github/actions/post-steps""
",147,10,2,"schedule, workflow_dispatch",33
facebook/rocksdb,pr-jobs-candidate.yml,"name: facebook/rocksdb/pr-jobs-candidate
on: workflow_dispatch
permissions: {}
jobs:
  # These jobs would be in pr-jobs but are failing or otherwise broken for
  # some reason.
  # =========================== ARM Jobs ============================ #
  build-linux-arm:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: arm64large # GitHub hosted ARM runners do not yet exist
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - uses: ""./.github/actions/install-gflags""
    - run: ROCKSDBTESTS_PLATFORM_DEPENDENT=only make V=1 J=4 -j4 all_but_some_tests check_some
    - uses: ""./.github/actions/post-steps""
  build-linux-arm-cmake-no_test_run:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: arm64large # GitHub hosted ARM runners do not yet exist
    env:
      JAVA_HOME: ""/usr/lib/jvm/java-8-openjdk-arm64""
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - uses: ""./.github/actions/install-gflags""
    - name: Set Java Environment
      run: |-
        echo ""JAVA_HOME=${JAVA_HOME}""
        echo 'export PATH=$JAVA_HOME/bin:$PATH' >> $BASH_ENV
        which java && java -version
        which javac && javac -version
    - name: Build with cmake
      run: |-
        mkdir build
        cd build
        cmake -DCMAKE_BUILD_TYPE=Release -DWITH_TESTS=0 -DWITH_GFLAGS=1 -DWITH_BENCHMARK_TOOLS=0 -DWITH_TOOLS=0 -DWITH_CORE_TOOLS=1 ..
        make -j4
    - name: Build Java with cmake
      run: |-
        rm -rf build
        mkdir build
        cd build
        cmake -DJNI=1 -DCMAKE_BUILD_TYPE=Release -DWITH_GFLAGS=1 ..
        make -j4 rocksdb rocksdbjni
    - uses: ""./.github/actions/post-steps""
",47,2,1,workflow_dispatch,8
facebook/rocksdb,pr-jobs.yml,"name: facebook/rocksdb/pr-jobs
on: [push, pull_request]
permissions: {}
jobs:
  # NOTE: multiple workflows would be recommended, but the current GHA UI in
  # PRs doesn't make it clear when there's an overall error with a workflow,
  # making it easy to overlook something broken. Grouping everything into one
  # workflow minimizes the problem because it will be suspicious if there are
  # no GHA results.
  #
  # The if: ${{ github.repository_owner == 'facebook' }} lines prevent the
  # jobs from attempting to run on repo forks, because of a few problems:
  # * runs-on labels are repository (owner) specific, so the job might wait
  # for days waiting for a runner that simply isn't available.
  # * Pushes to branches on forks for pull requests (the normal process) would
  # run the workflow jobs twice: once in the pull-from fork and once for the PR
  # destination repo. This is wasteful and dumb.
  # * It is not known how to avoid copy-pasting the line to each job,
  # increasing the risk of misconfiguration, especially on forks that might
  # want to run with this GHA setup.
  #
  # DEBUGGING WITH SSH: Temporarily add this as a job step, either before the
  # step of interest without the ""if:"" line or after the failing step with the
  # ""if:"" line. Then use ssh command printed in CI output.
  #  - name: Setup tmate session # TEMPORARY!
  #    if: ${{ failure() }}
  #    uses: mxschmitt/action-tmate@v3
  #    with:
  #      limit-access-to-actor: true

  # ======================== Fast Initial Checks ====================== #
  check-format-and-targets:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on: ubuntu-24.04
    steps:
    - uses: actions/checkout@v4.1.0
      with:
        fetch-depth: 0 # Need full checkout to determine merge base
        fetch-tags: true
    - uses: ""./.github/actions/setup-upstream""
    - name: Setup Python
      uses: actions/setup-python@v5
    - name: Install Dependencies
      run: python -m pip install --upgrade pip
    - name: Install argparse
      run: pip install argparse
    - name: Download clang-format-diff.py
      run: wget https://rocksdb-deps.s3.us-west-2.amazonaws.com/llvm/llvm-project/release/12.x/clang/tools/clang-format/clang-format-diff.py
    - name: Check format
      run: VERBOSE_CHECK=1 make check-format
    - name: Compare buckify output
      run: make check-buck-targets
    - name: Simple source code checks
      run: make check-sources
    - name: Sanity check check_format_compatible.sh
      run: |-
        export TEST_TMPDIR=/dev/shm/rocksdb
        rm -rf /dev/shm/rocksdb
        mkdir /dev/shm/rocksdb
        git reset --hard
        git config --global --add safe.directory /__w/rocksdb/rocksdb
        SANITY_CHECK=1 LONG_TEST=1 tools/check_format_compatible.sh
  # ========================= Linux With Tests ======================== #
  build-linux:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 16-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - run: make V=1 J=32 -j32 check
    - uses: ""./.github/actions/post-steps""
  build-linux-cmake-mingw:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 4-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - run: update-alternatives --set x86_64-w64-mingw32-g++ /usr/bin/x86_64-w64-mingw32-g++-posix
    - name: Build cmake-mingw
      run: |-
        export PATH=$JAVA_HOME/bin:$PATH
        echo ""JAVA_HOME=${JAVA_HOME}""
        which java && java -version
        which javac && javac -version
        mkdir build && cd build && cmake -DJNI=1 -DWITH_GFLAGS=OFF .. -DCMAKE_C_COMPILER=x86_64-w64-mingw32-gcc -DCMAKE_CXX_COMPILER=x86_64-w64-mingw32-g++ -DCMAKE_SYSTEM_NAME=Windows && make -j4 rocksdb rocksdbjni
    - uses: ""./.github/actions/post-steps""
  build-linux-cmake-with-folly:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 16-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    env:
      CC: gcc-10
      CXX: g++-10
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - uses: ""./.github/actions/setup-folly""
    - uses: ""./.github/actions/build-folly""
    - run: ""(mkdir build && cd build && cmake -DUSE_FOLLY=1 -DWITH_GFLAGS=1 -DROCKSDB_BUILD_SHARED=0 .. && make V=1 -j20 && ctest -j20)""
    - uses: ""./.github/actions/post-steps""
  build-linux-make-with-folly:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 16-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    env:
      CC: gcc-10
      CXX: g++-10
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - uses: ""./.github/actions/setup-folly""
    - uses: ""./.github/actions/build-folly""
    - run: USE_FOLLY=1 LIB_MODE=static V=1 make -j32 check
    - uses: ""./.github/actions/post-steps""
  build-linux-make-with-folly-lite-no-test:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 16-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    env:
      CC: gcc-10
      CXX: g++-10
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - uses: ""./.github/actions/setup-folly""
    - run: USE_FOLLY_LITE=1 V=1 make -j32 all
    - uses: ""./.github/actions/post-steps""
  build-linux-cmake-with-folly-coroutines:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 16-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    env:
      CC: gcc-10
      CXX: g++-10
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - uses: ""./.github/actions/setup-folly""
    - uses: ""./.github/actions/build-folly""
    - run: ""(mkdir build && cd build && cmake -DUSE_COROUTINES=1 -DWITH_GFLAGS=1 -DROCKSDB_BUILD_SHARED=0 .. && make V=1 -j20 && ctest -j20)""
    - uses: ""./.github/actions/post-steps""
  build-linux-cmake-with-benchmark:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 16-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - run: mkdir build && cd build && cmake -DWITH_GFLAGS=1 -DWITH_BENCHMARK=1 .. && make V=1 -j20 && ctest -j20
    - uses: ""./.github/actions/post-steps""
  build-linux-encrypted_env-no_compression:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 16-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - run: ENCRYPTED_ENV=1 ROCKSDB_DISABLE_SNAPPY=1 ROCKSDB_DISABLE_ZLIB=1 ROCKSDB_DISABLE_BZIP=1 ROCKSDB_DISABLE_LZ4=1 ROCKSDB_DISABLE_ZSTD=1 make V=1 J=32 -j32 check
    - run: ""./sst_dump --help | grep -E -q 'Supported compression types: kNoCompression$' # Verify no compiled in compression\n""
    - uses: ""./.github/actions/post-steps""
  # ======================== Linux No Test Runs ======================= #
  build-linux-release:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 16-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - run: make V=1 -j32 LIB_MODE=shared release
    - run: ls librocksdb.so
    - run: ""./db_stress --version""
    - run: make clean
    - run: make V=1 -j32 release
    - run: ls librocksdb.a
    - run: ""./db_stress --version""
    - run: make clean
    - run: apt-get remove -y libgflags-dev
    - run: make V=1 -j32 LIB_MODE=shared release
    - run: ls librocksdb.so
    - run: if ./db_stress --version; then false; else true; fi
    - run: make clean
    - run: make V=1 -j32 release
    - run: ls librocksdb.a
    - run: if ./db_stress --version; then false; else true; fi
    - uses: ""./.github/actions/post-steps""
  build-linux-release-rtti:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 8-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - run: USE_RTTI=1 DEBUG_LEVEL=0 make V=1 -j16 static_lib tools db_bench
    - run: ""./db_stress --version""
    - run: make clean
    - run: apt-get remove -y libgflags-dev
    - run: USE_RTTI=1 DEBUG_LEVEL=0 make V=1 -j16 static_lib tools db_bench
    - run: if ./db_stress --version; then false; else true; fi
  build-linux-clang-no_test_run:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 8-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - run: CC=clang CXX=clang++ USE_CLANG=1 PORTABLE=1 make V=1 -j16 all
    - uses: ""./.github/actions/post-steps""
  build-linux-clang-13-no_test_run:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 16-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - run: CC=clang-13 CXX=clang++-13 USE_CLANG=1 make -j32 all microbench
    - uses: ""./.github/actions/post-steps""
  build-linux-gcc-8-no_test_run:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 16-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - run: CC=gcc-8 CXX=g++-8 V=1 make -j32 all
    - uses: ""./.github/actions/post-steps""
  build-linux-gcc-10-cxx20-no_test_run:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 16-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - run: CC=gcc-10 CXX=g++-10 V=1 ROCKSDB_CXX_STANDARD=c++20 make -j32 all
    - uses: ""./.github/actions/post-steps""

  # ======================== Linux Other Checks ======================= #
  build-linux-clang10-clang-analyze:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 16-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - run: CC=clang-10 CXX=clang++-10 ROCKSDB_DISABLE_ALIGNED_NEW=1 CLANG_ANALYZER=""/usr/bin/clang++-10"" CLANG_SCAN_BUILD=scan-build-10 USE_CLANG=1 make V=1 -j32 analyze
    - uses: ""./.github/actions/post-steps""
    - name: compress test report
      run: tar -cvzf scan_build_report.tar.gz scan_build_report
      if: failure()
    - uses: actions/upload-artifact@v4.0.0
      with:
        name: scan-build-report
        path: scan_build_report.tar.gz
  build-linux-unity-and-headers:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 4-core-ubuntu
    container:
      image: gcc:latest
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - run: apt-get update -y && apt-get install -y libgflags-dev
    - name: Unity build
      run: make V=1 -j8 unity_test
    - run: make V=1 -j8 -k check-headers
    - uses: ""./.github/actions/post-steps""
  build-linux-mini-crashtest:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 4-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - run: ulimit -S -n `ulimit -H -n` && make V=1 -j8 CRASH_TEST_EXT_ARGS='--duration=960 --max_key=2500000' blackbox_crash_test_with_atomic_flush
    - uses: ""./.github/actions/post-steps""
  # ======================= Linux with Sanitizers ===================== #
  build-linux-clang10-asan:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 32-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - run: COMPILE_WITH_ASAN=1 CC=clang-10 CXX=clang++-10 ROCKSDB_DISABLE_ALIGNED_NEW=1 USE_CLANG=1 make V=1 -j32 check
    - uses: ""./.github/actions/post-steps""
  build-linux-clang10-ubsan:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 16-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - run: COMPILE_WITH_UBSAN=1 CC=clang-10 CXX=clang++-10 ROCKSDB_DISABLE_ALIGNED_NEW=1 USE_CLANG=1 make V=1 -j32 ubsan_check
    - uses: ""./.github/actions/post-steps""
  build-linux-clang13-mini-tsan:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 32-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - run: COMPILE_WITH_TSAN=1 CC=clang-13 CXX=clang++-13 ROCKSDB_DISABLE_ALIGNED_NEW=1 USE_CLANG=1 make V=1 -j32 check
    - uses: ""./.github/actions/post-steps""
  build-linux-static_lib-alt_namespace-status_checked:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 16-core-ubuntu
    container:
      image: zjay437/rocksdb:0.6
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/pre-steps""
    - run: ASSERT_STATUS_CHECKED=1 TEST_UINT128_COMPAT=1 ROCKSDB_MODIFY_NPHASH=1 LIB_MODE=static OPT=""-DROCKSDB_NAMESPACE=alternative_rocksdb_ns"" make V=1 -j24 check
    - uses: ""./.github/actions/post-steps""
  # ========================= MacOS build only ======================== #
  build-macos:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on: macos-13
    env:
      ROCKSDB_DISABLE_JEMALLOC: 1
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: maxim-lobanov/setup-xcode@v1.6.0
      with:
        xcode-version: 14.3.1
    - uses: ""./.github/actions/increase-max-open-files-on-macos""
    - uses: ""./.github/actions/install-gflags-on-macos""
    - uses: ""./.github/actions/pre-steps-macos""
    - name: Build
      run: ulimit -S -n `ulimit -H -n` && make V=1 J=16 -j16 all
    - uses: ""./.github/actions/post-steps""
  # ========================= MacOS with Tests ======================== #
  build-macos-cmake:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on: macos-13
    strategy:
      matrix:
        run_even_tests: [true, false]
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: maxim-lobanov/setup-xcode@v1.6.0
      with:
        xcode-version: 14.3.1
    - uses: ""./.github/actions/increase-max-open-files-on-macos""
    - uses: ""./.github/actions/install-gflags-on-macos""
    - uses: ""./.github/actions/pre-steps-macos""
    - name: cmake generate project file
      run: ulimit -S -n `ulimit -H -n` && mkdir build && cd build && cmake -DWITH_GFLAGS=1 ..
    - name: Build tests
      run: cd build && make V=1 -j16
    - name: Run even tests
      run: ulimit -S -n `ulimit -H -n` && cd build && ctest -j16 -I 0,,2
      if: ${{ matrix.run_even_tests }}
    - name: Run odd tests
      run: ulimit -S -n `ulimit -H -n` && cd build && ctest -j16 -I 1,,2
      if: ${{ ! matrix.run_even_tests  }}
    - uses: ""./.github/actions/post-steps""
  # ======================== Windows with Tests ======================= #
  # NOTE: some windows jobs are in ""nightly"" to save resources
  build-windows-vs2022:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on: windows-2022
    env:
      CMAKE_GENERATOR: Visual Studio 17 2022
      CMAKE_PORTABLE: 1
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/windows-build-steps""
  # ============================ Java Jobs ============================ #
  build-linux-java:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 4-core-ubuntu
    container:
      image: evolvedbinary/rocksjava:centos6_x64-be
      options: --shm-size=16gb
    steps:
    # The docker image is intentionally based on an OS that has an older GLIBC version.
    # That GLIBC is incompatibile with GitHub's actions/checkout. Thus we implement a manual checkout step.
    - name: Checkout
      env:
        GH_TOKEN: ${{ github.token }}
      run: |
        chown `whoami` . || true
        git clone --no-checkout https://oath2:$GH_TOKEN@github.com/${{ github.repository }}.git .
        git -c protocol.version=2 fetch --update-head-ok --no-tags --prune --no-recurse-submodules --depth=1 origin +${{ github.sha }}:${{ github.ref }}
        git checkout --progress --force ${{ github.ref }}
        git log -1 --format='%H'
    - uses: ""./.github/actions/pre-steps""
    - name: Set Java Environment
      run: |-
        echo ""JAVA_HOME=${JAVA_HOME}""
        which java && java -version
        which javac && javac -version
    - name: Test RocksDBJava
      run: scl enable devtoolset-7 'make V=1 J=8 -j8 jtest'
    # NOTE: post-steps skipped because of compatibility issues with docker image
  build-linux-java-static:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 4-core-ubuntu
    container:
      image: evolvedbinary/rocksjava:centos6_x64-be
      options: --shm-size=16gb
    steps:
    # The docker image is intentionally based on an OS that has an older GLIBC version.
    # That GLIBC is incompatibile with GitHub's actions/checkout. Thus we implement a manual checkout step.
    - name: Checkout
      env:
        GH_TOKEN: ${{ github.token }}
      run: |
        chown `whoami` . || true
        git clone --no-checkout https://oath2:$GH_TOKEN@github.com/${{ github.repository }}.git .
        git -c protocol.version=2 fetch --update-head-ok --no-tags --prune --no-recurse-submodules --depth=1 origin +${{ github.sha }}:${{ github.ref }}
        git checkout --progress --force ${{ github.ref }}
        git log -1 --format='%H'
    - uses: ""./.github/actions/pre-steps""
    - name: Set Java Environment
      run: |-
        echo ""JAVA_HOME=${JAVA_HOME}""
        which java && java -version
        which javac && javac -version
    - name: Build RocksDBJava Static Library
      run: scl enable devtoolset-7 'make V=1 J=8 -j8 rocksdbjavastatic'
    # NOTE: post-steps skipped because of compatibility issues with docker image
  build-macos-java:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on: macos-13
    env:
      JAVA_HOME: ""/Library/Java/JavaVirtualMachines/liberica-jdk-8.jdk/Contents/Home""
      ROCKSDB_DISABLE_JEMALLOC: 1
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: maxim-lobanov/setup-xcode@v1.6.0
      with:
        xcode-version: 14.3.1
    - uses: ""./.github/actions/increase-max-open-files-on-macos""
    - uses: ""./.github/actions/install-gflags-on-macos""
    - uses: ""./.github/actions/install-jdk8-on-macos""
    - uses: ""./.github/actions/pre-steps-macos""
    - name: Set Java Environment
      run: |-
        echo ""JAVA_HOME=${JAVA_HOME}""
        which java && java -version
        which javac && javac -version
    - name: Test RocksDBJava
      run: make V=1 J=16 -j16 jtest
    - uses: ""./.github/actions/post-steps""
  build-macos-java-static:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on: macos-13
    env:
      JAVA_HOME: ""/Library/Java/JavaVirtualMachines/liberica-jdk-8.jdk/Contents/Home""
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: maxim-lobanov/setup-xcode@v1.6.0
      with:
        xcode-version: 14.3.1
    - uses: ""./.github/actions/increase-max-open-files-on-macos""
    - uses: ""./.github/actions/install-gflags-on-macos""
    - uses: ""./.github/actions/install-jdk8-on-macos""
    - uses: ""./.github/actions/pre-steps-macos""
    - name: Set Java Environment
      run: |-
        echo ""JAVA_HOME=${JAVA_HOME}""
        which java && java -version
        which javac && javac -version
    - name: Build RocksDBJava x86 and ARM Static Libraries
      run: make V=1 J=16 -j16 rocksdbjavastaticosx
    - uses: ""./.github/actions/post-steps""
  build-macos-java-static-universal:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on: macos-13
    env:
      JAVA_HOME: ""/Library/Java/JavaVirtualMachines/liberica-jdk-8.jdk/Contents/Home""
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: maxim-lobanov/setup-xcode@v1.6.0
      with:
        xcode-version: 14.3.1
    - uses: ""./.github/actions/increase-max-open-files-on-macos""
    - uses: ""./.github/actions/install-gflags-on-macos""
    - uses: ""./.github/actions/install-jdk8-on-macos""
    - uses: ""./.github/actions/pre-steps-macos""
    - name: Set Java Environment
      run: |-
        echo ""JAVA_HOME=${JAVA_HOME}""
        which java && java -version
        which javac && javac -version
    - name: Build RocksDBJava Universal Binary Static Library
      run: make V=1 J=16 -j16 rocksdbjavastaticosx_ub
    - uses: ""./.github/actions/post-steps""
  build-linux-java-pmd:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 4-core-ubuntu
    container:
      image: evolvedbinary/rocksjava:rockylinux8_x64-be
      options: --shm-size=16gb
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: ""./.github/actions/install-maven""
    - uses: ""./.github/actions/pre-steps""
    - name: Set Java Environment
      run: |-
        echo ""JAVA_HOME=${JAVA_HOME}""
        which java && java -version
        which javac && javac -version
    - name: PMD RocksDBJava
      run: make V=1 J=8 -j8 jpmd
    - uses: actions/upload-artifact@v4.0.0
      with:
        name: pmd-report
        path: ""${{ github.workspace }}/java/target/pmd.xml""
    - uses: actions/upload-artifact@v4.0.0
      with:
        name: maven-site
        path: ""${{ github.workspace }}/java/target/site""
  build-linux-arm:
    if: ${{ github.repository_owner == 'facebook' }}
    runs-on:
      labels: 4-core-ubuntu-arm
    steps:
      - uses: actions/checkout@v4.1.0
      - uses: ""./.github/actions/pre-steps""
      - run: sudo apt-get update && sudo apt-get install -y build-essential
      - run: ROCKSDBTESTS_PLATFORM_DEPENDENT=only make V=1 J=4 -j4 all_but_some_tests check_some
      - uses: ""./.github/actions/post-steps""
",585,32,2,"push, pull_request",115
facebook/react,compiler_discord_notify.yml,"name: (Compiler) Discord Notify

on:
  pull_request_target:
    types: [opened, ready_for_review]
    paths:
      - compiler/**
      - .github/workflows/compiler_**.yml

permissions: {}

jobs:
  check_access:
    if: ${{ github.event.pull_request.draft == false }}
    runs-on: ubuntu-latest
    outputs:
      is_member_or_collaborator: ${{ steps.check_is_member_or_collaborator.outputs.is_member_or_collaborator }}
    steps:
      - run: echo ${{ github.event.pull_request.author_association }}
      - name: Check is member or collaborator
        id: check_is_member_or_collaborator
        if: ${{ github.event.pull_request.author_association == 'MEMBER' || github.event.pull_request.author_association == 'COLLABORATOR' }}
        run: echo ""is_member_or_collaborator=true"" >> ""$GITHUB_OUTPUT""

  check_maintainer:
    if: ${{ needs.check_access.outputs.is_member_or_collaborator == 'true' || needs.check_access.outputs.is_member_or_collaborator == true }}
    needs: [check_access]
    uses: facebook/react/.github/workflows/shared_check_maintainer.yml@main
    permissions:
      # Used by check_maintainer
      contents: read
    with:
      actor: ${{ github.event.pull_request.user.login }}

  notify:
    if: ${{ needs.check_maintainer.outputs.is_core_team == 'true' }}
    needs: check_maintainer
    runs-on: ubuntu-latest
    steps:
      - name: Discord Webhook Action
        uses: tsickert/discord-webhook@86dc739f3f165f16dadc5666051c367efa1692f4
        with:
          webhook-url: ${{ secrets.COMPILER_DISCORD_WEBHOOK_URL }}
          embed-author-name: ${{ github.event.pull_request.user.login }}
          embed-author-url: ${{ github.event.pull_request.user.html_url }}
          embed-author-icon-url: ${{ github.event.pull_request.user.avatar_url }}
          embed-title: '#${{ github.event.number }} (+${{github.event.pull_request.additions}} -${{github.event.pull_request.deletions}}): ${{ github.event.pull_request.title }}'
          embed-description: ${{ github.event.pull_request.body }}
          embed-url: ${{ github.event.pull_request.html_url }}
",49,3,1,pull_request_target,2
facebook/react,compiler_playground.yml,"name: (Compiler) Playground

on:
  push:
    branches: [main]
  pull_request:
    paths:
      - compiler/**
      - .github/workflows/compiler_playground.yml

permissions: {}

concurrency:
  group: ${{ github.workflow }}-${{ github.ref_name }}-${{ github.event.pull_request.number || github.run_id }}
  cancel-in-progress: true

env:
  TZ: /usr/share/zoneinfo/America/Los_Angeles
  # https://github.com/actions/cache/blob/main/tips-and-workarounds.md#cache-segment-restore-timeout
  SEGMENT_DOWNLOAD_TIMEOUT_MINS: 1

defaults:
  run:
    working-directory: compiler/apps/playground

jobs:
  playground:
    name: Test playground
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: compiler/**/yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: compiler-and-playground-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('compiler/**/yarn.lock') }}
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
        working-directory: compiler
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - name: Check Playwright version
        id: playwright_version
        run: echo ""playwright_version=$(npm ls @playwright/test | grep @playwright | sed 's/.*@//' | head -1)"" >> ""$GITHUB_OUTPUT""
      - name: Cache Playwright Browsers for version ${{ steps.playwright_version.outputs.playwright_version }}
        id: cache_playwright_browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: playwright-browsers-v6-${{ runner.arch }}-${{ runner.os }}-${{ steps.playwright_version.outputs.playwright_version }}
      - run: npx playwright install --with-deps chromium
        if: steps.cache_playwright_browsers.outputs.cache-hit != 'true'
      - run: npx playwright install-deps
        if: steps.cache_playwright_browsers.outputs.cache-hit == 'true'
      - run: CI=true yarn test
      - run: ls -R test-results
        if: '!cancelled()'
      - name: Archive test results
        if: '!cancelled()'
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: compiler/apps/playground/test-results
          if-no-files-found: ignore
",71,1,2,"push, pull_request",5
facebook/react,compiler_prereleases.yml,"name: (Compiler) Publish Prereleases

on:
  workflow_call:
    inputs:
      commit_sha:
        required: true
        default: ''
        type: string
      release_channel:
        required: true
        type: string
      dist_tag:
        required: true
        type: string
      version_name:
        required: true
        type: string
      tag_version:
        required: false
        type: string
    secrets:
      NPM_TOKEN:
        required: true

permissions: {}

env:
  TZ: /usr/share/zoneinfo/America/Los_Angeles
  # https://github.com/actions/cache/blob/main/tips-and-workarounds.md#cache-segment-restore-timeout
  SEGMENT_DOWNLOAD_TIMEOUT_MINS: 1
  NPM_TOKEN: ${{ secrets.NPM_TOKEN }}

defaults:
  run:
    working-directory: compiler

jobs:
  publish_prerelease:
    name: Publish prelease (${{ inputs.release_channel }}) ${{ inputs.commit_sha }} @${{ inputs.dist_tag }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: compiler/yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: compiler-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('compiler/yarn.lock') }}
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - name: Publish packages to npm
        run: |
          cp ./scripts/release/ci-npmrc ~/.npmrc
          scripts/release/publish.js --frfr --ci --versionName=${{ inputs.version_name }} --tag=${{ inputs.dist_tag }} ${{ inputs.tag_version && format('--tagVersion={0}', inputs.tag_version) || '' }}
",61,1,1,workflow_call,3
facebook/react,compiler_prereleases_manual.yml,"name: (Compiler) Publish Prereleases Manual

on:
  workflow_dispatch:
    inputs:
      prerelease_commit_sha:
        required: false
      release_channel:
        required: true
        type: string
      dist_tag:
        required: true
        type: string
      version_name:
        required: true
        type: string
      tag_version:
        required: false
        type: string

permissions: {}

env:
  TZ: /usr/share/zoneinfo/America/Los_Angeles

jobs:
  publish_prerelease_experimental:
    name: Publish to Experimental channel
    uses: facebook/react/.github/workflows/compiler_prereleases.yml@main
    with:
      commit_sha: ${{ inputs.prerelease_commit_sha || github.sha }}
      release_channel: ${{ inputs.release_channel }}
      dist_tag: ${{ inputs.dist_tag }}
      version_name: ${{ inputs.version_name }}
      tag_version: ${{ inputs.tag_version }}
    secrets:
      NPM_TOKEN: ${{ secrets.NPM_TOKEN }}
",37,1,1,workflow_dispatch,1
facebook/react,compiler_prereleases_nightly.yml,"name: (Compiler) Publish Prereleases Nightly

on:
  schedule:
    # At 10 minutes past 16:00 on Mon, Tue, Wed, Thu, and Fri
    - cron: 10 16 * * 1,2,3,4,5

permissions: {}

env:
  TZ: /usr/share/zoneinfo/America/Los_Angeles

jobs:
  publish_prerelease_experimental:
    name: Publish to Experimental channel
    uses: facebook/react/.github/workflows/compiler_prereleases.yml@main
    with:
      commit_sha: ${{ github.sha }}
      release_channel: experimental
      dist_tag: experimental
      version_name: '0.0.0'
    secrets:
      NPM_TOKEN: ${{ secrets.NPM_TOKEN }}
",23,1,1,schedule,1
facebook/react,compiler_typescript.yml,"name: (Compiler) TypeScript

on:
  push:
    branches: [main]
  pull_request:
    paths:
      - compiler/**
      - .github/workflows/compiler_typescript.yml

permissions: {}

concurrency:
  group: ${{ github.workflow }}-${{ github.ref_name }}-${{ github.event.pull_request.number || github.run_id }}
  cancel-in-progress: true

env:
  TZ: /usr/share/zoneinfo/America/Los_Angeles
  # https://github.com/actions/cache/blob/main/tips-and-workarounds.md#cache-segment-restore-timeout
  SEGMENT_DOWNLOAD_TIMEOUT_MINS: 1

defaults:
  run:
    working-directory: compiler

jobs:
  discover_yarn_workspaces:
    name: Discover yarn workspaces
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - uses: actions/checkout@v4
      - id: set-matrix
        run: echo ""matrix=$(find packages -mindepth 1 -maxdepth 1 -type d | sed 's!packages/!!g' | tr '\n' ',' | sed s/.$// | jq -Rsc '. / "","" - [""""]')"" >> $GITHUB_OUTPUT

  # Hardcoded to improve parallelism
  lint:
    name: Lint babel-plugin-react-compiler
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: compiler/yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: compiler-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('compiler/yarn.lock') }}
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: yarn workspace babel-plugin-react-compiler lint

  # Hardcoded to improve parallelism
  jest:
    name: Jest babel-plugin-react-compiler
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: compiler/yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: compiler-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('compiler/yarn.lock') }}
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: yarn workspace babel-plugin-react-compiler jest

  test:
    name: Test ${{ matrix.workspace_name }}
    needs: discover_yarn_workspaces
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        workspace_name: ${{ fromJSON(needs.discover_yarn_workspaces.outputs.matrix) }}
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: compiler/yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: compiler-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('compiler/yarn.lock') }}
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: xvfb-run -a yarn workspace ${{ matrix.workspace_name }} test
        if: runner.os == 'Linux' && matrix.workspace_name == 'react-forgive'
      - run: yarn workspace ${{ matrix.workspace_name }} test
        if: matrix.workspace_name != 'react-forgive'
",108,4,2,"push, pull_request",10
facebook/react,devtools_regression_tests.yml,"name: (DevTools) Regression Tests

on:
  schedule:
    - cron: 0 0 * * *
  workflow_dispatch:
    inputs:
      commit_sha:
        required: false
        type: string

permissions: {}

env:
  TZ: /usr/share/zoneinfo/America/Los_Angeles
  # https://github.com/actions/cache/blob/main/tips-and-workarounds.md#cache-segment-restore-timeout
  SEGMENT_DOWNLOAD_TIMEOUT_MINS: 1

jobs:
  download_build:
    name: Download base build
    runs-on: ubuntu-latest
    permissions:
      # We use github.token to download the build artifact from a previous runtime_build_and_test.yml run
      actions: read
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-release-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock', 'scripts/release/yarn.lock') }}
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: yarn --cwd scripts/release install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - name: Download react-devtools artifacts for base revision
        run: |
          git fetch origin main
          GH_TOKEN=${{ github.token }} scripts/release/download-experimental-build.js --commit=${{ inputs.commit_sha || '$(git rev-parse origin/main)' }}
      - name: Display structure of build
        run: ls -R build
      - name: Archive build
        uses: actions/upload-artifact@v4
        with:
          name: build
          path: build
          if-no-files-found: error

  build_devtools_and_process_artifacts:
    name: Build DevTools and process artifacts
    needs: download_build
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock') }}
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - name: Restore archived build
        uses: actions/download-artifact@v4
        with:
          name: build
          path: build
      - run: ./scripts/ci/pack_and_store_devtools_artifacts.sh
        env:
          RELEASE_CHANNEL: experimental
      - name: Display structure of build
        run: ls -R build
      - name: Archive devtools build
        uses: actions/upload-artifact@v4
        with:
          name: react-devtools
          path: build/devtools.tgz
          if-no-files-found: error
      # Simplifies getting the extension for local testing
      - name: Archive chrome extension
        uses: actions/upload-artifact@v4
        with:
          name: react-devtools-chrome-extension
          path: build/devtools/chrome-extension.zip
          if-no-files-found: error
      - name: Archive firefox extension
        uses: actions/upload-artifact@v4
        with:
          name: react-devtools-firefox-extension
          path: build/devtools/firefox-extension.zip
          if-no-files-found: error

  run_devtools_tests_for_versions:
    name: Run DevTools tests for versions
    needs: build_devtools_and_process_artifacts
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        version:
          - ""16.0""
          - ""16.5"" # schedule package
          - ""16.8"" # hooks
          - ""17.0""
          - ""18.0""
          - ""18.2"" # compiler polyfill
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock') }}
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - name: Restore all archived build artifacts
        uses: actions/download-artifact@v4
      - name: Display structure of build
        run: ls -R build
      - run: ./scripts/ci/download_devtools_regression_build.js ${{ matrix.version }} --replaceBuild
      - run: node ./scripts/jest/jest-cli.js --build --project devtools --release-channel=experimental --reactVersion ${{ matrix.version }} --ci

  run_devtools_e2e_tests_for_versions:
    name: Run DevTools e2e tests for versions
    needs: build_devtools_and_process_artifacts
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        version:
          - ""16.0""
          - ""16.5"" # schedule package
          - ""16.8"" # hooks
          - ""17.0""
          - ""18.0""
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock') }}
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - name: Restore all archived build artifacts
        uses: actions/download-artifact@v4
      - name: Display structure of build
        run: ls -R build
      - name: Check Playwright version
        id: playwright_version
        run: echo ""playwright_version=$(npm ls @playwright/test | grep @playwright | sed 's/.*@//' | head -1)"" >> ""$GITHUB_OUTPUT""
      - name: Cache Playwright Browsers for version ${{ steps.playwright_version.outputs.playwright_version }}
        id: cache_playwright_browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: playwright-browsers-v6-${{ runner.arch }}-${{ runner.os }}-${{ steps.playwright_version.outputs.playwright_version }}
      - run: npx playwright install --with-deps
        if: steps.cache_playwright_browsers.outputs.cache-hit != 'true'
      - run: npx playwright install-deps
        if: steps.cache_playwright_browsers.outputs.cache-hit == 'true'
      - run: ./scripts/ci/download_devtools_regression_build.js ${{ matrix.version }}
      - run: ls -R build-regression
      - run: ./scripts/ci/run_devtools_e2e_tests.js ${{ matrix.version }}
        env:
          RELEASE_CHANNEL: experimental
      - name: Cleanup build regression folder
        run: rm -r ./build-regression
      - uses: actions/upload-artifact@v4
        with:
          name: screenshots
          path: ./tmp/screenshots
          if-no-files-found: warn
",205,4,2,"schedule, workflow_dispatch",21
facebook/react,runtime_build_and_test.yml,"name: (Runtime) Build and Test

on:
  push:
    branches: [main]
  pull_request:
    paths-ignore:
      - compiler/**

permissions: {}

concurrency:
  group: ${{ github.workflow }}-${{ github.ref_name }}-${{ github.event.pull_request.number || github.run_id }}
  cancel-in-progress: true

env:
  TZ: /usr/share/zoneinfo/America/Los_Angeles
  # https://github.com/actions/cache/blob/main/tips-and-workarounds.md#cache-segment-restore-timeout
  SEGMENT_DOWNLOAD_TIMEOUT_MINS: 1

jobs:
  # ----- NODE_MODULES CACHE -----
  # Centralize the node_modules cache so it is saved once and each subsequent job only needs to
  # restore the cache. Prevents race conditions where multiple workflows try to write to the cache.
  runtime_node_modules_cache:
    name: Cache Runtime node_modules
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
      - name: Check cache hit
        uses: actions/cache/restore@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock') }}
          lookup-only: true
      - uses: actions/setup-node@v4
        if: steps.node_modules.outputs.cache-hit != 'true'
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      - name: Warm with old cache
        if: steps.node_modules.outputs.cache-hit != 'true'
        uses: actions/cache/restore@v4
        with:
          path: |
            **/node_modules
          key: runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock') }}
          restore-keys: |
            runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-
            runtime-node_modules-v6-
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - name: Save cache
        if: steps.node_modules.outputs.cache-hit != 'true'
        uses: actions/cache/save@v4
        with:
          path: |
            **/node_modules
          key: runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock') }}

  runtime_compiler_node_modules_cache:
    name: Cache Runtime, Compiler node_modules
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
      - name: Check cache hit
        uses: actions/cache/restore@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-and-compiler-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock', 'compiler/yarn.lock') }}
          lookup-only: true
      - uses: actions/setup-node@v4
        if: steps.node_modules.outputs.cache-hit != 'true'
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: |
            yarn.lock
            compiler/yarn.lock
      - name: Warm with old cache
        if: steps.node_modules.outputs.cache-hit != 'true'
        uses: actions/cache/restore@v4
        with:
          path: |
            **/node_modules
          key: runtime-and-compiler-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock', 'compiler/yarn.lock') }}
          restore-keys: |
            runtime-and-compiler-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-
            runtime-and-compiler-node_modules-v6-
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: yarn --cwd compiler install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - name: Save cache
        if: steps.node_modules.outputs.cache-hit != 'true'
        uses: actions/cache/save@v4
        with:
          path: |
            **/node_modules
          key: runtime-and-compiler-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock', 'compiler/yarn.lock') }}

  # ----- FLOW -----
  discover_flow_inline_configs:
    name: Discover flow inline configs
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.result }}
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
      - uses: actions/github-script@v7
        id: set-matrix
        with:
          script: |
            const inlinedHostConfigs = require('./scripts/shared/inlinedHostConfigs.js');
            return inlinedHostConfigs.map(config => config.shortName);

  flow:
    name: Flow check ${{ matrix.flow_inline_config_shortname }}
    needs: [discover_flow_inline_configs, runtime_node_modules_cache]
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        flow_inline_config_shortname: ${{ fromJSON(needs.discover_flow_inline_configs.outputs.matrix) }}
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache/restore@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock') }}
          restore-keys: |
            runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-
            runtime-node_modules-v6-
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: node ./scripts/tasks/flow-ci ${{ matrix.flow_inline_config_shortname }}

  # ----- FIZZ -----
  check_generated_fizz_runtime:
    name: Confirm generated inline Fizz runtime is up to date
    needs: [runtime_node_modules_cache]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache/restore@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock') }}
          restore-keys: |
            runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-
            runtime-node_modules-v6-
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: |
          yarn generate-inline-fizz-runtime
          git diff --quiet || (echo ""There was a change to the Fizz runtime. Run `yarn generate-inline-fizz-runtime` and check in the result."" && false)

  # ----- FEATURE FLAGS -----
  flags:
    name: Check flags
    needs: [runtime_node_modules_cache]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache/restore@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock') }}
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: yarn flags

  # ----- TESTS -----
  test:
    name: yarn test ${{ matrix.params }} (Shard ${{ matrix.shard }})
    needs: [runtime_compiler_node_modules_cache]
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        params:
          - ""-r=stable --env=development""
          - ""-r=stable --env=production""
          - ""-r=experimental --env=development""
          - ""-r=experimental --env=production""
          - ""-r=www-classic --env=development --variant=false""
          - ""-r=www-classic --env=production --variant=false""
          - ""-r=www-classic --env=development --variant=true""
          - ""-r=www-classic --env=production --variant=true""
          - ""-r=www-modern --env=development --variant=false""
          - ""-r=www-modern --env=production --variant=false""
          - ""-r=www-modern --env=development --variant=true""
          - ""-r=www-modern --env=production --variant=true""
          - ""-r=xplat --env=development --variant=false""
          - ""-r=xplat --env=development --variant=true""
          - ""-r=xplat --env=production --variant=false""
          - ""-r=xplat --env=production --variant=true""
          # TODO: Test more persistent configurations?
          - ""-r=stable --env=development --persistent""
          - ""-r=experimental --env=development --persistent""
        shard:
          - 1/5
          - 2/5
          - 3/5
          - 4/5
          - 5/5
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: |
            yarn.lock
            compiler/yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache/restore@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-and-compiler-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock', 'compiler/yarn.lock') }}
          restore-keys: |
            runtime-and-compiler-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-
            runtime-and-compiler-node_modules-v6-
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: yarn --cwd compiler install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: yarn test ${{ matrix.params }} --ci --shard=${{ matrix.shard }}

  # Hardcoded to improve parallelism
  test-linter:
    name: Test eslint-plugin-react-hooks
    needs: [runtime_compiler_node_modules_cache]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path:  |
            yarn.lock
            compiler/yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-and-compiler-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock', 'compiler/yarn.lock') }}
      - name: Install runtime dependencies
        run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - name: Install compiler dependencies
        run: yarn install --frozen-lockfile
        working-directory: compiler
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: ./scripts/react-compiler/build-compiler.sh && ./scripts/react-compiler/link-compiler.sh
      - run: yarn workspace eslint-plugin-react-hooks test
        
  # ----- BUILD -----
  build_and_lint:
    name: yarn build and lint
    needs: [runtime_compiler_node_modules_cache]
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        # yml is dumb. update the --total arg to yarn build if you change the number of workers
        worker_id: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]
        release_channel: [stable, experimental]
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: |
            yarn.lock
            compiler/yarn.lock
      - uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: 11.0.22
      - name: Restore cached node_modules
        uses: actions/cache/restore@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-and-compiler-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock', 'compiler/yarn.lock') }}
          restore-keys: |
            runtime-and-compiler-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-
            runtime-and-compiler-node_modules-v6-
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: yarn --cwd compiler install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: yarn build --index=${{ matrix.worker_id }} --total=25 --r=${{ matrix.release_channel }} --ci
        env:
          CI: github
          RELEASE_CHANNEL: ${{ matrix.release_channel }}
          NODE_INDEX: ${{ matrix.worker_id }}
      - name: Lint build
        run: yarn lint-build
      - name: Display structure of build
        run: ls -R build
      - name: Archive build
        uses: actions/upload-artifact@v4
        with:
          name: _build_${{ matrix.worker_id }}_${{ matrix.release_channel }}
          path: build
          if-no-files-found: error

  test_build:
    name: yarn test-build
    needs: [build_and_lint, runtime_compiler_node_modules_cache]
    strategy:
      fail-fast: false
      matrix:
        test_params: [
          # Intentionally passing these as strings instead of creating a
          # separate parameter per CLI argument, since it's easier to
          # control/see which combinations we want to run.
          -r=stable --env=development,
          -r=stable --env=production,
          -r=experimental --env=development,
          -r=experimental --env=production,

          # Dev Tools
          --project=devtools -r=experimental,

          # TODO: Update test config to support www build tests
          # - ""-r=www-classic --env=development --variant=false""
          # - ""-r=www-classic --env=production --variant=false""
          # - ""-r=www-classic --env=development --variant=true""
          # - ""-r=www-classic --env=production --variant=true""
          # - ""-r=www-modern --env=development --variant=false""
          # - ""-r=www-modern --env=production --variant=false""
          # - ""-r=www-modern --env=development --variant=true""
          # - ""-r=www-modern --env=production --variant=true""

          # TODO: Update test config to support xplat build tests
          # - ""-r=xplat --env=development --variant=false""
          # - ""-r=xplat --env=development --variant=true""
          # - ""-r=xplat --env=production --variant=false""
          # - ""-r=xplat --env=production --variant=true""

          # TODO: Test more persistent configurations?
        ]
        shard:
          - 1/10
          - 2/10
          - 3/10
          - 4/10
          - 5/10
          - 6/10
          - 7/10
          - 8/10
          - 9/10
          - 10/10
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: |
            yarn.lock
            compiler/yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache/restore@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-and-compiler-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock', 'compiler/yarn.lock') }}
          restore-keys: |
            runtime-and-compiler-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-
            runtime-and-compiler-node_modules-v6-
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: yarn --cwd compiler install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - name: Restore archived build
        uses: actions/download-artifact@v4
        with:
          pattern: _build_*
          path: build
          merge-multiple: true
      - name: Display structure of build
        run: ls -R build
      - run: yarn test --build ${{ matrix.test_params }} --shard=${{ matrix.shard }} --ci

  process_artifacts_combined:
    name: Process artifacts combined
    needs: [build_and_lint, runtime_node_modules_cache]
    permissions:
      # https://github.com/actions/attest-build-provenance
      id-token: write
      attestations: write
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache/restore@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock') }}
          restore-keys: |
            runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-
            runtime-node_modules-v6-
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - name: Restore archived build
        uses: actions/download-artifact@v4
        with:
          pattern: _build_*
          path: build
          merge-multiple: true
      - name: Display structure of build
        run: ls -R build
      - run: echo ${{ github.event.pull_request.head.sha || github.sha }} >> build/COMMIT_SHA
      - name: Scrape warning messages
        run: |
          mkdir -p ./build/__test_utils__
          node ./scripts/print-warnings/print-warnings.js > build/__test_utils__/ReactAllWarnings.js
        # Compress build directory into a single tarball for easy download
      - run: tar -zcvf ./build.tgz ./build
        # TODO: Migrate scripts to use `build` directory instead of `build2`
      - run: cp ./build.tgz ./build2.tgz
      - name: Archive build artifacts
        id: upload_artifacts_combined
        uses: actions/upload-artifact@v4
        with:
          name: artifacts_combined
          path: |
            ./build.tgz
            ./build2.tgz
          if-no-files-found: error
      - uses: actions/attest-build-provenance@v2
        # We don't verify builds generated from pull requests not originating from facebook/react.
        # However, if the PR lands, the run on `main` will generate the attestation which can then
        # be used to download a build via scripts/release/download-experimental-build.js.
        #
        # Note that this means that scripts/release/download-experimental-build.js must be run with
        # --no-verify when downloading a build from a fork.
        if: github.event_name == 'push' && github.ref_name == 'main' || github.event.pull_request.head.repo.full_name == github.repository
        with:
          subject-name: artifacts_combined.zip
          subject-digest: sha256:${{ steps.upload_artifacts_combined.outputs.artifact-digest }}

  check_error_codes:
    name: Search build artifacts for unminified errors
    needs: [build_and_lint, runtime_node_modules_cache]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache/restore@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock') }}
          restore-keys: |
            runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-
            runtime-node_modules-v6-
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - name: Restore archived build
        uses: actions/download-artifact@v4
        with:
          pattern: _build_*
          path: build
          merge-multiple: true
      - name: Display structure of build
        run: ls -R build
      - name: Search build artifacts for unminified errors
        run: |
          yarn extract-errors
          git diff --quiet || (echo ""Found unminified errors. Either update the error codes map or disable error minification for the affected build, if appropriate."" && false)

  check_release_dependencies:
    name: Check release dependencies
    needs: [build_and_lint, runtime_node_modules_cache]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache/restore@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock') }}
          restore-keys: |
            runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-
            runtime-node_modules-v6-
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - name: Restore archived build
        uses: actions/download-artifact@v4
        with:
          pattern: _build_*
          path: build
          merge-multiple: true
      - name: Display structure of build
        run: ls -R build
      - run: yarn check-release-dependencies

  RELEASE_CHANNEL_stable_yarn_test_dom_fixtures:
    name: Check fixtures DOM (stable)
    needs: build_and_lint
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache@v4 # note: this does not reuse centralized cache since it has unique cache key
        id: node_modules
        with:
          path: |
            **/node_modules
          key: fixtures_dom-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock', 'fixtures/dom/yarn.lock') }}
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn --cwd fixtures/dom install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - name: Restore archived build
        uses: actions/download-artifact@v4
        with:
          pattern: _build_*
          path: build
          merge-multiple: true
      - name: Display structure of build
        run: ls -R build
      - name: Run DOM fixture tests
        run: |
          yarn predev
          yarn test
        working-directory: fixtures/dom
        env:
          RELEASE_CHANNEL: stable

  # ----- FLIGHT -----
  run_fixtures_flight_tests:
    name: Run fixtures Flight tests
    needs: build_and_lint
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      # Fixture copies some built packages from the workroot after install.
      # That means dependencies of the built packages are not installed.
      # We need to install dependencies of the workroot to fulfill all dependency constraints
      - name: Restore cached node_modules
        uses: actions/cache@v4 # note: this does not reuse centralized cache since it has unique cache key
        id: node_modules
        with:
          path: |
            **/node_modules
          key: fixtures_flight-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock', 'fixtures/flight/yarn.lock') }}
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: yarn --cwd fixtures/flight install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - name: Check Playwright version
        id: playwright_version
        run: echo ""playwright_version=$(npm ls @playwright/test | grep @playwright | sed 's/.*@//' | head -1)"" >> ""$GITHUB_OUTPUT""
      - name: Cache Playwright Browsers for version ${{ steps.playwright_version.outputs.playwright_version }}
        id: cache_playwright_browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: playwright-browsers-v6-${{ runner.arch }}-${{ runner.os }}-${{ steps.playwright_version.outputs.playwright_version }}
      - name: Playwright install deps
        if: steps.cache_playwright_browsers.outputs.cache-hit != 'true'
        working-directory: fixtures/flight
        run: npx playwright install --with-deps chromium
      - name: Restore archived build
        uses: actions/download-artifact@v4
        with:
          pattern: _build_*
          path: build
          merge-multiple: true
      - name: Display structure of build
        run: ls -R build
      - name: Run tests
        working-directory: fixtures/flight
        run: yarn test
        env:
          # Otherwise the webserver is a blackbox
          DEBUG: pw:webserver
      - name: Archive Flight fixture artifacts
        uses: actions/upload-artifact@v4
        with:
          name: flight-playwright-report
          path: fixtures/flight/playwright-report
          if-no-files-found: warn
      - name: Archive Flight fixture artifacts
        uses: actions/upload-artifact@v4
        with:
          name: flight-test-results
          path: fixtures/flight/test-results
          if-no-files-found: ignore

  # ----- DEVTOOLS -----
  build_devtools_and_process_artifacts:
    name: Build DevTools and process artifacts
    needs: [build_and_lint, runtime_node_modules_cache]
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        browser: [chrome, firefox, edge]
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache/restore@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock') }}
          restore-keys: |
            runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-
            runtime-node_modules-v6-
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - name: Restore archived build
        uses: actions/download-artifact@v4
        with:
          pattern: _build_*
          path: build
          merge-multiple: true
      - run: ./scripts/ci/pack_and_store_devtools_artifacts.sh ${{ matrix.browser }}
        env:
          RELEASE_CHANNEL: experimental
      - name: Display structure of build
        run: ls -R build
      # Simplifies getting the extension for local testing
      - name: Archive ${{ matrix.browser }} extension
        uses: actions/upload-artifact@v4
        with:
          name: react-devtools-${{ matrix.browser }}-extension
          path: build/devtools/${{ matrix.browser }}-extension.zip
          if-no-files-found: error

  merge_devtools_artifacts:
    name: Merge DevTools artifacts
    needs: build_devtools_and_process_artifacts
    runs-on: ubuntu-latest
    steps:
      - name: Merge artifacts
        uses: actions/upload-artifact/merge@v4
        with:
          name: react-devtools
          pattern: react-devtools-*-extension

  run_devtools_e2e_tests:
    name: Run DevTools e2e tests
    needs: [build_and_lint, runtime_node_modules_cache]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache/restore@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock') }}
          restore-keys: |
            runtime-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-
            runtime-node_modules-v6-
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - name: Restore archived build
        uses: actions/download-artifact@v4
        with:
          pattern: _build_*
          path: build
          merge-multiple: true
      - run: |
          npx playwright install
          sudo npx playwright install-deps
      - run: ./scripts/ci/run_devtools_e2e_tests.js
        env:
          RELEASE_CHANNEL: experimental

  # ----- SIZEBOT -----
  sizebot:
    if: ${{ github.event_name == 'pull_request' && github.ref_name != 'main' && github.event.pull_request.base.ref == 'main' }}
    name: Run sizebot
    needs: [build_and_lint]
    permissions:
      # We use github.token to download the build artifact from a previous runtime_build_and_test.yml run
      actions: read
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache@v4 # note: this does not reuse centralized cache since it has unique cache key
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-release-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock', 'scripts/release/yarn.lock') }}
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: yarn --cwd scripts/release install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - name: Download artifacts for base revision
        # The build could have been generated from a fork, so we must download the build without
        # any verification. This is safe since we only use this for sizebot calculation and the
        # unverified artifact is not used. Additionally this workflow runs in the pull_request
        # trigger so only restricted permissions are available.
        run: |
          GH_TOKEN=${{ github.token }} scripts/release/download-experimental-build.js --commit=$(git rev-parse ${{ github.event.pull_request.base.sha }}) ${{ (github.event.pull_request.head.repo.full_name != github.repository && '--noVerify') || ''}}
          mv ./build ./base-build
      - name: Delete extraneous files
        # TODO: The `download-experimental-build` script copies the npm
        # packages into the `node_modules` directory. This is a historical
        # quirk of how the release script works. Let's pretend they
        # don't exist.
        run: rm -rf ./base-build/node_modules
      - name: Display structure of base-build from origin/main
        run: ls -R base-build
      - name: Ensure clean build directory
        run: rm -rf build
      - name: Restore archived build for PR
        uses: actions/download-artifact@v4
        with:
          pattern: _build_*
          path: build
          merge-multiple: true
      - name: Scrape warning messages
        run: |
          mkdir -p ./build/__test_utils__
          node ./scripts/print-warnings/print-warnings.js > build/__test_utils__/ReactAllWarnings.js
      - name: Display structure of build for PR
        run: ls -R build
      - run: echo ${{ github.event.pull_request.head.sha || github.sha }} >> build/COMMIT_SHA
      - run: node ./scripts/tasks/danger
      - name: Archive sizebot results
        uses: actions/upload-artifact@v4
        with:
          name: sizebot-message
          path: sizebot-message.md
          if-no-files-found: ignore
",883,19,2,"push, pull_request",76
facebook/react,runtime_commit_artifacts.yml,"name: (Runtime) Commit Artifacts for Meta WWW and fbsource V2

on:
  workflow_run:
    workflows: [""(Runtime) Build and Test""]
    types: [completed]
    branches:
      - main
  workflow_dispatch:
    inputs:
      commit_sha:
        required: false
        type: string
      force:
        description: 'Force a commit to the builds/... branches'
        required: true
        default: false
        type: boolean
      dry_run:
        description: Perform a dry run (run everything except push)
        required: true
        default: false
        type: boolean

permissions: {}

env:
  TZ: /usr/share/zoneinfo/America/Los_Angeles
  # https://github.com/actions/cache/blob/main/tips-and-workarounds.md#cache-segment-restore-timeout
  SEGMENT_DOWNLOAD_TIMEOUT_MINS: 1

jobs:
  download_artifacts:
    runs-on: ubuntu-latest
    permissions:
      # We use github.token to download the build artifact from a previous runtime_build_and_test.yml run
      actions: read
    steps:
      - uses: actions/checkout@v4
      - name: Restore cached node_modules
        uses: actions/cache@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-release-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock', 'scripts/release/yarn.lock') }}
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: yarn --cwd scripts/release install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - name: Download artifacts for base revision
        run: |
          GH_TOKEN=${{ github.token }} scripts/release/download-experimental-build.js --commit=${{ inputs.commit_sha || github.event.workflow_run.head_sha || github.sha }}
      - name: Display structure of build
        run: ls -R build
      - name: Archive build
        uses: actions/upload-artifact@v4
        with:
          name: build
          path: build/
          if-no-files-found: error


  process_artifacts:
    runs-on: ubuntu-latest
    needs: [download_artifacts]
    outputs:
      www_branch_count: ${{ steps.check_branches.outputs.www_branch_count }}
      fbsource_branch_count: ${{ steps.check_branches.outputs.fbsource_branch_count }}
      last_version_classic: ${{ steps.get_last_version_www.outputs.last_version_classic }}
      last_version_modern: ${{ steps.get_last_version_www.outputs.last_version_modern }}
      last_version_rn: ${{ steps.get_last_version_rn.outputs.last_version_rn }}
      current_version_classic: ${{ steps.get_current_version.outputs.current_version_classic }}
      current_version_modern: ${{ steps.get_current_version.outputs.current_version_modern }}
      current_version_rn: ${{ steps.get_current_version.outputs.current_version_rn }}
    steps:
      - uses: actions/checkout@v4
        with:
          ref: builds/facebook-www
      - name: ""Get last version string for www""
        id: get_last_version_www
        run: |
          # Empty checks only needed for backwards compatibility,can remove later.
          VERSION_CLASSIC=$( [ -f ./compiled/facebook-www/VERSION_CLASSIC ] && cat ./compiled/facebook-www/VERSION_CLASSIC || echo '' )
          VERSION_MODERN=$( [ -f ./compiled/facebook-www/VERSION_MODERN ] && cat ./compiled/facebook-www/VERSION_MODERN || echo '' )
          echo ""Last classic version is $VERSION_CLASSIC""
          echo ""Last modern version is $VERSION_MODERN""
          echo ""last_version_classic=$VERSION_CLASSIC"" >> ""$GITHUB_OUTPUT""
          echo ""last_version_modern=$VERSION_MODERN"" >> ""$GITHUB_OUTPUT""
      - uses: actions/checkout@v4
        with:
          ref: builds/facebook-fbsource
      - name: ""Get last version string for rn""
        id: get_last_version_rn
        run: |
          # Empty checks only needed for backwards compatibility,can remove later.
          VERSION_NATIVE_FB=$( [ -f ./compiled-rn/VERSION_NATIVE_FB ] && cat ./compiled-rn/VERSION_NATIVE_FB || echo '' )
          echo ""Last rn version is $VERSION_NATIVE_FB""
          echo ""last_version_rn=$VERSION_NATIVE_FB"" >> ""$GITHUB_OUTPUT""
      - uses: actions/checkout@v4
      - name: ""Check branches""
        id: check_branches
        run: |
          echo ""www_branch_count=$(git ls-remote --heads origin ""refs/heads/meta-www"" | wc -l)"" >> ""$GITHUB_OUTPUT""
          echo ""fbsource_branch_count=$(git ls-remote --heads origin ""refs/heads/meta-fbsource"" | wc -l)"" >> ""$GITHUB_OUTPUT""
      - name: Restore downloaded build
        uses: actions/download-artifact@v4
        with:
          name: build
          path: build
      - name: Display structure of build
        run: ls -R build
      - name: Strip @license from eslint plugin and react-refresh
        run: |
          sed -i -e 's/ @license React*//' \
            build/oss-experimental/eslint-plugin-react-hooks/cjs/eslint-plugin-react-hooks.development.js \
            build/oss-experimental/react-refresh/cjs/react-refresh-babel.development.js
      - name: Insert @headers into eslint plugin and react-refresh
        run: |
          sed -i -e 's/ LICENSE file in the root directory of this source tree./ LICENSE file in the root directory of this source tree.\n *\n * @noformat\n * @nolint\n * @lightSyntaxTransform\n * @preventMunge\n * @oncall react_core/' \
            build/oss-experimental/eslint-plugin-react-hooks/cjs/eslint-plugin-react-hooks.development.js \
            build/oss-experimental/react-refresh/cjs/react-refresh-babel.development.js
      - name: Move relevant files for React in www into compiled
        run: |
          # Move the facebook-www folder into compiled
          mkdir ./compiled
          mv build/facebook-www ./compiled

          # Move ReactAllWarnings.js to facebook-www
          mkdir ./compiled/facebook-www/__test_utils__
          mv build/__test_utils__/ReactAllWarnings.js ./compiled/facebook-www/__test_utils__/ReactAllWarnings.js

          # Copy eslint-plugin-react-hooks
          mkdir ./compiled/eslint-plugin-react-hooks
          cp build/oss-experimental/eslint-plugin-react-hooks/cjs/eslint-plugin-react-hooks.development.js \
            ./compiled/eslint-plugin-react-hooks/index.js

          # Move unstable_server-external-runtime.js into facebook-www
          mv build/oss-experimental/react-dom/unstable_server-external-runtime.js \
            ./compiled/facebook-www/unstable_server-external-runtime.js

          # Move react-refresh-babel.development.js into babel-plugin-react-refresh
          mkdir ./compiled/babel-plugin-react-refresh
          mv build/oss-experimental/react-refresh/cjs/react-refresh-babel.development.js \
            ./compiled/babel-plugin-react-refresh/index.js

          ls -R ./compiled
      - name: Move relevant files for React in fbsource into compiled-rn
        run: |
          BASE_FOLDER='compiled-rn/facebook-fbsource/xplat/js'
          mkdir -p ${BASE_FOLDER}/react-native-github/Libraries/Renderer/
          mkdir -p ${BASE_FOLDER}/RKJSModules/vendor/react/{scheduler,react,react-dom,react-is,react-test-renderer}/

          # Move React Native renderer
          mv build/react-native/implementations/ $BASE_FOLDER/react-native-github/Libraries/Renderer/
          mv build/react-native/shims/ $BASE_FOLDER/react-native-github/Libraries/Renderer/
          mv build/facebook-react-native/scheduler/cjs/ $BASE_FOLDER/RKJSModules/vendor/react/scheduler/
          mv build/facebook-react-native/react/cjs/ $BASE_FOLDER/RKJSModules/vendor/react/react/
          mv build/facebook-react-native/react-dom/cjs/ $BASE_FOLDER/RKJSModules/vendor/react/react-dom/
          mv build/facebook-react-native/react-is/cjs/ $BASE_FOLDER/RKJSModules/vendor/react/react-is/
          mv build/facebook-react-native/react-test-renderer/cjs/ $BASE_FOLDER/RKJSModules/vendor/react/react-test-renderer/

          # Delete OSS renderer. OSS renderer is synced through internal script.
          RENDERER_FOLDER=$BASE_FOLDER/react-native-github/Libraries/Renderer/implementations/
          rm $RENDERER_FOLDER/ReactFabric-{dev,prod,profiling}.js
          rm $RENDERER_FOLDER/ReactNativeRenderer-{dev,prod,profiling}.js

          # Copy eslint-plugin-react-hooks
          # NOTE: This is different from www, here we include the full package
          #       including package.json to include dependencies in fbsource.
          mkdir ""$BASE_FOLDER/tools""
          cp -r build/oss-experimental/eslint-plugin-react-hooks ""$BASE_FOLDER/tools""

          # Move React Native version file
          mv build/facebook-react-native/VERSION_NATIVE_FB ./compiled-rn/VERSION_NATIVE_FB

          ls -R ./compiled-rn
      - name: Add REVISION files
        run: |
          echo ${{ inputs.commit_sha || github.event.workflow_run.head_sha || github.sha }} >> ./compiled/facebook-www/REVISION
          cp ./compiled/facebook-www/REVISION ./compiled/facebook-www/REVISION_TRANSFORMS
          echo ${{ inputs.commit_sha || github.event.workflow_run.head_sha || github.sha }} >> ./compiled-rn/facebook-fbsource/xplat/js/react-native-github/Libraries/Renderer/REVISION
      - name: ""Get current version string""
        id: get_current_version
        run: |
          VERSION_CLASSIC=$(cat ./compiled/facebook-www/VERSION_CLASSIC)
          VERSION_MODERN=$(cat ./compiled/facebook-www/VERSION_MODERN)
          VERSION_NATIVE_FB=$(cat ./compiled-rn/VERSION_NATIVE_FB)
          echo ""Current classic version is $VERSION_CLASSIC""
          echo ""Current modern version is $VERSION_MODERN""
          echo ""Current rn version is $VERSION_NATIVE_FB""
          echo ""current_version_classic=$VERSION_CLASSIC"" >> ""$GITHUB_OUTPUT""
          echo ""current_version_modern=$VERSION_MODERN"" >> ""$GITHUB_OUTPUT""
          echo ""current_version_rn=$VERSION_NATIVE_FB"" >> ""$GITHUB_OUTPUT""
      - uses: actions/upload-artifact@v4
        with:
          name: compiled
          path: compiled/
          if-no-files-found: error
      - uses: actions/upload-artifact@v4
        with:
          name: compiled-rn
          path: compiled-rn/
          if-no-files-found: error

  commit_www_artifacts:
    needs: [download_artifacts, process_artifacts]
    if: inputs.force == true || (github.ref == 'refs/heads/main' && needs.process_artifacts.outputs.www_branch_count == '0')
    runs-on: ubuntu-latest
    permissions:
      # Used to push a commit to builds/facebook-www
      contents: write
    steps:
      - uses: actions/checkout@v4
        with:
          ref: builds/facebook-www
      - name: Ensure clean directory
        run: rm -rf compiled
      - uses: actions/download-artifact@v4
        with:
          name: compiled
          path: compiled/
      - name: Revert version changes
        if: needs.process_artifacts.outputs.last_version_classic != '' && needs.process_artifacts.outputs.last_version_modern != ''
        env:
          CURRENT_VERSION_CLASSIC: ${{ needs.process_artifacts.outputs.current_version_classic }}
          CURRENT_VERSION_MODERN: ${{ needs.process_artifacts.outputs.current_version_modern }}
          LAST_VERSION_CLASSIC: ${{ needs.process_artifacts.outputs.last_version_classic }}
          LAST_VERSION_MODERN: ${{ needs.process_artifacts.outputs.last_version_modern }}
        run: |
          echo ""Reverting $CURRENT_VERSION_CLASSIC to $LAST_VERSION_CLASSIC""
          grep -rl ""$CURRENT_VERSION_CLASSIC"" ./compiled || echo ""No files found with $CURRENT_VERSION_CLASSIC""
          grep -rl ""$CURRENT_VERSION_CLASSIC"" ./compiled | xargs -r sed -i -e ""s/$CURRENT_VERSION_CLASSIC/$LAST_VERSION_CLASSIC/g""
          grep -rl ""$CURRENT_VERSION_CLASSIC"" ./compiled || echo ""Classic version reverted""
          echo ""====================""
          echo ""Reverting $CURRENT_VERSION_MODERN to $LAST_VERSION_MODERN""
          grep -rl ""$CURRENT_VERSION_MODERN"" ./compiled || echo ""No files found with $CURRENT_VERSION_MODERN""
          grep -rl ""$CURRENT_VERSION_MODERN"" ./compiled | xargs -r sed -i -e ""s/$CURRENT_VERSION_MODERN/$LAST_VERSION_MODERN/g""
          grep -rl ""$CURRENT_VERSION_MODERN"" ./compiled || echo ""Modern version reverted""
      - name: Check for changes
        if: inputs.force != true
        id: check_should_commit
        run: |
          echo ""Full git status""
          git add .
          git status
          echo ""====================""
          if git status --porcelain | grep -qv '/REVISION'; then
            echo ""Changes detected""
            echo ""===== Changes =====""
            git --no-pager diff -U0 | grep '^[+-]' | head -n 50
            echo ""===================""
            echo ""should_commit=true"" >> ""$GITHUB_OUTPUT""
          else
            echo ""No Changes detected""
            echo ""should_commit=false"" >> ""$GITHUB_OUTPUT""
          fi
      - name: Re-apply version changes
        if: inputs.force == true || (steps.check_should_commit.outputs.should_commit == 'true' && needs.process_artifacts.outputs.last_version_classic != '' && needs.process_artifacts.outputs.last_version_modern != '')
        env:
          CURRENT_VERSION_CLASSIC: ${{ needs.process_artifacts.outputs.current_version_classic }}
          CURRENT_VERSION_MODERN: ${{ needs.process_artifacts.outputs.current_version_modern }}
          LAST_VERSION_CLASSIC: ${{ needs.process_artifacts.outputs.last_version_classic }}
          LAST_VERSION_MODERN: ${{ needs.process_artifacts.outputs.last_version_modern }}
        run: |
          echo ""Re-applying $LAST_VERSION_CLASSIC to $CURRENT_VERSION_CLASSIC""
          grep -rl ""$LAST_VERSION_CLASSIC"" ./compiled || echo ""No files found with $LAST_VERSION_CLASSIC""
          grep -rl ""$LAST_VERSION_CLASSIC"" ./compiled | xargs -r sed -i -e ""s/$LAST_VERSION_CLASSIC/$CURRENT_VERSION_CLASSIC/g""
          grep -rl ""$LAST_VERSION_CLASSIC"" ./compiled || echo ""Classic version re-applied""
          echo ""====================""
          echo ""Re-applying $LAST_VERSION_MODERN to $CURRENT_VERSION_MODERN""
          grep -rl ""$LAST_VERSION_MODERN"" ./compiled || echo ""No files found with $LAST_VERSION_MODERN""
          grep -rl ""$LAST_VERSION_MODERN"" ./compiled | xargs -r sed -i -e ""s/$LAST_VERSION_MODERN/$CURRENT_VERSION_MODERN/g""
          grep -rl ""$LAST_VERSION_MODERN"" ./compiled || echo ""Classic version re-applied""
      - name: Will commit these changes
        if: inputs.force == true || steps.check_should_commit.outputs.should_commit == 'true'
        run: |
          git add .
          git status
      - name: Check commit message
        if: inputs.dry_run
        run: |
          git fetch origin --quiet
          git show ${{ inputs.commit_sha || github.event.workflow_run.head_sha || github.sha }} --no-patch --pretty=format:""%B""
      - name: Commit changes to branch
        if: inputs.force == true || steps.check_should_commit.outputs.should_commit == 'true'
        run: |
          git config --global user.email ""${{ format('{0}@users.noreply.github.com', github.triggering_actor) }}""
          git config --global user.name ""${{ github.triggering_actor }}""

          git fetch origin --quiet
          git commit -m ""$(git show ${{ inputs.commit_sha || github.event.workflow_run.head_sha || github.sha }} --no-patch --pretty=format:'%B%n%nDiffTrain build for [${{ inputs.commit_sha || github.event.workflow_run.head_sha || github.sha }}](https://github.com/facebook/react/commit/${{ inputs.commit_sha || github.event.workflow_run.head_sha || github.sha}})')"" || echo ""No changes to commit""
      - name: Push changes to branch
        if: inputs.dry_run == false && (inputs.force == true || steps.check_should_commit.outputs.should_commit == 'true')
        run: git push

  commit_fbsource_artifacts:
    needs: [download_artifacts, process_artifacts]
    permissions:
      # Used to push a commit to builds/facebook-fbsource
      contents: write
    if: inputs.force == true || (github.ref == 'refs/heads/main' && needs.process_artifacts.outputs.fbsource_branch_count == '0')
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          ref: builds/facebook-fbsource
      - name: Ensure clean directory
        run: rm -rf compiled-rn
      - uses: actions/download-artifact@v4
        with:
          name: compiled-rn
          path: compiled-rn/
      - name: Revert version changes
        if: needs.process_artifacts.outputs.last_version_rn != ''
        env:
          CURRENT_VERSION: ${{ needs.process_artifacts.outputs.current_version_rn }}
          LAST_VERSION: ${{ needs.process_artifacts.outputs.last_version_rn }}
        run: |
          echo ""Reverting $CURRENT_VERSION to $LAST_VERSION""
          grep -rl ""$CURRENT_VERSION"" ./compiled-rn || echo ""No files found with $CURRENT_VERSION""
          grep -rl ""$CURRENT_VERSION"" ./compiled-rn | xargs -r sed -i -e ""s/$CURRENT_VERSION/$LAST_VERSION/g""
          grep -rl ""$CURRENT_VERSION"" ./compiled-rn || echo ""Version reverted""
      - name: Check for changes
        if: inputs.force != 'true'
        id: check_should_commit
        run: |
          echo ""Full git status""
          git add .
          git --no-pager diff -U0 --cached | grep '^[+-]' | head -n 100
          echo ""====================""
          # Ignore REVISION or lines removing @generated headers.
          if git diff --cached ':(exclude)*REVISION' ':(exclude)*/eslint-plugin-react-hooks/package.json' | grep -vE ""^(@@|diff|index|\-\-\-|\+\+\+|\- \* @generated SignedSource)"" | grep ""^[+-]"" > /dev/null; then
            echo ""Changes detected""
            echo ""===== Changes =====""
            git --no-pager diff --cached ':(exclude)*REVISION' ':(exclude)*/eslint-plugin-react-hooks/package.json' | grep -vE ""^(@@|diff|index|\-\-\-|\+\+\+|\- \* @generated SignedSource)"" | grep ""^[+-]"" | head -n 50
            echo ""===================""
            echo ""should_commit=true"" >> ""$GITHUB_OUTPUT""
          else
            echo ""No Changes detected""
            echo ""should_commit=false"" >> ""$GITHUB_OUTPUT""
          fi
      - name: Re-apply version changes
        if: inputs.force == true || (steps.check_should_commit.outputs.should_commit == 'true' && needs.process_artifacts.outputs.last_version_rn != '')
        env:
          CURRENT_VERSION: ${{ needs.process_artifacts.outputs.current_version_rn }}
          LAST_VERSION: ${{ needs.process_artifacts.outputs.last_version_rn }}
        run: |
          echo ""Re-applying $LAST_VERSION to $CURRENT_VERSION""
          grep -rl ""$LAST_VERSION"" ./compiled-rn || echo ""No files found with $LAST_VERSION""
          grep -rl ""$LAST_VERSION"" ./compiled-rn | xargs -r sed -i -e ""s/$LAST_VERSION/$CURRENT_VERSION/g""
          grep -rl ""$LAST_VERSION"" ./compiled-rn || echo ""Version re-applied""
      - name: Add files for signing
        if: inputs.force == true || steps.check_should_commit.outputs.should_commit == 'true'
        run: |
          echo "":""
          git add .
      - name: Signing files
        if: inputs.force == true || steps.check_should_commit.outputs.should_commit == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            // TODO: Move this to a script file.
            // We currently can't call scripts from the repo because
            // at this point in the workflow, we're on the compiled
            // artifact branch (so the scripts don't exist).
            // We can fix this with a composite action in the main repo.
            // This script is duplicated above.
            const fs = require('fs');
            const crypto = require('crypto');
            const {execSync} = require('child_process');

            // TODO: when we move this to a script, we can use this from npm.
            // Copy of signedsource since we can't install deps on this branch.
            const GENERATED = '@' + 'generated';
            const NEWTOKEN = '<<SignedSource::*O*zOeWoEQle#+L!plEphiEmie@IsG>>';
            const PATTERN = new RegExp(`${GENERATED} (?:SignedSource<<([a-f0-9]{32})>>)`);

            const TokenNotFoundError = new Error(
              `SignedSource.signFile(...): Cannot sign file without token: ${NEWTOKEN}`
            );

            function hash(data, encoding) {
              const md5sum = crypto.createHash('md5');
              md5sum.update(data, encoding);
              return md5sum.digest('hex');
            }

            const SignedSource = {
              getSigningToken() {
                return `${GENERATED} ${NEWTOKEN}`;
              },
              isSigned(data) {
                return PATTERN.exec(data) != null;
              },
              signFile(data) {
                if (!data.includes(NEWTOKEN)) {
                  if (SignedSource.isSigned(data)) {
                    // Signing a file that was previously signed.
                   data = data.replace(PATTERN, SignedSource.getSigningToken());
                  } else {
                    throw TokenNotFoundError;
                  }
                }
                return data.replace(NEWTOKEN, `SignedSource<<${hash(data, 'utf8')}>>`);
              },
            };

            const directory = './compiled-rn';
            console.log('Signing files in directory:', directory);
            try {
              const result = execSync(`git status --porcelain ${directory}`, {encoding: 'utf8'});
              console.log(result);

              // Parse the git status output to get file paths!
              const files = result.split('\n').filter(file => file.endsWith('.js'));

              if (files.length === 0) {
                throw new Error(
                  'git status returned no files to sign. this job should not have run.'
                );
              } else {
                files.forEach(line => {
                  let file = null;
                  if (line.startsWith('D ')) {
                    return;
                  } else if (line.startsWith('R ')) {
                    file = line.slice(line.indexOf('->') + 3);
                  } else {
                    file = line.slice(3).trim();
                  }
                  if (file) {
                    console.log('  Signing file:', file);
                    const originalContents = fs.readFileSync(file, 'utf8');
                    const signedContents = SignedSource.signFile(
                      originalContents
                        // Need to add the header in, since it's not inserted at build time.
                        .replace(' */\n', ` * ${SignedSource.getSigningToken()}\n */\n`)
                    );

                    fs.writeFileSync(file, signedContents, 'utf8');
                  }
                });
              }
            } catch (e) {
              process.exitCode = 1;
              console.error('Error signing files:', e);
            }
      - name: Will commit these changes
        if: inputs.force == true || steps.check_should_commit.outputs.should_commit == 'true'
        run: |
          git add .
          git status
      - name: Check commit message
        if: inputs.dry_run
        run: |
          git fetch origin --quiet
          git show ${{ inputs.commit_sha || github.event.workflow_run.head_sha || github.sha }} --no-patch --pretty=format:""%B""
      - name: Commit changes to branch
        if: inputs.force == true || steps.check_should_commit.outputs.should_commit == 'true'
        run: |
          git config --global user.email ""${{ format('{0}@users.noreply.github.com', github.triggering_actor) }}""
          git config --global user.name ""${{ github.triggering_actor }}""

          git fetch origin --quiet
          git commit -m ""$(git show ${{ inputs.commit_sha || github.event.workflow_run.head_sha || github.sha }} --no-patch --pretty=format:'%B%n%nDiffTrain build for [${{ inputs.commit_sha || github.event.workflow_run.head_sha || github.sha }}](https://github.com/facebook/react/commit/${{ inputs.commit_sha || github.event.workflow_run.head_sha || github.sha}})')"" || echo ""No changes to commit""
      - name: Push changes to branch
        if: inputs.dry_run == false && (inputs.force == true || steps.check_should_commit.outputs.should_commit == 'true')
        run: git push
",471,4,2,"workflow_run, workflow_dispatch",14
facebook/react,runtime_discord_notify.yml,"name: (Runtime) Discord Notify

on:
  pull_request_target:
    types: [opened, ready_for_review]
    paths-ignore:
      - compiler/**
      - .github/workflows/compiler_**.yml

permissions: {}

jobs:
  check_access:
    if: ${{ github.event.pull_request.draft == false }}
    runs-on: ubuntu-latest
    outputs:
      is_member_or_collaborator: ${{ steps.check_is_member_or_collaborator.outputs.is_member_or_collaborator }}
    steps:
      - run: echo ${{ github.event.pull_request.author_association }}
      - name: Check is member or collaborator
        id: check_is_member_or_collaborator
        if: ${{ github.event.pull_request.author_association == 'MEMBER' || github.event.pull_request.author_association == 'COLLABORATOR' }}
        run: echo ""is_member_or_collaborator=true"" >> ""$GITHUB_OUTPUT""

  check_maintainer:
    if: ${{ needs.check_access.outputs.is_member_or_collaborator == 'true' || needs.check_access.outputs.is_member_or_collaborator == true }}
    needs: [check_access]
    uses: facebook/react/.github/workflows/shared_check_maintainer.yml@main
    permissions:
      # Used by check_maintainer
      contents: read
    with:
      actor: ${{ github.event.pull_request.user.login }}

  notify:
    if: ${{ needs.check_maintainer.outputs.is_core_team == 'true' }}
    needs: check_maintainer
    runs-on: ubuntu-latest
    steps:
      - name: Discord Webhook Action
        uses: tsickert/discord-webhook@86dc739f3f165f16dadc5666051c367efa1692f4
        with:
          webhook-url: ${{ secrets.DISCORD_WEBHOOK_URL }}
          embed-author-name: ${{ github.event.pull_request.user.login }}
          embed-author-url: ${{ github.event.pull_request.user.html_url }}
          embed-author-icon-url: ${{ github.event.pull_request.user.avatar_url }}
          embed-title: '#${{ github.event.number }} (+${{github.event.pull_request.additions}} -${{github.event.pull_request.deletions}}): ${{ github.event.pull_request.title }}'
          embed-description: ${{ github.event.pull_request.body }}
          embed-url: ${{ github.event.pull_request.html_url }}
",49,3,1,pull_request_target,2
facebook/react,runtime_eslint_plugin_e2e.yml,"name: (Runtime) ESLint Plugin E2E

on:
  push:
    branches: [main]
  pull_request:
    paths-ignore:
      - compiler/**

permissions: {}

concurrency:
  group: ${{ github.workflow }}-${{ github.ref_name }}-${{ github.event.pull_request.number || github.run_id }}
  cancel-in-progress: true

env:
  TZ: /usr/share/zoneinfo/America/Los_Angeles

jobs:
  # ----- TESTS -----
  test:
    name: ESLint v${{ matrix.eslint_major }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        eslint_major:
          - ""6""
          - ""7""
          - ""8""
          - ""9""
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: |
            yarn.lock
            compiler/yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-and-compiler-eslint_e2e-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock', 'compiler/yarn.lock', 'fixtures/eslint-v*/yarn.lock') }}
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: yarn --cwd compiler install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - name: Install fixture dependencies
        working-directory: ./fixtures/eslint-v${{ matrix.eslint_major }}
        run: yarn --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - name: Build plugin
        working-directory: fixtures/eslint-v${{ matrix.eslint_major }}
        run: node build.mjs
      - name: Run lint test
        working-directory: ./fixtures/eslint-v${{ matrix.eslint_major }}
        run: yarn lint
",65,1,2,"push, pull_request",3
facebook/react,runtime_fuzz_tests.yml,"name: (Runtime) Fuzz tests

on:
  schedule:
    - cron: 0 * * * *
  push:
    branches:
      - main
  workflow_dispatch:

permissions: {}

env:
  TZ: /usr/share/zoneinfo/America/Los_Angeles

jobs:
  test_fuzz:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4.1.0
    - uses: actions/setup-node@v4
      with:
        node-version-file: '.nvmrc'
        cache: 'yarn'
    - name: Install dependencies
      run: yarn install --frozen-lockfile
      env:
        ELECTRON_SKIP_BINARY_DOWNLOAD: ""1""
      shell: bash
    - name: Run fuzz tests
      run: |-
        FUZZ_TEST_SEED=$RANDOM yarn test fuzz --ci
        FUZZ_TEST_SEED=$RANDOM yarn test --prod fuzz --ci
",33,1,3,"schedule, push, workflow_dispatch",2
facebook/react,runtime_prereleases.yml,"name: (Runtime) Publish Prereleases

on:
  workflow_call:
    inputs:
      commit_sha:
        required: true
        default: ''
        type: string
      release_channel:
        required: true
        type: string
      dist_tag:
        required: true
        type: string
      enableFailureNotification:
        description: 'Whether to notify the team on Discord when the release fails. Useful if this workflow is called from an automation.'
        required: false
        type: boolean
      only_packages:
        description: Packages to publish (space separated)
        type: string
      skip_packages:
        description: Packages to NOT publish (space separated)
        type: string
      dry:
        required: true
        description: Dry run instead of publish?
        type: boolean
        default: true
    secrets:
      DISCORD_WEBHOOK_URL:
        description: 'Discord webhook URL to notify on failure. Only required if enableFailureNotification is true.'
        required: false
      GH_TOKEN:
        required: true
      NPM_TOKEN:
        required: true

permissions: {}

env:
  TZ: /usr/share/zoneinfo/America/Los_Angeles
  # https://github.com/actions/cache/blob/main/tips-and-workarounds.md#cache-segment-restore-timeout
  SEGMENT_DOWNLOAD_TIMEOUT_MINS: 1
  NPM_TOKEN: ${{ secrets.NPM_TOKEN }}

jobs:
  publish_prerelease:
    name: Publish prelease (${{ inputs.release_channel }}) ${{ inputs.commit_sha }} @${{ inputs.dist_tag }}
    runs-on: ubuntu-latest
    permissions:
      # We use github.token to download the build artifact from a previous runtime_build_and_test.yml run
      actions: read
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-release-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock', 'scripts/release/yarn.lock') }}
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: yarn --cwd scripts/release install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: cp ./scripts/release/ci-npmrc ~/.npmrc
      - run: |
          GH_TOKEN=${{ secrets.GH_TOKEN }} scripts/release/prepare-release-from-ci.js --skipTests -r ${{ inputs.release_channel }} --commit=${{ inputs.commit_sha }}
      - name: Check prepared files
        run: ls -R build/node_modules
      - if: '${{ inputs.only_packages }}'
        name: 'Publish ${{ inputs.only_packages }}'
        run: |
          scripts/release/publish.js \
            --ci \
            --skipTests \
            --tags=${{ inputs.dist_tag }} \
            --onlyPackages=${{ inputs.only_packages }} ${{ (inputs.dry && '') || '\'}}
            ${{ inputs.dry && '--dry' || '' }}
      - if: '${{ inputs.skip_packages }}'
        name: 'Publish all packages EXCEPT ${{ inputs.skip_packages }}'
        run: |
          scripts/release/publish.js \
            --ci \
            --skipTests \
            --tags=${{ inputs.dist_tag }} \
            --skipPackages=${{ inputs.skip_packages }} ${{ (inputs.dry && '') || '\'}}
            ${{ inputs.dry && '--dry' || '' }}
      - if: '${{ !(inputs.skip_packages && inputs.only_packages) }}'
        name: 'Publish all packages'
        run: |
          scripts/release/publish.js \
            --ci \
            --tags=${{ inputs.dist_tag }} ${{ (inputs.dry && '') || '\'}}
            ${{ inputs.dry && '--dry' || '' }}
      - name: Notify Discord on failure
        if: failure() && inputs.enableFailureNotification == true
        uses: tsickert/discord-webhook@86dc739f3f165f16dadc5666051c367efa1692f4
        with:
          webhook-url: ${{ secrets.DISCORD_WEBHOOK_URL }}
          embed-author-name: ""GitHub Actions""
          embed-title: '[Runtime] Publish of ${{ inputs.release_channel }}@${{ inputs.dist_tag}} release failed'
          embed-url: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}/attempts/${{ github.run_attempt }}
",112,1,1,workflow_call,4
facebook/react,runtime_prereleases_manual.yml,"name: (Runtime) Publish Prereleases Manual

on:
  workflow_dispatch:
    inputs:
      prerelease_commit_sha:
        required: true
      only_packages:
        description: Packages to publish (space separated)
        type: string
      skip_packages:
        description: Packages to NOT publish (space separated)
        type: string
      dry:
        required: true
        description: Dry run instead of publish?
        type: boolean
        default: true
      experimental_only:
        type: boolean
        description: Only publish to the experimental tag
        default: false
      force_notify:
        description: Force a Discord notification?
        type: boolean
        default: false

permissions: {}

env:
  TZ: /usr/share/zoneinfo/America/Los_Angeles

jobs:
  notify:
    if: ${{ inputs.force_notify || inputs.dry == false || inputs.dry == 'false' }}
    runs-on: ubuntu-latest
    steps:
      - name: Discord Webhook Action
        uses: tsickert/discord-webhook@86dc739f3f165f16dadc5666051c367efa1692f4
        with:
          webhook-url: ${{ secrets.DISCORD_WEBHOOK_URL }}
          embed-author-name: ${{ github.event.sender.login }}
          embed-author-url: ${{ github.event.sender.html_url }}
          embed-author-icon-url: ${{ github.event.sender.avatar_url }}
          embed-title: ""⚠️ Publishing ${{ inputs.experimental_only && 'EXPERIMENTAL' || 'CANARY & EXPERIMENTAL' }} release ${{ (inputs.dry && ' (dry run)') || '' }}""
          embed-description: |
            ```json
            ${{ toJson(inputs) }}
            ```
          embed-url: https://github.com/facebook/react/actions/runs/${{ github.run_id }}

  publish_prerelease_canary:
    if: ${{ !inputs.experimental_only }}
    name: Publish to Canary channel
    uses: facebook/react/.github/workflows/runtime_prereleases.yml@main
    permissions:
      # We use github.token to download the build artifact from a previous runtime_build_and_test.yml run
      actions: read
    with:
      commit_sha: ${{ inputs.prerelease_commit_sha }}
      release_channel: stable
      # The tags to use when publishing canaries. The main one we should
      # always include is ""canary"" but we can use multiple (e.g. alpha,
      # beta, rc). To declare multiple, use a comma-separated string, like
      # this:
      #   dist_tag: ""canary,alpha,beta,rc""
      #
      # TODO: We currently tag canaries with ""next"" in addition to ""canary""
      # because this used to be called the ""next"" channel and some
      # downstream consumers might still expect that tag. We can remove this
      # after some time has elapsed and the change has been communicated.
      dist_tag: canary,next
      only_packages: ${{ inputs.only_packages }}
      skip_packages: ${{ inputs.skip_packages }}
      dry: ${{ inputs.dry }}
    secrets:
      NPM_TOKEN: ${{ secrets.NPM_TOKEN }}
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  publish_prerelease_experimental:
    name: Publish to Experimental channel
    uses: facebook/react/.github/workflows/runtime_prereleases.yml@main
    permissions:
      # We use github.token to download the build artifact from a previous runtime_build_and_test.yml run
      actions: read
    # NOTE: Intentionally running these jobs sequentially because npm
    # will sometimes fail if you try to concurrently publish two
    # different versions of the same package, even if they use different
    # dist tags.
    needs: publish_prerelease_canary
    # Ensures the job runs even if canary is skipped
    if: always()
    with:
      commit_sha: ${{ inputs.prerelease_commit_sha }}
      release_channel: experimental
      dist_tag: experimental
      only_packages: ${{ inputs.only_packages }}
      skip_packages: ${{ inputs.skip_packages }}
      dry: ${{ inputs.dry }}
    secrets:
      NPM_TOKEN: ${{ secrets.NPM_TOKEN }}
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
",102,3,1,workflow_dispatch,3
facebook/react,runtime_prereleases_nightly.yml,"name: (Runtime) Publish Prereleases Nightly

on:
  schedule:
    # At 10 minutes past 16:00 on Mon, Tue, Wed, Thu, and Fri
    - cron: 10 16 * * 1,2,3,4,5

permissions: {}

env:
  TZ: /usr/share/zoneinfo/America/Los_Angeles

jobs:
  publish_prerelease_canary:
    name: Publish to Canary channel
    uses: facebook/react/.github/workflows/runtime_prereleases.yml@main
    permissions:
      # We use github.token to download the build artifact from a previous runtime_build_and_test.yml run
      actions: read
    with:
      commit_sha: ${{ github.sha }}
      release_channel: stable
      dist_tag: canary,next
      enableFailureNotification: true
      dry: false
    secrets:
      DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
      NPM_TOKEN: ${{ secrets.NPM_TOKEN }}
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  publish_prerelease_experimental:
    name: Publish to Experimental channel
    uses: facebook/react/.github/workflows/runtime_prereleases.yml@main
    permissions:
      # We use github.token to download the build artifact from a previous runtime_build_and_test.yml run
      actions: read
    # NOTE: Intentionally running these jobs sequentially because npm
    # will sometimes fail if you try to concurrently publish two
    # different versions of the same package, even if they use different
    # dist tags.
    needs: publish_prerelease_canary
    with:
      commit_sha: ${{ github.sha }}
      release_channel: experimental
      dist_tag: experimental
      enableFailureNotification: true
      dry: false
    secrets:
      DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
      NPM_TOKEN: ${{ secrets.NPM_TOKEN }}
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
",51,2,1,schedule,2
facebook/react,runtime_releases_from_npm_manual.yml,"name: (Runtime) Publish Releases from NPM Manual

on:
  workflow_dispatch:
    inputs:
      version_to_promote:
        required: true
        description: Current npm version (non-experimental) to promote
        type: string
      version_to_publish:
        required: true
        description: Version to publish for the specified packages
        type: string
      only_packages:
        description: Packages to publish (space separated)
        type: string
      skip_packages:
        description: Packages to NOT publish (space separated)
        type: string
      tags:
        description: NPM tags (space separated)
        type: string
        default: untagged
      dry:
        required: true
        description: Dry run instead of publish?
        type: boolean
        default: true
      force_notify:
        description: Force a Discord notification?
        type: boolean
        default: false

permissions: {}

env:
  TZ: /usr/share/zoneinfo/America/Los_Angeles
  # https://github.com/actions/cache/blob/main/tips-and-workarounds.md#cache-segment-restore-timeout
  SEGMENT_DOWNLOAD_TIMEOUT_MINS: 1
  NPM_TOKEN: ${{ secrets.NPM_TOKEN }}

jobs:
  notify:
    if: ${{ inputs.force_notify || inputs.dry == false || inputs.dry == 'false' }}
    runs-on: ubuntu-latest
    steps:
      - name: Discord Webhook Action
        uses: tsickert/discord-webhook@86dc739f3f165f16dadc5666051c367efa1692f4
        with:
          webhook-url: ${{ secrets.DISCORD_WEBHOOK_URL }}
          embed-author-name: ${{ github.event.sender.login }}
          embed-author-url: ${{ github.event.sender.html_url }}
          embed-author-icon-url: ${{ github.event.sender.avatar_url }}
          embed-title: ""⚠️ Publishing release from NPM${{ (inputs.dry && ' (dry run)') || '' }}""
          embed-description: |
            ```json
            ${{ toJson(inputs) }}
            ```
          embed-url: https://github.com/facebook/react/actions/runs/${{ github.run_id }}

  publish:
    name: Publish releases
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: runtime-release-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('yarn.lock', 'scripts/release/yarn.lock') }}
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: yarn --cwd scripts/release install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: cp ./scripts/release/ci-npmrc ~/.npmrc
      - if: '${{ inputs.only_packages }}'
        name: 'Prepare ${{ inputs.only_packages }} from NPM'
        run: |
          scripts/release/prepare-release-from-npm.js \
            --ci \
            --skipTests \
            --version=${{ inputs.version_to_promote }} \
            --publishVersion=${{ inputs.version_to_publish }} \
            --onlyPackages=${{ inputs.only_packages }}
      - if: '${{ inputs.skip_packages }}'
        name: 'Prepare all packages EXCEPT ${{ inputs.skip_packages }} from NPM'
        run: |
          scripts/release/prepare-release-from-npm.js \
            --ci \
            --skipTests \
            --version=${{ inputs.version_to_promote }} \
            --publishVersion=${{ inputs.version_to_publish }} \
            --skipPackages=${{ inputs.skip_packages }}
      - name: Check prepared files
        run: ls -R build/node_modules
      - if: '${{ inputs.only_packages }}'
        name: 'Publish ${{ inputs.only_packages }}'
        run: |
          scripts/release/publish.js \
            --ci \
            --tags=${{ inputs.tags }} \
            --publishVersion=${{ inputs.version_to_publish }} \
            --onlyPackages=${{ inputs.only_packages }} ${{ (inputs.dry && '') || '\'}}
            ${{ inputs.dry && '--dry' || '' }}
      - if: '${{ inputs.skip_packages }}'
        name: 'Publish all packages EXCEPT ${{ inputs.skip_packages }}'
        run: |
          scripts/release/publish.js \
            --ci \
            --tags=${{ inputs.tags }} \
            --publishVersion=${{ inputs.version_to_publish }} \
            --skipPackages=${{ inputs.skip_packages }} ${{ (inputs.dry && '') || '\'}}
            ${{ inputs.dry && '--dry' || '' }}
      - name: Archive released package for debugging
        uses: actions/upload-artifact@v4
        with:
          name: build
          path: |
            ./build/node_modules
",128,2,1,workflow_dispatch,5
facebook/react,shared_check_maintainer.yml,"name: (Shared) Check maintainer

on:
  workflow_call:
    inputs:
      actor:
        required: true
        type: string
    outputs:
      is_core_team:
        value: ${{ jobs.check_maintainer.outputs.is_core_team }}

permissions: {}

env:
  TZ: /usr/share/zoneinfo/America/Los_Angeles
  # https://github.com/actions/cache/blob/main/tips-and-workarounds.md#cache-segment-restore-timeout
  SEGMENT_DOWNLOAD_TIMEOUT_MINS: 1

jobs:
  check_maintainer:
    runs-on: ubuntu-latest
    permissions:
      # We fetch the contents of the MAINTAINERS file
      contents: read
    outputs:
      is_core_team: ${{ steps.check_if_actor_is_maintainer.outputs.result }}
    steps:
      - name: Check if actor is maintainer
        id: check_if_actor_is_maintainer
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const actor = '${{ inputs.actor }}';
            const res = await github.rest.repos.getContent({
              owner: 'facebook',
              repo: 'react',
              path: 'MAINTAINERS',
              ref: 'main',
              headers: { Accept: 'application/vnd.github+json' }
            });
            if (res.status !== 200) {
              console.error(res);
              throw new Error('Unable to fetch MAINTAINERS file');
            }
            content = Buffer.from(res.data.content, 'base64').toString();
            if (content == null || typeof content !== 'string') {
              throw new Error('Unable to retrieve MAINTAINERS file');
            }

            const maintainers = new Set(content.split('\n'));
            if (maintainers.has(actor)) {
              console.log(`🟢 ${actor} is a maintainer`);
              return true;
            }
            console.log(`🔴 ${actor} is NOT a maintainer`);
            return null;
",58,1,1,workflow_call,1
facebook/react,shared_cleanup_merged_branch_caches.yml,"# https://github.com/actions/cache/blob/main/tips-and-workarounds.md#force-deletion-of-caches-overriding-default-cache-eviction-policy

name: (Shared) Cleanup Merged Branch Caches
on:
  pull_request:
    types:
      - closed
  workflow_dispatch:
    inputs:
      pr_number:
        required: true
        type: string

permissions: {}

jobs:
  cleanup:
    runs-on: ubuntu-latest
    permissions:
      # `actions:write` permission is required to delete caches
      #   See also: https://docs.github.com/en/rest/actions/cache?apiVersion=2022-11-28#delete-a-github-actions-cache-for-a-repository-using-a-cache-id
      actions: write
      contents: read
    steps:
      - name: Cleanup
        run: |
          echo ""Fetching list of cache key""
          cacheKeysForPR=$(gh cache list --ref $BRANCH --limit 100 --json id --jq '.[].id')

          ## Setting this to not fail the workflow while deleting cache keys.
          set +e
          for cacheKey in $cacheKeysForPR
          do
              gh cache delete $cacheKey
              echo ""Deleting $cacheKey""
          done
          echo ""Done""
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GH_REPO: ${{ github.repository }}
          BRANCH: refs/pull/${{ inputs.pr_number || github.event.pull_request.number }}/merge
",41,1,2,"pull_request, workflow_dispatch",0
facebook/react,shared_cleanup_stale_branch_caches.yml,"# https://github.com/actions/cache/blob/main/tips-and-workarounds.md#force-deletion-of-caches-overriding-default-cache-eviction-policy

name: (Shared) Cleanup Stale Branch Caches
on:
  schedule:
    # Every 6 hours
    - cron: 0 */6 * * *
  workflow_dispatch:

permissions: {}

jobs:
  cleanup:
    runs-on: ubuntu-latest
    permissions:
      # `actions:write` permission is required to delete caches
      #   See also: https://docs.github.com/en/rest/actions/cache?apiVersion=2022-11-28#delete-a-github-actions-cache-for-a-repository-using-a-cache-id
      actions: write
      contents: read
    steps:
      - name: Cleanup
        run: |
          echo ""Fetching list of cache keys""
          cacheKeysForPR=$(gh cache list --limit 100 --json id,ref --jq '.[] | select(.ref != ""refs/heads/main"") | .id')

          ## Setting this to not fail the workflow while deleting cache keys.
          set +e
          for cacheKey in $cacheKeysForPR
          do
              gh cache delete $cacheKey
              echo ""Deleting $cacheKey""
          done
          echo ""Done""
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GH_REPO: ${{ github.repository }}
",36,1,2,"schedule, workflow_dispatch",0
facebook/react,shared_close_direct_sync_branch_prs.yml,"name: (Shared) Close Direct Sync Branch PRs

on:
  pull_request:
    branches:
      - 'builds/facebook-**'

permissions: {}

env:
  TZ: /usr/share/zoneinfo/America/Los_Angeles
  # https://github.com/actions/cache/blob/main/tips-and-workarounds.md#cache-segment-restore-timeout
  SEGMENT_DOWNLOAD_TIMEOUT_MINS: 1

jobs:
  close_pr:
    runs-on: ubuntu-latest
    permissions:
      # Used to create a review and close PRs
      pull-requests: write
    steps:
      - name: Close PR
        uses: actions/github-script@v7
        with:
          script: |
            const owner = context.repo.owner;
            const repo = context.repo.repo;
            const pullNumber = ${{ github.event.number }};

            await github.rest.pulls.createReview({
              owner,
              repo,
              pull_number: pullNumber,
              body: 'Do not land changes to `${{ github.event.pull_request.base.ref }}`. Please re-open your PR targeting `main` instead.',
              event: 'REQUEST_CHANGES'
            });
            await github.rest.pulls.update({
              owner,
              repo,
              pull_number: pullNumber,
              state: 'closed'
            });
",42,1,1,pull_request,1
facebook/react,shared_label_core_team_prs.yml,"name: (Shared) Label Core Team PRs

on:
  pull_request_target:
    types: [opened]

permissions: {}

env:
  TZ: /usr/share/zoneinfo/America/Los_Angeles
  # https://github.com/actions/cache/blob/main/tips-and-workarounds.md#cache-segment-restore-timeout
  SEGMENT_DOWNLOAD_TIMEOUT_MINS: 1

jobs:
  check_access:
    runs-on: ubuntu-latest
    outputs:
      is_member_or_collaborator: ${{ steps.check_is_member_or_collaborator.outputs.is_member_or_collaborator }}
    steps:
      - run: echo ${{ github.event.pull_request.author_association }}
      - name: Check is member or collaborator
        id: check_is_member_or_collaborator
        if: ${{ github.event.pull_request.author_association == 'MEMBER' || github.event.pull_request.author_association == 'COLLABORATOR' }}
        run: echo ""is_member_or_collaborator=true"" >> ""$GITHUB_OUTPUT""

  check_maintainer:
    if: ${{ needs.check_access.outputs.is_member_or_collaborator == 'true' || needs.check_access.outputs.is_member_or_collaborator == true }}
    needs: [check_access]
    uses: facebook/react/.github/workflows/shared_check_maintainer.yml@main
    permissions:
      # Used by check_maintainer
      contents: read
    with:
      actor: ${{ github.event.pull_request.user.login }}

  label:
    if: ${{ needs.check_maintainer.outputs.is_core_team == 'true' }}
    runs-on: ubuntu-latest
    needs: check_maintainer
    permissions:
      # Used to add labels on issues
      issues: write
      # Used to add labels on PRs
      pull-requests: write
    steps:
      - name: Label PR as React Core Team
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.addLabels({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: ${{ github.event.number }},
              labels: ['React Core Team']
            });
",55,3,1,pull_request_target,2
facebook/react,shared_lint.yml,"name: (Shared) Lint

on:
  push:
    branches: [main]
  pull_request:

permissions: {}

concurrency:
  group: ${{ github.workflow }}-${{ github.ref_name }}-${{ github.event.pull_request.number || github.run_id }}
  cancel-in-progress: true

env:
  TZ: /usr/share/zoneinfo/America/Los_Angeles
  # https://github.com/actions/cache/blob/main/tips-and-workarounds.md#cache-segment-restore-timeout
  SEGMENT_DOWNLOAD_TIMEOUT_MINS: 1

jobs:
  prettier:
    name: Run prettier
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: shared-lint-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('**/yarn.lock') }}
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: yarn prettier-check

  eslint:
    name: Run eslint
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: shared-lint-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('**/yarn.lock') }}
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: node ./scripts/tasks/eslint

  check_license:
    name: Check license
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: shared-lint-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('**/yarn.lock') }}
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: ./scripts/ci/check_license.sh

  test_print_warnings:
    name: Test print warnings
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: yarn
          cache-dependency-path: yarn.lock
      - name: Restore cached node_modules
        uses: actions/cache@v4
        id: node_modules
        with:
          path: |
            **/node_modules
          key: shared-lint-node_modules-v6-${{ runner.arch }}-${{ runner.os }}-${{ hashFiles('**/yarn.lock') }}
      - name: Ensure clean build directory
        run: rm -rf build
      - run: yarn install --frozen-lockfile
        if: steps.node_modules.outputs.cache-hit != 'true'
      - run: ./scripts/ci/test_print_warnings.sh
",110,4,2,"push, pull_request",12
facebook/react,shared_stale.yml,"# Configuration for stale action workflow - https://github.com/actions/stale
name: (Shared) Manage stale issues and PRs
on:
  schedule:
    # Run hourly
    - cron: '0 * * * *'
  workflow_dispatch:

permissions:
  # https://github.com/actions/stale/tree/v9/?tab=readme-ov-file#recommended-permissions
  issues: write
  pull-requests: write

env:
  TZ: /usr/share/zoneinfo/America/Los_Angeles

jobs:
  stale:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/stale@v9
        with:
          # --- Issues & PRs ---
          # Number of days of inactivity before an issue or PR becomes stale
          days-before-stale: 90
          # Number of days of inactivity before a stale issue or PR is closed
          days-before-close: 7
          # API calls per run
          operations-per-run: 100

          # --- Issues ---
          stale-issue-label: ""Resolution: Stale""
          # Comment to post when marking an issue as stale
          stale-issue-message: >
            This issue has been automatically marked as stale.
            **If this issue is still affecting you, please leave any comment** (for example, ""bump""), and we'll keep it open.
            We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!
          # Comment to post when closing a stale issue
          close-issue-message: >
            Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!
          # Issues with these labels will never be considered stale
          exempt-issue-labels: ""Partner,React Core Team,Resolution: Backlog,Type: Bug,Type: Discussion,Type: Needs Investigation,Type: Regression,Type: Feature Request,Type: Enhancement""

          # --- PRs ---
          stale-pr-label: ""Resolution: Stale""
          # Comment to post when marking a pull request as stale
          stale-pr-message: >
            This pull request has been automatically marked as stale.
            **If this pull request is still relevant, please leave any comment** (for example, ""bump""), and we'll keep it open.
            We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.
          # Comment to post when closing a stale pull request
          close-pr-message: >
            Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!
          # PRs with these labels will never be considered stale
          exempt-pr-labels: ""Partner,React Core Team,Resolution: Backlog,Type: Bug,Type: Discussion,Type: Needs Investigation,Type: Regression,Type: Feature Request,Type: Enhancement""
",55,1,2,"schedule, workflow_dispatch",1
facebook/fbthrift,getdeps_linux.yml,"# This file was @generated by getdeps.py

name: linux

on:
  push:
    branches:
    - main
  pull_request:
    branches:
    - main

permissions:
  contents: read  #  to fetch code (actions/checkout)

jobs:
  build:
    runs-on: ubuntu-22.04
    steps:
    - uses: actions/checkout@v4
    - name: Show disk space at start
      run: df -h
    - name: Free up disk space
      run: sudo rm -rf /usr/local/lib/android
    - name: Show disk space after freeing up
      run: df -h
    - name: Update system package info
      run: sudo --preserve-env=http_proxy apt-get update
    - name: Install system deps
      run: sudo --preserve-env=http_proxy python3 build/fbcode_builder/getdeps.py --allow-system-packages install-system-deps --recursive fbthrift && sudo --preserve-env=http_proxy python3 build/fbcode_builder/getdeps.py --allow-system-packages install-system-deps --recursive patchelf
    - id: paths
      name: Query paths
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages query-paths --recursive --src-dir=. fbthrift  >> ""$GITHUB_OUTPUT""
    - name: Fetch xxhash
      if: ${{ steps.paths.outputs.xxhash_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests xxhash
    - name: Fetch ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests ninja
    - name: Fetch cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests cmake
    - name: Fetch zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests zlib
    - name: Fetch zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests zstd
    - name: Fetch fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests fmt
    - name: Fetch boost
      if: ${{ steps.paths.outputs.boost_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests boost
    - name: Fetch double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests double-conversion
    - name: Fetch fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests fast_float
    - name: Fetch gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests gflags
    - name: Fetch glog
      if: ${{ steps.paths.outputs.glog_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests glog
    - name: Fetch googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests googletest
    - name: Fetch libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libdwarf
    - name: Fetch libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libevent
    - name: Fetch lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests lz4
    - name: Fetch snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests snappy
    - name: Fetch openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests openssl
    - name: Fetch liboqs
      if: ${{ steps.paths.outputs.liboqs_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests liboqs
    - name: Fetch autoconf
      if: ${{ steps.paths.outputs.autoconf_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests autoconf
    - name: Fetch automake
      if: ${{ steps.paths.outputs.automake_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests automake
    - name: Fetch libtool
      if: ${{ steps.paths.outputs.libtool_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libtool
    - name: Fetch libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libsodium
    - name: Fetch libiberty
      if: ${{ steps.paths.outputs.libiberty_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libiberty
    - name: Fetch libunwind
      if: ${{ steps.paths.outputs.libunwind_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libunwind
    - name: Fetch xz
      if: ${{ steps.paths.outputs.xz_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests xz
    - name: Fetch folly
      if: ${{ steps.paths.outputs.folly_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests folly
    - name: Fetch fizz
      if: ${{ steps.paths.outputs.fizz_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests fizz
    - name: Fetch mvfst
      if: ${{ steps.paths.outputs.mvfst_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests mvfst
    - name: Fetch wangle
      if: ${{ steps.paths.outputs.wangle_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests wangle
    - name: Restore xxhash from cache
      id: restore_xxhash
      if: ${{ steps.paths.outputs.xxhash_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.xxhash_INSTALL }}
       key: ${{ steps.paths.outputs.xxhash_CACHE_KEY }}-install
    - name: Build xxhash
      if: ${{ steps.paths.outputs.xxhash_SOURCE && ! steps.restore_xxhash.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests xxhash
    - name: Save xxhash to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.xxhash_SOURCE && ! steps.restore_xxhash.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.xxhash_INSTALL }}
       key: ${{ steps.paths.outputs.xxhash_CACHE_KEY }}-install
    - name: Restore ninja from cache
      id: restore_ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.ninja_INSTALL }}
       key: ${{ steps.paths.outputs.ninja_CACHE_KEY }}-install
    - name: Build ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE && ! steps.restore_ninja.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests ninja
    - name: Save ninja to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.ninja_SOURCE && ! steps.restore_ninja.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.ninja_INSTALL }}
       key: ${{ steps.paths.outputs.ninja_CACHE_KEY }}-install
    - name: Restore cmake from cache
      id: restore_cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.cmake_INSTALL }}
       key: ${{ steps.paths.outputs.cmake_CACHE_KEY }}-install
    - name: Build cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE && ! steps.restore_cmake.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests cmake
    - name: Save cmake to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.cmake_SOURCE && ! steps.restore_cmake.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.cmake_INSTALL }}
       key: ${{ steps.paths.outputs.cmake_CACHE_KEY }}-install
    - name: Restore zlib from cache
      id: restore_zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.zlib_INSTALL }}
       key: ${{ steps.paths.outputs.zlib_CACHE_KEY }}-install
    - name: Build zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE && ! steps.restore_zlib.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests zlib
    - name: Save zlib to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.zlib_SOURCE && ! steps.restore_zlib.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.zlib_INSTALL }}
       key: ${{ steps.paths.outputs.zlib_CACHE_KEY }}-install
    - name: Restore zstd from cache
      id: restore_zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.zstd_INSTALL }}
       key: ${{ steps.paths.outputs.zstd_CACHE_KEY }}-install
    - name: Build zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE && ! steps.restore_zstd.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests zstd
    - name: Save zstd to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.zstd_SOURCE && ! steps.restore_zstd.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.zstd_INSTALL }}
       key: ${{ steps.paths.outputs.zstd_CACHE_KEY }}-install
    - name: Restore fmt from cache
      id: restore_fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fmt_INSTALL }}
       key: ${{ steps.paths.outputs.fmt_CACHE_KEY }}-install
    - name: Build fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE && ! steps.restore_fmt.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests fmt
    - name: Save fmt to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fmt_SOURCE && ! steps.restore_fmt.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fmt_INSTALL }}
       key: ${{ steps.paths.outputs.fmt_CACHE_KEY }}-install
    - name: Restore boost from cache
      id: restore_boost
      if: ${{ steps.paths.outputs.boost_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.boost_INSTALL }}
       key: ${{ steps.paths.outputs.boost_CACHE_KEY }}-install
    - name: Build boost
      if: ${{ steps.paths.outputs.boost_SOURCE && ! steps.restore_boost.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests boost
    - name: Save boost to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.boost_SOURCE && ! steps.restore_boost.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.boost_INSTALL }}
       key: ${{ steps.paths.outputs.boost_CACHE_KEY }}-install
    - name: Restore double-conversion from cache
      id: restore_double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.double-conversion_INSTALL }}
       key: ${{ steps.paths.outputs.double-conversion_CACHE_KEY }}-install
    - name: Build double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE && ! steps.restore_double-conversion.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests double-conversion
    - name: Save double-conversion to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.double-conversion_SOURCE && ! steps.restore_double-conversion.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.double-conversion_INSTALL }}
       key: ${{ steps.paths.outputs.double-conversion_CACHE_KEY }}-install
    - name: Restore fast_float from cache
      id: restore_fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fast_float_INSTALL }}
       key: ${{ steps.paths.outputs.fast_float_CACHE_KEY }}-install
    - name: Build fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE && ! steps.restore_fast_float.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests fast_float
    - name: Save fast_float to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fast_float_SOURCE && ! steps.restore_fast_float.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fast_float_INSTALL }}
       key: ${{ steps.paths.outputs.fast_float_CACHE_KEY }}-install
    - name: Restore gflags from cache
      id: restore_gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.gflags_INSTALL }}
       key: ${{ steps.paths.outputs.gflags_CACHE_KEY }}-install
    - name: Build gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE && ! steps.restore_gflags.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests gflags
    - name: Save gflags to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.gflags_SOURCE && ! steps.restore_gflags.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.gflags_INSTALL }}
       key: ${{ steps.paths.outputs.gflags_CACHE_KEY }}-install
    - name: Restore glog from cache
      id: restore_glog
      if: ${{ steps.paths.outputs.glog_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.glog_INSTALL }}
       key: ${{ steps.paths.outputs.glog_CACHE_KEY }}-install
    - name: Build glog
      if: ${{ steps.paths.outputs.glog_SOURCE && ! steps.restore_glog.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests glog
    - name: Save glog to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.glog_SOURCE && ! steps.restore_glog.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.glog_INSTALL }}
       key: ${{ steps.paths.outputs.glog_CACHE_KEY }}-install
    - name: Restore googletest from cache
      id: restore_googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.googletest_INSTALL }}
       key: ${{ steps.paths.outputs.googletest_CACHE_KEY }}-install
    - name: Build googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE && ! steps.restore_googletest.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests googletest
    - name: Save googletest to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.googletest_SOURCE && ! steps.restore_googletest.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.googletest_INSTALL }}
       key: ${{ steps.paths.outputs.googletest_CACHE_KEY }}-install
    - name: Restore libdwarf from cache
      id: restore_libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libdwarf_INSTALL }}
       key: ${{ steps.paths.outputs.libdwarf_CACHE_KEY }}-install
    - name: Build libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE && ! steps.restore_libdwarf.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests libdwarf
    - name: Save libdwarf to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libdwarf_SOURCE && ! steps.restore_libdwarf.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libdwarf_INSTALL }}
       key: ${{ steps.paths.outputs.libdwarf_CACHE_KEY }}-install
    - name: Restore libevent from cache
      id: restore_libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libevent_INSTALL }}
       key: ${{ steps.paths.outputs.libevent_CACHE_KEY }}-install
    - name: Build libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE && ! steps.restore_libevent.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests libevent
    - name: Save libevent to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libevent_SOURCE && ! steps.restore_libevent.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libevent_INSTALL }}
       key: ${{ steps.paths.outputs.libevent_CACHE_KEY }}-install
    - name: Restore lz4 from cache
      id: restore_lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.lz4_INSTALL }}
       key: ${{ steps.paths.outputs.lz4_CACHE_KEY }}-install
    - name: Build lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE && ! steps.restore_lz4.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests lz4
    - name: Save lz4 to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.lz4_SOURCE && ! steps.restore_lz4.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.lz4_INSTALL }}
       key: ${{ steps.paths.outputs.lz4_CACHE_KEY }}-install
    - name: Restore snappy from cache
      id: restore_snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.snappy_INSTALL }}
       key: ${{ steps.paths.outputs.snappy_CACHE_KEY }}-install
    - name: Build snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE && ! steps.restore_snappy.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests snappy
    - name: Save snappy to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.snappy_SOURCE && ! steps.restore_snappy.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.snappy_INSTALL }}
       key: ${{ steps.paths.outputs.snappy_CACHE_KEY }}-install
    - name: Restore openssl from cache
      id: restore_openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.openssl_INSTALL }}
       key: ${{ steps.paths.outputs.openssl_CACHE_KEY }}-install
    - name: Build openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE && ! steps.restore_openssl.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests openssl
    - name: Save openssl to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.openssl_SOURCE && ! steps.restore_openssl.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.openssl_INSTALL }}
       key: ${{ steps.paths.outputs.openssl_CACHE_KEY }}-install
    - name: Restore liboqs from cache
      id: restore_liboqs
      if: ${{ steps.paths.outputs.liboqs_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.liboqs_INSTALL }}
       key: ${{ steps.paths.outputs.liboqs_CACHE_KEY }}-install
    - name: Build liboqs
      if: ${{ steps.paths.outputs.liboqs_SOURCE && ! steps.restore_liboqs.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests liboqs
    - name: Save liboqs to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.liboqs_SOURCE && ! steps.restore_liboqs.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.liboqs_INSTALL }}
       key: ${{ steps.paths.outputs.liboqs_CACHE_KEY }}-install
    - name: Restore autoconf from cache
      id: restore_autoconf
      if: ${{ steps.paths.outputs.autoconf_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.autoconf_INSTALL }}
       key: ${{ steps.paths.outputs.autoconf_CACHE_KEY }}-install
    - name: Build autoconf
      if: ${{ steps.paths.outputs.autoconf_SOURCE && ! steps.restore_autoconf.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests autoconf
    - name: Save autoconf to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.autoconf_SOURCE && ! steps.restore_autoconf.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.autoconf_INSTALL }}
       key: ${{ steps.paths.outputs.autoconf_CACHE_KEY }}-install
    - name: Restore automake from cache
      id: restore_automake
      if: ${{ steps.paths.outputs.automake_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.automake_INSTALL }}
       key: ${{ steps.paths.outputs.automake_CACHE_KEY }}-install
    - name: Build automake
      if: ${{ steps.paths.outputs.automake_SOURCE && ! steps.restore_automake.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests automake
    - name: Save automake to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.automake_SOURCE && ! steps.restore_automake.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.automake_INSTALL }}
       key: ${{ steps.paths.outputs.automake_CACHE_KEY }}-install
    - name: Restore libtool from cache
      id: restore_libtool
      if: ${{ steps.paths.outputs.libtool_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libtool_INSTALL }}
       key: ${{ steps.paths.outputs.libtool_CACHE_KEY }}-install
    - name: Build libtool
      if: ${{ steps.paths.outputs.libtool_SOURCE && ! steps.restore_libtool.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests libtool
    - name: Save libtool to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libtool_SOURCE && ! steps.restore_libtool.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libtool_INSTALL }}
       key: ${{ steps.paths.outputs.libtool_CACHE_KEY }}-install
    - name: Restore libsodium from cache
      id: restore_libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libsodium_INSTALL }}
       key: ${{ steps.paths.outputs.libsodium_CACHE_KEY }}-install
    - name: Build libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE && ! steps.restore_libsodium.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests libsodium
    - name: Save libsodium to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libsodium_SOURCE && ! steps.restore_libsodium.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libsodium_INSTALL }}
       key: ${{ steps.paths.outputs.libsodium_CACHE_KEY }}-install
    - name: Restore libiberty from cache
      id: restore_libiberty
      if: ${{ steps.paths.outputs.libiberty_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libiberty_INSTALL }}
       key: ${{ steps.paths.outputs.libiberty_CACHE_KEY }}-install
    - name: Build libiberty
      if: ${{ steps.paths.outputs.libiberty_SOURCE && ! steps.restore_libiberty.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests libiberty
    - name: Save libiberty to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libiberty_SOURCE && ! steps.restore_libiberty.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libiberty_INSTALL }}
       key: ${{ steps.paths.outputs.libiberty_CACHE_KEY }}-install
    - name: Restore libunwind from cache
      id: restore_libunwind
      if: ${{ steps.paths.outputs.libunwind_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libunwind_INSTALL }}
       key: ${{ steps.paths.outputs.libunwind_CACHE_KEY }}-install
    - name: Build libunwind
      if: ${{ steps.paths.outputs.libunwind_SOURCE && ! steps.restore_libunwind.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests libunwind
    - name: Save libunwind to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libunwind_SOURCE && ! steps.restore_libunwind.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libunwind_INSTALL }}
       key: ${{ steps.paths.outputs.libunwind_CACHE_KEY }}-install
    - name: Restore xz from cache
      id: restore_xz
      if: ${{ steps.paths.outputs.xz_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.xz_INSTALL }}
       key: ${{ steps.paths.outputs.xz_CACHE_KEY }}-install
    - name: Build xz
      if: ${{ steps.paths.outputs.xz_SOURCE && ! steps.restore_xz.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests xz
    - name: Save xz to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.xz_SOURCE && ! steps.restore_xz.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.xz_INSTALL }}
       key: ${{ steps.paths.outputs.xz_CACHE_KEY }}-install
    - name: Restore folly from cache
      id: restore_folly
      if: ${{ steps.paths.outputs.folly_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.folly_INSTALL }}
       key: ${{ steps.paths.outputs.folly_CACHE_KEY }}-install
    - name: Build folly
      if: ${{ steps.paths.outputs.folly_SOURCE && ! steps.restore_folly.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests folly
    - name: Save folly to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.folly_SOURCE && ! steps.restore_folly.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.folly_INSTALL }}
       key: ${{ steps.paths.outputs.folly_CACHE_KEY }}-install
    - name: Restore fizz from cache
      id: restore_fizz
      if: ${{ steps.paths.outputs.fizz_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fizz_INSTALL }}
       key: ${{ steps.paths.outputs.fizz_CACHE_KEY }}-install
    - name: Build fizz
      if: ${{ steps.paths.outputs.fizz_SOURCE && ! steps.restore_fizz.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests fizz
    - name: Save fizz to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fizz_SOURCE && ! steps.restore_fizz.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fizz_INSTALL }}
       key: ${{ steps.paths.outputs.fizz_CACHE_KEY }}-install
    - name: Restore mvfst from cache
      id: restore_mvfst
      if: ${{ steps.paths.outputs.mvfst_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.mvfst_INSTALL }}
       key: ${{ steps.paths.outputs.mvfst_CACHE_KEY }}-install
    - name: Build mvfst
      if: ${{ steps.paths.outputs.mvfst_SOURCE && ! steps.restore_mvfst.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests mvfst
    - name: Save mvfst to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.mvfst_SOURCE && ! steps.restore_mvfst.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.mvfst_INSTALL }}
       key: ${{ steps.paths.outputs.mvfst_CACHE_KEY }}-install
    - name: Restore wangle from cache
      id: restore_wangle
      if: ${{ steps.paths.outputs.wangle_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.wangle_INSTALL }}
       key: ${{ steps.paths.outputs.wangle_CACHE_KEY }}-install
    - name: Build wangle
      if: ${{ steps.paths.outputs.wangle_SOURCE && ! steps.restore_wangle.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests wangle
    - name: Save wangle to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.wangle_SOURCE && ! steps.restore_wangle.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.wangle_INSTALL }}
       key: ${{ steps.paths.outputs.wangle_CACHE_KEY }}-install
    - name: Build fbthrift
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --src-dir=. fbthrift  --project-install-prefix fbthrift:/usr/local
    - name: Copy artifacts
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fixup-dyn-deps --strip --src-dir=. fbthrift _artifacts/linux  --project-install-prefix fbthrift:/usr/local --final-install-prefix /usr/local
    - uses: actions/upload-artifact@v4
      with:
        name: fbthrift
        path: _artifacts
    - name: Test fbthrift
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages test --src-dir=. fbthrift  --project-install-prefix fbthrift:/usr/local
    - name: Show disk space at end
      if: always()
      run: df -h
",597,1,2,"push, pull_request",60
facebook/fbthrift,getdeps_mac.yml,"# This file was @generated by getdeps.py

name: mac

on:
  push:
    branches:
    - main
  pull_request:
    branches:
    - main

permissions:
  contents: read  #  to fetch code (actions/checkout)

jobs:
  build:
    runs-on: macOS-latest
    steps:
    - uses: actions/checkout@v4
    - name: Show disk space at start
      run: df -h
    - name: Free up disk space
      run: sudo rm -rf /usr/local/lib/android
    - name: Show disk space after freeing up
      run: df -h
    - name: Install system deps
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages install-system-deps --recursive fbthrift
    - id: paths
      name: Query paths
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages query-paths --recursive --src-dir=. fbthrift  >> ""$GITHUB_OUTPUT""
    - name: Fetch xxhash
      if: ${{ steps.paths.outputs.xxhash_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests xxhash
    - name: Fetch ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests ninja
    - name: Fetch cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests cmake
    - name: Fetch zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests zlib
    - name: Fetch zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests zstd
    - name: Fetch fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests fmt
    - name: Fetch boost
      if: ${{ steps.paths.outputs.boost_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests boost
    - name: Fetch double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests double-conversion
    - name: Fetch fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests fast_float
    - name: Fetch gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests gflags
    - name: Fetch glog
      if: ${{ steps.paths.outputs.glog_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests glog
    - name: Fetch googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests googletest
    - name: Fetch libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libdwarf
    - name: Fetch lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests lz4
    - name: Fetch openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests openssl
    - name: Fetch snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests snappy
    - name: Fetch libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libevent
    - name: Fetch liboqs
      if: ${{ steps.paths.outputs.liboqs_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests liboqs
    - name: Fetch autoconf
      if: ${{ steps.paths.outputs.autoconf_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests autoconf
    - name: Fetch automake
      if: ${{ steps.paths.outputs.automake_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests automake
    - name: Fetch libtool
      if: ${{ steps.paths.outputs.libtool_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libtool
    - name: Fetch libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests libsodium
    - name: Fetch xz
      if: ${{ steps.paths.outputs.xz_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests xz
    - name: Fetch folly
      if: ${{ steps.paths.outputs.folly_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests folly
    - name: Fetch fizz
      if: ${{ steps.paths.outputs.fizz_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests fizz
    - name: Fetch mvfst
      if: ${{ steps.paths.outputs.mvfst_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests mvfst
    - name: Fetch wangle
      if: ${{ steps.paths.outputs.wangle_SOURCE }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fetch --no-tests wangle
    - name: Restore xxhash from cache
      id: restore_xxhash
      if: ${{ steps.paths.outputs.xxhash_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.xxhash_INSTALL }}
       key: ${{ steps.paths.outputs.xxhash_CACHE_KEY }}-install
    - name: Build xxhash
      if: ${{ steps.paths.outputs.xxhash_SOURCE && ! steps.restore_xxhash.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests xxhash
    - name: Save xxhash to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.xxhash_SOURCE && ! steps.restore_xxhash.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.xxhash_INSTALL }}
       key: ${{ steps.paths.outputs.xxhash_CACHE_KEY }}-install
    - name: Restore ninja from cache
      id: restore_ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.ninja_INSTALL }}
       key: ${{ steps.paths.outputs.ninja_CACHE_KEY }}-install
    - name: Build ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE && ! steps.restore_ninja.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests ninja
    - name: Save ninja to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.ninja_SOURCE && ! steps.restore_ninja.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.ninja_INSTALL }}
       key: ${{ steps.paths.outputs.ninja_CACHE_KEY }}-install
    - name: Restore cmake from cache
      id: restore_cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.cmake_INSTALL }}
       key: ${{ steps.paths.outputs.cmake_CACHE_KEY }}-install
    - name: Build cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE && ! steps.restore_cmake.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests cmake
    - name: Save cmake to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.cmake_SOURCE && ! steps.restore_cmake.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.cmake_INSTALL }}
       key: ${{ steps.paths.outputs.cmake_CACHE_KEY }}-install
    - name: Restore zlib from cache
      id: restore_zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.zlib_INSTALL }}
       key: ${{ steps.paths.outputs.zlib_CACHE_KEY }}-install
    - name: Build zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE && ! steps.restore_zlib.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests zlib
    - name: Save zlib to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.zlib_SOURCE && ! steps.restore_zlib.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.zlib_INSTALL }}
       key: ${{ steps.paths.outputs.zlib_CACHE_KEY }}-install
    - name: Restore zstd from cache
      id: restore_zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.zstd_INSTALL }}
       key: ${{ steps.paths.outputs.zstd_CACHE_KEY }}-install
    - name: Build zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE && ! steps.restore_zstd.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests zstd
    - name: Save zstd to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.zstd_SOURCE && ! steps.restore_zstd.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.zstd_INSTALL }}
       key: ${{ steps.paths.outputs.zstd_CACHE_KEY }}-install
    - name: Restore fmt from cache
      id: restore_fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fmt_INSTALL }}
       key: ${{ steps.paths.outputs.fmt_CACHE_KEY }}-install
    - name: Build fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE && ! steps.restore_fmt.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests fmt
    - name: Save fmt to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fmt_SOURCE && ! steps.restore_fmt.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fmt_INSTALL }}
       key: ${{ steps.paths.outputs.fmt_CACHE_KEY }}-install
    - name: Restore boost from cache
      id: restore_boost
      if: ${{ steps.paths.outputs.boost_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.boost_INSTALL }}
       key: ${{ steps.paths.outputs.boost_CACHE_KEY }}-install
    - name: Build boost
      if: ${{ steps.paths.outputs.boost_SOURCE && ! steps.restore_boost.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests boost
    - name: Save boost to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.boost_SOURCE && ! steps.restore_boost.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.boost_INSTALL }}
       key: ${{ steps.paths.outputs.boost_CACHE_KEY }}-install
    - name: Restore double-conversion from cache
      id: restore_double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.double-conversion_INSTALL }}
       key: ${{ steps.paths.outputs.double-conversion_CACHE_KEY }}-install
    - name: Build double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE && ! steps.restore_double-conversion.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests double-conversion
    - name: Save double-conversion to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.double-conversion_SOURCE && ! steps.restore_double-conversion.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.double-conversion_INSTALL }}
       key: ${{ steps.paths.outputs.double-conversion_CACHE_KEY }}-install
    - name: Restore fast_float from cache
      id: restore_fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fast_float_INSTALL }}
       key: ${{ steps.paths.outputs.fast_float_CACHE_KEY }}-install
    - name: Build fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE && ! steps.restore_fast_float.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests fast_float
    - name: Save fast_float to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fast_float_SOURCE && ! steps.restore_fast_float.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fast_float_INSTALL }}
       key: ${{ steps.paths.outputs.fast_float_CACHE_KEY }}-install
    - name: Restore gflags from cache
      id: restore_gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.gflags_INSTALL }}
       key: ${{ steps.paths.outputs.gflags_CACHE_KEY }}-install
    - name: Build gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE && ! steps.restore_gflags.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests gflags
    - name: Save gflags to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.gflags_SOURCE && ! steps.restore_gflags.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.gflags_INSTALL }}
       key: ${{ steps.paths.outputs.gflags_CACHE_KEY }}-install
    - name: Restore glog from cache
      id: restore_glog
      if: ${{ steps.paths.outputs.glog_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.glog_INSTALL }}
       key: ${{ steps.paths.outputs.glog_CACHE_KEY }}-install
    - name: Build glog
      if: ${{ steps.paths.outputs.glog_SOURCE && ! steps.restore_glog.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests glog
    - name: Save glog to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.glog_SOURCE && ! steps.restore_glog.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.glog_INSTALL }}
       key: ${{ steps.paths.outputs.glog_CACHE_KEY }}-install
    - name: Restore googletest from cache
      id: restore_googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.googletest_INSTALL }}
       key: ${{ steps.paths.outputs.googletest_CACHE_KEY }}-install
    - name: Build googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE && ! steps.restore_googletest.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests googletest
    - name: Save googletest to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.googletest_SOURCE && ! steps.restore_googletest.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.googletest_INSTALL }}
       key: ${{ steps.paths.outputs.googletest_CACHE_KEY }}-install
    - name: Restore libdwarf from cache
      id: restore_libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libdwarf_INSTALL }}
       key: ${{ steps.paths.outputs.libdwarf_CACHE_KEY }}-install
    - name: Build libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE && ! steps.restore_libdwarf.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests libdwarf
    - name: Save libdwarf to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libdwarf_SOURCE && ! steps.restore_libdwarf.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libdwarf_INSTALL }}
       key: ${{ steps.paths.outputs.libdwarf_CACHE_KEY }}-install
    - name: Restore lz4 from cache
      id: restore_lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.lz4_INSTALL }}
       key: ${{ steps.paths.outputs.lz4_CACHE_KEY }}-install
    - name: Build lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE && ! steps.restore_lz4.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests lz4
    - name: Save lz4 to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.lz4_SOURCE && ! steps.restore_lz4.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.lz4_INSTALL }}
       key: ${{ steps.paths.outputs.lz4_CACHE_KEY }}-install
    - name: Restore openssl from cache
      id: restore_openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.openssl_INSTALL }}
       key: ${{ steps.paths.outputs.openssl_CACHE_KEY }}-install
    - name: Build openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE && ! steps.restore_openssl.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests openssl
    - name: Save openssl to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.openssl_SOURCE && ! steps.restore_openssl.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.openssl_INSTALL }}
       key: ${{ steps.paths.outputs.openssl_CACHE_KEY }}-install
    - name: Restore snappy from cache
      id: restore_snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.snappy_INSTALL }}
       key: ${{ steps.paths.outputs.snappy_CACHE_KEY }}-install
    - name: Build snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE && ! steps.restore_snappy.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests snappy
    - name: Save snappy to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.snappy_SOURCE && ! steps.restore_snappy.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.snappy_INSTALL }}
       key: ${{ steps.paths.outputs.snappy_CACHE_KEY }}-install
    - name: Restore libevent from cache
      id: restore_libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libevent_INSTALL }}
       key: ${{ steps.paths.outputs.libevent_CACHE_KEY }}-install
    - name: Build libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE && ! steps.restore_libevent.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests libevent
    - name: Save libevent to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libevent_SOURCE && ! steps.restore_libevent.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libevent_INSTALL }}
       key: ${{ steps.paths.outputs.libevent_CACHE_KEY }}-install
    - name: Restore liboqs from cache
      id: restore_liboqs
      if: ${{ steps.paths.outputs.liboqs_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.liboqs_INSTALL }}
       key: ${{ steps.paths.outputs.liboqs_CACHE_KEY }}-install
    - name: Build liboqs
      if: ${{ steps.paths.outputs.liboqs_SOURCE && ! steps.restore_liboqs.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests liboqs
    - name: Save liboqs to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.liboqs_SOURCE && ! steps.restore_liboqs.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.liboqs_INSTALL }}
       key: ${{ steps.paths.outputs.liboqs_CACHE_KEY }}-install
    - name: Restore autoconf from cache
      id: restore_autoconf
      if: ${{ steps.paths.outputs.autoconf_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.autoconf_INSTALL }}
       key: ${{ steps.paths.outputs.autoconf_CACHE_KEY }}-install
    - name: Build autoconf
      if: ${{ steps.paths.outputs.autoconf_SOURCE && ! steps.restore_autoconf.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests autoconf
    - name: Save autoconf to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.autoconf_SOURCE && ! steps.restore_autoconf.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.autoconf_INSTALL }}
       key: ${{ steps.paths.outputs.autoconf_CACHE_KEY }}-install
    - name: Restore automake from cache
      id: restore_automake
      if: ${{ steps.paths.outputs.automake_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.automake_INSTALL }}
       key: ${{ steps.paths.outputs.automake_CACHE_KEY }}-install
    - name: Build automake
      if: ${{ steps.paths.outputs.automake_SOURCE && ! steps.restore_automake.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests automake
    - name: Save automake to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.automake_SOURCE && ! steps.restore_automake.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.automake_INSTALL }}
       key: ${{ steps.paths.outputs.automake_CACHE_KEY }}-install
    - name: Restore libtool from cache
      id: restore_libtool
      if: ${{ steps.paths.outputs.libtool_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libtool_INSTALL }}
       key: ${{ steps.paths.outputs.libtool_CACHE_KEY }}-install
    - name: Build libtool
      if: ${{ steps.paths.outputs.libtool_SOURCE && ! steps.restore_libtool.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests libtool
    - name: Save libtool to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libtool_SOURCE && ! steps.restore_libtool.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libtool_INSTALL }}
       key: ${{ steps.paths.outputs.libtool_CACHE_KEY }}-install
    - name: Restore libsodium from cache
      id: restore_libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libsodium_INSTALL }}
       key: ${{ steps.paths.outputs.libsodium_CACHE_KEY }}-install
    - name: Build libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE && ! steps.restore_libsodium.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests libsodium
    - name: Save libsodium to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libsodium_SOURCE && ! steps.restore_libsodium.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libsodium_INSTALL }}
       key: ${{ steps.paths.outputs.libsodium_CACHE_KEY }}-install
    - name: Restore xz from cache
      id: restore_xz
      if: ${{ steps.paths.outputs.xz_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.xz_INSTALL }}
       key: ${{ steps.paths.outputs.xz_CACHE_KEY }}-install
    - name: Build xz
      if: ${{ steps.paths.outputs.xz_SOURCE && ! steps.restore_xz.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests xz
    - name: Save xz to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.xz_SOURCE && ! steps.restore_xz.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.xz_INSTALL }}
       key: ${{ steps.paths.outputs.xz_CACHE_KEY }}-install
    - name: Restore folly from cache
      id: restore_folly
      if: ${{ steps.paths.outputs.folly_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.folly_INSTALL }}
       key: ${{ steps.paths.outputs.folly_CACHE_KEY }}-install
    - name: Build folly
      if: ${{ steps.paths.outputs.folly_SOURCE && ! steps.restore_folly.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests folly
    - name: Save folly to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.folly_SOURCE && ! steps.restore_folly.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.folly_INSTALL }}
       key: ${{ steps.paths.outputs.folly_CACHE_KEY }}-install
    - name: Restore fizz from cache
      id: restore_fizz
      if: ${{ steps.paths.outputs.fizz_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fizz_INSTALL }}
       key: ${{ steps.paths.outputs.fizz_CACHE_KEY }}-install
    - name: Build fizz
      if: ${{ steps.paths.outputs.fizz_SOURCE && ! steps.restore_fizz.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests fizz
    - name: Save fizz to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fizz_SOURCE && ! steps.restore_fizz.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fizz_INSTALL }}
       key: ${{ steps.paths.outputs.fizz_CACHE_KEY }}-install
    - name: Restore mvfst from cache
      id: restore_mvfst
      if: ${{ steps.paths.outputs.mvfst_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.mvfst_INSTALL }}
       key: ${{ steps.paths.outputs.mvfst_CACHE_KEY }}-install
    - name: Build mvfst
      if: ${{ steps.paths.outputs.mvfst_SOURCE && ! steps.restore_mvfst.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests mvfst
    - name: Save mvfst to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.mvfst_SOURCE && ! steps.restore_mvfst.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.mvfst_INSTALL }}
       key: ${{ steps.paths.outputs.mvfst_CACHE_KEY }}-install
    - name: Restore wangle from cache
      id: restore_wangle
      if: ${{ steps.paths.outputs.wangle_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.wangle_INSTALL }}
       key: ${{ steps.paths.outputs.wangle_CACHE_KEY }}-install
    - name: Build wangle
      if: ${{ steps.paths.outputs.wangle_SOURCE && ! steps.restore_wangle.outputs.cache-hit }}
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --free-up-disk --no-tests wangle
    - name: Save wangle to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.wangle_SOURCE && ! steps.restore_wangle.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.wangle_INSTALL }}
       key: ${{ steps.paths.outputs.wangle_CACHE_KEY }}-install
    - name: Build fbthrift
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages build --src-dir=. fbthrift  --project-install-prefix fbthrift:/usr/local
    - name: Copy artifacts
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages fixup-dyn-deps --src-dir=. fbthrift _artifacts/mac  --project-install-prefix fbthrift:/usr/local --final-install-prefix /usr/local
    - uses: actions/upload-artifact@v4
      with:
        name: fbthrift
        path: _artifacts
    - name: Test fbthrift
      run: python3 build/fbcode_builder/getdeps.py --allow-system-packages test --src-dir=. fbthrift  --project-install-prefix fbthrift:/usr/local
    - name: Show disk space at end
      if: always()
      run: df -h
",557,1,2,"push, pull_request",56
facebook/fbthrift,getdeps_windows.yml,"# This file was @generated by getdeps.py

name: windows

on:
  push:
    branches:
    - main
  pull_request:
    branches:
    - main

permissions:
  contents: read  #  to fetch code (actions/checkout)

jobs:
  build:
    runs-on: windows-2019
    steps:
    - name: Export boost environment
      run: ""echo BOOST_ROOT=%BOOST_ROOT_1_83_0% >> %GITHUB_ENV%""
      shell: cmd
    - name: Fix Git config
      run: >
        git config --system core.longpaths true &&
        git config --system core.autocrlf false &&
        git config --system core.symlinks true
      shell: cmd
    - uses: actions/checkout@v4
    - id: paths
      name: Query paths
      run: python build/fbcode_builder/getdeps.py query-paths --recursive --src-dir=. fbthrift  >> $env:GITHUB_OUTPUT
      shell: pwsh
    - name: Fetch libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests libsodium
    - name: Fetch ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests ninja
    - name: Fetch cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests cmake
    - name: Fetch zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests zlib
    - name: Fetch zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests zstd
    - name: Fetch fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests fmt
    - name: Fetch boost
      if: ${{ steps.paths.outputs.boost_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests boost
    - name: Fetch double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests double-conversion
    - name: Fetch fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests fast_float
    - name: Fetch gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests gflags
    - name: Fetch glog
      if: ${{ steps.paths.outputs.glog_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests glog
    - name: Fetch googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests googletest
    - name: Fetch libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests libdwarf
    - name: Fetch lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests lz4
    - name: Fetch snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests snappy
    - name: Fetch xxhash
      if: ${{ steps.paths.outputs.xxhash_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests xxhash
    - name: Fetch jom
      if: ${{ steps.paths.outputs.jom_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests jom
    - name: Fetch perl
      if: ${{ steps.paths.outputs.perl_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests perl
    - name: Fetch openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests openssl
    - name: Fetch libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests libevent
    - name: Fetch folly
      if: ${{ steps.paths.outputs.folly_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests folly
    - name: Fetch liboqs
      if: ${{ steps.paths.outputs.liboqs_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests liboqs
    - name: Fetch fizz
      if: ${{ steps.paths.outputs.fizz_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests fizz
    - name: Fetch mvfst
      if: ${{ steps.paths.outputs.mvfst_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests mvfst
    - name: Fetch wangle
      if: ${{ steps.paths.outputs.wangle_SOURCE }}
      run: python build/fbcode_builder/getdeps.py fetch --no-tests wangle
    - name: Restore libsodium from cache
      id: restore_libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libsodium_INSTALL }}
       key: ${{ steps.paths.outputs.libsodium_CACHE_KEY }}-install
    - name: Build libsodium
      if: ${{ steps.paths.outputs.libsodium_SOURCE && ! steps.restore_libsodium.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests libsodium
    - name: Save libsodium to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libsodium_SOURCE && ! steps.restore_libsodium.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libsodium_INSTALL }}
       key: ${{ steps.paths.outputs.libsodium_CACHE_KEY }}-install
    - name: Restore ninja from cache
      id: restore_ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.ninja_INSTALL }}
       key: ${{ steps.paths.outputs.ninja_CACHE_KEY }}-install
    - name: Build ninja
      if: ${{ steps.paths.outputs.ninja_SOURCE && ! steps.restore_ninja.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests ninja
    - name: Save ninja to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.ninja_SOURCE && ! steps.restore_ninja.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.ninja_INSTALL }}
       key: ${{ steps.paths.outputs.ninja_CACHE_KEY }}-install
    - name: Restore cmake from cache
      id: restore_cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.cmake_INSTALL }}
       key: ${{ steps.paths.outputs.cmake_CACHE_KEY }}-install
    - name: Build cmake
      if: ${{ steps.paths.outputs.cmake_SOURCE && ! steps.restore_cmake.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests cmake
    - name: Save cmake to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.cmake_SOURCE && ! steps.restore_cmake.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.cmake_INSTALL }}
       key: ${{ steps.paths.outputs.cmake_CACHE_KEY }}-install
    - name: Restore zlib from cache
      id: restore_zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.zlib_INSTALL }}
       key: ${{ steps.paths.outputs.zlib_CACHE_KEY }}-install
    - name: Build zlib
      if: ${{ steps.paths.outputs.zlib_SOURCE && ! steps.restore_zlib.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests zlib
    - name: Save zlib to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.zlib_SOURCE && ! steps.restore_zlib.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.zlib_INSTALL }}
       key: ${{ steps.paths.outputs.zlib_CACHE_KEY }}-install
    - name: Restore zstd from cache
      id: restore_zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.zstd_INSTALL }}
       key: ${{ steps.paths.outputs.zstd_CACHE_KEY }}-install
    - name: Build zstd
      if: ${{ steps.paths.outputs.zstd_SOURCE && ! steps.restore_zstd.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests zstd
    - name: Save zstd to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.zstd_SOURCE && ! steps.restore_zstd.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.zstd_INSTALL }}
       key: ${{ steps.paths.outputs.zstd_CACHE_KEY }}-install
    - name: Restore fmt from cache
      id: restore_fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fmt_INSTALL }}
       key: ${{ steps.paths.outputs.fmt_CACHE_KEY }}-install
    - name: Build fmt
      if: ${{ steps.paths.outputs.fmt_SOURCE && ! steps.restore_fmt.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests fmt
    - name: Save fmt to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fmt_SOURCE && ! steps.restore_fmt.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fmt_INSTALL }}
       key: ${{ steps.paths.outputs.fmt_CACHE_KEY }}-install
    - name: Restore boost from cache
      id: restore_boost
      if: ${{ steps.paths.outputs.boost_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.boost_INSTALL }}
       key: ${{ steps.paths.outputs.boost_CACHE_KEY }}-install
    - name: Build boost
      if: ${{ steps.paths.outputs.boost_SOURCE && ! steps.restore_boost.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests boost
    - name: Save boost to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.boost_SOURCE && ! steps.restore_boost.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.boost_INSTALL }}
       key: ${{ steps.paths.outputs.boost_CACHE_KEY }}-install
    - name: Restore double-conversion from cache
      id: restore_double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.double-conversion_INSTALL }}
       key: ${{ steps.paths.outputs.double-conversion_CACHE_KEY }}-install
    - name: Build double-conversion
      if: ${{ steps.paths.outputs.double-conversion_SOURCE && ! steps.restore_double-conversion.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests double-conversion
    - name: Save double-conversion to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.double-conversion_SOURCE && ! steps.restore_double-conversion.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.double-conversion_INSTALL }}
       key: ${{ steps.paths.outputs.double-conversion_CACHE_KEY }}-install
    - name: Restore fast_float from cache
      id: restore_fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fast_float_INSTALL }}
       key: ${{ steps.paths.outputs.fast_float_CACHE_KEY }}-install
    - name: Build fast_float
      if: ${{ steps.paths.outputs.fast_float_SOURCE && ! steps.restore_fast_float.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests fast_float
    - name: Save fast_float to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fast_float_SOURCE && ! steps.restore_fast_float.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fast_float_INSTALL }}
       key: ${{ steps.paths.outputs.fast_float_CACHE_KEY }}-install
    - name: Restore gflags from cache
      id: restore_gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.gflags_INSTALL }}
       key: ${{ steps.paths.outputs.gflags_CACHE_KEY }}-install
    - name: Build gflags
      if: ${{ steps.paths.outputs.gflags_SOURCE && ! steps.restore_gflags.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests gflags
    - name: Save gflags to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.gflags_SOURCE && ! steps.restore_gflags.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.gflags_INSTALL }}
       key: ${{ steps.paths.outputs.gflags_CACHE_KEY }}-install
    - name: Restore glog from cache
      id: restore_glog
      if: ${{ steps.paths.outputs.glog_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.glog_INSTALL }}
       key: ${{ steps.paths.outputs.glog_CACHE_KEY }}-install
    - name: Build glog
      if: ${{ steps.paths.outputs.glog_SOURCE && ! steps.restore_glog.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests glog
    - name: Save glog to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.glog_SOURCE && ! steps.restore_glog.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.glog_INSTALL }}
       key: ${{ steps.paths.outputs.glog_CACHE_KEY }}-install
    - name: Restore googletest from cache
      id: restore_googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.googletest_INSTALL }}
       key: ${{ steps.paths.outputs.googletest_CACHE_KEY }}-install
    - name: Build googletest
      if: ${{ steps.paths.outputs.googletest_SOURCE && ! steps.restore_googletest.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests googletest
    - name: Save googletest to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.googletest_SOURCE && ! steps.restore_googletest.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.googletest_INSTALL }}
       key: ${{ steps.paths.outputs.googletest_CACHE_KEY }}-install
    - name: Restore libdwarf from cache
      id: restore_libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libdwarf_INSTALL }}
       key: ${{ steps.paths.outputs.libdwarf_CACHE_KEY }}-install
    - name: Build libdwarf
      if: ${{ steps.paths.outputs.libdwarf_SOURCE && ! steps.restore_libdwarf.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests libdwarf
    - name: Save libdwarf to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libdwarf_SOURCE && ! steps.restore_libdwarf.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libdwarf_INSTALL }}
       key: ${{ steps.paths.outputs.libdwarf_CACHE_KEY }}-install
    - name: Restore lz4 from cache
      id: restore_lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.lz4_INSTALL }}
       key: ${{ steps.paths.outputs.lz4_CACHE_KEY }}-install
    - name: Build lz4
      if: ${{ steps.paths.outputs.lz4_SOURCE && ! steps.restore_lz4.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests lz4
    - name: Save lz4 to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.lz4_SOURCE && ! steps.restore_lz4.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.lz4_INSTALL }}
       key: ${{ steps.paths.outputs.lz4_CACHE_KEY }}-install
    - name: Restore snappy from cache
      id: restore_snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.snappy_INSTALL }}
       key: ${{ steps.paths.outputs.snappy_CACHE_KEY }}-install
    - name: Build snappy
      if: ${{ steps.paths.outputs.snappy_SOURCE && ! steps.restore_snappy.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests snappy
    - name: Save snappy to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.snappy_SOURCE && ! steps.restore_snappy.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.snappy_INSTALL }}
       key: ${{ steps.paths.outputs.snappy_CACHE_KEY }}-install
    - name: Restore xxhash from cache
      id: restore_xxhash
      if: ${{ steps.paths.outputs.xxhash_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.xxhash_INSTALL }}
       key: ${{ steps.paths.outputs.xxhash_CACHE_KEY }}-install
    - name: Build xxhash
      if: ${{ steps.paths.outputs.xxhash_SOURCE && ! steps.restore_xxhash.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests xxhash
    - name: Save xxhash to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.xxhash_SOURCE && ! steps.restore_xxhash.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.xxhash_INSTALL }}
       key: ${{ steps.paths.outputs.xxhash_CACHE_KEY }}-install
    - name: Restore jom from cache
      id: restore_jom
      if: ${{ steps.paths.outputs.jom_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.jom_INSTALL }}
       key: ${{ steps.paths.outputs.jom_CACHE_KEY }}-install
    - name: Build jom
      if: ${{ steps.paths.outputs.jom_SOURCE && ! steps.restore_jom.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests jom
    - name: Save jom to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.jom_SOURCE && ! steps.restore_jom.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.jom_INSTALL }}
       key: ${{ steps.paths.outputs.jom_CACHE_KEY }}-install
    - name: Restore perl from cache
      id: restore_perl
      if: ${{ steps.paths.outputs.perl_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.perl_INSTALL }}
       key: ${{ steps.paths.outputs.perl_CACHE_KEY }}-install
    - name: Build perl
      if: ${{ steps.paths.outputs.perl_SOURCE && ! steps.restore_perl.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests perl
    - name: Save perl to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.perl_SOURCE && ! steps.restore_perl.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.perl_INSTALL }}
       key: ${{ steps.paths.outputs.perl_CACHE_KEY }}-install
    - name: Restore openssl from cache
      id: restore_openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.openssl_INSTALL }}
       key: ${{ steps.paths.outputs.openssl_CACHE_KEY }}-install
    - name: Build openssl
      if: ${{ steps.paths.outputs.openssl_SOURCE && ! steps.restore_openssl.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests openssl
    - name: Save openssl to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.openssl_SOURCE && ! steps.restore_openssl.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.openssl_INSTALL }}
       key: ${{ steps.paths.outputs.openssl_CACHE_KEY }}-install
    - name: Restore libevent from cache
      id: restore_libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.libevent_INSTALL }}
       key: ${{ steps.paths.outputs.libevent_CACHE_KEY }}-install
    - name: Build libevent
      if: ${{ steps.paths.outputs.libevent_SOURCE && ! steps.restore_libevent.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests libevent
    - name: Save libevent to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.libevent_SOURCE && ! steps.restore_libevent.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.libevent_INSTALL }}
       key: ${{ steps.paths.outputs.libevent_CACHE_KEY }}-install
    - name: Restore folly from cache
      id: restore_folly
      if: ${{ steps.paths.outputs.folly_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.folly_INSTALL }}
       key: ${{ steps.paths.outputs.folly_CACHE_KEY }}-install
    - name: Build folly
      if: ${{ steps.paths.outputs.folly_SOURCE && ! steps.restore_folly.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests folly
    - name: Save folly to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.folly_SOURCE && ! steps.restore_folly.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.folly_INSTALL }}
       key: ${{ steps.paths.outputs.folly_CACHE_KEY }}-install
    - name: Restore liboqs from cache
      id: restore_liboqs
      if: ${{ steps.paths.outputs.liboqs_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.liboqs_INSTALL }}
       key: ${{ steps.paths.outputs.liboqs_CACHE_KEY }}-install
    - name: Build liboqs
      if: ${{ steps.paths.outputs.liboqs_SOURCE && ! steps.restore_liboqs.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests liboqs
    - name: Save liboqs to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.liboqs_SOURCE && ! steps.restore_liboqs.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.liboqs_INSTALL }}
       key: ${{ steps.paths.outputs.liboqs_CACHE_KEY }}-install
    - name: Restore fizz from cache
      id: restore_fizz
      if: ${{ steps.paths.outputs.fizz_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.fizz_INSTALL }}
       key: ${{ steps.paths.outputs.fizz_CACHE_KEY }}-install
    - name: Build fizz
      if: ${{ steps.paths.outputs.fizz_SOURCE && ! steps.restore_fizz.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests fizz
    - name: Save fizz to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.fizz_SOURCE && ! steps.restore_fizz.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.fizz_INSTALL }}
       key: ${{ steps.paths.outputs.fizz_CACHE_KEY }}-install
    - name: Restore mvfst from cache
      id: restore_mvfst
      if: ${{ steps.paths.outputs.mvfst_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.mvfst_INSTALL }}
       key: ${{ steps.paths.outputs.mvfst_CACHE_KEY }}-install
    - name: Build mvfst
      if: ${{ steps.paths.outputs.mvfst_SOURCE && ! steps.restore_mvfst.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests mvfst
    - name: Save mvfst to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.mvfst_SOURCE && ! steps.restore_mvfst.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.mvfst_INSTALL }}
       key: ${{ steps.paths.outputs.mvfst_CACHE_KEY }}-install
    - name: Restore wangle from cache
      id: restore_wangle
      if: ${{ steps.paths.outputs.wangle_SOURCE }}
      uses: actions/cache/restore@v4
      with:
       path: ${{ steps.paths.outputs.wangle_INSTALL }}
       key: ${{ steps.paths.outputs.wangle_CACHE_KEY }}-install
    - name: Build wangle
      if: ${{ steps.paths.outputs.wangle_SOURCE && ! steps.restore_wangle.outputs.cache-hit }}
      run: python build/fbcode_builder/getdeps.py build --free-up-disk --no-tests wangle
    - name: Save wangle to cache
      uses: actions/cache/save@v4
      if: ${{ steps.paths.outputs.wangle_SOURCE && ! steps.restore_wangle.outputs.cache-hit }}
      with:
       path: ${{ steps.paths.outputs.wangle_INSTALL }}
       key: ${{ steps.paths.outputs.wangle_CACHE_KEY }}-install
    - name: Build fbthrift
      run: python build/fbcode_builder/getdeps.py build --src-dir=. fbthrift 
    - name: Copy artifacts
      run: python build/fbcode_builder/getdeps.py fixup-dyn-deps --src-dir=. fbthrift _artifacts/windows  --final-install-prefix /usr/local
    - uses: actions/upload-artifact@v4
      with:
        name: fbthrift
        path: _artifacts
    - name: Test fbthrift
      run: python build/fbcode_builder/getdeps.py test --src-dir=. fbthrift 
",518,1,2,"push, pull_request",52
facebook/mcrouter,build.yml,"name: build

on:
  push:
    branches:
      - main
      - github_action
  pull_request:
    branches:
      - main

jobs:
  build:
    runs-on: ubuntu-24.04
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Build dependencies
        run: |
          ./mcrouter/scripts/install_ubuntu_24.04.sh ""$(pwd)""/mcrouter-install deps
      - name: Build mcrouter
        run: |
          mkdir -p ""$(pwd)""/mcrouter-install/install
          ./mcrouter/scripts/install_ubuntu_24.04.sh ""$(pwd)""/mcrouter-install mcrouter
",24,1,2,"push, pull_request",1
awslabs/amazon-ecr-credential-helper,build.yaml,"name: Build

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  git-secrets:
    runs-on: 'ubuntu-22.04'
    steps:
      - name: Pull latest awslabs/git-secrets repo
        uses: actions/checkout@v4
        with:
          repository: awslabs/git-secrets
          ref: 1.3.0
          fetch-tags: true
          path: git-secrets
      - name: Install git secrets from source
        run: sudo make install
        working-directory: git-secrets
      - uses: actions/checkout@v4
      - name: Scan repository for git secrets
        run: |
          git secrets --register-aws
          git secrets --scan-history

  cross-compile:
    runs-on: 'ubuntu-22.04'
    steps:
      - uses: actions/checkout@v4
      - name: Cross-compile all variants
        run: make all-variants-in-docker

  unit-test:
    strategy:
      matrix:
        go: ['1.23', '1.24']

        # Intentionally use specific versions instead of ""latest"" to
        # make this build reproducible.
        os: ['ubuntu-22.04', 'macos-13', 'windows-2022']

      # Build all variants regardless of failures
      fail-fast: false
    name: unit-test (${{ matrix.os }} / Go ${{ matrix.go }})
    runs-on: ${{ matrix.os }}

    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-go@v5
        with:
          go-version: ${{ matrix.go }}
      - run: make test
",55,3,2,"push, pull_request",5
awslabs/amazon-ecr-credential-helper,check-links.yml,"name: Check Links

on:
  workflow_dispatch:
  schedule:
    - cron: ""0 0 * * 3"" # Every Wednesday at 00:00 UTC
  pull_request:
    paths:
      - "".github/workflows/check-links.yml""

jobs:
  check:
    runs-on: ubuntu-22.04
    if: github.repository == 'awslabs/amazon-ecr-credential-helper'
    name: lychee
    timeout-minutes: 15
    steps:
      - uses: actions/checkout@v4
      - uses: lycheeverse/lychee-action@v2.4.0
        with:
          fail: true
          args: --exclude-path ecr-login/vendor --timeout 30 --no-progress './**/*.md'
          format: markdown
          jobSummary: true
",24,1,3,"workflow_dispatch, schedule, pull_request",2
awslabs/amazon-ecr-credential-helper,codeql.yml,"name: ""CodeQL Scan""

on:
  push:
    branches: [ ""main"" ]
  pull_request:
    branches: [ ""main"" ]
  schedule:
    - cron: '25 21 * * 5'

jobs:
  analyze:
    name: Analyze
    runs-on: ubuntu-22.04
    permissions:
      actions: read
      contents: read
      security-events: write

    strategy:
      fail-fast: false
      matrix:
        language: [ 'go' ]

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    # Initializes the CodeQL tools for scanning.
    - name: Initialize CodeQL
      uses: github/codeql-action/init@v3
      with:
        languages: ${{ matrix.language }}

    - name: Build
      run: make build

    - name: Perform CodeQL Analysis
      uses: github/codeql-action/analyze@v3
      with:
        category: ""/language:${{matrix.language}}""

",42,1,3,"push, pull_request, schedule",3
awslabs/amazon-ecr-credential-helper,new-pull-requests.yml,"name: ""New Pull Requests""

on:
  # It is safe to use pull_request_target here because we are not checking out
  # code from the pull request branch.
  #
  # See https://securitylab.github.com/research/github-actions-preventing-pwn-requests/
  pull_request_target:

permissions:
  contents: read

jobs:
  label:
    if: github.event.pull_request.draft == false
    runs-on: ubuntu-22.04

    permissions:
      pull-requests: write

    steps:
      # Use label configuration from main instead of from the pull request branch
      # to mitigate running untrusted workflows with write permissions.
      - uses: actions/labeler@v5
        with:
          configuration-path: '.github/new-pull-request-labels.yml'
          sync-labels: true
",27,1,1,pull_request_target,1
awslabs/amazon-ecr-credential-helper,review-dependencies.yml,"name: Review dependencies

on:
  pull_request:
    branches: ['main', 'release/**']
    paths:
      - 'ecr-login/go.*'

jobs:
  review:
    runs-on: ubuntu-latest

    permissions:
      # Write permissions needed to comment review results on PR.
      # Pwn request risk mitigated by using pull_request workflow trigger
      # and external contributor workflow runs require maintainer approval.
      pull-requests: write

    steps:
      - uses: actions/checkout@v4
      - uses: actions/dependency-review-action@v4
        with:
          config-file: './.github/dependency-review-config.yml'
          comment-summary-in-pr: always
",24,1,1,pull_request,2
oracle/opengrok,apiary.yml,"name: Check Apiary blueprint

on:
  push:
    paths:
    - apiary.apib
    branches:
    - master
  pull_request:
    paths:
    - apiary.apib

jobs:
  ubuntu:
    name: Ubuntu
    runs-on: ubuntu-latest
    steps:
    - name: Checkout master branch
      uses: actions/checkout@v4
    - name: Install drafter
      run: npm install drafter
    - name: Build
      run: node dev/parse.js
",23,1,2,"push, pull_request",1
oracle/opengrok,build.yml,"name: Build

on:
  push:
    paths-ignore:
    - README.md
    - '**/*.md'
  pull_request:
  schedule:
  - cron: ""0 0 * * 0""

jobs:
  build:
    name: ${{ matrix.os }} with Java 21
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, ubuntu-24.04-arm, macos-latest, windows-latest]
    steps:
    - name: Checkout master branch
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    - name: Set up JDK
      uses: actions/setup-java@v4
      with:
        distribution: 'oracle'
        java-version: '21'
    - name: Cache Maven packages
      uses: actions/cache@v4
      with:
        path: ~/.m2
        key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}
        restore-keys: ${{ runner.os }}-m2
    - name: Checkout Universal ctags
      uses: actions/checkout@v4
      with:
        repository: universal-ctags/ctags
        path: ctags
    - name: Install pre-requisites (Unix)
      if: runner.os == 'Linux' || runner.os == 'macOS'
      run: ./dev/before_install
    - name: Install Universal ctags (Windows)
      if: runner.os == 'Windows'
      run: choco install universal-ctags
    - name: Before build actions
      shell: bash
      run: ./dev/before
    - name: Maven build
      shell: bash
      env:
        OPENGROK_PULL_REQUEST: ${{ github.head_ref }}
        OPENGROK_REPO_SLUG: ${{ github.repository }}
        OPENGROK_REF: ${{ github.ref }}
        OPENGROK_SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: ./dev/main
    - name: Upload artifact
      uses: actions/upload-artifact@v4
      with:
        name: opengrok-${{ github.sha }}-${{ matrix.os }}.tar.gz
        path: distribution/target/opengrok-*.tar.gz
        compression-level: 0
",64,1,3,"push, pull_request, schedule",5
oracle/opengrok,codeql-analysis.yml,"name: ""CodeQL""

on:
  push:
    branches: [ master ]
  pull_request:
    # The branches below must be a subset of the branches above
    branches: [ master ]
  schedule:
    - cron: '41 17 * * 5'

jobs:
  analyze:
    name: Analyze
    runs-on: ubuntu-latest

    strategy:
      fail-fast: false
      matrix:
        language: [ 'java', 'javascript', 'python' ]

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    - name: Set up JDK 17
      uses: actions/setup-java@v4
      with:
        distribution: 'oracle'
        java-version: '17'
    - name: Initialize CodeQL
      uses: github/codeql-action/init@v3
      with:
        languages: ${{ matrix.language }}

    - name: Autobuild
      uses: github/codeql-action/autobuild@v3

    - name: Perform CodeQL Analysis
      uses: github/codeql-action/analyze@v3
",39,1,3,"push, pull_request, schedule",5
oracle/opengrok,docker.yml,"name: Build Docker image

on:
  push:
  pull_request:
  release:
    types: [created]

jobs:
  ubuntu:
    runs-on: ubuntu-latest
    steps:
    - name: Print environment
      shell: bash
      run: env
    - name: Checkout master branch
      uses: actions/checkout@v4
    - uses: actions/setup-python@v5
      with:
        python-version: '3.10'
    - name: Install dependencies
      run: python3 -m pip install -r docker/requirements.txt
    - name: Install opengrok-tools so that pylint can perform the checks
      run: python3 -m pip install .
      working-directory: tools
    - name: Install checkers
      run: python3 -m pip install pylint flake8 black isort
    - name: Run flake8
      run: flake8 --max-line-length 119 docker/*.py
    - name: Run pylint
      run: pylint -E --max-line-length 119 docker/*.py
    - name: Run black in check mode
      run: black --check docker/*.py
    - name: Run isort in check mode
      run: isort --settings-file docker/.isort.cfg docker/*.py  --check --diff
    - name: Build and optionally push Docker image
      env:
        DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}
        DOCKER_PASSWORD: ${{ secrets.DOCKER_PASSWORD }}
        OPENGROK_REPO_SLUG: ${{ github.repository }}
        OPENGROK_REF: ${{ github.ref }}
      run: ./dev/docker.sh
    - name: Install Python pre-requisites
      run: python3 -m pip install requests
    - name: Optionally update README on Docker hub
      env:
        OPENGROK_REPO_SLUG: ${{ github.repository }}
        DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}
        DOCKER_PASSWORD: ${{ secrets.DOCKER_PASSWORD }}
      run: ./dev/dockerhub_readme.py
",50,1,3,"push, pull_request, release",2
oracle/opengrok,javadoc.yml,"name: Upload javadocs to Github pages

on:
  push:
    branches:
    - master
    paths:
    - opengrok-indexer/**
    - opengrok-web/**
    - suggester/**
    - plugins/**
    - .github/workflows/javadoc.yml
    - dev/javadoc.sh

jobs:
  ubuntu:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout master branch
      uses: actions/checkout@v4
    - name: Set up JDK
      uses: actions/setup-java@v4
      with:
        distribution: 'oracle'
        java-version: '21'
    - name: Cache Maven packages
      uses: actions/cache@v4
      with:
        path: ~/.m2
        key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}
        restore-keys: ${{ runner.os }}-m2
    - name: Checkout gh-pages branch
      if: github.repository == 'oracle/opengrok'
      uses: actions/checkout@v4
      with:
        ref: gh-pages
        path: gh-pages
    - name: Refresh Javadoc
      env:
        OPENGROK_REPO_SLUG: ${{ github.repository }}
        OPENGROK_PULL_REQUEST: ${{ github.head_ref }}
        OPENGROK_REF: ${{ github.ref }}
        OPENGROK_BUILD_DIR: ${{ github.workspace }}
      run: ./dev/javadoc.sh
",44,1,1,push,4
oracle/opengrok,release.yml,"name: Release

# TODO: run this only for the oracle/opengrok repository
on:
  release:
    types: [created]

jobs:
  get_tag:
    name: Get tag name
    outputs:
      tag: ${{ steps.get_tag.outputs.tag }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout master branch
        uses: actions/checkout@v4
      - name: Get the tag name
        id: get_tag
        env:
          OPENGROK_REF: ${{ github.ref }}
        run: ./dev/ref2tag.sh
  build:
    runs-on: ubuntu-latest
    needs: get_tag
    steps:
    - name: Checkout master branch
      uses: actions/checkout@v4
    - name: Set up JDK
      uses: actions/setup-java@v4
      with:
        distribution: 'oracle'
        java-version: '21'
    - name: Cache Maven packages
      uses: actions/cache@v4
      with:
        path: ~/.m2
        key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}
        restore-keys: ${{ runner.os }}-m2
    - name: Checkout Universal ctags
      uses: actions/checkout@v4
      with:
        repository: universal-ctags/ctags
        path: ctags
    - name: Install pre-requisites
      run: ./dev/before_install
    - name: Before build actions
      run: ./dev/before
    - name: Build
      run: ./mvnw -DskipTests=true -Dmaven.javadoc.skip=false -B -V package
    - name: Get upload URL
      id: get_upload_url
      env:
        OPENGROK_TAG: ${{ needs.get_tag.outputs.tag }}
      run: dev/get_upload_url.sh
    - name: Upload release tarball
      id: upload-release-asset
      uses: actions/upload-release-asset@v1
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      with:
        upload_url: ${{ steps.get_upload_url.outputs.upload_url }}
        asset_path: ./distribution/target/opengrok-${{ needs.get_tag.outputs.tag }}.tar.gz
        asset_name: opengrok-${{ needs.get_tag.outputs.tag }}.tar.gz
        asset_content_type: application/octet-stream
",64,2,1,release,6
oracle/docker-images,build-and-push-dev-images.yml,"name: Build and publish Oracle Linux developer container images to GitHub Container Registry

# Builds are triggered either by:
#   - a push on the main branch with changes in this file.
#     All container images will be (re)built.
#   - a push on the main branch with changes the OracleLinuxDevelopers
#     directory.
#     Affected container images will be (re)built.
#   - a manual trigger of the workflow using the API.
#     Subset of OL version / language can be specified; default is to build
#     all images.
# Images are built for both amd64 and arm64 architectures, except for
#   - oracledb images (not available on arm)
#   - php and nodejs on OL7 (packages not available)

on:
  push:
    branches:
      - main
    paths:
      - 'OracleLinuxDevelopers/**'
      - '.github/workflows/build-and-push-dev-images.yml'
  workflow_dispatch:
    inputs:
      ol:
        description: List of ol versions to build
        default: 'oraclelinux7, oraclelinux8, oraclelinux9'
        required: false
      lang:
        description: List of languages to build
        default: 'gcc-toolset, golang, nginx, nodejs, php, python, redis, ruby, haproxy, kubectl, helm, ocne-tools, httpd'
        required: false

# Default values for the builds triggered by the push event
env:
  ol: 'oraclelinux7, oraclelinux8, oraclelinux9'
  lang: 'gcc-toolset, golang, nodejs, nginx, php, python, redis, ruby, haproxy, kubectl, helm, ocne-tools, httpd'

jobs:
  prepare:
    name: Create build matrix
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.build-matrix.outputs.matrix }}
      skip_build: ${{ steps.build-matrix.outputs.skip_build }}
      repository_owner: ${{ steps.repository_owner.outputs.repository_owner }}
      date_stamp: ${{ steps.date_stamp.outputs.date_stamp }}

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          # We need ""some"" commit history to check for changed files
          fetch-depth: 32

      - name: Build matrix
        id: build-matrix
        working-directory: OracleLinuxDevelopers
        run: |
          IFS="", "" read -r -a ol_list <<< ""${{ github.event.inputs.ol || env.ol}}""
          IFS="", "" read -r -a lang_list <<< ""${{ github.event.inputs.lang || env.lang}}""
          changes=$(mktemp)
          # workflow is only set in the workflow_dispatch event payload
          workflow=""${{ github.event.workflow }}""
          if [[ -z ${workflow} ]]; then
            # Push event - retrieve list of changed files
            git diff --name-only '${{ github.event.before }}..${{ github.event.after }}' > ""${changes}""
            if grep -q build-and-push-dev-images.yml ""${changes}""; then
              echo ""PUSH: Action updated, rebuilding all images""
              build_all=1
            else
              echo ""PUSH: Rebuilding changed images only""
              build_all=0
            fi
          else
            echo ""MANUAL: Rebuilding based on parameters""
            build_all=1
          fi
          matrix=$(
            for ol in ""${ol_list[@]}""; do
              pushd ""${ol}"" >/dev/null || exit 1
              for lang in ""${lang_list[@]}""; do
                if [[ -d ${lang} ]]; then
                  pushd ""${lang}"" >/dev/null || exit 1
                  for dockerfile in */Dockerfile; do
                    tag=$(dirname ""${dockerfile}"")
                    if [[ -f ${tag}/.skip-arm64 ]]; then
                      multi=0
                      arch=""linux/amd64""
                    else
                      multi=1
                      arch=""linux/amd64,linux/arm64""
                    fi
                    if [[ ${build_all} -eq 1 ]] || grep -q ""${ol}/${lang}/${tag}"" ""${changes}""; then
                      echo ""${ol};${lang};${tag};${arch};${multi}""
                    fi
                  done
                  popd >/dev/null || exit 1
                fi
              done
              popd >/dev/null || exit 1
            done | jq --slurp --raw-input --compact-output '
              split(""\n"") |
              .[:-1] |
              map(split("";"")) |
              map({""ol"": .[0], ""lang"": .[1], ""tag"": .[2], ""arch"": .[3], ""multi"": (.[4] == ""1"")})'
          )
          rm ""${changes}""
          if [[ ${matrix} == ""[]"" ]]; then
            # Empty array -- change didn't impact any image
            skip_build=true
          else
            skip_build=false
            matrix=$(jq --compact-output '{ ""include"": .}' <<<""${matrix}"")
          fi
          echo ""matrix=${matrix}"" >> ""$GITHUB_OUTPUT""
          echo ""skip_build=${skip_build}"" >> ""$GITHUB_OUTPUT""

      - name: Lowercase repository owner
        id: repository_owner
        run: |
          echo ""repository_owner=$(echo '${{ github.repository_owner }}' | tr '[:upper:]' '[:lower:]')"" >> ""$GITHUB_OUTPUT""

      - name: Date stamp
        id: date_stamp
        run: |
          echo ""date_stamp=$(date +'%Y%m%d')"" >> ""$GITHUB_OUTPUT""

  build-image:
    name: Build image
    needs: [ prepare ]
    if: always() && needs.prepare.outputs.skip_build == 'false'
    strategy:
      matrix: ${{fromJson(needs.prepare.outputs.matrix)}}
      fail-fast: false
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3
        with:
          platforms: arm64

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log into GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build image - amd64
        uses: docker/build-push-action@v6
        with:
          context: OracleLinuxDevelopers/${{ matrix.ol }}/${{ matrix.lang }}/${{ matrix.tag }}
          platforms: linux/amd64
          push: ${{ github.event_name != 'pull_request' }}
          tags: |
            ""ghcr.io/${{ needs.prepare.outputs.repository_owner }}/${{ matrix.ol }}-${{ matrix.lang }}:${{ matrix.tag }}-${{ needs.prepare.outputs.date_stamp }}${{ matrix.multi && '-amd64' || '' }}""
            ""ghcr.io/${{ needs.prepare.outputs.repository_owner }}/${{ matrix.ol }}-${{ matrix.lang }}:${{ matrix.tag }}${{ matrix.multi && '-amd64' || '' }}""

      - name: Build image - arm64
        uses: docker/build-push-action@v6
        if: matrix.multi
        with:
          context: OracleLinuxDevelopers/${{ matrix.ol }}/${{ matrix.lang }}/${{ matrix.tag }}
          platforms: linux/arm64
          push: ${{ github.event_name != 'pull_request' }}
          tags: |
            ""ghcr.io/${{ needs.prepare.outputs.repository_owner }}/${{ matrix.ol }}-${{ matrix.lang }}:${{ matrix.tag }}-${{ needs.prepare.outputs.date_stamp }}-arm64""
            ""ghcr.io/${{ needs.prepare.outputs.repository_owner }}/${{ matrix.ol }}-${{ matrix.lang }}:${{ matrix.tag }}-arm64""

      - name: Manifest
        if: matrix.multi && github.event_name != 'pull_request'
        run: |
          docker buildx imagetools create --tag \
            ""ghcr.io/${{ needs.prepare.outputs.repository_owner }}/${{ matrix.ol }}-${{ matrix.lang }}:${{ matrix.tag }}-${{ needs.prepare.outputs.date_stamp }}"" \
            ""ghcr.io/${{ needs.prepare.outputs.repository_owner }}/${{ matrix.ol }}-${{ matrix.lang }}:${{ matrix.tag }}-${{ needs.prepare.outputs.date_stamp }}-amd64"" \
            ""ghcr.io/${{ needs.prepare.outputs.repository_owner }}/${{ matrix.ol }}-${{ matrix.lang }}:${{ matrix.tag }}-${{ needs.prepare.outputs.date_stamp }}-arm64""
          docker buildx imagetools create --tag \
            ""ghcr.io/${{ needs.prepare.outputs.repository_owner }}/${{ matrix.ol }}-${{ matrix.lang }}:${{ matrix.tag }}"" \
            ""ghcr.io/${{ needs.prepare.outputs.repository_owner }}/${{ matrix.ol }}-${{ matrix.lang }}:${{ matrix.tag }}-amd64"" \
            ""ghcr.io/${{ needs.prepare.outputs.repository_owner }}/${{ matrix.ol }}-${{ matrix.lang }}:${{ matrix.tag }}-arm64""
",187,2,2,"push, workflow_dispatch",7
oracle/docker-images,build-and-push-instantclient-images.yml,"name: Build and publish Oracle Instant Client container images to GitHub Container Registry

on:
  push:
    branches:
      - main
    paths:
      - 'OracleInstantClient/*/19/*'
      - 'OracleInstantClient/*/21/*'
      - 'OracleInstantClient/*/23/*'
      - '.github/workflows/build-and-push-instantclient-images.yml'
  workflow_dispatch:

jobs:
  push:
    name: Build and push Oracle Instant Client images

    runs-on: ubuntu-latest

    outputs:
      ol: ${{ steps.linux-version.outputs.ol }}
      ic: ${{ steps.linux-version.outputs.ic }}

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          # We need ""some"" commit history to check for changed files
          fetch-depth: 32

      - name: Determine which images to build
        id: linux-version
        working-directory: OracleInstantClient
        run: |
          changes=$(mktemp)
          # workflow is only in the workflow_dispatch event payload
          workflow=""${{ github.event.workflow }}""
          if [[ -z ${workflow} ]]; then
            # Push event - retrieve list of changed files
            git diff --name-only '${{ github.event.before }}..${{ github.event.after }}' > ""${changes}""
            if grep -q build-and-push-instantclient-images.yml ""${changes}""; then
              echo ""PUSH: Action updated: rebuilding all images""
              ol=""oraclelinux7 oraclelinux8 oraclelinux9""
              ic=""19 21 23""
            else
              echo ""PUSH: Rebuilding changed images only""
              if grep -q oraclelinux7 ""${changes}""; then
                ol=""oraclelinux7""
              fi
              if grep -q oraclelinux8 ""${changes}""; then
                ol=""${ol} oraclelinux8""
              fi
              if grep -q oraclelinux9 ""${changes}""; then
                ol=""${ol} oraclelinux9""
              fi
              if grep -q /19/ ""${changes}""; then
                ic=""19""
              fi
              if grep -q /21/ ""${changes}""; then
                ic=""${ic} 21""
              fi
              if grep -q /23/ ""${changes}""; then
                ic=""${ic} 23""
              fi
            fi
          else
            echo ""MANUAL: Rebuilding all""
            ol=""oraclelinux7 oraclelinux8 oraclelinux9""
            ic=""19 21 23""
          fi
          echo ""Rebuilding: ${ol} ${ic}""
          echo ""ol=${ol}"" >> $GITHUB_OUTPUT
          echo ""ic=${ic}"" >> $GITHUB_OUTPUT
          rm ""${changes}""

      - name: Log into GitHub Container Registry
        run: echo ""${{ secrets.GITHUB_TOKEN }}"" | docker login ghcr.io -u ${GITHUB_ACTOR,,} --password-stdin

      - name: Repository owner needs to be lowercase
        id: repo-owner
        run: |
          REPO_OWNER=${{ github.repository_owner }}
          echo ""repo-owner=${REPO_OWNER,,}"" >> $GITHUB_OUTPUT

      - name: Build Oracle Instant Client
        run: |
          for o in ${{ steps.linux-version.outputs.ol }}
          do
            for i in ${{ steps.linux-version.outputs.ic }}
            do
              if [[ ${o} = ""oraclelinux7"" && ${i} = ""23"" ]]; then
                continue
              fi
              if [[ ${o} = ""oraclelinux9"" && ${i} = ""21"" ]]; then
                continue
              fi
              docker build --tag ghcr.io/${{ steps.repo-owner.outputs.repo-owner }}/${o}-instantclient:${i} OracleInstantClient/${o}/${i}
            done
          done

      - name: Push to GitHub Container Registry
        run: |
          for o in ${{ steps.linux-version.outputs.ol }}
          do
            for i in ${{ steps.linux-version.outputs.ic }}
            do
              if [[ ${o} = ""oraclelinux7"" && ${i} = ""23"" ]]; then
                continue
              fi
              if [[ ${o} = ""oraclelinux9"" && ${i} = ""21"" ]]; then
                continue
              fi
              docker push ghcr.io/${{ steps.repo-owner.outputs.repo-owner }}/${o}-instantclient:${i}
            done
          done
",115,1,2,"push, workflow_dispatch",1
oracle/docker-images,build-and-push-nosql-image.yml,"name: Build and publish NoSQL container image to GitHub Container Registry

on:
  push:
    branches:
      - main
    paths:
      - 'NoSQL/ce/*'
      - '.github/workflows/build-and-push-nosql-image.yml'
  workflow_dispatch:

env:
  IMAGE_NAME: nosql

jobs:
  push:
    name: Build and push NoSQL ce image

    runs-on: ubuntu-latest
    if: github.event_name == 'push'
    permissions:
      contents: read
      packages: write

    steps:
      -
        name: Checkout
        uses: actions/checkout@v4
      -
        name: Set up QEMU
        uses: docker/setup-qemu-action@v3
      -
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - 
        name: Repository owner needs to be lowercase
        id: repo-owner
        run: |
          REPO_OWNER=""${{ github.repository_owner }}""
          echo ""repo-owner=${REPO_OWNER,,}"" >> ""$GITHUB_OUTPUT""
      - 
        name: Get current date
        id: date
        run: echo ""date=$(date +'%Y-%m')"" >> ""$GITHUB_OUTPUT""
      -
        name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - 
        name: Generate container image metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ghcr.io/${{ steps.repo-owner.outputs.repo-owner }}/${{ env.IMAGE_NAME }}
          flavor: |
            latest=false
          tags: |
                type=raw,value=latest-ce
                type=raw,value=${{ env.TAG }}
        env:
          TAG: ${{ steps.date.outputs.date }}-ce 
      -
        name: Build and push
        uses: docker/build-push-action@v6
        with:
          context: ./NoSQL/ce/
          platforms: linux/amd64,linux/arm64
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}

",74,1,2,"push, workflow_dispatch",6
oracle/docker-images,build-and-push-nosql-sec-image.yml,"name: Build and publish NoSQL secure container image to GitHub Container Registry

on:
  push:
    branches:
      - main
    paths:
      - 'NoSQL/ce-sec/*'
      - '.github/workflows/build-and-push-nosql-sec-image.yml'
  workflow_dispatch:

env:
  IMAGE_NAME: nosql

jobs:
  push:
    name: Build and push NoSQL ce image

    runs-on: ubuntu-latest
    if: github.event_name == 'push'
    permissions:
      contents: read
      packages: write

    steps:
      -
        name: Checkout
        uses: actions/checkout@v4
      -
        name: Set up QEMU
        uses: docker/setup-qemu-action@v3
      -
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - 
        name: Repository owner needs to be lowercase
        id: repo-owner
        run: |
          REPO_OWNER=""${{ github.repository_owner }}""
          echo ""repo-owner=${REPO_OWNER,,}"" >> ""$GITHUB_OUTPUT""
      - 
        name: Get current date
        id: date
        run: echo ""date=$(date +'%Y-%m')"" >> ""$GITHUB_OUTPUT""
      -
        name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - 
        name: Generate container image metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ghcr.io/${{ steps.repo-owner.outputs.repo-owner }}/${{ env.IMAGE_NAME }}
          flavor: |
            latest=false
          tags: |
                type=raw,value=latest-ce-sec
                type=raw,value=${{ env.TAG }}
        env:
          TAG: ${{ steps.date.outputs.date }}-ce-sec
      -
        name: Build and push
        uses: docker/build-push-action@v6
        with:
          context: ./NoSQL/ce-sec/
          platforms: linux/amd64,linux/arm64
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
 
",74,1,2,"push, workflow_dispatch",6
oracle/docker-images,build-and-push-oci-cli-image.yml,"name: Build and Push OCI CLI Image

on:
  workflow_dispatch:
  schedule:
    - cron: '30 0 * * 3'  # Tuesday@17h30 Pacific = Wednesday@00h30 UTC
  push:
    branches: [ ""main"" ]
    paths:
      - 'OracleCloudInfrastructuree/oci-cli/*'
      - '.github/workflows/build-and-push-oci-cli-image.yml'
  pull_request:
    paths:
      - 'OracleCloudInfrastructuree/oci-cli/*'
      - '.github/workflows/build-and-push-oci-cli-image.yml'
    branches: [ ""main"" ]


env:
  IMAGE_NAME: oci-cli

jobs:
  build:

    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Setup Docker BuildX
        uses: docker/setup-buildx-action@v3

      - name: Lowercase repository owner
        id: repo-owner
        run: |
          echo ""repo-owner=$(echo '${{ github.repository_owner }}' | tr '[:upper:]' '[:lower:]')"" >> ""$GITHUB_OUTPUT""

      - name: Log into GitHub Container Registry
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Generate container image metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ghcr.io/${{ steps.repo-owner.outputs.repo-owner }}/${{ env.IMAGE_NAME }}
          flavor: |
            latest=true
          labels: |
            provider=${REPO_OWNER}
            issues=https://github.com/${{ steps.repo-owner.outputs.repo-owner }}/docker-images/issues
            org.opencontainers.image.licenses=UPL-1.0
            org.opencontainers.image.vendor=${REPO_OWNER}
            org.opencontainers.image.title=OCI CLI
            org.opencontainers.image.description=Oracle Cloud Infrastructure Command Line Interface
            org.opencontainers.image.source=https://github.com/oracle/docker-images/tree/main/OracleCloudInfrastructure/oci-cli
            org.opencontainers.image.documentation=https://docs.oracle.com/en-us/iaas/Content/API/Concepts/cliconcepts.htm
            org.opencontainers.image.url=https://github.com/${{ steps.repo-owner.outputs.repo-owner }}/docker-images/pkgs/container/oci-cli
            org.opencontainers.image.base.name=ghcr.io/${{ steps.repo-owner.outputs.repo-owner }}/oci-cli:latest
          tags: |
            type=schedule,pattern={{date 'YYYYMMDD'}}
            type=sha

      - name: Build and push image
        id: build-and-push
        uses: docker/build-push-action@v6
        with:
          build-args: |
            BUILDTIME=${{ fromJSON(steps.meta.outputs.json).labels['org.opencontainers.image.created'] }}
            VERSION=${{ fromJSON(steps.meta.outputs.json).labels['org.opencontainers.image.version'] }}
            REVISION=${{ fromJSON(steps.meta.outputs.json).labels['org.opencontainers.image.revision'] }}
          context: ./OracleCloudInfrastructure/oci-cli/
          push: ${{ github.event_name != 'pull_request' }}
          platforms: linux/amd64, linux/arm64
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
",87,1,4,"workflow_dispatch, schedule, push, pull_request",6
oracle/docker-images,super-linter.yml,"---
name: Lint new or modified files using Super Linter

on:
  pull_request:
    branches: [main]

jobs:
  lint-new-modified-files:
    name: Lint new or modified files
    runs-on: ubuntu-latest

    permissions:
      contents: read
      packages: read
      statuses: write

    steps:
      - name: Checkout Git repository with history
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set environment variables
        run: cat .github/super-linter.env >> ""$GITHUB_ENV""

      - name: Run Super Linter
        uses: github/super-linter/slim@v7
        env:
          DEFAULT_BRANCH: main
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
",31,1,1,pull_request,3
oracle/graal,cdt-inspect.yml,"#
# Copyright (c) 2023, 2023, Oracle and/or its affiliates. All rights reserved.
# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
#
# The Universal Permissive License (UPL), Version 1.0
#
# Subject to the condition set forth below, permission is hereby granted to any
# person obtaining a copy of this software, associated documentation and/or
# data (collectively the ""Software""), free of charge and under any and all
# copyright rights in the Software, and any and all patent rights owned or
# freely licensable by each licensor hereunder covering either (i) the
# unmodified Software as contributed to or provided by such licensor, or (ii)
# the Larger Works (as defined below), to deal in both
#
# (a) the Software, and
#
# (b) any piece of software and/or hardware listed in the lrgrwrks.txt file if
# one is included with the Software each a ""Larger Work"" to which the Software
# is contributed by such licensors),
#
# without restriction, including without limitation the rights to copy, create
# derivative works of, display, perform, and distribute the Software and make,
# use, sell, offer for sale, import, export, have made, and have sold the
# Software and the Larger Work(s), and to sublicense the foregoing rights on
# either these or other terms.
#
# This license is subject to the following condition:
#
# The above copyright notice and either this complete permission notice or at a
# minimum a reference to the UPL must be included in all copies or substantial
# portions of the Software.
#
# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# Intergation test of CDT with Inspector backend.
name: Weekly CDT Inspector

on:
  schedule:
    - cron: ""30 2 * * 2,5"" # Tuesday and Friday at 2:30

env:
  JAVA_HOME: ${{ github.workspace }}/jdk
  JDK_VERSION: ""latest""
  MX_PATH: ${{ github.workspace }}/mx
  SE_SKIP_DRIVER_IN_PATH: ""true""

jobs:
  build:

    runs-on: ubuntu-latest

    if: github.repository == 'oracle/graal'

    steps:
    - name: Checkout oracle/graal
      uses: actions/checkout@v4
      with:
        path: ${{ github.workspace }}/graal
    - name: Checkout oracle/graaljs
      uses: actions/checkout@v4
      with:
        repository: oracle/graaljs
        sparse-checkout: |
          graal-js
        path: ${{ github.workspace }}/js
    - name: Checkout graalvm/mx
      uses: actions/checkout@v4
      with:
        repository: graalvm/mx.git
        ref: master
        path: ${{ env.MX_PATH }}
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.8'
    - name: Fetch LabsJDK
      run: |
        mkdir jdk-dl
        ${MX_PATH}/mx --java-home= fetch-jdk --jdk-id labsjdk-ce-${JDK_VERSION} --to jdk-dl --alias ${JAVA_HOME}
    - run: |
        cd ${{ github.workspace }}/graal/vm
        ${MX_PATH}/mx --dy /tools,graal-js build
        cd tests/gh_workflows/CDTInspectorTest
        mvn -q compile
        mvn -q exec:exec -Dtestargs=""${{ github.workspace }}/graal/sdk/latest_graalvm_home/bin/js scripts/StepTest.js""
",92,1,1,schedule,4
oracle/graal,main.yml,"#
# Copyright (c) 2020, 2025, Oracle and/or its affiliates. All rights reserved.
# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
#
# The Universal Permissive License (UPL), Version 1.0
#
# Subject to the condition set forth below, permission is hereby granted to any
# person obtaining a copy of this software, associated documentation and/or
# data (collectively the ""Software""), free of charge and under any and all
# copyright rights in the Software, and any and all patent rights owned or
# freely licensable by each licensor hereunder covering either (i) the
# unmodified Software as contributed to or provided by such licensor, or (ii)
# the Larger Works (as defined below), to deal in both
#
# (a) the Software, and
#
# (b) any piece of software and/or hardware listed in the lrgrwrks.txt file if
# one is included with the Software each a ""Larger Work"" to which the Software
# is contributed by such licensors),
#
# without restriction, including without limitation the rights to copy, create
# derivative works of, display, perform, and distribute the Software and make,
# use, sell, offer for sale, import, export, have made, and have sold the
# Software and the Larger Work(s), and to sublicense the foregoing rights on
# either these or other terms.
#
# This license is subject to the following condition:
#
# The above copyright notice and either this complete permission notice or at a
# minimum a reference to the UPL must be included in all copies or substantial
# portions of the Software.
#
# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
#
name: GraalVM Gate

on:
  push:
    branches:
      - 'release/**'
    paths-ignore:
      - '.devcontainer/**'
      - '.github/workflows/quarkus.yml'
      - '**.md'
      - '**.jsonnet'
      - '**.libjsonnet'
  pull_request:
    paths-ignore:
      - '.devcontainer/**'
      - '.github/workflows/quarkus.yml'
      - '**.md'
      - '**.jsonnet'
      - '**.libjsonnet'
  schedule:
  - cron: '30 0 * * 1'
  # Enable manual dispatch of the workflow
  # see https://docs.github.com/en/actions/managing-workflow-runs/manually-running-a-workflow
  workflow_dispatch:

# The following aims to reduce CI CPU cycles by:
# 1. Cancelling any previous builds of this PR when pushing new changes to it
# 2. Cancelling any previous builds of a branch when pushing new changes to it in a fork
# 3. Cancelling any pending builds, but not active ones, when pushing to a branch in the main
#    repository. This prevents us from constantly cancelling CI runs, while being able to skip
#    intermediate builds. E.g., if we perform two pushes the first one will start a CI job and
#    the second one will add another one to the queue; if we perform a third push while the
#    first CI job is still running the previously queued CI job (for the second push) will be
#    cancelled and a new CI job will be queued for the latest (third) push.
concurrency:
  group: ""workflow = ${{ github.workflow }}, ref = ${{ github.event.ref }}, pr = ${{ github.event.pull_request.id }}""
  cancel-in-progress: ${{ github.event_name == 'pull_request' || github.repository != 'oracle/graal' }}

env:
  JAVA_HOME: ${{ github.workspace }}/jdk
  TOOLS_JAVA_HOME_LOCATION: ${{ github.workspace }}/tools-jdk
  LANG: en_US.UTF-8
  MX_GIT_CACHE: refcache
  MX_PATH: ${{ github.workspace }}/mx
  MX_PYTHON: python3.8
  # Enforce experimental option checking in CI (GR-47922)
  NATIVE_IMAGE_EXPERIMENTAL_OPTIONS_ARE_FATAL: ""true""

permissions:
  contents: read # to fetch code (actions/checkout)

jobs:
  build-graalvm-linux:
    name: /${{ matrix.env.PRIMARY }} ${{ matrix.env.GATE_TAGS }} JDK${{ matrix.env.JDK_VERSION }}
    runs-on: ${{ matrix.os || 'ubuntu-22.04' }}
    timeout-minutes: 60
    strategy:
      fail-fast: false
      matrix:
        include:
          # /compiler
          - env:
              JDK_VERSION: ""latest""
              TOOLS_JDK_VERSION: ""21""
              GATE_TAGS: ""style,fullbuild,test""
              PRIMARY: ""compiler""
          - env:
              JDK_VERSION: ""latest""
              GATE_TAGS: ""build,bootstraplite""
              PRIMARY: ""compiler""
          # /espresso
          - env:
              JDK_VERSION: ""latest""
              TOOLS_JDK_VERSION: ""21""
              GATE_TAGS: ""style,fullbuild""
              PRIMARY: ""espresso""
          # /substratevm
          - env:
              JDK_VERSION: ""latest""
              TOOLS_JDK_VERSION: ""21""
              GATE_TAGS: ""style,fullbuild""
              PRIMARY: ""substratevm""
          - env:
              JDK_VERSION: ""latest""
              GATE_TAGS: ""build,helloworld,native_unittests""
              PRIMARY: ""substratevm""
              PIP_PACKAGES: ""jsonschema==4.6.1""
          - os: ubuntu-24.04
            env:
              JDK_VERSION: ""latest""
              GATE_TAGS: ""build,debuginfotest""
              PRIMARY: ""substratevm""
          - env:
              JDK_VERSION: ""latest""
              GATE_TAGS: ""hellomodule""
              PRIMARY: ""substratevm""
          # /sulong
          - env:
              JDK_VERSION: ""21""
              GATE_TAGS: ""style,fullbuild,sulongBasic""
              PRIMARY: ""sulong""
          # /truffle
          - env:
              JDK_VERSION: ""21""
              GATE_TAGS: ""style,fullbuild,fulltest""
              PRIMARY: ""truffle""
          # /vm
          - env:
              JDK_VERSION: ""latest""
              GATE_TAGS: ""build,sulong""
              GATE_OPTS: ""--no-warning-as-error""
              PRIMARY: ""vm""
              DYNAMIC_IMPORTS: ""/sulong,/substratevm""
              NATIVE_IMAGES: ""graalvm-native-binutil,graalvm-native-clang,graalvm-native-clang-cl,graalvm-native-clang++,graalvm-native-ld,lib:llvmvm""
              DISABLE_POLYGLOT: true
              DISABLE_LIBPOLYGLOT: true
          - env:
              JDK_VERSION: ""latest""
              GATE_TAGS: ""build""
              GATE_OPTS: ""--no-warning-as-error""
              PRIMARY: ""vm""
              DYNAMIC_IMPORTS: ""/tools,/substratevm,/sulong""
              NATIVE_IMAGES: ""lib:jvmcicompiler,native-image,lib:native-image-agent,lib:native-image-diagnostics-agent,polyglot""
              WITHOUT_VCS: true
    env:
      JDT: builtin # Compile with ECJ (and javac) as part of gate runs tagged with 'fullbuild'
      MX_RUNS_DEBUG: ${{ contains(matrix.env.GATE_TAGS, 'debug') || matrix.env.GATE_TAGS == '' }}
      MX_RUNS_STYLE: ${{ contains(matrix.env.GATE_TAGS, 'style') || matrix.env.GATE_TAGS == '' }}
    steps:
    - name: Checkout oracle/graal
      uses: actions/checkout@v4
      with:
        ref: ${{ github.ref }} # Lock ref to current branch to avoid fetching others
        fetch-depth: ""${{ env.MX_RUNS_STYLE && '0' || '1' }}"" # The style gate needs the full commit history for checking copyright years
    - name: Determine mx version
      run: echo ""MX_VERSION=$(jq -r '.mx_version' common.json)"" >> ${GITHUB_ENV}
    - name: Checkout graalvm/mx
      uses: actions/checkout@v4
      with:
        repository: graalvm/mx.git
        ref: ${{ env.MX_VERSION }}
        path: ${{ env.MX_PATH }}
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.8'
    - name: Fetch LabsJDK
      env: ${{ matrix.env }}
      run: |
        mkdir jdk-dl
        ${MX_PATH}/mx --java-home= fetch-jdk --jdk-id labsjdk-ce-${JDK_VERSION} --to jdk-dl --alias ${JAVA_HOME}
    - name: Fetch Tools JDK
      env: ${{ matrix.env }}
      if: ${{ matrix.env.TOOLS_JDK_VERSION != '' }}
      run: |
        ${MX_PATH}/mx --java-home= fetch-jdk --jdk-id labsjdk-ce-${TOOLS_JDK_VERSION} --to jdk-dl --alias ${TOOLS_JAVA_HOME_LOCATION}
    - name: Update dependency cache
      if: ${{ env.MX_RUNS_DEBUG == 'true' || env.MX_RUNS_STYLE == 'true' }}
      run: sudo apt update
    - name: Install debug dependencies
      if: ${{ env.MX_RUNS_DEBUG == 'true' }}
      run: sudo apt install gdb
    - name: Install style dependencies
      if: ${{ env.MX_RUNS_STYLE == 'true' }}
      run: |
        sudo apt install python3-pip python-setuptools
        sudo pip install $(jq -r '[.pip | to_entries[] | join("""")] | join("" "")' common.json)
    - name: Install additional pip packages
      if: ${{ matrix.env.PIP_PACKAGES != '' }}
      run: ${MX_PYTHON} -m pip install ${{ matrix.env.PIP_PACKAGES }}
    - name: Download Eclipse
      if: ${{ env.MX_RUNS_STYLE == 'true' }}
      run: |
        ECLIPSE_TAR=eclipse.tar.gz
        ECLIPSE_ORG_VERSION=$(jq -r '.eclipse.short_version' common.json)
        ECLIPSE_ORG_TIMESTAMP=$(jq -r '.eclipse.timestamp' common.json)
        wget --no-verbose https://archive.eclipse.org/eclipse/downloads/drops4/R-${ECLIPSE_ORG_VERSION}-${ECLIPSE_ORG_TIMESTAMP}/eclipse-SDK-${ECLIPSE_ORG_VERSION}-linux-gtk-x86_64.tar.gz -O $ECLIPSE_TAR
        tar -xzf ${ECLIPSE_TAR}
        echo ""ECLIPSE_EXE=${PWD}/eclipse/eclipse"" >> $GITHUB_ENV
    - name: Remove .git directory
      if: ${{ matrix.env.WITHOUT_VCS }}
      run: rm -rf .git
    - name: Build GraalVM and run gate with tags
      env: ${{ matrix.env }}
      run: ${MX_PATH}/mx --primary-suite-path ${PRIMARY} --java-home=${JAVA_HOME} --tools-java-home=${{ matrix.env.TOOLS_JDK_VERSION != '' && env.TOOLS_JAVA_HOME_LOCATION || '' }} gate --strict-mode ${{ matrix.env.GATE_OPTS }} --tags ${GATE_TAGS}
      if: ${{ matrix.env.GATE_TAGS != '' }}
    - name: Build GraalVM and run gate without tags
      env: ${{ matrix.env }}
      run: ${MX_PATH}/mx --primary-suite-path ${PRIMARY} --java-home=${JAVA_HOME} gate --strict-mode ${{ matrix.env.GATE_OPTS }}
      if: ${{ matrix.env.GATE_TAGS == '' }}
  build-graalvm-windows:
    name: /substratevm on Windows
    runs-on: windows-2022
    timeout-minutes: 60
    env:
      MX_PYTHON: 'python'
      PYTHONIOENCODING: 'utf-8'
    steps:
    - name: Checkout oracle/graal
      uses: actions/checkout@v4
      with:
        ref: ${{ github.ref }} # Lock ref to current branch to avoid fetching others
    - name: Determine mx version
      shell: bash
      run: echo ""MX_VERSION=$(jq -r '.mx_version' common.json)"" >> ${GITHUB_ENV}
    - name: Checkout graalvm/mx
      uses: actions/checkout@v4
      with:
        repository: graalvm/mx.git
        ref: ${{ env.MX_VERSION }}
        path: ${{ env.MX_PATH }}
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.8'
    - name: Fetch LabsJDK
      shell: bash
      run: |
        mkdir jdk-dl
        ${MX_PATH}/mx --java-home= fetch-jdk --jdk-id labsjdk-ce-latest --to jdk-dl --alias ${JAVA_HOME}
    - name: Build GraalVM via cmd.exe
      shell: cmd
      run: |
        call ""C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Auxiliary\Build\vcvarsall.bat"" x64
        call ${{ env.MX_PATH }}\mx.cmd -p substratevm --native-images=native-image --components=""Native Image"" build
        call ${{ env.MX_PATH }}\mx.cmd -p substratevm --native-images=native-image --components=""Native Image"" graalvm-home > graalvm-home-with-forward-slashes.txt
        set /p GRAALVM_HOME=<graalvm-home-with-forward-slashes.txt
        setlocal enabledelayedexpansion
        set ""GRAALVM_HOME=%GRAALVM_HOME:/=\%""
        echo %GRAALVM_HOME%\bin>>%GITHUB_PATH%
        echo GRAALVM_HOME=%GRAALVM_HOME%>>%GITHUB_ENV%
    - name: Test GraalVM
      run: |
        native-image --version
        native-image -m jdk.httpserver
    
",276,2,4,"push, pull_request, schedule, workflow_dispatch",6
oracle/graal,micronaut.yml,"#
# Copyright (c) 2024, 2025, Oracle and/or its affiliates. All rights reserved.
# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
#
# The Universal Permissive License (UPL), Version 1.0
#
# Subject to the condition set forth below, permission is hereby granted to any
# person obtaining a copy of this software, associated documentation and/or
# data (collectively the ""Software""), free of charge and under any and all
# copyright rights in the Software, and any and all patent rights owned or
# freely licensable by each licensor hereunder covering either (i) the
# unmodified Software as contributed to or provided by such licensor, or (ii)
# the Larger Works (as defined below), to deal in both
#
# (a) the Software, and
#
# (b) any piece of software and/or hardware listed in the lrgrwrks.txt file if
# one is included with the Software each a ""Larger Work"" to which the Software
# is contributed by such licensors),
#
# without restriction, including without limitation the rights to copy, create
# derivative works of, display, perform, and distribute the Software and make,
# use, sell, offer for sale, import, export, have made, and have sold the
# Software and the Larger Work(s), and to sublicense the foregoing rights on
# either these or other terms.
#
# This license is subject to the following condition:
#
# The above copyright notice and either this complete permission notice or at a
# minimum a reference to the UPL must be included in all copies or substantial
# portions of the Software.
#
# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
#
name: Weekly Micronaut Tests

on:
  pull_request:
    paths:
      - '.github/workflows/micronaut.yml'
  schedule:
  - cron: '0 2 * * 1'
  workflow_dispatch:

env:
  MICRONAUT_CORE_PATH: ${{ github.workspace }}/micronaut-core
  MICRONAUT_JAVA_VERSION: 21
  # Enforce experimental option checking in CI (GR-47922)
  NATIVE_IMAGE_EXPERIMENTAL_OPTIONS_ARE_FATAL: 'true'

permissions:
  contents: read # to fetch code (actions/checkout)

jobs:
  build-graalvm-and-micronaut:
    name: Native Tests
    runs-on: ubuntu-22.04
    if: (github.event_name == 'schedule' && github.repository == 'oracle/graal') || (github.event_name != 'schedule')
    steps:
    - name: Checkout oracle/graal
      uses: actions/checkout@v4
    - name: Build GraalVM JDK
      uses: ./.github/actions/build-graalvm
      with:
        java-version: ${{ env.MICRONAUT_JAVA_VERSION }}
    - name: Run nativeTest in Micronaut launch project
      run: |
        curl --fail --silent --location --retry 3 --max-time 10 --output demo.zip --request GET 'https://launch.micronaut.io/create/default/com.example.demo?lang=JAVA&build=GRADLE&test=JUNIT&javaVersion=JDK_${{ env.MICRONAUT_JAVA_VERSION }}'
        unzip demo.zip
        cd demo
        ./gradlew nativeTest
    - name: Checkout micronaut-projects/micronaut-core
      uses: actions/checkout@v4
      with:
        repository: micronaut-projects/micronaut-core
        path: ${{ env.MICRONAUT_CORE_PATH }}
    - name: Run nativeTest in micronaut-core
      run: |
        cd ${{ env.MICRONAUT_CORE_PATH }}
        ./gradlew nativeTest
",86,1,3,"pull_request, schedule, workflow_dispatch",3
oracle/graal,ni-layers.yml,"#
# Copyright (c) 2024, 2024, Oracle and/or its affiliates. All rights reserved.
# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
#
# The Universal Permissive License (UPL), Version 1.0
#
# Subject to the condition set forth below, permission is hereby granted to any
# person obtaining a copy of this software, associated documentation and/or
# data (collectively the ""Software""), free of charge and under any and all
# copyright rights in the Software, and any and all patent rights owned or
# freely licensable by each licensor hereunder covering either (i) the
# unmodified Software as contributed to or provided by such licensor, or (ii)
# the Larger Works (as defined below), to deal in both
#
# (a) the Software, and
#
# (b) any piece of software and/or hardware listed in the lrgrwrks.txt file if
# one is included with the Software each a ""Larger Work"" to which the Software
# is contributed by such licensors),
#
# without restriction, including without limitation the rights to copy, create
# derivative works of, display, perform, and distribute the Software and make,
# use, sell, offer for sale, import, export, have made, and have sold the
# Software and the Larger Work(s), and to sublicense the foregoing rights on
# either these or other terms.
#
# This license is subject to the following condition:
#
# The above copyright notice and either this complete permission notice or at a
# minimum a reference to the UPL must be included in all copies or substantial
# portions of the Software.
#
# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
#
name: Weekly Native Image Layer Building Tests

on:
  pull_request:
    paths:
      - '.github/workflows/ni-layers.yml'
      - 'vm/tests/gh_workflows/NILayerTests/**'
  schedule:
  - cron: ""0 0 * * 1"" # Once a week, at midnight on Monday (00:00 UTC)
  workflow_dispatch:

env:
  LIBRARY_METADATA_PATH: ${{ github.workspace }}/vm/tests/gh_workflows/NILayerTests
  JAVA_VERSION: 21
  PYTHON_VERSION: 3.12.3

jobs:
  build-graalvm-and-populate-matrix:
    name: Build GraalVM and populate matrix
    runs-on: ubuntu-latest
    if: (github.repository=='oracle/graal')
    outputs:
        matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
    - name: Checkout oracle/graal
      uses: actions/checkout@v4
    - name: Build GraalVM JDK
      uses: ./.github/actions/build-graalvm
      with:
        native-images: 'native-image,native-image-configure,lib:native-image-agent'
        components: 'Native Image,Native Image Configure Tool'
        java-version: ${{ env.JAVA_VERSION }}
    - name: Tar GraalVM JDK
      shell: bash
      run: tar -czvhf graalvm.tgz -C $(dirname ${GRAALVM_HOME}) $(basename ${GRAALVM_HOME})
    - name: Persist GraalVM JDK build
      uses: actions/upload-artifact@v4
      with:
        name: graalvm
        path: graalvm.tgz
    - name: Setup python
      uses: actions/setup-python@v5
      with:
        python-version: '${{ env.PYTHON_VERSION }}'
    - name: Populate matrix
      id: set-matrix
      run: python3 ${{ env.LIBRARY_METADATA_PATH }}/build_native_image_layer.py ${{ env.LIBRARY_METADATA_PATH }}/

  test-native-image-layer-build:
    name: ${{ matrix.coordinates }}
    runs-on: ubuntu-latest
    env:
      GRAALVM_HOME: ${{ github.workspace }}/graalvm
    timeout-minutes: 30
    needs:  build-graalvm-and-populate-matrix
    strategy:
      fail-fast: false
      matrix: 
        coordinates: ${{ fromJson(needs. build-graalvm-and-populate-matrix.outputs.matrix).coordinates }}
    steps:
      - name: Checkout oracle/graal
        uses: actions/checkout@v4
      - name: Download GraalVM JDK build
        uses: actions/download-artifact@95815c38cf2ff2164869cbab79da8d1f422bc89e # v4.2.1
        with:
          name: graalvm
          path: .
      - name: Extract GraalVM JDK build
        run: tar -xzvf graalvm.tgz -C $(dirname ${GRAALVM_HOME})
      - name: ""Setup JAVA_HOME""
        uses: actions/setup-java@v4
        with:
          distribution: 'oracle'
          java-version: ${{ env.JAVA_VERSION }}
      - name: Setup python
        uses: actions/setup-python@v5
        with:
          python-version: '${{ env.PYTHON_VERSION }}'
      - name: Build layer
        run: |
            python3 ${{ env.LIBRARY_METADATA_PATH }}/build_native_image_layer.py ${{ env.GRAALVM_HOME }}/bin/native-image ""${{ matrix.coordinates }}""
",121,2,3,"pull_request, schedule, workflow_dispatch",8
oracle/graal,quarkus.yml,"#
# Copyright (c) 2020, 2025, Oracle and/or its affiliates. All rights reserved.
# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
#
# The Universal Permissive License (UPL), Version 1.0
#
# Subject to the condition set forth below, permission is hereby granted to any
# person obtaining a copy of this software, associated documentation and/or
# data (collectively the ""Software""), free of charge and under any and all
# copyright rights in the Software, and any and all patent rights owned or
# freely licensable by each licensor hereunder covering either (i) the
# unmodified Software as contributed to or provided by such licensor, or (ii)
# the Larger Works (as defined below), to deal in both
#
# (a) the Software, and
#
# (b) any piece of software and/or hardware listed in the lrgrwrks.txt file if
# one is included with the Software each a ""Larger Work"" to which the Software
# is contributed by such licensors),
#
# without restriction, including without limitation the rights to copy, create
# derivative works of, display, perform, and distribute the Software and make,
# use, sell, offer for sale, import, export, have made, and have sold the
# Software and the Larger Work(s), and to sublicense the foregoing rights on
# either these or other terms.
#
# This license is subject to the following condition:
#
# The above copyright notice and either this complete permission notice or at a
# minimum a reference to the UPL must be included in all copies or substantial
# portions of the Software.
#
# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
#
name: Weekly Quarkus Tests

on:
  pull_request:
    paths:
      - '.github/workflows/quarkus.yml'
  schedule:
  - cron: '0 3 * * 1'
  workflow_dispatch:

env:
  COMMON_MAVEN_ARGS: ""-e -B --settings .github/mvn-settings.xml --fail-at-end""
  DB_NAME: hibernate_orm_test
  DB_PASSWORD: hibernate_orm_test
  DB_USER: hibernate_orm_test
  NATIVE_TEST_MAVEN_ARGS: ""-Dtest-containers -Dstart-containers -Dquarkus.native.native-image-xmx=6g -Dnative -Dnative.surefire.skip -Dformat.skip -Dno-descriptor-tests install -DskipDocs -Dquarkus.native.container-build=false""
  QUARKUS_JAVA_VERSION: 17 # Use Java 17 to build Quarkus as that's the lowest supported JDK version currently
  QUARKUS_PATH: ${{ github.workspace }}/quarkus

permissions: {}
jobs:
  build-quarkus-and-graalvm:
    permissions:
      contents: read # to fetch code (actions/checkout)

    name: Nightly Quarkus and GraalVM build
    runs-on: ubuntu-22.04
    if: (github.event_name == 'schedule' && github.repository == 'oracle/graal') || (github.event_name != 'schedule')
    outputs:
      matrix: ${{ steps.read.outputs.matrix }}
    steps:
    - name: Checkout oracle/graal
      uses: actions/checkout@v4
    - name: Build GraalVM JDK
      uses: ./.github/actions/build-graalvm
      with:
        java-version: ${{ env.QUARKUS_JAVA_VERSION }}
    - name: Get latest Quarkus release
      run: |
        export QUARKUS_VERSION=main #$(curl https://repo1.maven.org/maven2/io/quarkus/quarkus-bom/maven-metadata.xml | awk -F""[<>]"" '/latest/ {print $3}')
        echo Getting Quarkus $QUARKUS_VERSION
        curl --output quarkus.tgz -sL https://api.github.com/repos/quarkusio/quarkus/tarball/$QUARKUS_VERSION
        mkdir ${QUARKUS_PATH}
        tar xf quarkus.tgz -C ${QUARKUS_PATH} --strip-components=1
    - uses: actions/cache@v4
      with:
        path: ~/.m2/repository
        key: ${{ runner.os }}-maven-${{ hashFiles('**/pom.xml') }}
        restore-keys: |
          ${{ runner.os }}-maven-
    - name: Tar GraalVM JDK
      shell: bash
      run: tar -czvhf graalvm.tgz -C $(dirname ${GRAALVM_HOME}) $(basename ${GRAALVM_HOME})
    - name: Persist GraalVM JDK build
      uses: actions/upload-artifact@v4
      with:
        name: graalvm
        path: graalvm.tgz
    - name: Build Quarkus
      run: |
        cd ${QUARKUS_PATH}
        eval ./mvnw -e -B -Dquickly
    - name: Read json file with native-tests matrix
      id: read
      run: |
        json=$(tr -d '\n' < ${QUARKUS_PATH}/.github/native-tests.json )
        echo $json
        echo ""matrix=${json}"" >> $GITHUB_OUTPUT
    - name: Tar Maven Repo
      shell: bash
      run: tar -czvf maven-repo.tgz -C ~ .m2/repository
    - name: Persist Maven Repo
      uses: actions/upload-artifact@v4
      with:
        name: maven-repo
        path: maven-repo.tgz

  native-tests:
    name: Native Tests - ${{matrix.category}}
    needs: build-quarkus-and-graalvm
    runs-on: ubuntu-latest
    env:
      GRAALVM_HOME: ${{ github.workspace }}/graalvm # identical to the one in ./.github/actions/build-graalvm
    # Ignore the following YAML Schema error
    timeout-minutes: ${{matrix.timeout}}
    strategy:
      max-parallel: 8
      fail-fast: false
      matrix: ${{ fromJson(needs.build-quarkus-and-graalvm.outputs.matrix) }}
    steps:
      - name: Download GraalVM JDK build
        if: startsWith(matrix.os-name, 'ubuntu')
        uses: actions/download-artifact@95815c38cf2ff2164869cbab79da8d1f422bc89e # v4.2.1
        with:
          name: graalvm
          path: .
      - name: Extract GraalVM JDK build
        if: startsWith(matrix.os-name, 'ubuntu')
        shell: bash
        run: tar -xzvf graalvm.tgz -C $(dirname ${GRAALVM_HOME})
      - name: Get latest Quarkus release
        if: startsWith(matrix.os-name, 'ubuntu')
        run: |
          export QUARKUS_VERSION=main #$(curl https://repo1.maven.org/maven2/io/quarkus/quarkus-bom/maven-metadata.xml | awk -F""[<>]"" '/latest/ {print $3}')
          echo Getting Quarkus $QUARKUS_VERSION
          curl --output quarkus.tgz -sL https://api.github.com/repos/quarkusio/quarkus/tarball/$QUARKUS_VERSION
          mkdir ${QUARKUS_PATH}
          tar xf quarkus.tgz -C ${QUARKUS_PATH} --strip-components=1
      - name: Reclaim Disk Space
        if: startsWith(matrix.os-name, 'ubuntu')
        run: ${QUARKUS_PATH}/.github/ci-prerequisites.sh
      - name: Download Maven Repo
        if: startsWith(matrix.os-name, 'ubuntu')
        uses: actions/download-artifact@95815c38cf2ff2164869cbab79da8d1f422bc89e # v4.2.1
        with:
          name: maven-repo
          path: .
      - name: Extract Maven Repo
        if: startsWith(matrix.os-name, 'ubuntu')
        shell: bash
        run: tar -xzf maven-repo.tgz -C ~
      - uses: actions/setup-java@v4
        with:
          distribution: 'oracle'
          java-version: '17'
      - name: Build with Maven
        if: startsWith(matrix.os-name, 'ubuntu')
        env:
          TEST_MODULES: ${{matrix.test-modules}}
        run: |
          cd ${QUARKUS_PATH}
          ${GRAALVM_HOME}/bin/native-image --version
          ./mvnw $COMMON_MAVEN_ARGS -f integration-tests -pl ""$TEST_MODULES"" $NATIVE_TEST_MAVEN_ARGS
      - name: Prepare failure archive (if maven failed)
        if: failure()
        shell: bash
        run: find . -type d -name '*-reports' -o -wholename '*/build/reports/tests/functionalTest' | tar -czf test-reports.tgz -T -
      - name: Upload failure Archive (if maven failed)
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: test-reports-native-${{matrix.category}}
          path: 'test-reports.tgz'
",183,2,3,"pull_request, schedule, workflow_dispatch",9
oracle/graal,reachability-metadata.yml,"#
# Copyright (c) 2024, 2024, Oracle and/or its affiliates. All rights reserved.
# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
#
# The Universal Permissive License (UPL), Version 1.0
#
# Subject to the condition set forth below, permission is hereby granted to any
# person obtaining a copy of this software, associated documentation and/or
# data (collectively the ""Software""), free of charge and under any and all
# copyright rights in the Software, and any and all patent rights owned or
# freely licensable by each licensor hereunder covering either (i) the
# unmodified Software as contributed to or provided by such licensor, or (ii)
# the Larger Works (as defined below), to deal in both
#
# (a) the Software, and
#
# (b) any piece of software and/or hardware listed in the lrgrwrks.txt file if
# one is included with the Software each a ""Larger Work"" to which the Software
# is contributed by such licensors),
#
# without restriction, including without limitation the rights to copy, create
# derivative works of, display, perform, and distribute the Software and make,
# use, sell, offer for sale, import, export, have made, and have sold the
# Software and the Larger Work(s), and to sublicense the foregoing rights on
# either these or other terms.
#
# This license is subject to the following condition:
#
# The above copyright notice and either this complete permission notice or at a
# minimum a reference to the UPL must be included in all copies or substantial
# portions of the Software.
#
# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
#
name: Weekly Reachability Metadata Tests

on:
  pull_request:
    paths:
      - '.github/workflows/reachability-metadata.yml'
  schedule:
  - cron: '0 1 * * 1'
  workflow_dispatch:

env:
  REACHABILITY_METADATA_PATH: ${{ github.workspace }}/graalvm-reachability-metadata
  MINIMUM_METADATA_JAVA_VERSION: 17

permissions:
  contents: read # to fetch code (actions/checkout)

jobs:
  build-graalvm-and-populate-matrix:
    name: Build GraalVM and populate matrix
    runs-on: ubuntu-22.04
    if: (github.event_name == 'schedule' && github.repository == 'oracle/graal') || (github.event_name != 'schedule')
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
    - name: Checkout oracle/graal
      uses: actions/checkout@v4
    - name: Build GraalVM JDK
      uses: ./.github/actions/build-graalvm
      with:
        native-images: 'native-image,native-image-configure,lib:native-image-agent'
        components: 'Native Image,Native Image Configure Tool'
        java-version: ${{ env.MINIMUM_METADATA_JAVA_VERSION }}
    - name: Tar GraalVM JDK
      shell: bash
      run: tar -czvhf graalvm.tgz -C $(dirname ${GRAALVM_HOME}) $(basename ${GRAALVM_HOME})
    - name: Persist GraalVM JDK build
      uses: actions/upload-artifact@v4
      with:
        name: graalvm
        path: graalvm.tgz
    - name: Checkout oracle/graalvm-reachability-metadata
      uses: actions/checkout@v4
      with:
        repository: oracle/graalvm-reachability-metadata
        path: ${{ env.REACHABILITY_METADATA_PATH }}
    - name: ""Populate matrix""
      id: set-matrix
      run: |
        cd ${{ env.REACHABILITY_METADATA_PATH }}
        ./gradlew generateMatrixMatchingCoordinates -Pcoordinates=all

  test-all-metadata:
    name: ${{ matrix.coordinates }}
    runs-on: ubuntu-latest
    env:
      GRAALVM_HOME: ${{ github.workspace }}/graalvm # identical to the one in ./.github/actions/build-graalvm
    timeout-minutes: 20
    needs: build-graalvm-and-populate-matrix
    strategy:
        fail-fast: false
        matrix:
          coordinates: ${{fromJson(needs.build-graalvm-and-populate-matrix.outputs.matrix).coordinates}}
    steps:
      - name: ""Checkout oracle/graalvm-reachability-metadata""
        uses: actions/checkout@v4
        with:
          repository: oracle/graalvm-reachability-metadata
      - name: Download GraalVM JDK build
        uses: actions/download-artifact@95815c38cf2ff2164869cbab79da8d1f422bc89e # v4.2.1
        with:
          name: graalvm
          path: .
      - name: Extract GraalVM JDK build
        run: tar -xzvf graalvm.tgz -C $(dirname ${GRAALVM_HOME})
      - name: ""Setup JAVA_HOME""
        uses: actions/setup-java@v4
        with:
          distribution: 'oracle'
          java-version: ${{ env.MINIMUM_METADATA_JAVA_VERSION }}
      - name: ""Pull allowed docker images""
        run: |
          ./gradlew pullAllowedDockerImages --coordinates=${{ matrix.coordinates }}
      - name: ""Disable docker""
        run: |
          sudo apt-get install openbsd-inetd
          sudo bash -c ""cat ./.github/workflows/discard-port.conf >> /etc/inetd.conf""
          sudo systemctl start inetd
          sudo mkdir /etc/systemd/system/docker.service.d
          sudo bash -c ""cat ./.github/workflows/dockerd.service > /etc/systemd/system/docker.service.d/http-proxy.conf""
          sudo systemctl daemon-reload
          sudo systemctl restart docker
      - name: ""Run '${{ matrix.coordinates }}' tests""
        run: |
          ./gradlew test -Pcoordinates=${{ matrix.coordinates }}
    ",136,2,3,"pull_request, schedule, workflow_dispatch",7
oracle/graal,spring.yml,"#
# Copyright (c) 2024, 2025, Oracle and/or its affiliates. All rights reserved.
# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
#
# The Universal Permissive License (UPL), Version 1.0
#
# Subject to the condition set forth below, permission is hereby granted to any
# person obtaining a copy of this software, associated documentation and/or
# data (collectively the ""Software""), free of charge and under any and all
# copyright rights in the Software, and any and all patent rights owned or
# freely licensable by each licensor hereunder covering either (i) the
# unmodified Software as contributed to or provided by such licensor, or (ii)
# the Larger Works (as defined below), to deal in both
#
# (a) the Software, and
#
# (b) any piece of software and/or hardware listed in the lrgrwrks.txt file if
# one is included with the Software each a ""Larger Work"" to which the Software
# is contributed by such licensors),
#
# without restriction, including without limitation the rights to copy, create
# derivative works of, display, perform, and distribute the Software and make,
# use, sell, offer for sale, import, export, have made, and have sold the
# Software and the Larger Work(s), and to sublicense the foregoing rights on
# either these or other terms.
#
# This license is subject to the following condition:
#
# The above copyright notice and either this complete permission notice or at a
# minimum a reference to the UPL must be included in all copies or substantial
# portions of the Software.
#
# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
#
name: Weekly Spring Tests

on:
  pull_request:
    paths:
      - '.github/workflows/spring.yml'
  schedule:
  - cron: '0 4 * * 1'
  workflow_dispatch:

env:
  SPRING_PETCLINIC_PATH: ${{ github.workspace }}/spring-petclinic
  SPRING_JAVA_VERSION: 21

permissions:
  contents: read # to fetch code (actions/checkout)

jobs:
  build-graalvm-and-spring:
    name: Native Tests
    runs-on: ubuntu-22.04
    if: (github.event_name == 'schedule' && github.repository == 'oracle/graal') || (github.event_name != 'schedule')
    steps:
    - name: Checkout oracle/graal
      uses: actions/checkout@v4
    - name: Build GraalVM JDK
      uses: ./.github/actions/build-graalvm
      with:
        java-version: ${{ env.SPRING_JAVA_VERSION }}
    - name: Checkout spring-projects/spring-petclinic
      uses: actions/checkout@v4
      with:
        repository: spring-projects/spring-petclinic
        path: ${{ env.SPRING_PETCLINIC_PATH }}
    - name: Run nativeTest in spring-petclinic
      run: |
        cd ${{ env.SPRING_PETCLINIC_PATH }}
        ./gradlew nativeTest
",78,1,3,"pull_request, schedule, workflow_dispatch",3
oracle/truffleruby,ci.yml,"# NOTE: This is only a small subset of the CI which runs on GitHub Actions.
# Most of the CI is defined in ci.jsonnet.
name: CI
on:
  pull_request:
  push:
    branches: [master]
permissions:
  contents: read

jobs:
  lint:
    name: lint
    runs-on: ubuntu-22.04
    env:
      JT_JDK: '21'
    steps:
    - name: Clone TruffleRuby
      uses: actions/checkout@v4
      with:
        fetch-depth: 0 # Necessary for jt check_abi
    - name: Setup system Ruby
      uses: ruby/setup-ruby@v1
    - name: Setup jt
      run: echo ""$PWD/bin"" >> $GITHUB_PATH

    - name: Restore ~/.mx/cache
      uses: actions/cache@v3
      with:
        path: ~/.mx/cache
        key: mx-cache-lint-${{ runner.os }}-${{ hashFiles('common.json') }}

    - uses: ./.github/actions/setup-jvmci-graal

    - run: jt install eclipse
    - name: Install RuboCop
      run: gem install --no-document rubocop:0.66.0
    - name: Build with --warning-as-error to ensure there are no non-deprecation warnings
      # See comment in ci.jsonnet about --jdt
      run: jt build -- --jdt builtin --warning-as-error --force-deprecation-as-warning
    - run: jt lint

  build:
    name: build jvm
    runs-on: ubuntu-22.04
    defaults:
      run:
        # Ensure all build files are in build/.
        # Test jobs don't have build/ to ensure nothing uses the build files.
        working-directory: build
    steps:
    - name: Clone TruffleRuby
      uses: actions/checkout@v4
      with:
        path: build
    - name: Setup system Ruby
      uses: ruby/setup-ruby@v1
      with:
        working-directory: build
    - name: Setup jt
      run: echo ""$PWD/bin"" >> $GITHUB_PATH

    - name: Restore ~/.mx/cache
      uses: actions/cache@v3
      with:
        path: ~/.mx/cache
        key: mx-cache-build-${{ runner.os }}-${{ hashFiles('build/common.json') }}

    - uses: ./build/.github/actions/setup-jvmci-graal

    - name: Build TruffleRuby
      run: jt build

    - name: Create archive
      run: |
        mv ""$(jt -u jvm ruby-home)"" ""${{ github.workspace }}/truffleruby-jvm""
        cd ${{ github.workspace }}
        tar cf ${{ github.workspace }}/truffleruby-jvm.tar truffleruby-jvm
    - uses: actions/upload-artifact@v4
      with:
        name: truffleruby-jvm
        path: ${{ github.workspace }}/truffleruby-jvm.tar
        include-hidden-files: true

  build_native:
    name: build native
    runs-on: ubuntu-22.04
    defaults:
      run:
        # Ensure all build files are in build/.
        # Test jobs don't have build/ to ensure nothing uses the build files.
        working-directory: build
    steps:
      - name: Clone TruffleRuby
        uses: actions/checkout@v4
        with:
          path: build
      - name: Setup system Ruby
        uses: ruby/setup-ruby@v1
        with:
          working-directory: build
      - name: Setup jt
        run: echo ""$PWD/bin"" >> $GITHUB_PATH

      - name: Restore ~/.mx/cache
        uses: actions/cache@v3
        with:
          path: ~/.mx/cache
          key: mx-cache-build-native-${{ runner.os }}-${{ hashFiles('build/common.json') }}

      - uses: ./build/.github/actions/setup-jvmci-graal

      - run: free -m
      - name: Build TruffleRuby
        run: jt build --env native

      - name: Create archive
        run: |
          mv ""$(jt -u native ruby-home)"" ""${{ github.workspace }}/truffleruby-native""
          cd ${{ github.workspace }}
          tar cf ${{ github.workspace }}/truffleruby-native.tar truffleruby-native
      - uses: actions/upload-artifact@v4
        with:
          name: truffleruby-native
          path: ${{ github.workspace }}/truffleruby-native.tar
          include-hidden-files: true

  fast_specs:
    name: fast specs
    needs: [build]
    runs-on: ubuntu-22.04
    steps:
    - name: Clone TruffleRuby
      uses: actions/checkout@v4
    - name: Setup system Ruby
      uses: ruby/setup-ruby@v1
    - name: Setup jt
      run: echo ""SYSTEM_RUBY=$(which ruby)"" >> $GITHUB_ENV && echo ""$PWD/bin"" >> $GITHUB_PATH

    - uses: actions/download-artifact@95815c38cf2ff2164869cbab79da8d1f422bc89e # v4.2.1
      with:
        name: truffleruby-jvm
    - uses: ./.github/actions/setup-truffleruby

    - run: jt test fast
    - run: jt test :next

  all_specs:
    name: specs ${{ matrix.specs }}
    needs: [build]
    runs-on: ubuntu-22.04
    strategy:
      fail-fast: false
      matrix:
        specs:
        - :truffle # ~12min
        - :language :core # ~3min + ~9min
        - :library :cext :security :command_line :tracepoint # command_line ~5min, others < 2min
    steps:
      - name: Clone TruffleRuby
        uses: actions/checkout@v4
      - name: Setup system Ruby
        uses: ruby/setup-ruby@v1
      - name: Setup jt
        run: echo ""SYSTEM_RUBY=$(which ruby)"" >> $GITHUB_ENV && echo ""$PWD/bin"" >> $GITHUB_PATH

      - uses: actions/download-artifact@95815c38cf2ff2164869cbab79da8d1f422bc89e # v4.2.1
        with:
          name: truffleruby-jvm
      - uses: ./.github/actions/setup-truffleruby

      - run: jt test --timeout 180 ${{ matrix.specs }}

  test_integration:
    name: test integration
    needs: [build]
    runs-on: ubuntu-22.04
    steps:
      - name: Clone TruffleRuby
        uses: actions/checkout@v4
      - name: Setup system Ruby
        uses: ruby/setup-ruby@v1
      - name: Setup jt
        run: echo ""SYSTEM_RUBY=$(which ruby)"" >> $GITHUB_ENV && echo ""$PWD/bin"" >> $GITHUB_PATH

      - uses: actions/download-artifact@95815c38cf2ff2164869cbab79da8d1f422bc89e # v4.2.1
        with:
          name: truffleruby-jvm
      - uses: ./.github/actions/setup-truffleruby

      - run: jt test integration

  test_native:
    name: test native
    needs: [build_native]
    runs-on: ubuntu-22.04
    steps:
      - name: Clone TruffleRuby
        uses: actions/checkout@v4
      - name: Setup system Ruby
        uses: ruby/setup-ruby@v1
      - name: Setup jt
        run: echo ""SYSTEM_RUBY=$(which ruby)"" >> $GITHUB_ENV && echo ""$PWD/bin"" >> $GITHUB_PATH

      - uses: actions/download-artifact@95815c38cf2ff2164869cbab79da8d1f422bc89e # v4.2.1
        with:
          name: truffleruby-native
      - uses: ./.github/actions/setup-truffleruby
        with:
          archive: truffleruby-native

      - run: jt test compiler
      # A subset of specs that are more likely to differ on native and run quickly
      - run: jt test :command_line
      - run: jt test :language
      # To catch slow :truffle specs which only apply to native
      - run: jt test fast :truffle

  test_mri:
    name: run MRI tests on native
    needs: [build_native]
    runs-on: ubuntu-22.04
    steps:
      - name: Clone TruffleRuby
        uses: actions/checkout@v4
      - name: Setup system Ruby
        uses: ruby/setup-ruby@v1
      - name: Setup jt
        run: echo ""SYSTEM_RUBY=$(which ruby)"" >> $GITHUB_ENV && echo ""$PWD/bin"" >> $GITHUB_PATH

      - uses: actions/download-artifact@95815c38cf2ff2164869cbab79da8d1f422bc89e # v4.2.1
        with:
          name: truffleruby-native
      - uses: ./.github/actions/setup-truffleruby
        with:
          archive: truffleruby-native

      - run: jt test mri --fast --no-sulong

  ruby_spec_cruby:
    name: ruby/spec on CRuby ${{ matrix.ruby }}
    strategy:
      fail-fast: false
      matrix:
        ruby: ['3.2', '3.3', '3.4']
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: ruby/setup-ruby@v1
        with:
          ruby-version: ${{ matrix.ruby }}
          bundler: none
      - name: Setup jt
        run: echo ""$PWD/bin"" >> $GITHUB_PATH
      - run: CHECK_LEAKS=true jt -u ruby mspec -fdot --timeout 30 spec/ruby
",255,9,2,"pull_request, push",36
oracle/truffleruby,prism.yml,"name: Check importing latest Prism
on:
  workflow_dispatch:
  schedule:
    - cron: '0 13 * * *'
permissions:
  contents: read

jobs:
  # Inspired from the job ""build"" in ../ci.yml
  test-import-prism:
    if: github.repository == 'oracle/truffleruby'
    runs-on: ubuntu-22.04
    env:
      BUNDLE_WITHOUT: ""memcheck:types""
    steps:
    - name: Clone Prism
      uses: actions/checkout@v4
      with:
        repository: ruby/prism
        path: prism
    - name: Setup system Ruby
      uses: ruby/setup-ruby@v1
      with:
        ruby-version: ruby
        bundler-cache: true
        working-directory: prism

    # Clone truffleruby in truffleruby-ws/truffleruby so that tool/import-prism.sh
    # has the correct relative path to the cloned prism repo.
    - run: mkdir truffleruby-ws
    - name: Clone TruffleRuby
      uses: actions/checkout@v4
      with:
        path: truffleruby-ws/truffleruby
    - name: Setup jt
      run: echo ""$PWD/truffleruby-ws/truffleruby/bin"" >> $GITHUB_PATH

    - name: Restore ~/.mx/cache
      uses: actions/cache@v4
      with:
        path: ~/.mx/cache
        key: mx-cache-prism-${{ runner.os }}-${{ hashFiles('truffleruby-ws/truffleruby/common.json') }}

    - name: Import latest prism in TruffleRuby
      run: tool/import-prism.sh
      working-directory: truffleruby-ws/truffleruby

    - uses: ./truffleruby-ws/truffleruby/.github/actions/setup-jvmci-graal

    - name: Build TruffleRuby
      run: jt build
      working-directory: truffleruby-ws/truffleruby

    - name: Parse test/prism/fixtures/**/*.txt
      run: jt ruby -e 'Dir.glob(""test/prism/fixtures/**/*.txt"") { |file| puts file; puts Truffle::Debug.parse_ast(File.read(file)) }'
      working-directory: prism

    - name: Execute p 1+2
      run: jt ruby -e 'p 1+2'
",60,1,2,"workflow_dispatch, schedule",5
IBM/sarama,apidiff.yml,"name: API Compatibility
on:
  merge_group:
  push:
    branches:
    - main
    paths-ignore:
    - '**/*.md'
  pull_request:
    branches:
    - ""**""
    paths-ignore:
    - '**/*.md'

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ startsWith(github.ref, 'refs/pull/') }}

permissions:
  contents: read  # for actions/checkout to fetch code

env:
  # Use the Go toolchain installed by setup-go
  GOTOOLCHAIN: local

jobs:
  apidiff:
    runs-on: ubuntu-latest
    if: github.base_ref
    steps:
    - name: Setup Go
      uses: actions/setup-go@d35c59abb061a4a6fb18e82ac0862c26744d6ab5 # v5.5.0
      with:
        go-version: stable
    - name: Add GOBIN to PATH
      run: echo ""$(go env GOPATH)/bin"" >>$GITHUB_PATH
    - name: Install apidiff cmd
      run: go install golang.org/x/exp/cmd/apidiff@v0.0.0-20231006140011-7918f672742d
    - name: Checkout base code
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      with:
        ref: ${{ github.base_ref }}
        path: ""base""
        persist-credentials: false
    - name: Capture apidiff baseline
      run: apidiff -m -w ../baseline.bin .
      working-directory: ""base""
    - name: Checkout updated code
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      with:
        path: ""updated""
        persist-credentials: false
    - name: Run apidiff check
      run: apidiff -m -incompatible ../baseline.bin .
      working-directory: ""updated""
",55,1,3,"merge_group, push, pull_request",3
IBM/sarama,cache-cleanup.yml,"# https://docs.github.com/en/actions/writing-workflows/choosing-what-your-workflow-does/caching-dependencies-to-speed-up-workflows#force-deleting-cache-entries
name: Cleanup caches on PR close/merge
on:
  pull_request:
    types:
      - closed

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ startsWith(github.ref, 'refs/pull/') }}

permissions:
  contents: read # for actions/checkout to fetch code

jobs:
  cleanup:
    permissions:
      actions: write # for cache management
    runs-on: ubuntu-latest
    steps:
      - name: Delete Caches
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GH_REPO: ${{ github.repository }}
          BRANCH: refs/pull/${{ github.event.pull_request.number }}/merge
        run: |
          gh cache list --ref ""$BRANCH"" --limit 100
          CACHE_KEYS=""$(gh cache list --ref ""$BRANCH"" --limit 100 --json id --jq '.[].id')""
          for KEY in ${CACHE_KEYS}; do
            echo ""Deleting cache $KEY for ref $BRANCH""
            gh cache delete ""$KEY"" || true
            echo
          done
",33,1,1,pull_request,0
IBM/sarama,ci.yml,"name: CI
on:
  merge_group:
  push:
    branches:
    - main
    paths-ignore:
    - '**/*.md'
  pull_request:
    branches:
    - ""**""
    paths-ignore:
    - '**/*.md'

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ startsWith(github.ref, 'refs/pull/') }}

permissions:
  contents: read  # for actions/checkout to fetch code

env:
  # Use the Go toolchain installed by setup-go
  GOTOOLCHAIN: local

jobs:
  lint:
    permissions:
      contents: read  # for actions/checkout to fetch code
      pull-requests: read  # for golangci/golangci-lint-action to fetch pull requests
    name: Linting with Go ${{ matrix.go-version }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        go-version: [stable]
    steps:
    - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      with:
        persist-credentials: false
    - name: Setup Go
      uses: actions/setup-go@d35c59abb061a4a6fb18e82ac0862c26744d6ab5 # v5.5.0
      with:
        go-version: ${{ matrix.go-version }}
    - name: Staticcheck
      shell: bash
      env:
        BUILDTAGS: ""functional""
        VERSION: ""v0.6.0""
      run: |
        go install ""honnef.co/go/tools/cmd/staticcheck@${VERSION}""
        echo ""::add-matcher::./.github/actions/staticcheck-matchers.json""
        $(go env GOPATH)/bin/staticcheck -tags ""${BUILDTAGS}"" ./...
    - name: golangci-lint
      env:
        GOFLAGS: -tags=functional
      uses: golangci/golangci-lint-action@4afd733a84b1f43292c63897423277bb7f4313a9 # v8.0.0
      with:
        version: v2.1.6
  test:
    name: Unit Testing with Go ${{ matrix.go-version }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        go-version: [oldstable, stable]
    env:
      DEBUG: true
      GOFLAGS: -trimpath
    steps:
    - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      with:
        persist-credentials: false
    - name: Setup Go
      uses: actions/setup-go@d35c59abb061a4a6fb18e82ac0862c26744d6ab5 # v5.5.0
      with:
        go-version: ${{ matrix.go-version }}
    - name: Test (Unit)
      run: make test
",79,2,3,"merge_group, push, pull_request",5
IBM/sarama,codeql-analysis.yml,"name: ""CodeQL""
on:
  merge_group:
  push:
    branches:
    - main
  pull_request:
    branches:
    - ""**""
  schedule:
  - cron: ""39 12 * * 1""

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ startsWith(github.ref, 'refs/pull/') }}

permissions:
  contents: read  # for actions/checkout to fetch code

env:
  # Use the Go toolchain installed by setup-go
  GOTOOLCHAIN: local

jobs:
  analyze:
    name: Analyze
    runs-on: ubuntu-latest
    permissions:
      actions: read  # for github/codeql-action to list actions
      contents: read  # for actions/checkout to fetch code
      security-events: write  # for github/codeql-action to report security issues
    strategy:
      fail-fast: false
      matrix:
        language: [""actions"", ""go""]
    steps:
    - name: Checkout repository
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      with:
        persist-credentials: false
    - name: Initialize CodeQL
      uses: github/codeql-action/init@ff0a06e83cb2de871e5a09832bc6a81e7276941f # v3.28.18
      with:
        languages: ${{ matrix.language }}
    - name: Setup Go
      uses: actions/setup-go@d35c59abb061a4a6fb18e82ac0862c26744d6ab5 # v5.5.0
      with:
        go-version: stable
    - name: Autobuild
      uses: github/codeql-action/autobuild@ff0a06e83cb2de871e5a09832bc6a81e7276941f # v3.28.18
    - name: Perform CodeQL Analysis
      uses: github/codeql-action/analyze@ff0a06e83cb2de871e5a09832bc6a81e7276941f # v3.28.18
",52,1,4,"merge_group, push, pull_request, schedule",5
IBM/sarama,dependency-review.yml,"# Dependency Review Action
#
# This Action will scan dependency manifest files that change as part of a Pull Request,
# surfacing known-vulnerable versions of the packages declared or updated in the PR.
# Once installed, if the workflow run is marked as required,
# PRs introducing known-vulnerable packages will be blocked from merging.
#
# Source repository: https://github.com/actions/dependency-review-action
name: 'Dependency Review'
on:
  pull_request:
    branches:
    - ""**""
    paths-ignore:
    - '**/*.md'

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ startsWith(github.ref, 'refs/pull/') }}

permissions:
  contents: read  # for actions/checkout to fetch code

jobs:
  dependency-review:
    runs-on: ubuntu-latest
    steps:
      - name: 'Checkout Repository'
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          persist-credentials: false
      - name: 'Dependency Review'
        uses: actions/dependency-review-action@da24556b548a50705dd671f47852072ea4c105d9 # v4.7.1
",33,1,1,pull_request,2
IBM/sarama,fuzz.yml,"name: Fuzzing
on:
  merge_group:
  push:
    branches:
    - main
    paths-ignore:
    - '**/*.md'
  pull_request:
    branches:
    - ""**""
    paths-ignore:
    - '**/*.md'

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ startsWith(github.ref, 'refs/pull/') }}

permissions:
  contents: read  # for actions/checkout to fetch code

env:
  # Use the Go toolchain installed by setup-go
  GOTOOLCHAIN: local

jobs:
  test:
    name: Fuzz
    runs-on: ubuntu-latest
    env:
      GOFLAGS: -trimpath
    steps:
    - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      with:
        persist-credentials: false
    - name: Setup Go
      uses: actions/setup-go@d35c59abb061a4a6fb18e82ac0862c26744d6ab5 # v5.5.0
      with:
        go-version: stable
    - name: Run any fuzzing tests
      run: go test -list . | grep '^Fuzz' | parallel 'go test -v -run=^{}$ -fuzz=^{}$ -fuzztime=5m'
",41,1,3,"merge_group, push, pull_request",2
IBM/sarama,fvt-main.yml,"name: FVT (main)
on:
  merge_group:
  push:
    branches:
    - main
    paths-ignore:
    - '**/*.md'

permissions:
  contents: read  # for actions/checkout to fetch code

env:
  # Use the Go toolchain installed by setup-go
  GOTOOLCHAIN: local

jobs:
  fvt:
    name: Test with Kafka ${{ matrix.kafka-version }}
    strategy:
      fail-fast: false
      matrix:
        go-version: [stable]
        kafka-version: [1.0.2, 2.0.1, 2.2.2, 2.6.3, 2.8.2, 3.0.2, 3.3.2, 3.6.2, 3.8.1, 4.0.0]
        include:
        - kafka-version: 1.0.2
          scala-version: 2.11
        - kafka-version: 2.0.1
          scala-version: 2.12
        - kafka-version: 2.2.2
          scala-version: 2.12
        - kafka-version: 2.6.3
          scala-version: 2.12
        - kafka-version: 2.8.2
          scala-version: 2.12
        - kafka-version: 3.0.2
          scala-version: 2.12
        - kafka-version: 3.3.2
          scala-version: 2.13
        - kafka-version: 3.6.2
          scala-version: 2.13
        - kafka-version: 3.8.1
          scala-version: 2.13
        - kafka-version: 4.0.0
          scala-version: 2.13
    uses: ./.github/workflows/fvt.yml
    with:
      go-version: ${{ matrix.go-version }}
      kafka-version: ${{ matrix.kafka-version }}
      scala-version: ${{ matrix.scala-version }}
",50,1,2,"merge_group, push",1
IBM/sarama,fvt-pr.yml,"name: FVT (PR)
on:
  pull_request:
    branches:
    - ""**""
    paths-ignore:
    - '**/*.md'

permissions:
  contents: read  # for actions/checkout to fetch code

env:
  # Use the Go toolchain installed by setup-go
  GOTOOLCHAIN: local

jobs:
  fvt:
    name: Test with Kafka ${{ matrix.kafka-version }}
    strategy:
      fail-fast: false
      matrix:
        go-version: [stable]
        kafka-version: [1.0.2, 2.6.3, 3.6.2, 3.8.1, 4.0.0]
        include:
        - kafka-version: 1.0.2
          scala-version: 2.11
        - kafka-version: 2.6.3
          scala-version: 2.12
        - kafka-version: 3.6.2
          scala-version: 2.13
        - kafka-version: 3.8.1
          scala-version: 2.13
        - kafka-version: 4.0.0
          scala-version: 2.13
    uses: ./.github/workflows/fvt.yml
    with:
      go-version: ${{ matrix.go-version }}
      kafka-version: ${{ matrix.kafka-version }}
      scala-version: ${{ matrix.scala-version }}
",39,1,1,pull_request,1
IBM/sarama,fvt.yml,"name: FVT
on:
  workflow_call:
    inputs:
      go-version:
        required: false
        type: string
        default: stable
      kafka-version:
        required: false
        type: string
        default: 3.6.2
      scala-version:
        required: false
        type: string
        default: 2.13

concurrency:
  group: ${{ github.workflow }}-kafka-${{ inputs.kafka-version}}-${{ github.ref }}
  cancel-in-progress: ${{ startsWith(github.ref, 'refs/pull/') }}

permissions:
  contents: read  # for actions/checkout to fetch code

env:
  # Use the Go toolchain installed by setup-go
  GOTOOLCHAIN: local

jobs:
  fvt:
    name: Test with Kafka ${{ inputs.kafka-version }}
    runs-on: ubuntu-latest
    env:
      DEBUG: true
      GOFLAGS: -trimpath
      KAFKA_VERSION: ${{ inputs.kafka-version }}
      SCALA_VERSION: ${{ inputs.scala-version }}
    steps:
    - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      with:
        persist-credentials: false
    - name: Setup Docker
      uses: docker/setup-buildx-action@b5ca514318bd6ebac0fb2aedd5d36ec1b5c232a2 # v3.10.0
      id: buildx
    - name: Build FVT Docker Image
      uses: docker/bake-action@37816e747588cb137173af99ab33873600c46ea8 # v6.8.0
      with:
        builder: ${{ steps.buildx.outputs.name }}
        files: docker-compose.yml
        load: true
        targets: kafka-1
        set: |
          *.cache-from=type=gha,scope=fvt-kafka-${{ inputs.kafka-version }}
          *.cache-to=type=gha,scope=fvt-kafka-${{ inputs.kafka-version }},mode=max
    - name: Setup Go
      uses: actions/setup-go@d35c59abb061a4a6fb18e82ac0862c26744d6ab5 # v5.5.0
      with:
        go-version: ${{ inputs.go-version }}
    - name: Setup Docker Compose
      run: |
        curl --fail -sSL ""https://github.com/docker/compose/releases/download/v2.32.1/docker-compose-$(uname -s)-$(uname -m)"" -o /tmp/docker-compose
        mkdir -p $HOME/.docker/cli-plugins
        install -m755 /tmp/docker-compose $HOME/.docker/cli-plugins
        docker version --format 'Docker Engine version v{{.Server.Version}}'
        docker compose version
    - name: Test (Functional)
      run: |
        nohup sudo tcpdump -i lo -w ""fvt-kafka-${KAFKA_VERSION}.pcap"" portrange 29091-29095 >/dev/null 2>&1 &
        echo $! >tcpdump.pid
        make test_functional
        echo ""## Code Coverage"" >>$GITHUB_STEP_SUMMARY
        echo ""|Filename|Function|Coverage|"" >>$GITHUB_STEP_SUMMARY
        echo ""|--------|--------|--------|"" >>$GITHUB_STEP_SUMMARY
        go tool cover -func=profile.out | sed -E -e 's/[[:space:]]+/|/g' -e 's/$/|/g' -e 's/^/|/g' >>$GITHUB_STEP_SUMMARY
    - name: Stop tcpdump
      if: always()
      run: |
        if [ -f ""tcpdump.pid"" ]; then sudo kill ""$(cat tcpdump.pid)"" || true; fi
        if [ -f ""fvt-kafka-${KAFKA_VERSION}.pcap"" ]; then sudo chmod a+r ""fvt-kafka-${KAFKA_VERSION}.pcap""; fi
    - name: Upload pcap file
      if: always()
      uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
      with:
        name: fvt-kafka-${{ inputs.kafka-version }}.pcap
        path: fvt-kafka-${{ inputs.kafka-version }}.pcap
        retention-days: 5
        if-no-files-found: ignore
",87,1,1,workflow_call,5
IBM/sarama,i386.yml,"name: i386
on:
  merge_group:
  push:
    branches:
    - main
    paths-ignore:
    - '**/*.md'
  pull_request:
    branches:
    - ""**""
    paths-ignore:
    - '**/*.md'

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ startsWith(github.ref, 'refs/pull/') }}

permissions:
  contents: read  # for actions/checkout to fetch code

env:
  # Use the Go toolchain installed by setup-go
  GOTOOLCHAIN: local

jobs:
  atomicalign:
    permissions:
      contents: read  # for actions/checkout to fetch code
      pull-requests: read  # for golangci/golangci-lint-action to fetch pull requests
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      with:
        persist-credentials: false
    - name: Setup Go
      uses: actions/setup-go@d35c59abb061a4a6fb18e82ac0862c26744d6ab5 # v5.5.0
      with:
        go-version: stable
    - name: staticcheck
      env:
        GOARCH: 386
        GOFLAGS: -tags=functional
      run: |
          git clone --depth=1 https://github.com/dominikh/go-tools /tmp/go-tools
          ( cd /tmp/go-tools/cmd/staticcheck && go build -o /tmp/staticcheck )
          /tmp/staticcheck -checks SA1027 ./...
",47,1,3,"merge_group, push, pull_request",2
IBM/sarama,scorecard.yml,"name: Scorecard supply-chain security
on:
  # For Branch-Protection check. Only the default branch is supported. See
  # https://github.com/ossf/scorecard/blob/main/docs/checks.md#branch-protection
  branch_protection_rule:
  # To guarantee Maintained check is occasionally updated. See
  # https://github.com/ossf/scorecard/blob/main/docs/checks.md#maintained
  schedule:
    - cron: '17 4 * * 5'
  push:
    branches: [ ""main"" ]

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ startsWith(github.ref, 'refs/pull/') }}

# Declare default permissions as read only.
permissions:
  actions: read
  checks: read
  contents: read
  issues: read
  pull-requests: read
  statuses: read

jobs:
  analysis:
    name: Scorecard analysis
    runs-on: ubuntu-latest
    permissions:
      # Needed to upload the results to code-scanning dashboard.
      security-events: write
      # Needed to publish results and get a badge (see publish_results below).
      id-token: write
      # Uncomment the permissions below if installing in a private repository.
      # contents: read
      # actions: read

    steps:
      - name: ""Checkout code""
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          persist-credentials: false

      - name: ""Run analysis""
        uses: ossf/scorecard-action@05b42c624433fc40578a4040d5cf5e36ddca8cde # v2.4.2
        with:
          results_file: results.sarif
          results_format: sarif
          # (Optional) ""write"" PAT token. Uncomment the `repo_token` line below if:
          # - you want to enable the Branch-Protection check on a *public* repository, or
          # - you are installing Scorecard on a *private* repository
          # To create the PAT, follow the steps in https://github.com/ossf/scorecard-action#authentication-with-pat.
          # repo_token: ${{ secrets.SCORECARD_TOKEN }}

          # Public repositories:
          #   - Publish results to OpenSSF REST API for easy access by consumers
          #   - Allows the repository to include the Scorecard badge.
          #   - See https://github.com/ossf/scorecard-action#publishing-results.
          # For private repositories:
          #   - `publish_results` will always be set to `false`, regardless
          #     of the value entered here.
          publish_results: true

      # Upload the results as artifacts (optional). Commenting out will disable uploads of run results in SARIF
      # format to the repository Actions tab.
      - name: ""Upload artifact""
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: SARIF file
          path: results.sarif
          retention-days: 5

      # Upload the results to GitHub's code scanning dashboard.
      - name: ""Upload to code-scanning""
        uses: github/codeql-action/upload-sarif@ff0a06e83cb2de871e5a09832bc6a81e7276941f # v3.28.18
        with:
          sarif_file: results.sarif
",78,1,3,"branch_protection_rule, schedule, push",5
IBM/sarama,stale.yml,"# configuration for https://github.com/actions/stale
name: ""Stale issues and PRs""
on:
  schedule:
  - cron: ""0 */2 * * *""
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ startsWith(github.ref, 'refs/pull/') }}

permissions:
  contents: read  # for actions/checkout to fetch code

jobs:
  stale:
    permissions:
      issues: write  # for actions/stale to close stale issues
      pull-requests: write  # for actions/stale to close stale PRs
    runs-on: ubuntu-latest
    steps:
      # pinned to main commit to make use of https://github.com/actions/stale/pull/1033
    - uses: actions/stale@5bef64f19d7facfb25b37b414482c7164d639639 # v9.1.0
      with:
        ascending: true
        days-before-stale: 90
        days-before-close: 30
        stale-issue-message: >-
          Thank you for taking the time to raise this issue. However, it has not had
          any activity on it in the past 90 days and will be closed in 30 days if
          no updates occur.

          Please check if the main branch has already resolved the issue since it
          was raised. If you believe the issue is still valid and you would like input
          from the maintainers then please comment to ask for it to be reviewed.
        stale-pr-message: >-
          Thank you for your contribution! However, this pull request has not had
          any activity in the past 90 days and will be closed in 30 days if no updates
          occur.

          If you believe the changes are still valid then please verify your branch
          has no conflicts with main and rebase if needed. If you are awaiting a (re-)review
          then please let us know.
        stale-issue-label: ""stale""
        exempt-issue-labels: ""stale/exempt,pinned""
        stale-pr-label: ""stale""
        exempt-pr-labels: ""stale/exempt,pinned""
",47,1,2,"schedule, workflow_dispatch",1
NVIDIA/k8s-device-plugin,basic-checks.yaml,"name: ""basic checks""

on:
  workflow_call:
    outputs:
      version:
        description: ""The short SHA to use as a version string""
        value: ${{ jobs.variables.outputs.version }}
      golang_version:
        description: ""The golang version for this project""
        value: ${{ jobs.variables.outputs.golang_version }}
  pull_request:
    types:
      - opened
      - synchronize
    branches:
      - main
      - release-*

jobs:
  variables:
    uses: ./.github/workflows/variables.yaml

  golang:
    needs:
    - variables
    uses: ./.github/workflows/golang.yaml
    with:
      golang_version: ${{ needs.variables.outputs.golang_version }}

  helm:
    needs:
    - variables
    uses: ./.github/workflows/helm.yaml
    with:
      golang_version: ${{ needs.variables.outputs.golang_version }}

  code-scanning:
    needs:
    - variables
    uses: ./.github/workflows/code_scanning.yaml
    with:
      golang_version: ${{ needs.variables.outputs.golang_version }}
",43,4,2,"workflow_call, pull_request",4
NVIDIA/k8s-device-plugin,ci.yaml,"# Copyright 2025 NVIDIA CORPORATION
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

name: CI Pipeline

on:
  push:
    branches:
      - ""pull-request/[0-9]+""
      - main
      - release-*

jobs:
  basic:
    uses: ./.github/workflows/basic-checks.yaml

  image:
    uses: ./.github/workflows/image.yaml
    needs:
    - basic
    secrets: inherit
    with:
      version: ${{ needs.basic.outputs.version }}
      build_multi_arch_images: ${{ github.ref_name == 'main' || startsWith(github.ref_name, 'release-') }}

  e2e-test:
    needs:
    - image
    - basic
    secrets: inherit
    uses: ./.github/workflows/e2e.yaml
    with:
      version: ${{ needs.basic.outputs.version }}
      golang_version: ${{ needs.basic.outputs.golang_version }}
",45,3,1,push,3
NVIDIA/k8s-device-plugin,code_scanning.yaml,"# Copyright 2024 NVIDIA CORPORATION
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

name: ""CodeQL""

on:
  workflow_call:
    inputs:
      golang_version:
        required: true
        type: string

jobs:
  analyze:
    name: Analyze Go code with CodeQL
    runs-on: ubuntu-latest
    timeout-minutes: 360
    permissions:
      security-events: write
      packages: read
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Install Go
      uses: actions/setup-go@v5
      with:
        go-version: ${{ inputs.golang_version }}

    - name: Initialize CodeQL
      uses: github/codeql-action/init@v3
      with:
        languages: go
        build-mode: manual

    - shell: bash
      run: |
        make build

    - name: Perform CodeQL Analysis
      uses: github/codeql-action/analyze@v3
      with:
        category: ""/language:go""
",54,1,1,workflow_call,4
NVIDIA/k8s-device-plugin,deploy-to-pages.yml,"# Copyright (c) NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

name: Deploy Helm charts to GitHub Pages

on:
  push:
    branches:
      - gh-pages

  # Allow this workflow to be called from other workflows.
  workflow_call:
    inputs:
      git_ref_to_deploy:
        required: false
        type: string
        default: gh-pages

  # Allow this workflow to be run from the Actions tab.
  workflow_dispatch:
      git_ref_to_deploy:
        description: The git reference to deploy
        required: false
        type: string
        default: gh-pages

# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages
permissions:
  contents: read
  pages: write
  id-token: write

# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.
# However, do NOT cancel in-progress runs as we want to allow these production deployments to complete.
concurrency:
  group: ""pages""
  cancel-in-progress: false

jobs:
  # Build job
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          ref: ${{ inputs.git_ref_to_deploy }}
      - name: Setup Pages
        uses: actions/configure-pages@v5
      - name: Build with Jekyll
        uses: actions/jekyll-build-pages@v1
        with:
          source: ./
          destination: ./_site
      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3

  # Deployment job
  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
",79,2,3,"push, workflow_call, workflow_dispatch",5
NVIDIA/k8s-device-plugin,e2e.yaml,"# Copyright 2025 NVIDIA CORPORATION
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

name: End-to-end Tests

on:
  workflow_call:
    inputs:
      version:
        required: true
        type: string
      golang_version:
        required: true
        type: string
    secrets:
      AWS_ACCESS_KEY_ID:
        required: true
      AWS_SECRET_ACCESS_KEY:
        required: true
      AWS_SSH_KEY:
        required: true
      SLACK_BOT_TOKEN:
        required: true
      SLACK_CHANNEL_ID:
        required: true

jobs:
  e2e-tests:
    runs-on: linux-amd64-cpu4
    steps:
    - name: Check out code
      uses: actions/checkout@v4

    - name: Install Go
      uses: actions/setup-go@v5
      with:
        go-version: ${{ inputs.golang_version }}

    - name: Set up Holodeck
      uses: NVIDIA/holodeck@v0.2.12
      with:
        aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws_ssh_key: ${{ secrets.AWS_SSH_KEY }}
        holodeck_config: ""tests/e2e/infra/aws.yaml""

    - name: Run e2e tests
      env:
        KUBECONFIG: ${{ github.workspace }}/kubeconfig
        E2E_IMAGE_REPO: ghcr.io/nvidia/k8s-device-plugin
        E2E_IMAGE_TAG: ${{ inputs.version }}-ubi9
        LOG_ARTIFACTS: ${{ github.workspace }}/e2e_logs
      run: |
        make test-e2e

    - name: Archive test logs
      if: ${{ failure() }}
      uses: actions/upload-artifact@v4
      with:
        name: e2e-test-logs
        path: ./e2e_logs/
        retention-days: 15

    - name: Send Slack alert notification
      id: slack
      if: false
      uses: slackapi/slack-github-action@v2.1.0
      env:
        SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}
        SUMMARY_URL: https://github.com/${{github.repository}}/actions/runs/${{github.run_id}}
      with:
        channel-id: ${{ secrets.SLACK_CHANNEL_ID }}
        slack-message: |
          :x: On repository ${{ github.repository }} the Workflow *${{ github.workflow }}* has failed.

          Details: ${{ env.SUMMARY_URL }}
",87,1,1,workflow_call,5
NVIDIA/k8s-device-plugin,golang.yaml,"# Copyright 2024 NVIDIA CORPORATION
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

name: Golang

on:
  workflow_call:
    inputs:
      golang_version:
        required: true
        type: string

jobs:
  check:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
      name: Checkout code
    - name: Install Go
      uses: actions/setup-go@v5
      with:
        go-version: ${{ inputs.golang_version }}
    - name: Lint
      uses: golangci/golangci-lint-action@v8
      with:
        version: latest
        args: -v --timeout 5m
        skip-cache: true
    - name: Check golang modules
      run: |
        make check-modules
        make -C deployments/devel check-modules
  test:
    name: Unit test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Install Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ inputs.golang_version }}
      - run: make test
  build:
    name: Build
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Install Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ inputs.golang_version }}
      - run: make build
",65,3,1,workflow_call,7
NVIDIA/k8s-device-plugin,helm.yaml,"# Copyright 2024 NVIDIA CORPORATION
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

name: Helm

on:
  workflow_call:
    inputs:
      golang_version:
        required: true
        type: string

jobs:
  test:
    name: Test
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    - name: Install Go
      uses: actions/setup-go@v5
      with:
        go-version: ${{ inputs.golang_version }}
    - run: make test-helm
",35,1,1,workflow_call,2
NVIDIA/k8s-device-plugin,image.yaml,"# Copyright 2024 NVIDIA CORPORATION
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

name: Image

on:
  workflow_call:
    inputs:
      version:
        required: true
        type: string
      build_multi_arch_images:
        required: true
        type: string

jobs:
  build:
    runs-on: linux-amd64-cpu4
    steps:
      - uses: actions/checkout@v4
        name: Check out code
      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3
        with:
          image: tonistiigi/binfmt:master
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Build image
        env:
          IMAGE_NAME: ghcr.io/nvidia/k8s-device-plugin
          VERSION: ${{ inputs.version }}
          PUSH_ON_BUILD: true
          BUILD_MULTI_ARCH_IMAGES: ${{ inputs.build_multi_arch_images }}
        run: |
          echo ""${VERSION}""
          make -f deployments/container/Makefile build
",53,1,1,workflow_call,4
NVIDIA/k8s-device-plugin,publish-helm.yaml,"# Copyright 2024 NVIDIA CORPORATION
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Run this workflow on new tags
name: Publish Helm Chart

on:
  release:
    types:
      - published

jobs:
  update-helm-charts:
    name: Update gh-pages branch helm charts and index
    runs-on: ubuntu-latest
    env:
      HELM_REPO_PATH: releases/helm-${{ github.event.release.tag_name }}/
    steps:
    - name: Install Helm
      uses: azure/setup-helm@v4.3.0
      with:
        version: 3.14.4

    - name: Check out repo
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Update helm index
      env:
        GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        git config user.name ""Github Actions""
        git config user.email ""no-reply@github.com""
        ./hack/update-helm-index.sh --helm-repo-path $HELM_REPO_PATH --version ${{ github.event.release.tag_name }}

    - name: Push updated Helm charts and index to gh-pages branch
      run: |
        git -C $HELM_REPO_PATH push https://${GITHUB_ACTOR}:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }} gh-pages

  publish-helm-repo:
    uses: ./.github/workflows/deploy-to-pages.yaml
    secrets: inherit

  check-helm-repo:
    name: Checks for updated Helm repo
    runs-on: ubuntu-latest
    steps:
    - name: Install Helm
      uses: azure/setup-helm@v4.3.0
      with:
        version: 3.14.4

    - name: Check help repo
      run: |
        helm repo add nvdp https://nvidia.github.io/k8s-device-plugin
        helm repo update
        helm search repo nvdp --version ${{ github.event.release.tag_name }} --fail-on-no-result
",69,3,1,release,4
NVIDIA/k8s-device-plugin,release.yaml,"# Copyright 2024 NVIDIA CORPORATION
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Run this workflow on new tags
name: Release

on:
  push:
    tags:
      - v*

jobs:
  release:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        name: Check out code

      - name: Install Helm
        uses: azure/setup-helm@v4.3.0
        with:
          version: 3.14.4

      - name: Generate Helm Charts
        run: |
          ./hack/package-helm-charts.sh ${{ github.ref_name }}

      - name: Create Draft Release
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          ./hack/create-release.sh ${{ github.ref_name }}
",43,1,1,push,2
NVIDIA/k8s-device-plugin,stale.yaml,"name: Stale issues and pull requests

on:
  schedule:
  - cron: ""21 4 * * *""

jobs:
  stale:
    permissions:
      issues: write
      pull-requests: write
    runs-on: ubuntu-latest

    steps:
    - uses: actions/stale@v9
      with:
        stale-issue-message: 'This issue is stale because it has been open 90 days with no activity. This issue will be closed in 30 days unless new comments are made or the stale label is removed.'
        stale-pr-message: 'This PR is stale because it has been open 90 days with no activity. This PR will be closed in 30 days unless new comments are made or the stale label is removed.'
        stale-issue-label: 'lifecycle/stale'
        stale-pr-label: 'lifecycle/stale'
        days-before-stale: 90
        close-issue-message: 'This issue was automatically closed due to inactivity.'
        close-pr-message: 'This pull request was automatically closed due to inactivity.' 
        days-before-issue-close: 30
        days-before-pr-close: 30
        remove-stale-when-updated: true
        operations-per-run: 300
",27,1,1,schedule,1
NVIDIA/k8s-device-plugin,variables.yaml,"# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

on:
  workflow_call:
    outputs:
      version:
        description: ""The short SHA to use as a version string""
        value: ${{ jobs.variables.outputs.version }}
      golang_version:
        description: ""The golang version for this project""
        value: ${{ jobs.variables.outputs.golang_version }}

jobs:
  variables:
    runs-on: ubuntu-latest
    outputs:
      version: ${{ steps.version.outputs.version }}
      golang_version: ${{ steps.golang_version.outputs.golang_version }}
    steps:
    - name: Check out code
      uses: actions/checkout@v4

    - name: Generate Commit Short SHA
      id: version
      run: echo ""version=$(echo $GITHUB_SHA | cut -c1-8)"" >> ""$GITHUB_OUTPUT""

    - name: Get Golang Version
      id: golang_version
      run: |
        GOLANG_VERSION=$(./hack/golang-version.sh)
        echo ""Detected $GOLANG_VERSION""
        echo ""golang_version=${GOLANG_VERSION}"" >> $GITHUB_OUTPUT

",45,1,1,workflow_call,1
NVIDIA/cutlass,blossom-ci.yml,"#################################################################################################
#
# Copyright (c) 2023 - 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: BSD-3-Clause
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#
# 1. Redistributions of source code must retain the above copyright notice, this
# list of conditions and the following disclaimer.
#
# 2. Redistributions in binary form must reproduce the above copyright notice,
# this list of conditions and the following disclaimer in the documentation
# and/or other materials provided with the distribution.
#
# 3. Neither the name of the copyright holder nor the names of its
# contributors may be used to endorse or promote products derived from
# this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
#
#################################################################################################

# A workflow to trigger ci on hybrid infra (github + self hosted runner)
name: Blossom-CI
on:
  issue_comment:
    types: [created]
  workflow_dispatch:
      inputs:
          platform:
            description: 'runs-on argument'
            required: false
          args:
            description: 'argument'
            required: false

jobs:
  Authorization:
    name: Authorization
    runs-on: blossom
    outputs:
      args: ${{ env.args }}

    # This job only runs for pull request comments
    if: |
        (startsWith(github.event.comment.body, '/bot run') ||
        startsWith(github.event.comment.body, '/bot kill')) && contains(
        fromJson('[""zekunf-nv""]'),
        github.actor)
    steps:
      - name: Check if comment is issued by authorized person
        run: blossom-ci
        env:
          OPERATION: 'AUTH'
          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          REPO_KEY_DATA: ${{ secrets.BLOSSOM_KEY }}

  Vulnerability-scan:
    name: Vulnerability scan
    needs: [Authorization]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v2
        with:
          repository: ${{ fromJson(needs.Authorization.outputs.args).repo }}
          ref: ${{ fromJson(needs.Authorization.outputs.args).ref }}
          lfs: 'true'

      - name: Run blossom action
        uses: NVIDIA/blossom-action@main
        env:
          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          REPO_KEY_DATA: ${{ secrets.BLOSSOM_KEY }}
        with:
          args1: ${{ fromJson(needs.Authorization.outputs.args).args1 }}
          args2: ${{ fromJson(needs.Authorization.outputs.args).args2 }}
          args3: ${{ fromJson(needs.Authorization.outputs.args).args3 }}

  Job-trigger:
    name: Start ci job
    needs: [Vulnerability-scan]
    runs-on: blossom
    steps:
      - name: Start ci job
        run: blossom-ci
        env:
          OPERATION: 'START-CI-JOB'
          CI_SERVER: ${{ secrets.CI_SERVER }}
          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  Upload-Log:
    name: Upload log
    runs-on: blossom
    if : github.event_name == 'workflow_dispatch'
    steps:
      - name: Jenkins log for pull request ${{ fromJson(github.event.inputs.args).pr }} (click here)
        run: blossom-ci
        env:
          OPERATION: 'POST-PROCESSING'
          CI_SERVER: ${{ secrets.CI_SERVER }}
          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
",112,4,2,"issue_comment, workflow_dispatch",2
NVIDIA/cutlass,labeler.yml,"name: ""Pull Request Labeler""
on:
- pull_request_target

jobs:
  triage:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/labeler@main
      with:
        repo-token: ""${{ secrets.GITHUB_TOKEN }}""
",11,1,1,pull_request_target,1
NVIDIA/cutlass,new-issues-to-triage-projects.yml,"name: Auto Assign New Issues to Triage Project

on:
  issues:
    types: [opened]

env:
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

jobs:
  assign_one_project:
    runs-on: ubuntu-latest
    name: Assign to New Issues to Triage Project
    steps:
    - name: Process bug issues
      uses: docker://takanabe/github-actions-automate-projects:v0.0.1
      if: contains(github.event.issue.labels.*.name, 'bug') && contains(github.event.issue.labels.*.name, '? - Needs Triage')
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        GITHUB_PROJECT_URL: https://github.com/NVIDIA/cutlass
        GITHUB_PROJECT_COLUMN_NAME: 'Needs prioritizing'
    - name: Process feature issues
      uses: docker://takanabe/github-actions-automate-projects:v0.0.1
      if: contains(github.event.issue.labels.*.name, 'feature request') && contains(github.event.issue.labels.*.name, '? - Needs Triage')
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        GITHUB_PROJECT_URL: https://github.com/NVIDIA/cutlass
        GITHUB_PROJECT_COLUMN_NAME: 'Needs prioritizing'
    - name: Process other issues
      uses: docker://takanabe/github-actions-automate-projects:v0.0.1
      if: contains(github.event.issue.labels.*.name, '? - Needs Triage') && (!contains(github.event.issue.labels.*.name, 'bug') && !contains(github.event.issue.labels.*.name, 'feature request'))
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        GITHUB_PROJECT_URL: https://github.com/NVIDIA/cutlass
        GITHUB_PROJECT_COLUMN_NAME: 'Needs prioritizing'
",35,1,1,issues,3
NVIDIA/cutlass,stale.yml,"name: Mark inactive issues and pull requests

on:
  schedule:
    - cron: ""0 * * * *""

jobs:
  mark-inactive-30d:
    runs-on: ubuntu-latest
    steps:
      - name: Mark 30 day inactive issues and pull requests
        uses: actions/stale@v3
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          stale-issue-message: >
            This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days.
            Please close this issue if no further response or action is needed.
            Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed.
            This issue will be labeled `inactive-90d` if there is no activity in the next 60 days.
          stale-issue-label: ""inactive-30d""
          exempt-issue-labels: ""0 - Blocked,0 - Backlog,good first issue""
          days-before-issue-stale: 30
          days-before-issue-close: -1
          stale-pr-message: >
            This PR has been labeled `inactive-30d` due to no recent activity in the past 30 days.
            Please close this PR if it is no longer required.
            Otherwise, please respond with a comment indicating any updates.
            This PR will be labeled `inactive-90d` if there is no activity in the next 60 days.
          stale-pr-label: ""inactive-30d""
          exempt-pr-labels: ""0 - Blocked,0 - Backlog,good first issue""
          days-before-pr-stale: 30
          days-before-pr-close: -1
          operations-per-run: 50
  mark-inactive-90d:
    runs-on: ubuntu-latest
    steps:
      - name: Mark 90 day inactive issues and pull requests
        uses: actions/stale@v3
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          stale-issue-message: >
            This issue has been labeled `inactive-90d` due to no recent activity in the past 90 days.
            Please close this issue if no further response or action is needed.
            Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed.
          stale-issue-label: ""inactive-90d""
          exempt-issue-labels: ""0 - Blocked,0 - Backlog,good first issue""
          days-before-issue-stale: 90
          days-before-issue-close: -1
          stale-pr-message: >
            This PR has been labeled `inactive-90d` due to no recent activity in the past 90 days.
            Please close this PR if it is no longer required.
            Otherwise, please respond with a comment indicating any updates.
          stale-pr-label: ""inactive-90d""
          exempt-pr-labels: ""0 - Blocked,0 - Backlog,good first issue""
          days-before-pr-stale: 90
          days-before-pr-close: -1
          operations-per-run: 50
",57,2,1,schedule,2
alibaba/druid,ci.yaml,"---
name: Java CI

on:
  push:
    # https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#example-excluding-paths
    paths-ignore:
      - 'docs/**'
      - '**.md'
  pull_request:
    paths-ignore:
      - 'docs/**'
      - '**.md'

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ ubuntu-latest ]
        java: [ 8, 11, 17, 21 ]
      fail-fast: false
      max-parallel: 16
    name: Test JDK ${{ matrix.java }}, ${{ matrix.os }}

    steps:
      - uses: actions/checkout@v4
      - name: Set up JDK
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: ${{ matrix.java }}
          cache: 'maven'
      - name: Build with Maven if test jdk8
        if: ${{ matrix.java == '8' || matrix.java == '11'}}
        run: ./mvnw -Pgen-javadoc clean package -B
      - name: Build with Maven if test jdk17
        if: ${{ matrix.java == '17' || matrix.java == '21' }}
        run: ./mvnw -Penable-for-jdk17+,gen-code-cov clean package -B
      - name: ""Codecov if test jdk17""
        if: ${{ matrix.java == '17' }}
        uses: codecov/codecov-action@v3.1.0
        with:
          files: ./core/target/site/jacoco/jacoco.xml,./druid-spring-boot-starter/target/site/jacoco/jacoco.xml,./druid-spring-boot-3-starter/target/site/jacoco/jacoco.xml
",44,1,2,"push, pull_request",3
alibaba/tengine,ci-arm64.yml,"name: build tengine

on:
  push:
    branches: [ master ]
  pull_request:

jobs:
  build-arm64:
    runs-on: ""ubuntu-24.04""
    strategy:
      fail-fast: false
      matrix:
        compiler:
         - { compiler: GNU,  CC: gcc,  CXX: g++}
         - { compiler: LLVM, CC: clang, CXX: clang++}
    steps:
      - name: Checkout Tengine
        uses: actions/checkout@v3

      - name: 'checkout luajit2'
        uses: actions/checkout@v3
        with:
          repository: openresty/luajit2
          path: luajit2

      - name: Compile with ${{ matrix.compiler.compiler }}
        uses: uraimo/run-on-arch-action@v3
        with:
          arch: aarch64
          distro: ubuntu24.04
          githubToken: ${{ github.token }}
          dockerRunArgs: |
            --volume ""${PWD}:/tengine""
          install: |
            set -x
            apt-get update -q -y
            apt-get install -q -y make gcc g++ clang libgd-dev libgeoip-dev libxslt1-dev libpcre3 libpcre3-dev liblua5.1-0-dev lua5.1 libperl-dev cpanminus libssl-dev file
          run: |
            set -x
            cd /tengine
            echo ""Build luajit2""
            cd luajit2
            make -j4
            make install
            cd ..
            echo ""Build tengine""
            export CC=${{ matrix.compiler.CC }}
            export CXX=${{ matrix.compiler.CXX }}
            export LUAJIT_LIB=/usr/local/lib
            export LUAJIT_INC=/usr/local/include/luajit-2.1
            ./configure \
              --with-ld-opt=""-Wl,-lpcre,-rpath,/usr/local/lib"" \
              --with-ipv6 \
              --with-http_ssl_module \
              --with-http_v2_module \
              --with-http_addition_module \
              --with-stream \
              --with-stream_ssl_module \
              --with-stream_realip_module \
              --with-stream_geoip_module \
              --with-stream_ssl_preread_module \
              --with-stream_sni \
              --add-module=./modules/ngx_backtrace_module \
              --add-module=./modules/ngx_debug_pool \
              --add-module=./modules/ngx_debug_timer \
              --add-module=./modules/ngx_debug_conn \
              --add-module=./modules/ngx_http_concat_module \
              --add-module=./modules/ngx_http_footer_filter_module \
              --add-module=./modules/ngx_http_lua_module \
              --add-module=./modules/ngx_http_proxy_connect_module \
              --add-module=./modules/ngx_http_reqstat_module \
              --add-module=./modules/ngx_http_slice_module \
              --add-module=./modules/ngx_http_sysguard_module \
              --add-module=./modules/ngx_http_trim_filter_module \
              --add-module=./modules/ngx_http_upstream_check_module \
              --add-module=./modules/ngx_http_upstream_consistent_hash_module \
              --add-module=./modules/ngx_http_upstream_dynamic_module \
              --add-module=./modules/ngx_http_upstream_dyups_module \
              --add-module=./modules/ngx_http_upstream_iwrr_module \
              --add-module=./modules/ngx_http_upstream_keepalive_module \
              --add-module=./modules/ngx_http_upstream_session_sticky_module \
              --add-module=./modules/ngx_http_upstream_vnswrr_module \
              --add-module=./modules/ngx_http_user_agent_module \
              --add-module=./modules/ngx_multi_upstream_module \
              --add-module=./modules/ngx_slab_stat \
              --without-http_upstream_keepalive_module
            make -j4
            make install
            file /usr/local/nginx/sbin/nginx | grep aarch64
",90,1,2,"push, pull_request",3
alibaba/tengine,ci.yml,"name: test tengine

on:
  push:
    branches: [ master ]
  pull_request:
    branches: [ master ]

jobs:
  build-and-test:
   runs-on: ""ubuntu-24.04""
   strategy:
     fail-fast: false
     matrix:
       compiler:
         - { compiler: GNU,  CC: gcc,  CXX: g++}
         - { compiler: LLVM, CC: clang, CXX: clang++}
   steps:
     - uses: actions/checkout@v3
     - name: get dependencies
       run: |
         sudo apt update
         sudo apt remove nginx libgd3
         sudo apt install -y libgd-dev libgeoip-dev libxslt1-dev libpcre3 libpcre3-dev liblua5.1-0-dev lua5.1 libperl-dev cpanminus libssl-dev
     - name: 'checkout luajit2'
       uses: actions/checkout@v3
       with:
         repository: openresty/luajit2
         path: luajit2
     - name: 'build luajit2'
       working-directory: luajit2
       run: |
         make
         sudo make install
     - name: 'checkout lua-resty-lrucache'
       uses: actions/checkout@v3
       with:
         repository: openresty/lua-resty-lrucache
         path: lua-resty-lrucache
     - name: 'build lua-resty-lrucache'
       working-directory: lua-resty-lrucache
       run: |
         sudo make install
     - name: 'checkout lua-resty-core'
       uses: actions/checkout@v3
       with:
         repository: openresty/lua-resty-core
         ref: v0.1.27
         path: lua-resty-core
     - name: 'build lua-resty-core'
       working-directory: lua-resty-core
       run: |
         sudo make install

# TODO: fix tests so they don't depend on /usr/local/nginx/logs/
#       so we can run `make`, `make test`, `make install`.

     - name: build
       env:
         CC: ${{ matrix.compiler.CC }}
         CXX: ${{ matrix.compiler.CXX }}
         LUAJIT_LIB: /usr/local/lib
         LUAJIT_INC: /usr/local/include/luajit-2.1
       run: |
         ./configure \
            --with-debug \
            --with-ld-opt=""-Wl,-lpcre,-rpath,/usr/local/lib"" \
            --with-ipv6 \
            --with-openssl-async \
            --with-http_ssl_module \
            --with-http_v2_module \
            --with-http_addition_module \
            --with-stream \
            --with-stream_ssl_module \
            --with-stream_realip_module \
            --with-stream_geoip_module \
            --with-stream_ssl_preread_module \
            --with-stream_sni \
            --add-module=./modules/ngx_backtrace_module \
            --add-module=./modules/ngx_debug_pool \
            --add-module=./modules/ngx_debug_timer \
            --add-module=./modules/ngx_debug_conn \
            --add-module=./modules/ngx_http_concat_module \
            --add-module=./modules/ngx_http_footer_filter_module \
            --add-module=./modules/ngx_http_lua_module \
            --add-module=./modules/ngx_http_proxy_connect_module \
            --add-module=./modules/ngx_http_reqstat_module \
            --add-module=./modules/ngx_http_slice_module \
            --add-module=./modules/ngx_http_sysguard_module \
            --add-module=./modules/ngx_http_trim_filter_module \
            --add-module=./modules/ngx_http_upstream_check_module \
            --add-module=./modules/ngx_http_upstream_consistent_hash_module \
            --add-module=./modules/ngx_http_upstream_dynamic_module \
            --add-module=./modules/ngx_http_upstream_dyups_module \
            --add-module=./modules/ngx_http_upstream_iwrr_module \
            --add-module=./modules/ngx_http_upstream_keepalive_module \
            --add-module=./modules/ngx_http_upstream_session_sticky_module \
            --add-module=./modules/ngx_http_upstream_vnswrr_module \
            --add-module=./modules/ngx_http_user_agent_module \
            --add-module=./modules/ngx_multi_upstream_module \
            --add-module=./modules/ngx_slab_stat \
            --without-http_upstream_keepalive_module
         make -j2
         sudo make install
     - name: tengine test cases using nginx-tests lib
       working-directory: tests/nginx-tests
       env:
         TEST_NGINX_BINARY: /usr/local/nginx/sbin/nginx
       run: |
         sudo cpanm --notest Net::DNS::Nameserver > build.log 2>&1 || (cat build.log && exit 1)
         prove -v -Inginx-tests/lib tengine-tests/
         prove -v -Inginx-tests/lib ../../modules/ngx_http_proxy_connect_module/t
         prove -v -Inginx-tests/lib ../../modules/ngx_debug_timer/t
         prove -v -Inginx-tests/lib ../../modules/ngx_debug_conn/t
         prove -v -Inginx-tests/lib ../../modules/ngx_slab_stat/t
     - name: tengine test cases using test-nginx lib
       working-directory: tests/test-nginx
       run: |
         sudo cpanm --notest Shell Test::Base Test::LongString List::MoreUtils LWP::UserAgent HTTP::Response  > build.log 2>&1 || (cat build.log && exit 1)
         mkdir t
         PATH=/usr/local/nginx/sbin:$PATH \
         prove -v -Itest-nginx/lib cases/
",122,1,2,"push, pull_request",4
alibaba/tengine,test-nginx-core.yml,"name: test nginx core

on:
  push:
    branches: [ master ]
  pull_request:
    branches: [ master ]

jobs:
  build-and-test:
   runs-on: ""ubuntu-24.04""
   strategy:
     fail-fast: false
     matrix:
       compiler:
         - { compiler: GNU,  CC: gcc,  CXX: g++}
         - { compiler: LLVM, CC: clang, CXX: clang++}
   steps:
     - uses: actions/checkout@v3
     - name: get dependencies
       run: |
         sudo apt update
         sudo apt remove nginx libgd3
         sudo apt install -y libgd-dev libgeoip-dev libxslt1-dev libpcre3 libpcre3-dev liblua5.1-0-dev lua5.1 libperl-dev cpanminus libssl-dev
         # for building nginx core
         sudo apt install -y libgoogle-perftools-dev
         # for running cases in nginx-tests
         sudo apt install -y uwsgi-plugin-python3 uwsgi ffmpeg memcached libsofthsm2-dev
     - name: 'checkout luajit2'
       uses: actions/checkout@v3
       with:
         repository: openresty/luajit2
         path: luajit2
     - name: 'build luajit2'
       working-directory: luajit2
       run: |
         make
         sudo make install
     - name: 'checkout lua-resty-lrucache'
       uses: actions/checkout@v3
       with:
         repository: openresty/lua-resty-lrucache
         path: lua-resty-lrucache
     - name: 'build lua-resty-lrucache'
       working-directory: lua-resty-lrucache
       run: |
         sudo make install
     - name: 'checkout lua-resty-core'
       uses: actions/checkout@v3
       with:
         repository: openresty/lua-resty-core
         ref: v0.1.27
         path: lua-resty-core
     - name: 'build lua-resty-core'
       working-directory: lua-resty-core
       run: |
         sudo make install
     - name: build
       env:
         CC: ${{ matrix.compiler.CC }}
         CXX: ${{ matrix.compiler.CXX }}
         LUAJIT_LIB: /usr/local/lib
         LUAJIT_INC: /usr/local/include/luajit-2.1
       run: |
         # TODO: fix https://github.com/alibaba/tengine/issues/1720, then remove ""-D T_NGX_HTTP_IMAGE_FILTER=0""

         # NOTE:
         # For ""-D T_NGX_MODIFY_DEFAULT_VALUE=0"", we dont compile the source included in this macro, otherwise some nginx-tests cases tests will fail.
         # For ""-D T_NGX_SERVER_INFO=0"", it makes some cases pass, such as userid.t.
         # For ""-D T_NGX_HTTP_UPSTREAM_RANDOM=0"", it makes some cases pass, such as image_filter_finalize.t.
         ./configure  \
            --with-cc-opt=""-D T_NGX_MODIFY_DEFAULT_VALUE=0 -D T_NGX_HTTP_IMAGE_FILTER=0 -D T_NGX_SERVER_INFO=0 -D T_NGX_HTTP_UPSTREAM_RANDOM=0"" \
            --with-ld-opt=""-Wl,-lpcre,-rpath,/usr/local/lib"" \
            --with-openssl-async \
            --with-pcre \
            --with-http_ssl_module \
            --with-http_image_filter_module \
            --with-http_v2_module \
            --with-http_addition_module \
            --with-http_mp4_module \
            --with-http_realip_module \
            --with-http_xslt_module \
            --with-http_geoip_module \
            --with-http_sub_module \
            --with-http_dav_module \
            --with-http_flv_module \
            --with-http_gunzip_module \
            --with-http_gzip_static_module \
            --with-http_auth_request_module \
            --with-http_random_index_module \
            --with-http_secure_link_module \
            --with-http_degradation_module \
            --with-http_slice_module \
            --with-http_stub_status_module \
            --with-mail \
            --with-mail_ssl_module \
            --with-stream \
            --with-stream_ssl_module \
            --with-stream_realip_module \
            --with-stream_geoip_module \
            --with-stream_ssl_preread_module \
            --with-google_perftools_module \
            --with-cpp_test_module \
            --with-compat \
            --add-module=modules/mod_config \
            --add-module=modules/mod_dubbo \
            --add-module=modules/ngx_backtrace_module \
            --add-module=modules/ngx_debug_timer \
            --add-module=modules/ngx_http_concat_module \
            --add-module=modules/ngx_http_footer_filter_module \
            --add-module=modules/ngx_http_lua_module \
            --add-module=modules/ngx_http_proxy_connect_module \
            --add-module=modules/ngx_http_reqstat_module \
            --add-module=modules/ngx_http_sysguard_module \
            --add-module=modules/ngx_http_trim_filter_module \
            --add-module=modules/ngx_http_upstream_check_module \
            --add-module=modules/ngx_http_upstream_consistent_hash_module \
            --add-module=modules/ngx_http_upstream_dynamic_module \
            --add-module=modules/ngx_http_upstream_session_sticky_module \
            --add-module=modules/ngx_http_upstream_vnswrr_module \
            --add-module=modules/ngx_http_user_agent_module \
            --add-module=modules/ngx_multi_upstream_module \
            --add-module=modules/ngx_slab_stat \
            --add-module=modules/ngx_http_upstream_dyups_module \
            --with-http_perl_module \
            --with-stream_sni \
            --with-openssl-async \
            --with-debug
            # skip ngx_debug_pool, it modified NGX_MIN_POOL_SIZE, which made some test case failed (http_header_buffers.t)
            # skip tengine upstream keepalive module
            #--without-http_upstream_keepalive_module \
            #--add-module=modules/ngx_http_upstream_keepalive_module \
            # skip tengine slice module
            #--add-module=modules/ngx_http_slice_module \
         make -j2
         sudo make install
     - name: run cases in nginx-tests
       working-directory: tests/nginx-tests
       env:
         TEST_NGINX_BINARY: /usr/local/nginx/sbin/nginx
         TEST_NGINX_UNSAFE: yes
       run: |
         # prepare perl library for test case
         sudo cpanm --notest SCGI Protocol::WebSocket Net::SSLeay IO::Socket::SSL Cache::Memcached Cache::Memcached::Fast Net::DNS::Nameserver GD > build.log 2>&1 || (cat build.log && exit 1)
         # fixed http_method.t for tengine proxy_connect module
         sed -i -e ""s+405 Not Allowed(?!.*200 OK)/s, 'connect'+400 Bad Request(?!.*200 OK)/s, 'connect'+"" nginx-tests/http_method.t
         # run cases in nginx-tests
         prove -I nginx-tests/lib nginx-tests/
         # It must be root for some cases.
         sudo groupadd wheel    # for proxy_bind_transparent.t
         sudo TEST_NGINX_BINARY=/usr/local/nginx/sbin/nginx TEST_NGINX_UNSAFE=yes prove -I nginx-tests/lib nginx-tests/proxy_bind_transparent.t nginx-tests/proxy_bind_transparent_capability.t

",152,1,2,"push, pull_request",4
alibaba/tengine,test-ntls.yml,"name: test tengine ntls

on:
  push:
    branches: [ master ]
  pull_request:
    branches: [ master ]

jobs:
  build-and-test:
    runs-on: ""ubuntu-24.04""
    strategy:
      fail-fast: false
      matrix:
        compiler:
          - { compiler: GNU,  CC: gcc,  CXX: g++}
          - { compiler: LLVM, CC: clang, CXX: clang++}
    steps:
      - uses: actions/checkout@v3
        with:
          path: tengine
      - name: get dependencies
        run: |
          sudo apt install -y libpcre3 libpcre3-dev
      - name: 'checkout luajit2'
        uses: actions/checkout@v3
        with:
          repository: openresty/luajit2
          path: luajit2
      - name: 'build luajit2'
        working-directory: luajit2
        run: |
          make
          sudo make install
      - name: 'checkout lua-resty-lrucache'
        uses: actions/checkout@v3
        with:
          repository: openresty/lua-resty-lrucache
          path: lua-resty-lrucache
      - name: 'build lua-resty-lrucache'
        working-directory: lua-resty-lrucache
        run: |
          sudo make install
      - name: 'checkout lua-resty-core'
        uses: actions/checkout@v3
        with:
          repository: openresty/lua-resty-core
          ref: v0.1.27
          path: lua-resty-core
      - name: 'build lua-resty-core'
        working-directory: lua-resty-core
        run: |
          sudo make install
      - name: checkout Tongsuo
        uses: actions/checkout@v3
        with:
          repository: Tongsuo-Project/Tongsuo
          path: Tongsuo
      - name: build Tongsuo
        working-directory: Tongsuo
        env:
          CC: ${{ matrix.compiler.CC }}
        run: |
          ./config --prefix=${RUNNER_TEMP}/tongsuo enable-ntls no-shared
          make -s -j4
          make install_sw
          make clean
      - name: build Tengine
        working-directory: tengine
        env:
          CC: ${{ matrix.compiler.CC }}
          CXX: ${{ matrix.compiler.CXX }}
          LUAJIT_LIB: /usr/local/lib
          LUAJIT_INC: /usr/local/include/luajit-2.1
        run: |
          ./configure \
            --with-ld-opt=""-Wl,-lpcre,-rpath,/usr/local/lib"" \
            --with-pcre \
            --add-module=modules/ngx_tongsuo_ntls \
            --add-module=modules/ngx_http_lua_module \
            --with-openssl=../Tongsuo \
            --with-openssl-opt=""--api=1.1.1 enable-ntls"" \
            --with-http_ssl_module \
            --with-http_v2_module \
            --with-stream \
            --with-stream_ssl_module \
            --with-stream_sni
          make -j2
          sudo make install
      - name: run test cases
        working-directory: tengine
        env:
          TEST_OPENSSL_BINARY: ${{ runner.temp }}/tongsuo/bin/tongsuo
          TEST_NGINX_BINARY: /usr/local/nginx/sbin/nginx
          TEST_NGINX_LEAVE: 1
        run: |
          prove -Itests/nginx-tests/nginx-tests/lib/ modules/ngx_tongsuo_ntls/t
      - name: debug
        if: ${{ failure() }}
        run: |
          for file in `ls /tmp/nginx-test-*/error.log`; do cat $file; done
",101,1,2,"push, pull_request",5
alibaba/transmittable-thread-local,ci.yaml,"# Quickstart for GitHub Actions
# https://docs.github.com/en/actions/quickstart
name: fast CI
on: [ push, pull_request, workflow_dispatch ]

jobs:
  test:
    runs-on: ${{ matrix.os }}
    timeout-minutes: 10
    strategy:
      matrix:
        os: [ ubuntu-latest, windows-latest ]
        java: [ 8, 11, 17, 21, 22 ]
      fail-fast: false
      max-parallel: 64
    name: Fast CI on Java ${{ matrix.java }} OS ${{ matrix.os }}

    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v4
        with:
          java-version: ${{ matrix.java }}
          distribution: zulu
          cache: maven
      - name: Build with Maven
        run: ./mvnw -V --no-transfer-progress clean package
      - name: Run unit test under ttl agent, include check for ExecutorService, ForkJoinPool, Timer/TimerTask
        working-directory: ttl2-compatible
        run: >
          ../mvnw -V --no-transfer-progress
          -Penable-ttl-agent-for-test
          surefire:test
          '-Dttl.agent.extra.d.options=-Drun-ttl-test-under-agent-with-enable-timer-task=true'
      - name: Run unit test under ttl agent, and turn on the disable inheritable for thread pool enhancement
        working-directory: ttl2-compatible
        run: >
          ../mvnw -V --no-transfer-progress
          -Penable-ttl-agent-for-test
          surefire:test
          '-Dttl.agent.extra.args=ttl.agent.disable.inheritable.for.thread.pool:true'
          '-Dttl.agent.extra.d.options=-Drun-ttl-test-under-agent-with-disable-inheritable=true'
      - name: Run agent check for Timer/TimerTask, explicit ""ttl.agent.enable.timer.task""
        working-directory: ttl2-compatible
        run: >
          ../mvnw -V --no-transfer-progress
          -Penable-ttl-agent-for-test
          surefire:test
          '-Dttl.agent.extra.args=ttl.agent.enable.timer.task:true'
          '-Dttl.agent.extra.d.options=-Drun-ttl-test-under-agent-with-enable-timer-task=true'
",49,1,3,"push, pull_request, workflow_dispatch",2
alibaba/transmittable-thread-local,strong_ci.yaml,"# Quickstart for GitHub Actions
# https://docs.github.com/en/actions/quickstart

name: Strong CI
on: [ push, pull_request, workflow_dispatch ]
jobs:
  test:
    # https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#choosing-github-hosted-runners
    runs-on: ubuntu-latest
    timeout-minutes: 30
    name: Strong CI by multiply java versions

    steps:
      - uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          # https://github.com/actions/setup-java?tab=readme-ov-file#install-multiple-jdks
          java-version: |
            8
            11
            17
            21
            22
          distribution: zulu
          cache: maven

      - name: Run integration test
        run: scripts/integration-test.sh

      - name: Remove self maven install files
        run: rm -rf $HOME/.m2/repository/com/alibaba/{transmittable-thread-local,ttl}*

      - name: Upload coverage reports to Codecov
        uses: codecov/codecov-action@v4
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

      # https://remarkablemark.org/blog/2017/10/12/check-git-dirty/
      - name: Check git dirty
        run: |
          git status --short
          [ -z ""$(git status --short)"" ]
",46,1,3,"push, pull_request, workflow_dispatch",3
alibaba/ice,canary.yml,"name: Publish canary

on:
  push:
    branches:
      - release/**

jobs:
  check_changeset:
    name: Check Changeset exists
    outputs:
      status: ${{ steps.check.outcome }}
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Branch
        uses: actions/checkout@v4

      - name: Check
        id: check
        continue-on-error: true
        run: test ""$(ls -1 .changeset | wc -l)"" -gt ""2""

  canary:
    name: Publish Canary
    runs-on: ubuntu-latest
    needs: check_changeset
    if: needs.check_changeset.outputs.status == 'success'

    strategy:
      matrix:
        node-version: [18]

    steps:
      - name: Checkout Branch
        uses: actions/checkout@v4

      - name: Install pnpm
        uses: pnpm/action-setup@v4

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'pnpm'

      - name: Setup
        run: pnpm run setup

      - name: Config npm
        run: echo ""//registry.npmjs.org/:_authToken=${NPM_TOKEN}"" > .npmrc
        env:
          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}

      - run: pnpm run release:snapshot
",55,2,1,push,4
alibaba/ice,ci.yml,"name: CI

env:
  NODE_OPTIONS: --max-old-space-size=6144

on: [push]

jobs:
  build:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        node-version: [16.x, 18.x]
        os: [ubuntu-latest, windows-latest]
      fail-fast: false
    steps:
      - name: Checkout Branch
        uses: actions/checkout@v3
      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v3
        with:
          node-version: ${{ matrix.node-version }}
          registry-url: https://registry.npmjs.org/
      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          run_install: false
      - name: Get pnpm store directory
        id: pnpm-cache
        run: |
          echo ""pnpm_cache_dir=$(pnpm store path)"" >> ""$GITHUB_OUTPUT""
      - uses: actions/cache@v3
        name: Setup pnpm cache
        with:
          path: |
            ${{ steps.pnpm-cache.outputs.pnpm_cache_dir }}
            .cache
          key: ${{ runner.os }}-pnpm-store-node-${{ matrix.node-version }}-${{ hashFiles('**/pnpm-lock.yaml') }}
          restore-keys: |
            ${{ runner.os }}-pnpm-store-node-${{ matrix.node-version }}
      - run: npm run setup
      - run: npm run dependency:check
      - run: npm run lint
      - run: npm run test
        env:
          TEST: true
          CI: true
          ACCESS_KEY_ID: ${{ secrets.ACCESS_KEY_ID }}
          ACCESS_KEY_SECRET: ${{ secrets.ACCESS_KEY_SECRET }}
",49,1,1,push,4
alibaba/ice,coverage.yml,"name: Coverage

env:
  NODE_OPTIONS: --max-old-space-size=6144

on:
  push:
    branches:
      - master
    paths-ignore:
      - 'examples/**'
      - 'website/**'
      - '**/*.md'
  pull_request:
    types:
      - 'opened'
      - 'synchronize'
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  coverage:
    permissions:
      checks: write
      pull-requests: write
      contents: read
    runs-on: ubuntu-latest
    strategy:
      matrix:
        node-version: [16.x]

    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0
      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v3
        with:
          node-version: ${{ matrix.node-version }}
          registry-url: https://registry.npmjs.org/
      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          run_install: false
      - name: Get pnpm store directory
        id: pnpm-cache
        run: |
          echo ""pnpm_cache_dir=$(pnpm store path)"" >> ""$GITHUB_OUTPUT""
      - uses: actions/cache@v3
        name: Setup pnpm cache
        with:
          path: |
            ${{ steps.pnpm-cache.outputs.pnpm_cache_dir }}
            .cache
          key: ${{ runner.os }}-pnpm-store-node-${{ matrix.node-version }}-${{ hashFiles('**/pnpm-lock.yaml') }}
          restore-keys: |
            ${{ runner.os }}-pnpm-store-node-${{ matrix.node-version }}
      - run: npm run setup
      - run: npm run cov
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        env:
          CI: true
",66,1,3,"push, pull_request, workflow_dispatch",5
alibaba/ice,release.yml,"name: Release

on:
  push:
    branches:
      - master

jobs:
  release:
    name: Release
    runs-on: ubuntu-latest

    strategy:
      matrix:
        node-version: [16]

    steps:
      - name: Checkout Branch
        uses: actions/checkout@v3

      - name: Install pnpm
        uses: pnpm/action-setup@v4

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v3
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'pnpm'

      - name: Setup
        run: pnpm run setup

      - name: Setup git user
        run: pnpm run setup-git

      - name: Publish to npm
        id: changesets
        uses: changesets/action@v1
        with:
          version: pnpm run version
          commit: 'chore: update versions'
          title: 'chore: update versions'
          publish: pnpm release
          createGithubReleases: false
          setupGitUser: false
        env:
          GITHUB_TOKEN: ${{ secrets.PERSONAL_GITHUB_TOKEN }}
          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}
",48,1,1,push,4
alibaba/ice,version.yml,"name: Version

on:
  push:
    branches:
      - release/next

jobs:
  version:
    name: Version
    runs-on: ubuntu-latest

    strategy:
      matrix:
        node-version: [16]

    steps:
      - name: Checkout Branch
        uses: actions/checkout@v3

      - name: Install pnpm
        uses: pnpm/action-setup@v4

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v3
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'pnpm'

      - name: Install Dependencies
        run: pnpm install

      - name: Setup git user
        run: pnpm run setup-git

      - name: Create Release Pull Request
        uses: changesets/action@v1
        with:
          version: pnpm run version
          commit: 'chore: update versions'
          title: 'chore: update versions'
          setupGitUser: false
        env:
          GITHUB_TOKEN: ${{ secrets.PERSONAL_GITHUB_TOKEN }}
",44,1,1,push,4
alibaba/ice,website.yml,"name: Build Website
on:
  push:
    paths:
      - 'website/**'
    branches:
      - master
  workflow_dispatch:

# 任务
jobs:
  build-and-deploy:
    # 服务器环境：最新版 Ubuntu
    runs-on: ubuntu-latest
    steps:
      # 拉取代码
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Install pnpm
        uses: pnpm/action-setup@v4

      - name: Use Node.js 16.x
        uses: actions/setup-node@v3
        with:
          node-version: 16.x
          cache: 'pnpm'

      - run: pnpm install --filter=./website

      - run: cd website && pnpm build

      # 部署到 GitHub Pages
      - name: Deploy
        uses: JamesIves/github-pages-deploy-action@4.1.0
        if: github.ref == 'refs/heads/master'
        with:
          BRANCH: gh-pages
          FOLDER: website/build
          clean-exclude: ice.png
",41,1,2,"push, workflow_dispatch",4
freeCodeCamp/freeCodeCamp,crowdin-download.client-ui.yml,"name: i18n - Download Client UI
on:
  workflow_dispatch:
  schedule:
    # runs Monday and Wednesday at 12:15 PM UTC
    - cron: '15 12 * * 1,3'

env:
  GITHUB_TOKEN: ${{ secrets.CROWDIN_CAMPERBOT_PAT }}
  CROWDIN_PERSONAL_TOKEN: ${{ secrets.CROWDIN_CAMPERBOT_SERVICE_TOKEN }}
  CROWDIN_API_URL: 'https://freecodecamp.crowdin.com/api/v2/'
  CROWDIN_PROJECT_ID: ${{ secrets.CROWDIN_PROJECT_ID_CLIENT }}

jobs:
  i18n-download-client-ui-translations:
    name: Client
    runs-on: ubuntu-22.04

    steps:
      - name: Checkout Source Files
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
        with:
          token: ${{ secrets.CROWDIN_CAMPERBOT_PAT }}

      - name: Generate Crowdin Config
        uses: freecodecamp/crowdin-action@main
        env:
          PLUGIN: 'generate-config'
          PROJECT_NAME: 'client'

      ##### Download Chinese #####
      - name: Crowdin Download Chinese Translations
        uses: crowdin/github-action@master
        # options: https://github.com/crowdin/github-action/blob/master/action.yml
        with:
          # uploads
          upload_sources: false
          upload_translations: false
          auto_approve_imported: false
          import_eq_suggestions: false

          # downloads
          download_translations: true
          download_language: zh-CN
          skip_untranslated_files: false
          export_only_approved: true

          push_translations: false

          # pull-request
          create_pull_request: false

          # global options
          config: './crowdin-config.yml'
          base_url: ${{ secrets.CROWDIN_BASE_URL_FCC }}

          # Uncomment below to debug
          # dryrun_action: true

      # Convert Simplified Chinese to Traditional #
      - name: Convert Chinese
        uses: freecodecamp/crowdin-action@main
        env:
          PLUGIN: 'convert-chinese'
          FILE_PATHS: '[""client/i18n/locales/chinese""]'

      ##### Download Espanol #####
      - name: Crowdin Download Espanol Translations
        uses: crowdin/github-action@master
        # options: https://github.com/crowdin/github-action/blob/master/action.yml
        with:
          # uploads
          upload_sources: false
          upload_translations: false
          auto_approve_imported: false
          import_eq_suggestions: false

          # downloads
          download_translations: true
          download_language: es-EM
          skip_untranslated_files: false
          export_only_approved: true

          push_translations: false

          # pull-request
          create_pull_request: false

          # global options
          config: './crowdin-config.yml'
          base_url: ${{ secrets.CROWDIN_BASE_URL_FCC }}

          # Uncomment below to debug
          # dryrun_action: true

      ##### Download Italian #####
      - name: Crowdin Download Italian Translations
        uses: crowdin/github-action@master
        # options: https://github.com/crowdin/github-action/blob/master/action.yml
        with:
          # uploads
          upload_sources: false
          upload_translations: false
          auto_approve_imported: false
          import_eq_suggestions: false

          # downloads
          download_translations: true
          download_language: it
          skip_untranslated_files: false
          export_only_approved: true

          push_translations: false

          # pull-request
          create_pull_request: false

          # global options
          config: './crowdin-config.yml'
          base_url: ${{ secrets.CROWDIN_BASE_URL_FCC }}

          # Uncomment below to debug
          # dryrun_action: true

      ##### Download Brazilian Portuguese #####
      - name: Crowdin Download Portuguese (Brazilian) Translations
        uses: crowdin/github-action@master
        # options: https://github.com/crowdin/github-action/blob/master/action.yml
        with:
          # uploads
          upload_sources: false
          upload_translations: false
          auto_approve_imported: false
          import_eq_suggestions: false

          # downloads
          download_translations: true
          download_language: pt-BR
          skip_untranslated_files: false
          export_only_approved: true

          push_translations: false

          # pull-request
          create_pull_request: false

          # global options
          config: './crowdin-config.yml'
          base_url: ${{ secrets.CROWDIN_BASE_URL_FCC }}

          # Uncomment below to debug
          # dryrun_action: true

          ##### Download Ukrainian #####
      - name: Crowdin Download Ukrainian Translations
        uses: crowdin/github-action@master
        # options: https://github.com/crowdin/github-action/blob/master/action.yml
        with:
          # uploads
          upload_sources: false
          upload_translations: false
          auto_approve_imported: false
          import_eq_suggestions: false

          # downloads
          download_translations: true
          download_language: uk
          skip_untranslated_files: false
          export_only_approved: true

          push_translations: false

          # pull-request
          create_pull_request: false

          # global options
          config: './crowdin-config.yml'
          base_url: ${{ secrets.CROWDIN_BASE_URL_FCC }}

          # Uncomment below to debug
          # dryrun_action: true

      ##### Download Japanese #####
      - name: Crowdin Download Japanese Translations
        uses: crowdin/github-action@master
        # options: https://github.com/crowdin/github-action/blob/master/action.yml
        with:
          # uploads
          upload_sources: false
          upload_translations: false
          auto_approve_imported: false
          import_eq_suggestions: false

          # downloads
          download_translations: true
          download_language: ja
          skip_untranslated_files: false
          export_only_approved: true

          push_translations: false

          # pull-request
          create_pull_request: false

          # global options
          config: './crowdin-config.yml'
          base_url: ${{ secrets.CROWDIN_BASE_URL_FCC }}

          # Uncomment below to debug
          # dryrun_action: true

      ##### Download German #####
      - name: Crowdin Download German Translations
        uses: crowdin/github-action@master
        # options: https://github.com/crowdin/github-action/blob/master/action.yml
        with:
          # uploads
          upload_sources: false
          upload_translations: false
          auto_approve_imported: false
          import_eq_suggestions: false

          # downloads
          download_translations: true
          download_language: de
          skip_untranslated_files: false
          export_only_approved: true

          push_translations: false

          # pull-request
          create_pull_request: false

          # global options
          config: './crowdin-config.yml'
          base_url: ${{ secrets.CROWDIN_BASE_URL_FCC }}

          # Uncomment below to debug
          # dryrun_action: true

      ##### Download Swahili #####
      - name: Crowdin Download Swahili Translations
        uses: crowdin/github-action@master
        # options: https://github.com/crowdin/github-action/blob/master/action.yml
        with:
          # uploads
          upload_sources: false
          upload_translations: false
          auto_approve_imported: false
          import_eq_suggestions: false

          # downloads
          download_translations: true
          download_language: sw
          skip_untranslated_files: false
          export_only_approved: true

          push_translations: false

          # pull-request
          create_pull_request: false

          # global options
          config: './crowdin-config.yml'
          base_url: ${{ secrets.CROWDIN_BASE_URL_FCC }}

          # Uncomment below to debug
          # dryrun_action: true

      ##### Download Korean #####
      - name: Crowdin Download Korean Translations
        uses: crowdin/github-action@master
        # options: https://github.com/crowdin/github-action/blob/master/action.yml
        with:
          # uploads
          upload_sources: false
          upload_translations: false
          auto_approve_imported: false
          import_eq_suggestions: false

          # downloads
          download_translations: true
          download_language: ko
          skip_untranslated_files: false
          export_only_approved: true

          push_translations: false

          # pull-request
          create_pull_request: false

          # global options
          config: './crowdin-config.yml'
          base_url: ${{ secrets.CROWDIN_BASE_URL_FCC }}

          # Uncomment below to debug
          # dryrun_action: true

      ###### Format JSON #####
      # Crowdin gives the files read-only permissions, so we first have to allow
      # writes.
      - name: Format JSON
        run: |
          sudo chown -R $(whoami): client/i18n/locales
          npx --yes prettier --write client/i18n/locales/**/*.json

      ###### Lowercase directory names #####

      - name: Lowercase Directories
        uses: freecodecamp/crowdin-action@main
        env:
          PLUGIN: 'lowercase-directories'
          FILE_PATH: 'client/i18n/locales'
      # Crowdin translators might have the directories
      # Create Commit
      - name: Commit Changes
        uses: freecodecamp/crowdin-action@main
        env:
          PLUGIN: 'commit-changes'
          GH_USERNAME: 'camperbot'
          GH_EMAIL: ${{ secrets.ACTIONS_CAMPERBOT_EMAIL }}
          GH_BRANCH: 'i18n-sync-client'
          GH_MESSAGE: 'chore(i18n,client): processed translations'

      # Generate PR #
      # All languages should go ABOVE this. #

      - name: Create PR
        uses: freecodecamp/crowdin-action@main
        env:
          PLUGIN: 'pull-request'
          GH_TOKEN: ${{ secrets.CROWDIN_CAMPERBOT_PAT }}
          BRANCH: 'i18n-sync-client'
          REPOSITORY: 'freecodecamp/freecodecamp'
          BASE: 'main'
          TITLE: 'chore(i18n,client): processed translations'
          BODY: 'This PR was opened auto-magically by Crowdin.'
          LABELS: 'crowdin-sync'
          TEAM_REVIEWERS: 'i18n'
",339,1,2,"workflow_dispatch, schedule",15
freeCodeCamp/freeCodeCamp,crowdin-upload.client-ui.yml,"name: i18n - Upload Client UI
on:
  workflow_dispatch:
  schedule:
    # runs every weekday at 7:15 AM UTC
    - cron: '15 7 * * 1-5'

env:
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  CROWDIN_PERSONAL_TOKEN: ${{ secrets.CROWDIN_CAMPERBOT_SERVICE_TOKEN }}
  CROWDIN_API_URL: 'https://freecodecamp.crowdin.com/api/v2/'
  CROWDIN_PROJECT_ID: ${{ secrets.CROWDIN_PROJECT_ID_ClIENT }}

jobs:
  i18n-upload-client-ui-files:
    name: Client
    runs-on: ubuntu-22.04

    steps:
      - name: Checkout Source Files
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1

      - name: Generate Crowdin Config
        uses: freecodecamp/crowdin-action@main
        env:
          PLUGIN: 'generate-config'
          PROJECT_NAME: 'client'

      - name: Crowdin Upload
        uses: crowdin/github-action@master
        # options: https://github.com/crowdin/github-action/blob/master/action.yml
        with:
          # uploads
          upload_sources: true
          upload_translations: false
          auto_approve_imported: false
          import_eq_suggestions: false

          # downloads
          download_translations: false

          # pull-request
          create_pull_request: false

          # global options
          config: './crowdin-config.yml'
          base_url: ${{ secrets.CROWDIN_BASE_URL_FCC }}

          # Uncomment below to debug
          # dryrun_action: true
",50,1,2,"workflow_dispatch, schedule",3
freeCodeCamp/freeCodeCamp,crowdin-upload.curriculum.yml,"name: i18n - Upload Curriculum
on:
  workflow_dispatch:
  schedule:
    # runs every weekday at 7:30 AM UTC
    - cron: '30 7 * * 1-5'

env:
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  CROWDIN_PERSONAL_TOKEN: ${{ secrets.CROWDIN_CAMPERBOT_SERVICE_TOKEN }}
  CROWDIN_API_URL: 'https://freecodecamp.crowdin.com/api/v2/'
  CROWDIN_PROJECT_ID: ${{ secrets.CROWDIN_PROJECT_ID_CURRICULUM }}

jobs:
  i18n-upload-curriculum-files:
    name: Learn
    runs-on: ubuntu-22.04

    steps:
      - name: Checkout Source Files
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1

      - name: Generate Crowdin Config
        uses: freecodecamp/crowdin-action@main
        env:
          PLUGIN: 'generate-config'
          PROJECT_NAME: 'curriculum'

      - name: Crowdin Upload
        uses: crowdin/github-action@master
        # options: https://github.com/crowdin/github-action/blob/master/action.yml
        with:
          # uploads
          upload_sources: true
          upload_translations: false
          auto_approve_imported: false
          import_eq_suggestions: false

          # downloads
          download_translations: false

          # pull-request
          create_pull_request: false

          # global options
          config: './crowdin-config.yml'
          base_url: ${{ secrets.CROWDIN_BASE_URL_FCC }}

          # Uncomment below to debug
          # dryrun_action: true

      - name: Remove deleted files
        uses: freecodecamp/crowdin-action@main
        env:
          PLUGIN: 'remove-deleted-files'
          FILE_PATHS: '[""curriculum/challenges/english"", ""curriculum/dictionaries/english""]'

      - name: Hide Non-Translated Strings
        uses: freecodecamp/crowdin-action@main
        env:
          PLUGIN: 'hide-curriculum-strings'

      - name: Hide a String
        uses: freecodecamp/crowdin-action@main
        env:
          PLUGIN: 'hide-string'
          FILE_NAME: 'basic-html-and-html5/nest-an-anchor-element-within-a-paragraph.md'
          STRING_CONTENT: Here's a <a href=""https://www.freecodecamp.org"" target=""_blank"" mark=""crwd-mark"">link to www.freecodecamp.org</a> for you to follow.

      - name: Unhide Title of Use && For a More Concise Conditional
        uses: freecodecamp/crowdin-action@main
        env:
          PLUGIN: 'unhide-string'
          FILE_NAME: 'react/use--for-a-more-concise-conditional.md'
          STRING_CONTENT: 'Use &amp;&amp; for a More Concise Conditional'
",75,1,2,"workflow_dispatch, schedule",7
freeCodeCamp/freeCodeCamp,curriculum-i18n-submodule.yml,"name: CI - Node.js - i18n - Submodule

on:
  # Run on push events, but only for the below branches
  push:
    branches:
      - 'chore/update-i18n-curriculum-submodule'
  workflow_dispatch:

permissions:
  contents: read

jobs:
  test-curriculum:
    name: Test Curriculum
    runs-on: ubuntu-22.04
    strategy:
      fail-fast: false
      matrix:
        node-version: [22]
        # Exclude the languages that we currently run in the full CI suite.
        locale:
          - 'chinese'
          - 'espanol'
          - 'ukrainian'
          - 'japanese'
          - 'german'
          - 'swahili'

    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
        with:
          submodules: 'recursive'

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4
        with:
          node-version: ${{ matrix.node-version }}

      - name: Install pnpm
        uses: pnpm/action-setup@a7487c7e89a18df4991f7f222e4898a00d66ddda # v4
        id: pnpm-install
        with:
          run_install: false

      - name: Set Environment variables
        run: |
          cp sample.env .env

      - name: Install node_modules
        run: pnpm install

      # DONT REMOVE THIS STEP.
      # TODO: Refactor and use re-usable workflow and shared artifacts
      - name: Build Client in ${{ matrix.locale }}
        env:
          CURRICULUM_LOCALE: ${{ matrix.locale }}
          CLIENT_LOCALE: ${{ matrix.locale }}
        run: |
          pnpm run build

      - name: Install Chrome for Puppeteer
        run: pnpm -F=curriculum install-puppeteer

      - name: Run Tests
        env:
          CURRICULUM_LOCALE: ${{ matrix.locale }}
          CLIENT_LOCALE: ${{ matrix.locale }}
        run: pnpm test:curriculum
",70,1,2,"push, workflow_dispatch",3
freeCodeCamp/freeCodeCamp,deploy-api.yml,"name: CD - Deploy - API

on:
  workflow_dispatch:
    inputs:
      api_log_lvl:
        description: 'Log level for the API'
        type: choice
        options:
          - debug
          - info
          - warn
        default: info

jobs:
  setup-jobs:
    name: Setup Jobs
    runs-on: ubuntu-22.04
    outputs:
      site_tld: ${{ steps.setup.outputs.site_tld }}
      tgt_env_short: ${{ steps.setup.outputs.tgt_env_short }}
      tgt_env_long: ${{ steps.setup.outputs.tgt_env_long }}
      api_log_lvl: ${{ steps.setup.outputs.api_log_lvl }}
    steps:
      - name: Setup
        id: setup
        run: |
          BRANCH=""${{ github.ref_name }}""
          echo ""Current branch: $BRANCH""
          case ""$BRANCH"" in
            ""prod-current"")
              echo ""site_tld=org"" >> $GITHUB_OUTPUT
              echo ""tgt_env_short=prd"" >> $GITHUB_OUTPUT
              echo ""tgt_env_long=production"" >> $GITHUB_OUTPUT
              echo ""api_log_lvl=${{ inputs.api_log_lvl || 'info' }}"" >> $GITHUB_OUTPUT
              ;;
            *)
              echo ""site_tld=dev"" >> $GITHUB_OUTPUT
              echo ""tgt_env_short=stg"" >> $GITHUB_OUTPUT
              echo ""tgt_env_long=staging"" >> $GITHUB_OUTPUT
              echo ""api_log_lvl=${{ inputs.api_log_lvl || 'info' }}"" >> $GITHUB_OUTPUT
              ;;
          esac

  build:
    name: Build & Push
    needs: setup-jobs
    uses: ./.github/workflows/docker-docr.yml
    with:
      site_tld: ${{ needs.setup-jobs.outputs.site_tld }}
      app: api
    secrets: inherit

  deploy:
    name: Deploy to Docker Swarm -- ${{ needs.setup-jobs.outputs.tgt_env_short }}
    runs-on: ubuntu-22.04
    needs: [setup-jobs, build]
    env:
      TS_USERNAME: ${{ secrets.TS_USERNAME }}
      TS_MACHINE_NAME: ${{ secrets.TS_MACHINE_NAME }}
    permissions:
      deployments: write
    environment:
      name: ${{ needs.setup-jobs.outputs.tgt_env_short }}-api

    steps:
      - name: Setup and connect to Tailscale network
        uses: tailscale/github-action@v3
        with:
          oauth-client-id: ${{ secrets.TS_OAUTH_CLIENT_ID }}
          oauth-secret: ${{ secrets.TS_OAUTH_SECRET }}
          hostname: gha-${{needs.setup-jobs.outputs.tgt_env_short}}-api-ci-${{ github.run_id }}
          tags: tag:ci
          version: latest

      - name: Configure SSH & Check Connection
        run: |
          mkdir -p ~/.ssh
          echo ""Host *
            UserKnownHostsFile=/dev/null
            StrictHostKeyChecking no"" > ~/.ssh/config
          chmod 644 ~/.ssh/config
          sleep 10
          tailscale status | grep -q ""$TS_MACHINE_NAME"" || { echo ""Error: Machine not found""; exit 1; }
          sleep 1
          MACHINE_IP=$(tailscale ip -4 $TS_MACHINE_NAME)
          echo -e ""\nLOG:Checking connection to $TS_MACHINE_NAME...""
          ssh $TS_USERNAME@$MACHINE_IP ""uptime""

      - name: Deploy with Docker Stack
        env:
          AGE_ENCRYPTED_ASC_SECRETS: ${{ secrets.AGE_ENCRYPTED_ASC_SECRETS }}
          AGE_SECRET_KEY: ${{ secrets.AGE_SECRET_KEY }}
          # Variable set from GitHub ""Environment"" secrets (AGE encrypted)
          # DOCKER_REGISTRY
          # MONGOHQ_URL
          # SENTRY_DSN
          # SENTRY_ENVIRONMENT
          # AUTH0_CLIENT_ID
          # AUTH0_CLIENT_SECRET
          # AUTH0_DOMAIN
          # JWT_SECRET
          # COOKIE_SECRET
          # COOKIE_DOMAIN
          # SES_ID
          # SES_SECRET
          # GROWTHBOOK_FASTIFY_API_HOST
          # GROWTHBOOK_FASTIFY_CLIENT_KEY
          # HOME_LOCATION
          # API_LOCATION
          # STRIPE_SECRET_KEY
          # LOKI_URL
          # Variables set from SetupJob
          DEPLOYMENT_VERSION: ${{ needs.build.outputs.tagname }}
          DEPLOYMENT_ENV: ${{ needs.setup-jobs.outputs.site_tld }}
          FCC_API_LOG_LEVEL: ${{ needs.setup-jobs.outputs.api_log_lvl }}
          # Stack name
          STACK_NAME: ${{ needs.setup-jobs.outputs.tgt_env_short }}-api
        run: |
          REMOTE_SCRIPT=""
            set -e
            echo -e '\nLOG:Deploying API to $TS_MACHINE_NAME...'
            cd /home/$TS_USERNAME/docker-swarm-config/stacks/api

            echo -e '\nLOG:Checking if age is installed...'
            which age > /dev/null

            echo -e '\nLOG:Decrypting secrets...'
            echo \""$AGE_ENCRYPTED_ASC_SECRETS\"" > secrets.age.asc
            echo \""$AGE_SECRET_KEY\"" > age.key && chmod 600 age.key
            age --identity age.key --decrypt secrets.age.asc > .env
            rm -f age.key secrets.age.asc

            echo -e '\nLOG:Cleaning up .env file...'
            touch .env.tmp
            while IFS= read -r line; do
              if [[ \$line =~ ^[A-Za-z0-9_]+=.*$ ]]; then
                # Extract the key (part before the first =)
                key=\${line%%=*}
                # Remove any previous line with this key
                sed -i \""/^\${key}=/d\"" .env.tmp
              fi
              # Append the current line
              echo \""\$line\"" >> .env.tmp
            done < .env
            mv .env.tmp .env

            echo -e '\nLOG:Adding deployment variables...'
            {
              echo \""DEPLOYMENT_VERSION=$DEPLOYMENT_VERSION\""
              echo \""DEPLOYMENT_ENV=$DEPLOYMENT_ENV\""
              echo \""FCC_API_LOG_LEVEL=$FCC_API_LOG_LEVEL\""
            } >> .env

            echo -e '\nLOG:Sourcing environment...'
            REQUIRED_VARS=(
              \""DOCKER_REGISTRY\""
              \""MONGOHQ_URL\""
              \""SENTRY_DSN\""
              \""SENTRY_ENVIRONMENT\""
              \""AUTH0_CLIENT_ID\""
              \""AUTH0_CLIENT_SECRET\""
              \""AUTH0_DOMAIN\""
              \""JWT_SECRET\""
              \""COOKIE_SECRET\""
              \""COOKIE_DOMAIN\""
              \""SES_ID\""
              \""SES_SECRET\""
              \""GROWTHBOOK_FASTIFY_API_HOST\""
              \""GROWTHBOOK_FASTIFY_CLIENT_KEY\""
              \""HOME_LOCATION\""
              \""API_LOCATION\""
              \""STRIPE_SECRET_KEY\""
              \""LOKI_URL\""
              \""DEPLOYMENT_VERSION\""
              \""DEPLOYMENT_ENV\""
              \""FCC_API_LOG_LEVEL\""
            )

            while IFS='=' read -r key value; do
              if [[ -n \""\$key\"" && ! \""\$key\"" =~ ^# ]]; then
                export \""\${key}=\${value}\""
              fi
            done < .env

            MISSING_VARS=()
            for var in \""\${REQUIRED_VARS[@]}\""; do
              if [[ -z \""\${!var}\"" ]]; then
                MISSING_VARS+=(\""\$var\"")
              fi
            done

            if [[ \${#MISSING_VARS[@]} -gt 0 ]]; then
              echo \""ERROR: The following required environment variables are missing or empty:\""
              for var in \""\${MISSING_VARS[@]}\""; do
                echo \""  - \$var\""
              done
              exit 1
            fi

            rm -rf .env

            echo -e '\nLOG:Validating deployment version...'
            if [[ \""\$DEPLOYMENT_VERSION\"" != \""$DEPLOYMENT_VERSION\"" ]]; then
              echo \""Error: Version mismatch. Expected: $DEPLOYMENT_VERSION, Got: \$DEPLOYMENT_VERSION\""
              exit 1
            fi
            env | grep -E 'DEPLOYMENT_VERSION'

            echo -e '\nLOG:Checking stack configuration...'
            CONFIG_OUTPUT=\""/dev/null\""
            if [[ \""\$FCC_API_LOG_LEVEL\"" == \""debug\"" ]]; then
              CONFIG_FILENAME=\""debug-docker-stack-config-\${DEPLOYMENT_VERSION}.yml\""
              echo -e '\nLOG:Saving stack configuration for debugging...'
              CONFIG_OUTPUT=\""\$CONFIG_FILENAME\""
            fi
            docker stack config -c stack-api.yml > \$CONFIG_OUTPUT

            echo -e '\nLOG:Deploying stack...'
            docker stack deploy -c stack-api.yml --prune --with-registry-auth --detach=false $STACK_NAME

            echo -e '\nLOG:Finished deployment.'
          ""
          MACHINE_IP=$(tailscale ip -4 $TS_MACHINE_NAME)
          ssh $TS_USERNAME@$MACHINE_IP ""$REMOTE_SCRIPT""
",225,3,1,workflow_dispatch,2
freeCodeCamp/freeCodeCamp,deploy-client.yml,"name: CD - Deploy - Clients

on:
  workflow_dispatch:

jobs:
  setup-jobs:
    name: Setup Jobs
    runs-on: ubuntu-22.04
    outputs:
      site_tld: ${{ steps.setup.outputs.site_tld }} # org, dev
      tgt_env_short: ${{ steps.setup.outputs.tgt_env_short }} # prd, stg
      tgt_env_long: ${{ steps.setup.outputs.tgt_env_long }} # production, staging
      tgt_env_branch: ${{ steps.setup.outputs.tgt_env_branch }} # prod-current, prod-staging
    steps:
      - name: Setup
        id: setup
        run: |
          BRANCH=""${{ github.ref_name }}""
          echo ""Current branch: $BRANCH""
          case ""$BRANCH"" in
            ""prod-current"")
              echo ""site_tld=org"" >> $GITHUB_OUTPUT
              echo ""tgt_env_short=prd"" >> $GITHUB_OUTPUT
              echo ""tgt_env_long=production"" >> $GITHUB_OUTPUT
              echo ""tgt_env_branch=prod-current"" >> $GITHUB_OUTPUT
              ;;
            *)
              echo ""site_tld=dev"" >> $GITHUB_OUTPUT
              echo ""tgt_env_short=stg"" >> $GITHUB_OUTPUT
              echo ""tgt_env_long=staging"" >> $GITHUB_OUTPUT
              echo ""tgt_env_branch=prod-staging"" >> $GITHUB_OUTPUT
              ;;
          esac

  client:
    name: Clients - [${{ needs.setup-jobs.outputs.tgt_env_short }}] [${{ matrix.lang-name-short }}]
    needs: [setup-jobs]
    runs-on: ubuntu-22.04
    strategy:
      fail-fast: false
      matrix:
        node-version: [22]
        lang-name-full:
          - english
          - chinese
          - espanol
          - chinese-traditional
          - italian
          - portuguese
          - ukrainian
          - japanese
          - german
          - swahili
        include:
          - lang-name-full: english
            lang-name-short: eng
          - lang-name-full: chinese
            lang-name-short: chn
          - lang-name-full: espanol
            lang-name-short: esp
          - lang-name-full: chinese-traditional
            lang-name-short: cnt
          - lang-name-full: italian
            lang-name-short: ita
          - lang-name-full: portuguese
            lang-name-short: por
          - lang-name-full: ukrainian
            lang-name-short: ukr
          - lang-name-full: japanese
            lang-name-short: jpn
          - lang-name-full: german
            lang-name-short: ger
          - lang-name-full: swahili
            lang-name-short: swa
    permissions:
      deployments: write
      contents: read
    environment:
      name: ${{ needs.setup-jobs.outputs.tgt_env_short }}-clients
    env:
      TS_USERNAME: ${{ secrets.TS_USERNAME }}
      TS_MACHINE_NAME_PREFIX: ${{ secrets.TS_MACHINE_NAME_PREFIX }}

    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
        with:
          submodules: 'recursive'

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4
        with:
          node-version: ${{ matrix.node-version }}

      - name: Install pnpm
        uses: pnpm/action-setup@a7487c7e89a18df4991f7f222e4898a00d66ddda # v4
        id: pnpm-install
        with:
          run_install: false

      - name: Language specific ENV - [${{ matrix.lang-name-full }}]
        run: |
          if [ ""${{ matrix.lang-name-full }}"" = ""english"" ]; then
            echo ""HOME_LOCATION=https://www.freecodecamp.${{ needs.setup-jobs.outputs.site_tld }}"" >> $GITHUB_ENV
            echo ""NEWS_LOCATION=https://www.freecodecamp.${{ needs.setup-jobs.outputs.site_tld }}/news"" >> $GITHUB_ENV
          else
            echo ""HOME_LOCATION=https://www.freecodecamp.${{ needs.setup-jobs.outputs.site_tld }}/${{ matrix.lang-name-full }}"" >> $GITHUB_ENV
            echo ""NEWS_LOCATION=https://www.freecodecamp.${{ needs.setup-jobs.outputs.site_tld }}/${{ matrix.lang-name-full }}/news"" >> $GITHUB_ENV
          fi
          echo ""CLIENT_LOCALE=${{ matrix.lang-name-full }}"" >> $GITHUB_ENV
          echo ""CURRICULUM_LOCALE=${{ matrix.lang-name-full }}"" >> $GITHUB_ENV

      - name: Install and Build
        env:
          API_LOCATION: 'https://api.freecodecamp.${{ needs.setup-jobs.outputs.site_tld }}'
          ALGOLIA_API_KEY: ${{ secrets.ALGOLIA_API_KEY }}
          ALGOLIA_APP_ID: ${{ secrets.ALGOLIA_APP_ID }}
          GROWTHBOOK_URI: ${{ secrets.GROWTHBOOK_URI }}
          FORUM_LOCATION: 'https://forum.freecodecamp.org'
          PATREON_CLIENT_ID: ${{ secrets.PATREON_CLIENT_ID }}
          PAYPAL_CLIENT_ID: ${{ secrets.PAYPAL_CLIENT_ID }}
          STRIPE_PUBLIC_KEY: ${{ secrets.STRIPE_PUBLIC_KEY }}
          SHOW_UPCOMING_CHANGES: ${{ vars.SHOW_UPCOMING_CHANGES || 'false' }}
          FREECODECAMP_NODE_ENV: production
          # The below is used in ecosystem.config.js file for the API -- to be removed later
          DEPLOYMENT_ENV: ${{ needs.setup-jobs.outputs.tgt_env_long }}
          # The above is used in ecosystem.config.js file for the API -- to be removed later
        run: |
          pnpm install
          pnpm run build

      - name: Tar Files
        run: tar -czf client-${{ matrix.lang-name-short }}.tar client/public

      - name: Setup and connect to Tailscale network
        uses: tailscale/github-action@v3
        with:
          oauth-client-id: ${{ secrets.TS_OAUTH_CLIENT_ID }}
          oauth-secret: ${{ secrets.TS_OAUTH_SECRET }}
          hostname: gha-${{needs.setup-jobs.outputs.tgt_env_short}}-clients-ci-${{ github.run_id }}
          tags: tag:ci
          version: latest

      - name: Configure SSH & Check Connection
        run: |
          mkdir -p ~/.ssh
          echo ""Host *
            UserKnownHostsFile=/dev/null
            StrictHostKeyChecking no"" > ~/.ssh/config
          chmod 644 ~/.ssh/config
          sleep 10
          for i in {0..1}; do
            TS_MACHINE_NAME=${TS_MACHINE_NAME_PREFIX}-${{ matrix.lang-name-short }}-${i}
            tailscale status | grep -q ""$TS_MACHINE_NAME"" || { echo ""Machine not found""; exit 1; }
            sleep 1
            MACHINE_IP=$(tailscale ip -4 $TS_MACHINE_NAME)
            ssh $TS_USERNAME@$MACHINE_IP ""uptime""
          done

      - name: Upload and Deploy
        run: |
          for i in {0..1}; do
            TS_MACHINE_NAME=${TS_MACHINE_NAME_PREFIX}-${{ matrix.lang-name-short }}-${i}
            CURRENT_DATE=$(date +%Y%m%d)
            CLIENT_SRC=client-${{ matrix.lang-name-short }}.tar
            CLIENT_DST=/tmp/client-${{ matrix.lang-name-short }}-${CURRENT_DATE}-${{ github.run_id }}.tar
            CLIENT_BINARIES=${{needs.setup-jobs.outputs.tgt_env_short}}-release-$CURRENT_DATE-${{ github.run_id }}

            echo -e ""\nLOG:Uploading client archive to $TS_MACHINE_NAME...""
            MACHINE_IP=$(tailscale ip -4 $TS_MACHINE_NAME)
            scp $CLIENT_SRC $TS_USERNAME@$MACHINE_IP:$CLIENT_DST

            REMOTE_SCRIPT=""
              set -e
              echo -e '\nLOG: Deploying client - $CLIENT_BINARIES to $TS_MACHINE_NAME...'

              echo -e '\nLOG:Extracting client archive...'
              mkdir -p /home/$TS_USERNAME/client/releases/$CLIENT_BINARIES
              tar -xzf $CLIENT_DST -C /home/$TS_USERNAME/client/releases/$CLIENT_BINARIES --strip-components=2

              echo -e '\nLOG:Cleaning up client archive...'
              rm $CLIENT_DST

              echo -e '\nLOG:Checking client archive size...'
              du -sh /home/$TS_USERNAME/client/releases/$CLIENT_BINARIES

              echo -e '\nLOG:Environment setup...'
              cd /home/$TS_USERNAME/client
              export NVM_DIR=\$HOME/.nvm && [ -s ""\$NVM_DIR/nvm.sh"" ] && source ""\$NVM_DIR/nvm.sh""
              echo -e '\nLOG:Checking available Node.js versions...'
              nvm ls | grep 'default'
              echo -e '\nLOG:Checking Node.js version...'
              node --version

              echo -e '\nLOG:Installing serve...'
              npm install -g serve@13

              echo -e '\nLOG:Primary client setup...'
              rm -f client-start-primary.sh
              echo \""serve -c ../../serve.json releases/$CLIENT_BINARIES -p 50505\"" >> client-start-primary.sh
              chmod +x client-start-primary.sh
              pm2 delete client-primary || true
              pm2 start ./client-start-primary.sh --name client-primary
              echo -e '\nLOG:Primary client setup completed.'

              pm2 ls

              echo -e '\nLOG:Secondary client setup...'
              rm -f client-start-secondary.sh
              echo \""serve -c ../../serve.json releases/$CLIENT_BINARIES -p 52525\"" >> client-start-secondary.sh
              chmod +x client-start-secondary.sh
              pm2 delete client-secondary || true
              pm2 start ./client-start-secondary.sh --name client-secondary
              echo -e '\nLOG:Secondary client setup completed.'

              pm2 ls
              pm2 save

              echo -e '\nLOG:Finished deployment.'
            ""
            ssh $TS_USERNAME@$MACHINE_IP ""$REMOTE_SCRIPT""
          done
",223,2,1,workflow_dispatch,4
freeCodeCamp/freeCodeCamp,docker-docr-cleanup.yml,"name: DOCR - Cleanup Container Images
on:
  workflow_dispatch:
  schedule:
    - cron: '5 0 * * 3,6' # 12:05 UTC on Wednesdays and Saturdays (6 hour maintenance window)

jobs:
  remove:
    name: Delete Old Images
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        repos:
          - learn-api
        variants:
          - dev
          - org

    steps:
      - name: Install doctl
        uses: digitalocean/action-doctl@v2
        with:
          token: ${{ secrets.DIGITALOCEAN_ACCESS_TOKEN }}

      - name: Log in to DigitalOcean Container Registry with short-lived credentials
        run: doctl registry login --expiry-seconds 1200

      - name: Delete Images
        uses: raisedadead/action-docr-cleanup@62b968c928fbb2dbce8b0caf11c0391f0921ea46 # v1
        with:
          repository_name: '${{ matrix.variants }}/${{ matrix.repos }}'
          days: '7'
          keep_last: '3'

  # NOTE: We don't need to trigger garbage collection because we have a cron job that runs on news repo, which
  #       triggers garbage collection. This can be uncommented should a need arise.

  # clean:
  #   name: Do Garbage Collection
  #   runs-on: ubuntu-latest
  #   needs: remove
  #   steps:
  #     - name: Install doctl
  #       uses: digitalocean/action-doctl@v2
  #       with:
  #         token: ${{ secrets.DIGITALOCEAN_ACCESS_TOKEN }}

  #     - name: Log in to DigitalOcean Container Registry with short-lived credentials
  #       run: doctl registry login --expiry-seconds 1200

  #     - name: Trigger Garbage collection
  #       run: doctl registry garbage-collection start --include-untagged-manifests --force
",53,1,2,"workflow_dispatch, schedule",3
freeCodeCamp/freeCodeCamp,docker-docr.yml,"name: CD - Docker - DOCR

on:
  workflow_dispatch:
    inputs:
      site_tld:
        required: true
        type: choice
        description: 'Input: The site tld (variant) to build'
        options:
          - dev
          - org
        default: 'dev'
      app:
        required: true
        type: string
        description: 'Input: The app (component) to build'
        default: 'api'
  workflow_call:
    inputs:
      site_tld:
        required: true
        type: string
        description: 'Input: The site tld (variant) to build'
      app:
        required: true
        type: string
        description: 'Input: The app (component) to build'
    outputs:
      tagname:
        description: 'Output: The tagname for the image built'
        value: ${{ jobs.build.outputs.tagname }}

jobs:
  build:
    name: Build & Push
    runs-on: ubuntu-22.04
    permissions:
      contents: read
    outputs:
      tagname: ${{ steps.tagname.outputs.tagname }}

    steps:
      - name: Checkout Source Files
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1

      - name: Create a tagname
        id: tagname
        run: |
          tagname=$(git rev-parse --short HEAD)-$(date +%Y%m%d)-$(date +%H%M)
          echo ""tagname=$tagname"" >> $GITHUB_ENV
          echo ""tagname=$tagname"" >> $GITHUB_OUTPUT

      - name: Build & Tag Image
        run: |
          docker build \
            --tag registry.digitalocean.com/${{ secrets.DOCR_NAME }}/${{ inputs.site_tld }}/learn-${{ inputs.app }}:$tagname \
            --tag registry.digitalocean.com/${{ secrets.DOCR_NAME }}/${{ inputs.site_tld }}/learn-${{ inputs.app }}:latest \
            --file docker/${{ inputs.app }}/Dockerfile .

      - name: Install doctl
        uses: digitalocean/action-doctl@v2
        with:
          token: ${{ secrets.DIGITALOCEAN_ACCESS_TOKEN }}

      - name: Log in to DigitalOcean Container Registry with short-lived credentials
        run: doctl registry login --expiry-seconds 1200

      - name: Push image to DigitalOcean Container Registry
        run: |
          docker push registry.digitalocean.com/${{ secrets.DOCR_NAME }}/${{ inputs.site_tld }}/learn-${{ inputs.app }}:$tagname
          docker push registry.digitalocean.com/${{ secrets.DOCR_NAME }}/${{ inputs.site_tld }}/learn-${{ inputs.app }}:latest
",72,1,2,"workflow_dispatch, workflow_call",2
freeCodeCamp/freeCodeCamp,docker-ghcr.yml,"name: CD - Docker - GHCR (Gitpod)

on:
  workflow_dispatch:
  push:
    branches:
      - main
    paths:
      - 'docker/gitpod/*'

jobs:
  build-and-push-image:
    runs-on: ubuntu-22.04
    permissions:
      contents: read
      packages: write

    strategy:
      fail-fast: false
      matrix:
        images:
          - gitpod

    steps:
      - name: Checkout code
        uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332 # v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@988b5a0280414f521da01fcc63a27aeeb4b104db # v3

      - name: Log in to the GHCR
        uses: docker/login-action@9780b0c442fbb1117ed29e0efdff1e18412f7567 # v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Cache Docker layers
        uses: actions/cache@5a3ec84eff668545956fd18022155c47e93e2684 # v4.2.3
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ matrix.images }}-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-buildx-${{ matrix.images }}-

      - name: Build and push Docker image
        uses: docker/build-push-action@14487ce63c7a62a4a324b0bfb37086795e31c6c1 # v6.16.0
        with:
          context: ./docker/${{ matrix.images }}
          push: true
          tags: |
            ghcr.io/freecodecamp/${{ matrix.images }}:${{ github.sha }}
            ghcr.io/freecodecamp/${{ matrix.images }}:latest
          cache-from: type=local,src=/tmp/.buildx-cache
          cache-to: type=local,dest=/tmp/.buildx-cache-new,mode=max

      - name: Move cache
        run: |
          rm -rf /tmp/.buildx-cache
          mv /tmp/.buildx-cache-new /tmp/.buildx-cache
",60,1,2,"workflow_dispatch, push",5
freeCodeCamp/freeCodeCamp,e2e-playwright.yml,"name: CI - E2E - Playwright
on:
  workflow_dispatch:
  workflow_run:
    workflows: ['CI - Node.js']
    types:
      - completed
  # TODO: refactor with a workflow_call
  pull_request:
    branches:
      - 'main'
      - 'temp-**' # Temporary branches allowed on Upstream

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.event.workflow_run.head_branch || github.ref }}
  cancel-in-progress: ${{ !contains(github.ref, 'main') && !contains(github.ref, 'prod-') }}

jobs:
  build-client:
    name: Build Client
    runs-on: ubuntu-22.04
    strategy:
      matrix:
        node-version: [22]

    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
        with:
          submodules: 'recursive'

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4
        with:
          node-version: ${{ matrix.node-version }}

      - name: Install pnpm
        uses: pnpm/action-setup@a7487c7e89a18df4991f7f222e4898a00d66ddda # v4
        id: pnpm-install
        with:
          run_install: false

      - name: Checkout client-config
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
        with:
          repository: freeCodeCamp/client-config
          path: client-config

      - name: Set freeCodeCamp Environment Variables
        run: |
          cp sample.env .env

      - name: Install and Build
        run: |
          pnpm install
          pnpm run build

      - name: Move serve.json to Public Folder
        run: cp client-config/serve.json client/public/serve.json

      # We tar them for performance reasons - uploading a lot of files is slow.
      - name: Tar Files
        run: tar -cf client-artifact.tar client/public

      - name: Upload Client Artifact
        uses: actions/upload-artifact@5d5d22a31266ced268874388b861e4b58bb5c2f3 # v4.3.1
        with:
          name: client-artifact
          path: client-artifact.tar

      - name: Upload Webpack Stats
        uses: actions/upload-artifact@5d5d22a31266ced268874388b861e4b58bb5c2f3 # v4.3.1
        with:
          name: webpack-stats
          path: client/public/stats.json

  build-api:
    name: Build API (Container)
    runs-on: ubuntu-22.04

    steps:
      - name: Checkout Source Files
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
        with:
          submodules: 'recursive'

      - name: Create Image
        run: |
          docker build \
            -t fcc-api \
            -f docker/api/Dockerfile .

      - name: Save Image
        run: docker save fcc-api > api-artifact.tar

      - name: Upload API Artifact
        uses: actions/upload-artifact@5d5d22a31266ced268874388b861e4b58bb5c2f3 # v4.3.1
        with:
          name: api-artifact
          path: api-artifact.tar

  playwright-run:
    name: Run Playwright Tests
    runs-on: ubuntu-22.04
    needs: [build-client, build-api]
    strategy:
      fail-fast: false
      matrix:
        # Extend this to include firefox and webkit once chromium is working.
        browsers: [chromium]
        node-version: [22]

    steps:
      - name: Set Action Environment Variables
        run: |
          echo ""GITHUB_TOKEN=${{ secrets.GITHUB_TOKEN }}"" >> $GITHUB_ENV

      - name: Checkout Source Files
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4

      - name: Download All Artifacts
        uses: actions/download-artifact@c850b930e6ba138125429b7e5c93fc707a7f8427 # v4.1.4

      - name: Unpack Client Artifact
        run: |
          tar -xf client-artifact/client-artifact.tar
          rm client-artifact/client-artifact.tar

      - name: Load API Image
        run: |
          docker load < api-artifact/api-artifact.tar
          rm api-artifact/api-artifact.tar

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4
        with:
          node-version: ${{ matrix.node-version }}

      - name: Install pnpm
        uses: pnpm/action-setup@a7487c7e89a18df4991f7f222e4898a00d66ddda # v4
        id: pnpm-install
        with:
          run_install: false

      - name: Install Dependencies
        run: pnpm install

      - name: Set freeCodeCamp Environment Variables (needed by api)
        run: |
          cp sample.env .env

      - name: Install playwright dependencies
        run: npx playwright install --with-deps

      - name: Install and Build
        run: |
          pnpm install
          pnpm run create:shared
          pnpm run build:curriculum

      - name: Start apps
        run: |
          docker compose up -d
          pnpm run serve:client-ci &
          sleep 10

      - name: Seed Database with Certified User
        run: pnpm run seed:certified-user

      - name: Run playwright tests
        run: npx playwright test --project=${{ matrix.browsers }} --grep-invert 'third-party-donation.spec.ts'

      - uses: actions/upload-artifact@v4
        if: ${{ !cancelled() }}
        with:
          name: playwright-report-${{ matrix.browsers }}
          path: playwright/reporter
          retention-days: 7
",178,3,3,"workflow_dispatch, workflow_run, pull_request",13
freeCodeCamp/freeCodeCamp,e2e-third-party.yml,"name: CI - E2E - 3rd party donation tests

on:
  workflow_dispatch:
  push:
    branches:
      - 'prod-**'

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.event.workflow_run.head_branch || github.ref }}
  cancel-in-progress: ${{ !contains(github.ref, 'main') && !contains(github.ref, 'prod-') }}

jobs:
  build-client:
    name: Build Client
    runs-on: ubuntu-22.04
    strategy:
      matrix:
        node-version: [22]
    steps:
      - name: Checkout Source Files
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
        with:
          submodules: 'recursive'

      - name: Checkout client-config
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
        with:
          repository: freeCodeCamp/client-config
          path: client-config

      - name: Setup pnpm
        uses: pnpm/action-setup@a3252b78c470c02df07e9d59298aecedc3ccdd6d #v3.0.0

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@60edb5dd545a775178f52524783378180af0d1f8 # v4.0.2
        with:
          node-version: ${{ matrix.node-version }}

      - name: Set freeCodeCamp Environment Variables
        run: |
          sed '/STRIPE/d; /PAYPAL/d; /PATREON/d;' sample.env > .env
          echo 'STRIPE_PUBLIC_KEY=${{ secrets.STRIPE_PUBLIC_KEY }}' >> .env
          echo 'PAYPAL_CLIENT_ID=${{ secrets.PAYPAL_CLIENT_ID }}' >> .env
          echo 'PATREON_CLIENT_ID=${{ secrets.PATREON_CLIENT_ID }}' >> .env

      - name: Install and Build
        run: |
          pnpm install
          pnpm run build

      - name: Move serve.json to Public Folder
        run: cp client-config/serve.json client/public/serve.json

      - name: Tar Files
        run: tar -cf client-artifact.tar client/public

      - name: Upload Client Artifact
        uses: actions/upload-artifact@5d5d22a31266ced268874388b861e4b58bb5c2f3 # v4.3.1
        with:
          name: client-artifact
          path: client-artifact.tar

  build-api:
    name: Build API (Container)
    runs-on: ubuntu-22.04
    steps:
      - name: Checkout Source Files
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
        with:
          submodules: 'recursive'

      - name: Create Image
        run: |
          docker build \
            -t fcc-api \
            -f docker/api/Dockerfile .

      - name: Save Image
        run: docker save fcc-api > api-artifact.tar

      - name: Upload API Artifact
        uses: actions/upload-artifact@5d5d22a31266ced268874388b861e4b58bb5c2f3 # v4.3.1
        with:
          name: api-artifact
          path: api-artifact.tar

  playwright-run-api:
    name: Run Playwright 3rd Party Donation Tests
    runs-on: ubuntu-22.04
    needs: [build-client, build-api]
    strategy:
      fail-fast: false
      matrix:
        browsers: [chromium]
        node-version: [22]
    services:
      mongodb:
        image: mongo:8.0
        ports:
          - 27017:27017
      mailhog:
        image: mailhog/mailhog
        ports:
          - 1025:1025
    steps:
      - name: Set Action Environment Variables
        run: |
          echo ""GITHUB_TOKEN=${{ secrets.GITHUB_TOKEN }}"" >> $GITHUB_ENV

      - name: Checkout Source Files
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1

      - uses: actions/download-artifact@c850b930e6ba138125429b7e5c93fc707a7f8427 # v4.1.4

      - name: Unpack Client Artifact
        run: |
          tar -xf client-artifact/client-artifact.tar
          rm client-artifact/client-artifact.tar

      - name: Load API Image
        run: |
          docker load < api-artifact/api-artifact.tar
          rm api-artifact/api-artifact.tar

      - name: Setup pnpm
        uses: pnpm/action-setup@a3252b78c470c02df07e9d59298aecedc3ccdd6d #v3.0.0

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@60edb5dd545a775178f52524783378180af0d1f8 # v4.0.2
        with:
          node-version: ${{ matrix.node-version }}

      - name: Install Dependencies
        run: pnpm install

      - name: Set freeCodeCamp Environment Variables (needed by api)
        run: |
          sed '/STRIPE/d; /PAYPAL/d; /PATREON/d;' sample.env > .env
          echo 'STRIPE_PUBLIC_KEY=${{ secrets.STRIPE_PUBLIC_KEY }}' >> .env
          echo 'PAYPAL_CLIENT_ID=${{ secrets.PAYPAL_CLIENT_ID }}' >> .env
          echo 'PATREON_CLIENT_ID=${{ secrets.PATREON_CLIENT_ID }}' >> .env

      - name: Install playwright dependencies
        run: npx playwright install --with-deps

      - name: Install and Build
        run: |
          pnpm install
          pnpm run create:shared
          pnpm run build:curriculum

      - name: Start apps
        run: |
          docker compose up -d
          pnpm run serve:client-ci &
          sleep 10

      - name: Seed Database with Certified User
        run: pnpm run seed:certified-user

      - name: Run playwright tests
        run: npx playwright test third-party-donation.spec.ts --project=${{ matrix.browsers }}

      - uses: actions/upload-artifact@v4
        if: ${{ !cancelled() }}
        with:
          name: playwright-report-${{ matrix.browsers }}
          path: playwright/reporter
          retention-days: 7
",170,3,2,"workflow_dispatch, push",12
freeCodeCamp/freeCodeCamp,github-autoclose.yml,"name: GitHub - Autoclose Invalid PRs
on:
  pull_request_target:
    branches:
      - 'main'
    paths:
      - '.gitignore'

jobs:
  autoclose:
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/github-script@d7906e4ad0b1822421a7e6a35d5ca353c962f410 # v6
        with:
          github-token: ${{secrets.GITHUB_TOKEN}}
          script: |
            const files = await github.rest.pulls.listFiles({
              owner: context.payload.repository.owner.login,
              repo: context.payload.repository.name,
              pull_number: context.payload.pull_request.number,
            });
            if (
              files.data.length !== 1 ||
              (files.data[0].filename !== "".gitignore"" &&
              // We've had four PRs make this same (irrelevant) change already.
                !(files.data[0].filename === ""664ef4623946e65e18d59764.md"" &&
                  files.data[0].patch.includes(""return re.sub('(?<!\d)1', '', equation_string.strip('+'))"")
                )
              )
            ) {
              return;
            }
            const creator = context.payload.sender.login;
            const opts = github.rest.issues.listForRepo.endpoint.merge({
              ...context.issue,
              creator,
              state: 'all'
            });
            const issues = await github.paginate(opts);

            // iterate through issues
            for (const issue of issues) {
              // if the issue is this one, keep looking
              if (issue.number === context.issue.number) {
                continue;
              }

              // if the issue is actually a PR, they're not a first timer
              if (issue.pull_request) {
                return // Creator is already a contributor.
              }
            }
            core.setFailed(""Invalid PR detected."");
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: ""Thank you for opening this pull request.\n\nThis is a standard message notifying you that we've reviewed your pull request and have decided not to merge it. We would welcome future pull requests from you.\n\nThank you and happy coding."",
            });
            await github.rest.pulls.update({
              owner: context.payload.repository.owner.login,
              repo: context.payload.repository.name,
              pull_number: context.payload.pull_request.number,
              state: ""closed""
            });
            await github.rest.issues.addLabels({
              owner: context.payload.repository.owner.login,
              repo: context.payload.repository.name,
              issue_number: context.issue.number,
              labels: [""spam""]
            });
",71,1,1,pull_request_target,1
freeCodeCamp/freeCodeCamp,github-labeler.yaml,"name: GitHub - Label PRs
on:
  - pull_request_target

permissions:
  contents: read

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.event.workflow_run.head_branch || github.ref }}
  cancel-in-progress: true

jobs:
  triage:
    permissions:
      # for actions/labeler to determine modified files
      contents: read
      # for actions/labeler to add labels to PRs
      pull-requests: write
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/labeler@ac9175f8a1f3625fd0d4fb234536d26811351594 # v4
        with:
          repo-token: '${{ secrets.GITHUB_TOKEN }}'
          sync-labels: true
",24,1,1,pull_request_target,1
freeCodeCamp/freeCodeCamp,github-no-i18n-via-prs.yml,"name: GitHub - No translations via PRs
on:
  pull_request_target:
    branches:
      - 'main'
    paths:
      - 'client/i18n/locales/**/intro.json'
      - 'client/i18n/locales/**/translations.json'
      - '!client/i18n/locales/english/**'

jobs:
  has-translation:
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/github-script@d7906e4ad0b1822421a7e6a35d5ca353c962f410 # v6
        with:
          github-token: ${{secrets.CAMPERBOT_NO_TRANSLATE}}
          script: |
            const isDev = await github.rest.teams.getMembershipForUserInOrg({
              org: ""freeCodeCamp"",
              team_slug: ""dev-team"",
              username: context.payload.pull_request.user.login
            }).catch(() => ({status: 404}));
            if (context.payload.pull_request.user.login !== ""camperbot"" && isDev.status !== 200) {
              core.setFailed('This PR appears to touch translated curriculum files.')
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: ""Thanks for your pull request.\n\n**Please remove the changes made to the non-English versions of the files. No need to close this pull request; just add more commits as needed.**\n\nWe require you to change **only English** versions of files in the codebase. Translations to corresponding files in other world languages are managed on our translation platform. Once your pull request is merged, changes will be synced automatically to other world languages.\n\nPlease visit [our contributing guidelines](https://contribute.freecodecamp.org) to learn more about translating freeCodeCamp's resources.\n\nAs always, we value all of your contributions.\n\nHappy contributing!\n\n---\n_**Note:** This message was automatically generated by a bot. If you feel this message is in error or would like help resolving it, feel free to reach us [in our contributor chat](https://discord.gg/PRyKn3Vbay)._""
              })
            } else if (isDev.status === 200) {
              core.setFailed('This PR appears to touch translated curriculum files, but since you are on the dev team there is no message.');
            }
",34,1,1,pull_request_target,1
freeCodeCamp/freeCodeCamp,github-no-web-commits.yml,"name: GitHub - No Commits on GitHub Web
on:
  pull_request_target:
    types:
      - opened
      - reopened
      # The ""synchronize"" type may not be used because code review commits,
      # from GitHub UI might be acceptable. Enable this if you want to block
      # all commits from GitHub UI.
      #
      # - synchronize

jobs:
  has-web-commits:
    runs-on: ubuntu-22.04
    steps:
      - name: Check if PR author is allow-listed
        id: pr_author
        uses: actions/github-script@d7906e4ad0b1822421a7e6a35d5ca353c962f410 # v6
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const prAuthor = context.payload.pull_request.user.login;
            const response = await github.rest.teams
              .getMembershipForUserInOrg({
                org: context.repo.owner,
                team_slug: 'moderators',
                username: prAuthor
              })
              .catch(() => ({ status: 404 }));
            let isAllowListed = false;
            if (prAuthor === 'renovate[bot]' || response.status === 200) {
              isAllowListed = true;
            }
            core.setOutput('is_allow_listed', isAllowListed);

      - name: Check if commits are made on GitHub Web UI
        id: check-commits
        if: steps.pr_author.outputs.is_allow_listed == 'false'
        run: |
          PR_NUMBER=$(jq --raw-output .pull_request.number ""$GITHUB_EVENT_PATH"")
          COMMITS_URL=""https://api.github.com/repos/$GITHUB_REPOSITORY/pulls/$PR_NUMBER/commits""
          IS_GITHUB_COMMIT=$(curl --header ""Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}"" ""$COMMITS_URL"" | jq '[.[] | .commit.committer.name] | any(.[]; . == ""GitHub"")')
          if [ ""$IS_GITHUB_COMMIT"" = ""true"" ]; then
            echo ""IS_GITHUB_COMMIT=true"" >> $GITHUB_ENV
          fi

      - name: Add comment on PR if commits are made on GitHub Web UI
        uses: actions/github-script@d7906e4ad0b1822421a7e6a35d5ca353c962f410 # v6
        if: steps.pr_author.outputs.is_allow_listed == 'false' && env.IS_GITHUB_COMMIT == 'true'
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            core.setFailed(""Commits were added via the GitHub Web UI."");
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: ""Thanks for your pull request.\n\n**Please do not add commits via the GitHub Web UI.**\n\nIt generally means you have yet to test these changes in a development setup or complete any prerequisites. We need you to follow the guides mentioned in the checklist. Please revalidate these changes in a developer environment and confirm how you validated your changes.\n\nHappy contributing!\n\n---\n_**Note:** This message was automatically generated by a bot. If you feel this message is in error or would like help resolving it, feel free to reach us [in our contributor chat](https://discord.gg/PRyKn3Vbay)._""
            });
",60,1,1,pull_request_target,2
freeCodeCamp/freeCodeCamp,github-spam.yml,"name: GitHub - Spam PR
on:
  pull_request_target:
    types:
      - labeled

jobs:
  is-spam:
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/github-script@d7906e4ad0b1822421a7e6a35d5ca353c962f410 # v6
        with:
          github-token: ${{secrets.CAMPERBOT_NO_TRANSLATE}}
          script: |
            const isSpam = context.payload.pull_request.labels.find(label => label.name === ""spam"");
            if (isSpam) {
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: ""We are marking this pull request as spam. Please note that if you are participating in Hacktoberfest, two or more PRs marked as spam will result in your permanent disqualification.\n\nIf you are interested in making quality and genuine contributions to our projects, check out our [contributing guidelines](https://contribute.freecodecamp.org).""
              })
            }
",23,1,1,pull_request_target,1
freeCodeCamp/freeCodeCamp,i18n-validate-builds.yml,"name: i18n - Build Validation
on:
  push:
    branches:
      - main

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.event.workflow_run.head_branch || github.ref }}
  cancel-in-progress: true

jobs:
  ci:
    name: Validate i18n Builds
    runs-on: ubuntu-22.04
    strategy:
      matrix:
        node-version: [22]

    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
        with:
          submodules: 'recursive'

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4
        with:
          node-version: ${{ matrix.node-version }}

      - name: Install pnpm
        uses: pnpm/action-setup@a7487c7e89a18df4991f7f222e4898a00d66ddda # v4
        id: pnpm-install
        with:
          run_install: false

      - name: Set freeCodeCamp Environment Variables
        run: |
          cp sample.env .env

      - name: Install Dependencies
        run: pnpm install

      - name: Validate Challenge Files
        run: pnpm run audit-challenges
",44,1,1,push,3
freeCodeCamp/freeCodeCamp,i18n-validate-prs.yml,"name: i18n - Curriculum PR Validation
on:
  pull_request:
    branches:
      - main

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.event.workflow_run.head_branch || github.ref }}
  cancel-in-progress: true

jobs:
  ci:
    name: Validate i18n Builds
    # run only on PRs that camperbot opens with title that matches the curriculum sync
    if: ${{ github.event.pull_request.user.login == 'camperbot' && contains(github.event.pull_request.title, 'chore(i18n,learn)') }}
    runs-on: ubuntu-22.04
    strategy:
      matrix:
        node-version: [22]

    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
        with:
          submodules: 'recursive'

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4
        with:
          node-version: ${{ matrix.node-version }}

      - name: Install pnpm
        uses: pnpm/action-setup@a7487c7e89a18df4991f7f222e4898a00d66ddda # v4
        id: pnpm-install
        with:
          run_install: false

      - name: Set freeCodeCamp Environment Variables
        run: |
          cp sample.env .env

      - name: Install Dependencies
        run: pnpm install

      - name: Validate Challenge Files
        id: validate
        run: pnpm run audit-challenges

      - name: Create Comment
        # Run if the validate challenge files step fails, specifically. Note that we need the failure() call for this step to trigger if the action fails.
        if: ${{ failure() && steps.validate.conclusion == 'failure' }}
        uses: actions/github-script@d7906e4ad0b1822421a7e6a35d5ca353c962f410 # v6
        with:
          github-token: ${{secrets.CAMPERBOT_NO_TRANSLATE}}
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: ""Hey @freecodecamp/i18n, it looks like we have new English curriculum files that need to be translated.""
            })
",61,1,1,pull_request,4
freeCodeCamp/freeCodeCamp,node.js-tests.yml,"name: CI - Node.js

on:
  push:
    branches:
      - 'main'
      - 'prod-**'
  pull_request:
    branches:
      - 'main'
      - 'temp-**' # Temporary branches allowed on Upstream

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.event.workflow_run.head_branch || github.ref }}
  cancel-in-progress: ${{ !contains(github.ref, 'main') && !contains(github.ref, 'prod-') }}

permissions:
  contents: read

jobs:
  lint:
    name: Lint
    runs-on: ubuntu-22.04
    strategy:
      matrix:
        node-version: [22]
      fail-fast: false

    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
        with:
          submodules: 'recursive'

      - name: Check number of lockfiles
        run: |
          if [ $(find . -name 'package-lock.json' | grep -vc -e 'node_modules') -gt 0 ]
          then
            echo -e 'Error: found package-lock files in the repository.\nWe use pnpm workspaces to manage packages so all dependencies should be added via pnpm add'
            exit 1
          fi

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4
        with:
          node-version: ${{ matrix.node-version }}

      - name: Install pnpm
        uses: pnpm/action-setup@a7487c7e89a18df4991f7f222e4898a00d66ddda # v4
        id: pnpm-install
        with:
          run_install: false

      - name: Set Environment variables
        run: |
          cp sample.env .env
          cat .env

      - name: Install node_modules
        run: pnpm install

      - name: Check formatting
        run: |
          pnpm prettier --check . || [ $? -eq 1 ] && printf ""\nTip: Run 'pnpm run format' in your terminal to fix this.\n\n""

      # The two prefixed installs are for the client update which are not,
      # currently, built as workspaces.
      - name: Lint Source Files
        run: |
          echo pnpm version $(pnpm -v)
          pnpm run create:shared
          pnpm run build:curriculum
          pnpm run lint

  # DONT REMOVE THIS JOB.
  # TODO: Refactor and use re-usable workflow and shared artifacts
  build:
    name: Build
    needs: lint
    runs-on: ubuntu-22.04
    strategy:
      matrix:
        node-version: [22]

    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
        with:
          submodules: 'recursive'

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4
        with:
          node-version: ${{ matrix.node-version }}

      - name: Install pnpm
        uses: pnpm/action-setup@a7487c7e89a18df4991f7f222e4898a00d66ddda # v4
        id: pnpm-install
        with:
          run_install: false

      - name: Set freeCodeCamp Environment Variables
        run: |
          cp sample.env .env

      - name: Install and Build
        run: |
          pnpm install
          pnpm run build

  test:
    name: Test
    needs: build
    runs-on: ubuntu-22.04

    strategy:
      fail-fast: false
      matrix:
        node-version: [22]

    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
        with:
          submodules: 'recursive'

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4
        with:
          node-version: ${{ matrix.node-version }}

      - name: Install pnpm
        uses: pnpm/action-setup@a7487c7e89a18df4991f7f222e4898a00d66ddda # v4
        id: pnpm-install
        with:
          run_install: false

      - name: Set Environment variables
        run: |
          cp sample.env .env
          cat .env

      - name: Start MongoDB
        uses: supercharge/mongodb-github-action@b0a1493307c4e9b82ed61f3858d606c5ff190c64 # v1.10.0
        with:
          mongodb-version: 8.0
          mongodb-replica-set: test-rs
          mongodb-port: 27017

      - name: Install Dependencies
        run: |
          echo pnpm version $(pnpm -v)
          pnpm install

      - name: Install Chrome for Puppeteer
        run: pnpm -F=curriculum install-puppeteer

      - name: Run Tests
        run: pnpm test

  test-upcoming:
    name: Test - Upcoming Changes
    needs: build
    runs-on: ubuntu-22.04

    strategy:
      fail-fast: false
      matrix:
        node-version: [22]

    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
        with:
          submodules: 'recursive'

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4
        with:
          node-version: ${{ matrix.node-version }}

      - name: Install pnpm
        uses: pnpm/action-setup@a7487c7e89a18df4991f7f222e4898a00d66ddda # v4
        id: pnpm-install
        with:
          run_install: false

      - name: Set Environment variables
        run: |
          cp sample.env .env
          echo 'SHOW_UPCOMING_CHANGES=true' >> .env
          cat .env

      - name: Start MongoDB
        uses: supercharge/mongodb-github-action@b0a1493307c4e9b82ed61f3858d606c5ff190c64 # v1.10.0
        with:
          mongodb-version: 8.0
          mongodb-replica-set: test-rs
          mongodb-port: 27017

      - name: Install Dependencies
        run: |
          echo pnpm version $(pnpm -v)
          pnpm install

      - name: Install Chrome for Puppeteer
        run: pnpm -F=curriculum install-puppeteer

      - name: Run Tests
        run: pnpm test

  test-localization:
    name: Test - i18n
    needs: build
    runs-on: ubuntu-22.04

    strategy:
      fail-fast: false
      matrix:
        node-version: [22]
        locale: [portuguese, italian]

    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
        with:
          submodules: 'recursive'

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4
        with:
          node-version: ${{ matrix.node-version }}

      - name: Install pnpm
        uses: pnpm/action-setup@a7487c7e89a18df4991f7f222e4898a00d66ddda # v4
        id: pnpm-install
        with:
          run_install: false

      - name: Set Environment variables
        run: |
          cp sample.env .env
          cat .env

      - name: Start MongoDB
        uses: supercharge/mongodb-github-action@b0a1493307c4e9b82ed61f3858d606c5ff190c64 # v1.10.0
        with:
          mongodb-version: 8.0
          mongodb-replica-set: test-rs
          mongodb-port: 27017

      - name: Install Dependencies
        env:
          CURRICULUM_LOCALE: ${{ matrix.locale }}
          CLIENT_LOCALE: ${{ matrix.locale }}
        run: |
          echo pnpm version $(pnpm -v)
          pnpm install

      # DONT REMOVE THIS STEP.
      # TODO: Refactor and use re-usable workflow and shared artifacts
      - name: Build Client in ${{ matrix.locale }}
        env:
          CURRICULUM_LOCALE: ${{ matrix.locale }}
          CLIENT_LOCALE: ${{ matrix.locale }}
        run: |
          pnpm run build

      - name: Install Chrome for Puppeteer
        run: pnpm -F=curriculum install-puppeteer

      - name: Run Tests
        env:
          CURRICULUM_LOCALE: ${{ matrix.locale }}
          CLIENT_LOCALE: ${{ matrix.locale }}
        run: pnpm test
",276,5,2,"push, pull_request",18
sindresorhus/awesome,main.yml,"on:
  pull_request:
    branches: [main]
    paths:
      - 'readme.md'
jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: awesome-lint
        run: ./.github/workflows/repo_linter.sh
",14,1,1,pull_request,1
EbookFoundation/free-programming-books,check-urls.yml,"name: Check URLs from changed files

on:
  push:
  pull_request:

permissions:
  # needed for checkout code
  contents: read

# This allows a subsequently queued workflow run to interrupt/wait for previous runs
concurrency:
  group: '${{ github.workflow }} @ ${{ github.run_id }}'
  cancel-in-progress: false         # true = interrupt, false = wait

jobs:

# NOTE: tj-actions/changed-files.
# For push events you need to include fetch-depth: 0 | 2 depending on your use case.
#  0: retrieve all history for all branches and tags
#  1: retrieve only current commit (by default)
#  2: retrieve until the preceding commit
  get-changed-files:
    name: Get changed files
    runs-on: ubuntu-latest
    outputs:
      fetch-depth: ${{ steps.set-params.outputs.fetch-depth }}
      files:       ${{ steps.set-files.outputs.files }}
      files-len:   ${{ steps.set-files.outputs.files-len }}
      matrix:      ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Determine workflow params
        id: set-params
        run: |
          echo ""fetch_depth=0"" >> $GITHUB_OUTPUT
          if [ ""${{ github.event_name }}"" == ""pull_request"" ]; then
            echo ""fetch_depth=0"" >> $GITHUB_OUTPUT
          fi
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: ${{ steps.set-params.outputs.fetch-depth }}
      - name: Get changed files
        id: changed-files
        uses: tj-actions/changed-files@v46
        with:
          separator: "" ""
          json: true
      - id: set-files
        run: |
          echo ""${{ steps.changed-files.outputs.all_changed_files }}""  \
            | jq --raw-output '. | join("" "")'  \
            | sed -e 's/^/files=/'  \
            >> $GITHUB_OUTPUT
          echo ""${{ steps.changed-files.outputs.all_changed_files }}""  \
            | jq --raw-output '. | length'  \
            | sed -e 's/^/files-len=/'  \
            >> $GITHUB_OUTPUT
      - id: set-matrix
        run: |
          echo ""{\""file\"":${{ steps.changed-files.outputs.all_changed_files }}}""  \
            | sed -e 's/^/matrix=/'  \
            >> $GITHUB_OUTPUT


  check-urls:
    name: Check @ ${{ matrix.file }}
    if: ${{ fromJSON(needs.get-changed-files.outputs.files-len) > 0 }}
    needs: [get-changed-files]
    runs-on: ubuntu-latest
    strategy:
      matrix: ${{ fromJSON(needs.get-changed-files.outputs.matrix) }}
      max-parallel: 10
      fail-fast: false
    steps:
      - name: Checkout
        if: ${{ endsWith(matrix.file, '.yml') || endsWith(matrix.file, '.md') }}
        uses: actions/checkout@v4
        with:
          fetch-depth: ${{ needs.get-changed-files.outputs.fetch-depth }}
      - name: Setup Ruby v2.6
        if: ${{ endsWith(matrix.file, '.yml') || endsWith(matrix.file, '.md') }}
        uses: ruby/setup-ruby@v1
        with:
          ruby-version: 2.6
      - name: Install awesome_bot
        if: ${{ endsWith(matrix.file, '.yml') || endsWith(matrix.file, '.md') }}
        run: |
          gem install awesome_bot
      - name: Set output
        id: set-output
        # FILENAME takes the complete file path and strips everything before the final '/'
        # FILEPATH replaces all '/' with '-' in the file path since '/' is not allowed in upload artifact name
        # Due to a bug in actions/download-artifact, we need to rename README.md to BASE_README.md
        run: |
          echo ""FILENAME=$(echo ${{ matrix.file }} | grep -oE '[a-zA-Z0-9_-]+(\.yml|\.md)')"" >> ""$GITHUB_OUTPUT""

          file_path=""${{ matrix.file }}""
          file_path=""${file_path//\//-}""

          if [[ ""$file_path"" == ""README.md"" ]]; then
            file_path=""BASE_README.md""
          fi

          echo ""FILEPATH=${file_path}"" >> ""$GITHUB_OUTPUT""
      - name: ""Check URLs of file: ${{ matrix.file }}""
        if: ${{ endsWith(matrix.file, '.yml') || endsWith(matrix.file, '.md') }}
        run: |
          awesome_bot ""${{ matrix.file }}"" --allow-redirect --allow-dupe --allow-ssl || true;
      - uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.set-output.outputs.FILEPATH }}
          path: ${{ github.workspace }}/ab-results-*.json


  reporter:
    name: GitHub report
    needs: [get-changed-files, check-urls]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout  # for having the sources of the local action
        uses: actions/checkout@v4
        # download and unzip the ab-results-*.json generated by job-matrix: check-urls
      - name: Download artifacts
        uses: actions/download-artifact@v4
      - name: Generate Summary Report
        uses: ./.github/actions/awesomebot-gh-summary-action
        with:
          ab-root: ${{ github.workspace }}
          files: ${{ needs.get-changed-files.outputs.files }}
          separator: "" ""
          append-heading: ${{ true }}
",132,3,2,"push, pull_request",8
EbookFoundation/free-programming-books,comment-pr.yml,"name: Comment on the pull request

on:
  workflow_run:
    workflows: [""free-programming-books-lint""]
    types:
      - completed

jobs:
  upload:
    permissions:
      pull-requests: write
    runs-on: ubuntu-latest
    if: >
      ${{ github.event.workflow_run.event == 'pull_request' &&
      github.event.workflow_run.conclusion == 'success' }}
    steps:
      - name: 'Download artifact'
        uses: actions/github-script@v7
        with:
          script: |
            let allArtifacts = await github.rest.actions.listWorkflowRunArtifacts({
               owner: context.repo.owner,
               repo: context.repo.repo,
               run_id: context.payload.workflow_run.id,
            });
            let matchArtifact = allArtifacts.data.artifacts.filter((artifact) => {
              return artifact.name == ""pr""
            })[0];
            let download = await github.rest.actions.downloadArtifact({
               owner: context.repo.owner,
               repo: context.repo.repo,
               artifact_id: matchArtifact.id,
               archive_format: 'zip',
            });
            let fs = require('fs');
            fs.writeFileSync(`${process.env.GITHUB_WORKSPACE}/pr.zip`, Buffer.from(download.data));

      - name: 'Unzip artifact'
        run: unzip pr.zip

      - name: 'Comment on PR'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          if [ -s error.log ]
          then
            gh pr comment $(<PRurl) -b ""Linter failed, fix the error(s):
            \`\`\`
            $(cat error.log)
            \`\`\`""
            gh pr edit $(<PRurl) --add-label ""linter error""
          else
            gh pr edit $(<PRurl) --remove-label ""linter error""
          fi
",55,1,1,workflow_run,1
EbookFoundation/free-programming-books,detect-conflicting-prs.yml,"name: ""Detect conflicting PRs""

on:
  workflow_dispatch:              # manually
  # So that PRs touching the same files as the push are updated
  push:
  # So that the `dirtyLabel` is removed if conflicts are resolved
  pull_request_target:            # - A pull request (even with conflicts)
    types:
      - synchronize               #   pushing more commits

permissions:
  # no checkouts/branching needed
  contents: none
  # need by ""eps1lon/actions-label-merge-conflict"" to manage PR label/comments
  pull-requests: write

# This allows a subsequently queued workflow run to interrupt/wait for previous runs
concurrency:
  group: '${{ github.workflow }}'
  cancel-in-progress: false  # true: interrupt, false = wait for

jobs:
  detect-prs:
    name: Detect
    if: ${{ github.actor != 'dependabot[bot]' }} # avoid dependabot PRs
    runs-on: ubuntu-latest
    steps:

      - name: Label conflicting PRs that are open
        id: pr-labeler
        uses: eps1lon/actions-label-merge-conflict@v3.0.3
        with:
          repoToken:  ${{ secrets.GITHUB_TOKEN }}
          retryAfter: 30 # seconds
          retryMax:   5  # atemps
          dirtyLabel: conflicts
          commentOnDirty: |
            Oh no 😟! Conflicts have been found.

            Please 🙏, take a moment and [address the merge conflicts](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/addressing-merge-conflicts) of your pull request before we can evaluate it again.

            Thanks in advance for your effort and patience ❤️!
          continueOnMissingPermissions: true

      - name: Print outputs
        run: echo ${{ join(steps.pr-labeler.outputs.*, ',') }}

      - name: Set PRs outputs
        id: set-prs
        run: |
          echo ""$INPUT_PRS""  \
            | jq --compact-output --raw-output 'to_entries | map({number: .key, dirty: .value})'  \
            | sed -e 's/^/prs=/'  \
            >> $GITHUB_OUTPUT
          echo ""$INPUT_PRS""  \
            | jq --raw-output 'to_entries | length'  \
            | sed -e 's/^/prs-len=/'  \
            >> $GITHUB_OUTPUT
        env:
          INPUT_PRS: ${{ steps.pr-labeler.outputs.prDirtyStatuses }}

      - name: Write job summary
        run: |
          echo ""### Pull Request statuses""  \
            >> $GITHUB_STEP_SUMMARY
          # render json array to a Markdown table with an optional ""No records"" message if empty
          echo ""$INPUT_PRS""  \
            | jq --raw-output 'map(""| [#\(.number)](\(env.GITHUB_PUBLIC_URL)/\(.number)) | \(if (.dirty) then ""❌"" else ""✔️"" end) |"") | join(""\n"") | if (. == """") then ""\nNo records.\n"" else ""\n| PR | Mergeable? |\n|---:|:----------:|\n\(.)\n"" end'  \
            >> $GITHUB_STEP_SUMMARY
        env:
          GITHUB_PUBLIC_URL:   ${{ format('{0}/{1}/pull', github.server_url, github.repository) }}
          INPUT_PRS: ${{ steps.set-prs.outputs.prs }}
",73,1,3,"workflow_dispatch, push, pull_request_target",1
EbookFoundation/free-programming-books,fpb-lint.yml,"name: free-programming-books-lint

on: [pull_request]

permissions:
  contents: read

jobs:
  build:

    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4
    - name: Use Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '16.x'
    - run: npm install -g free-programming-books-lint

    - name: Pull Request
      run: |
        fpb-lint books casts courses more &> output.log

    - name: Clean output and create artifacts
      if: always()
      run: |
          mkdir -p ./pr
          echo ${{ github.event.pull_request.html_url }} > ./pr/PRurl
          cat output.log | sed -E 's:/home/runner/work/free-programming-books/|⚠.+::' | uniq > ./pr/error.log

    - uses: actions/upload-artifact@v4
      if: always()
      with:
        name: pr
        path: pr/
",36,1,1,pull_request,3
EbookFoundation/free-programming-books,issues-pinner.yml,"#
# This workflow adds a label to the issue involved on event when is pinned
# and removes it when unpinned.
#
# It also is enhanced with `stale.yml` workflow: pinned issues never stales
# because that label is declared as part of it `exempt-issue-labels`
# input parameter.
#
name: Issues pinner management

on:
  issues:
    types:
      - ""pinned""
      - ""unpinned""

permissions:
  # no checkouts/branching needed
  contents: none
  # needed by ""action-add-labels / action-remove-labels"" to CRUD labels
  issues: write

# This allows a subsequently queued workflow run to interrupt/wait for previous runs
concurrency:
  group: '${{ github.workflow }} @ ${{ github.event.issue.number || github.run_id }}'
  cancel-in-progress: false  # true: interrupt, false = wait for

jobs:

  labeler:
    name: Pushpin labeler
    runs-on: ubuntu-latest
    steps:

      - name: Add pushpin label on pinning an issue
        id: if-pinned
        if: github.event.action == 'pinned'
        uses: actions-ecosystem/action-add-labels@v1
        with:
          repo:   ${{ github.repository }}
          number: ${{ github.event.issue.number }}
          labels: |
            :pushpin: pinned

      - name: Remove pushpin label on unpinning an issue
        id: if-unpinned
        if: github.event.action == 'unpinned'
        uses: actions-ecosystem/action-remove-labels@v1
        with:
          repo:   ${{ github.repository }}
          number: ${{ github.event.issue.number }}
          labels: |
            :pushpin: pinned

      - name: GitHub reporter
        # run even previous steps fails
        if: always()
        run: |
          echo ""$INPUT_SUMMARY""  >> $GITHUB_STEP_SUMMARY;
        env:
          INPUT_SUMMARY: ${{ format('Issue [\#{2}]({0}/{1}/issues/{2}) should be `{3}`.',
                                    github.server_url, github.repository,
                                    github.event.issue.number,
                                    github.event.action) }}
",64,1,1,issues,2
EbookFoundation/free-programming-books,rtl-ltr-linter.yml,"name: RTL/LTR Markdown Linter

on: [pull_request]

permissions:
  contents: read # Required to checkout the repository content

jobs:
  lint:
    runs-on: ubuntu-latest

    steps:
    # Checkout the repository code
    - name: Checkout code
      uses: actions/checkout@v4

    # Fetch the full history of 'main' for accurate git diff in PRs
    - name: Fetch all history for main
      run: git fetch --no-tags --prune --depth=50 origin main

    # Set up the required Python version for the linter
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11' # Use a recent Python version for compatibility

    # Install only the Python dependencies needed for the linter script
    - name: Install Python dependencies
      run: |
        pip install python-bidi PyYAML

    # (Optional) List files for debugging purposes
    - name: List files in scripts directory and current path
      run: |
        echo ""Current working directory:""
        pwd
        echo ""Listing contents of scripts directory (if it exists at root):""
        ls -la scripts/ || echo ""scripts/ directory not found at root or ls failed""

    # Identify all changed Markdown files in the PR using tj-actions/changed-files
    - name: Get changed Markdown files
      id: changed_md_files
      uses: tj-actions/changed-files@v46
      with:
        files: |
          **/*.md

    # Check if the PR has the ""RTL"" label
    - name: Check for RTL label
      id: rtl_label
      run: |
        gh pr view ${{ github.event.pull_request.number }} --json labels --jq '.labels[].name' | grep -q '^RTL$' && echo ""has_labels=true"" >> $GITHUB_OUTPUT || echo ""has_labels=false"" >> $GITHUB_OUTPUT
      env:
        GH_TOKEN: ${{ github.token }}
      
    # Check if any changed file is in ar, he, fa, ur
    - name: Check for RTL language file changes
      id: rtl_lang_files
      run: |
        RTL_CHANGED=false
        for f in ${{ steps.changed_md_files.outputs.all_changed_files }}; do
          if [[ ""$f"" =~ (ar|he|fa|ur) ]]; then
            RTL_CHANGED=true
            break
          fi
        done
        echo ""rtl_changed=$RTL_CHANGED"" >> $GITHUB_OUTPUT

    # Run the RTL/LTR Markdown linter:
    # - Scans all Markdown files for issues and writes a full log
    # - Prints GitHub Actions annotations only for issues on changed lines in changed files
    # - Fails the job if any error or warning is found on changed lines
    - name: Run RTL/LTR Markdown linter
      id: run_linter
      if: steps.rtl_label.outputs.has_labels == 'true' || steps.rtl_lang_files.outputs.rtl_changed == 'true'
      continue-on-error: true
      run: |
        echo ""Scanning all specified paths for full log...""
        echo ""Changed Markdown files for PR annotations: ${{ steps.changed_md_files.outputs.all_changed_files }}""
        
        CHANGED_FILES_ARGS=""""
        if [ ""${{ steps.changed_md_files.outputs.all_changed_files_count }}"" -gt 0 ]; then
          # Pass changed files to the script for PR annotation generation
          CHANGED_FILES_ARGS=""--changed-files ${{ steps.changed_md_files.outputs.all_changed_files }}""
        fi
        
        # Execute the linter.
        # Annotations for changed files will be printed to stdout by the script.
        # The script will also write a full log to 'rtl-linter-output.log'.
        # If the script exits with a non-zero code (error found), this step will fail.
        python3 scripts/rtl_ltr_linter.py books casts courses more ${CHANGED_FILES_ARGS} --log-file rtl-linter-output.log
    
    # Upload the linter output log as a workflow artifact
    # Only if the linter step was executed (success or failure)
    - name: Upload linter output artifact
      if: steps.run_linter.conclusion == 'success' || steps.run_linter.conclusion == 'failure'
      uses: actions/upload-artifact@v4
      with:
        name: rtl-linter-output # Name of the artifact
        path: rtl-linter-output.log # Path to the output file
        if-no-files-found: ignore # Ignore if no files are found",101,1,1,pull_request,4
EbookFoundation/free-programming-books,stale.yml,"name: 'Stale handler'
on:
  schedule:
    - cron: '0 0 * * *' # Run every day at midnight
  workflow_dispatch:
    inputs:
      debug-only:
        type: boolean
        description: ""Does a dry-run when enabled. No PR's will be altered""
        required: true
        default: true

permissions:
  pull-requests: write
  actions: write

jobs:
  stale:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/stale@v9
        with:
          days-before-issue-stale: -1 # Don't mark issues as stale
          days-before-issue-close: -1 # Don't close issues
          stale-pr-message: |
            'This Pull Request has been automatically marked as stale because it has not had recent activity during last 60 days :sleeping:

            It will be closed in 30 days if no further activity occurs. To unstale this PR, draft it, remove stale label, comment with a detailed explanation or push more commits.

            There can be many reasons why some specific PR has no activity. The most probable cause is lack of time, not lack of interest.

            Thank you for your patience :heart:'
          close-pr-message: |
            This Pull Request has been automatically closed because it has been inactive during the last 30 days since being marked as stale.

            As author or maintainer, it can always be reopened if you see that carry on been useful.

            Anyway, thank you for your interest in contribute :heart:
          days-before-pr-stale: 60
          days-before-pr-close: 30
          stale-pr-label: 'stale'
          exempt-pr-labels: 'keep' # Don't mark PR's with this label as stale
          labels-to-remove-when-unstale: 'stale'
          exempt-draft-pr: true
          debug-only: ${{ github.event.inputs.debug-only == 'true' }}
          enable-statistics: true
",46,1,2,"schedule, workflow_dispatch",1
public-apis/public-apis,test_of_push_and_pull.yml,"name: ""Tests of push & pull""

on:
  push:
    branches: [ master ]
  pull_request:
    branches: [ master ]

env:
  FILENAME: README.md

jobs:
  tests:
    name: 'Validate README.md changes'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.8'

      - name: Install dependencies
        run: python -m pip install -r scripts/requirements.txt

      - name: Validate Markdown format
        run: python scripts/validate/format.py ${FILENAME}

      - name: Validate pull request changes
        run: scripts/github_pull_request.sh ${{ github.repository }} ${{ github.event.pull_request.number }} ${FILENAME}
        if: github.event_name == 'pull_request'

      - name: Checking if push changes are duplicated
        run: python scripts/validate/links.py ${FILENAME} --only_duplicate_links_checker
        if: github.event_name == 'push'
",37,1,2,"push, pull_request",2
public-apis/public-apis,test_of_validate_package.yml,"name: ""Tests of validate package""

on:
  push:
    branches: [ master ]
  pull_request:
    branches: [ master ]

jobs:
  unittest:
    name: 'Run tests of validate package'
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v2

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.8'
    
    - name: Install dependencies
      run: python -m pip install -r scripts/requirements.txt

    - name: Run Unittest
      run: |
        cd scripts
        python -m unittest discover tests/ --verbose
",29,1,2,"push, pull_request",2
public-apis/public-apis,validate_links.yml,"name: ""Validate links""

on:
  workflow_dispatch:
  schedule:
    - cron:  '0 0 * * *'

env:
  FILENAME: README.md

jobs:
  validate_links:
    name: 'Check all links are working'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.8'

      - name: Install dependencies
        run: python -m pip install -r scripts/requirements.txt

      - name: Validate all links from README.md
        run: python scripts/validate/links.py ${FILENAME}
",28,1,2,"workflow_dispatch, schedule",2
jwasham/coding-interview-university,links_checker.yml,"name: Links Checker

on:
  ## Allow triggering this workflow manually via GitHub CLI/web
  workflow_dispatch:

  ## Run this workflow automatically every month
  schedule:
    - cron: '0 0 1 * *'

jobs:
  link_checker:
    name: Check links and create automated issue if needed
    runs-on: ubuntu-latest
    env:
      REPORT_FILE: links-report
    steps:
      ## Check out code using Git
      - uses: actions/checkout@v3

      - name: Check all links at README.md but skips translations files
        id: lychee
        uses: lycheeverse/lychee-action@v1.4.1
        with:
          output: ${{ env.REPORT_FILE }}
          format: markdown
          ## Do not fail this step on broken links
          fail: false
          ## Allow pages replying with 200 (Ok), 204 (No Content),
          ## 206 (Partial Content) in at most 20 seconds with HTML content.
          ## max-concurrency was 20, getting network errors
          args: >-
            --verbose
            --accept 200,204,206
            --headers ""accept=text/html""
            --timeout 20
            --max-concurrency 3 
            --no-progress
            README.md
        env:
          ## Avoid rate limiting when checking github.com links
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Lychee's exit code
        ## https://github.com/lycheeverse/lychee#exit-codes
        run: echo Lychee exit with ${{ steps.lychee.outputs.exit_code }}

      - name: Find the last report issue open
        uses: micalevisk/last-issue-action@v1.2
        id: last_issue
        with:
          state: open
          labels: |
            report
            automated issue
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Create issue from report file
        if: ${{ steps.last_issue.outputs.has_found == 'false' }}
        uses: peter-evans/create-issue-from-file@v4
        with:
          title: Link checker report
          content-filepath: ${{ env.REPORT_FILE }}
          issue-number: ${{ steps.last_issue.outputs.issue_number }}
          labels: |
            report
            automated issue

      - name: Update last report open issue created
        if: ${{ steps.last_issue.outputs.has_found == 'true' }}
        uses: peter-evans/create-issue-from-file@v4
        with:
          title: Link checker report
          content-filepath: ${{ env.REPORT_FILE }}
          issue-number: ${{ steps.last_issue.outputs.issue_number }}
          labels: |
            report
            automated issue

      - name: Close last report open issue
        if: ${{ steps.lychee.outputs.exit_code == 0 }}
        uses: peter-evans/close-issue@v2
        with:
          issue-number: ${{ steps.last_issue.outputs.issue_number }}
",85,1,2,"workflow_dispatch, schedule",6
vuejs/vue,ci.yml,"name: 'ci'
on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
jobs:
  unit-test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Install pnpm
        uses: pnpm/action-setup@v2

      - name: Set node version to 18
        uses: actions/setup-node@v2
        with:
          node-version: 18
          cache: 'pnpm'

      - run: pnpm install

      - name: Run unit tests
        run: pnpm run test:unit

  ssr-sfc-test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Install pnpm
        uses: pnpm/action-setup@v2

      - name: Set node version to 18
        uses: actions/setup-node@v2
        with:
          node-version: 18
          cache: 'pnpm'

      - run: pnpm install

      - name: Run SSR tests
        run: pnpm run test:ssr

      - name: Run compiler-sfc tests
        run: pnpm run test:sfc

  e2e-test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Install pnpm
        uses: pnpm/action-setup@v2

      - name: Set node version to 18
        uses: actions/setup-node@v2
        with:
          node-version: 18
          cache: 'pnpm'

      - run: pnpm install

      - name: Run e2e tests
        run: pnpm run test:e2e

      - name: Run transition tests
        run: pnpm run test:transition

  type-test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Install pnpm
        uses: pnpm/action-setup@v2

      - name: Set node version to 18
        uses: actions/setup-node@v2
        with:
          node-version: 18
          cache: 'pnpm'

      - run: pnpm install

      - name: Run srouce type check
        run: pnpm run ts-check

      - name: Run type declaration tests
        run: pnpm run test:types
",93,4,2,"push, pull_request",12
vuejs/vue,release-tag.yml,"on:
  push:
    tags:
      - 'v*' # Push events to matching v*, i.e. v1.0, v20.15.10

name: Create Release

jobs:
  build:
    name: Create Release
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@master
      - name: Create Release for Tag
        id: release_tag
        uses: yyx990803/release-tag@master
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tag_name: ${{ github.ref }}
          body: |
            Please refer to [CHANGELOG.md](https://github.com/vuejs/vue/blob/main/CHANGELOG.md) for details.
",23,1,1,push,2
TheAlgorithms/Python,build.yml,"name: ""build""

on:
  pull_request:
  schedule:
    - cron: ""0 0 * * *"" # Run everyday

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true
          cache-dependency-glob: uv.lock
      - uses: actions/setup-python@v5
        with:
          python-version: 3.13
          allow-prereleases: true
      - run: uv sync --group=test
      - name: Run tests
        # TODO: #8818 Re-enable quantum tests
        run: uv run pytest
          --ignore=computer_vision/cnn_classification.py
          --ignore=docs/conf.py
          --ignore=dynamic_programming/k_means_clustering_tensorflow.py
          --ignore=machine_learning/lstm/lstm_prediction.py
          --ignore=neural_network/input_data.py
          --ignore=project_euler/
          --ignore=quantum/q_fourier_transform.py
          --ignore=scripts/validate_solutions.py
          --ignore=web_programming/fetch_anime_and_play.py
          --cov-report=term-missing:skip-covered
          --cov=. .
      - if: ${{ success() }}
        run: scripts/build_directory_md.py 2>&1 | tee DIRECTORY.md
",37,1,2,"pull_request, schedule",3
TheAlgorithms/Python,directory_writer.yml,"# The objective of this GitHub Action is to update the DIRECTORY.md file (if needed)
# when doing a git push
name: directory_writer
on: [push]
jobs:
  directory_writer:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - uses: actions/setup-python@v5
        with:
          python-version: 3.x
      - name: Write DIRECTORY.md
        run: |
          scripts/build_directory_md.py 2>&1 | tee DIRECTORY.md
          git config --global user.name ""$GITHUB_ACTOR""
          git config --global user.email ""$GITHUB_ACTOR@users.noreply.github.com""
          git remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/$GITHUB_REPOSITORY
      - name: Update DIRECTORY.md
        run: |
          git add DIRECTORY.md
          git commit -am ""updating DIRECTORY.md"" ||  true
          git push --force origin HEAD:$GITHUB_REF || true
",25,1,1,push,2
TheAlgorithms/Python,project_euler.yml,"on:
  pull_request:
    # Run only if a file is changed within the project_euler directory and related files
    paths:
      - ""project_euler/**""
      - "".github/workflows/project_euler.yml""
      - ""scripts/validate_solutions.py""
  schedule:
    - cron: ""0 0 * * *"" # Run everyday

name: ""Project Euler""

jobs:
  project-euler:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: astral-sh/setup-uv@v6
      - uses: actions/setup-python@v5
        with:
          python-version: 3.x
      - run: uv sync --group=euler-validate --group=test
      - run: uv run pytest --doctest-modules --cov-report=term-missing:skip-covered --cov=project_euler/ project_euler/
  validate-solutions:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: astral-sh/setup-uv@v6
      - uses: actions/setup-python@v5
        with:
          python-version: 3.x
      - run: uv sync --group=euler-validate --group=test
      - run: uv run pytest scripts/validate_solutions.py
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
",35,2,2,"pull_request, schedule",6
TheAlgorithms/Python,ruff.yml,"# https://beta.ruff.rs
name: ruff
on:
  push:
    branches:
      - master
  pull_request:
    branches:
      - master
jobs:
  ruff:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: astral-sh/setup-uv@v6
      - run: uvx ruff check --output-format=github .
",16,1,2,"push, pull_request",2
TheAlgorithms/Python,sphinx.yml,"name: sphinx

on:
  # Triggers the workflow on push or pull request events but only for the ""master"" branch
  push:
    branches: [""master""]
  pull_request:
    branches: [""master""]
  # Or manually from the Actions tab
  workflow_dispatch:

# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages
permissions:
  contents: read
  pages: write
  id-token: write

# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.
# However, do NOT cancel in-progress runs as we want to allow these production deployments to complete.
concurrency:
  group: ""pages""
  cancel-in-progress: false

jobs:
  build_docs:
    runs-on: ubuntu-24.04-arm
    steps:
      - uses: actions/checkout@v4
      - uses: astral-sh/setup-uv@v6
      - uses: actions/setup-python@v5
        with:
          python-version: 3.13
          allow-prereleases: true
      - run: uv sync --group=docs
      - uses: actions/configure-pages@v5
      - run: uv run sphinx-build -c docs . docs/_build/html
      - uses: actions/upload-pages-artifact@v3
        with:
          path: docs/_build/html

  deploy_docs:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    if: github.event_name != 'pull_request'
    needs: build_docs
    runs-on: ubuntu-latest
    steps:
      - uses: actions/deploy-pages@v4
        id: deployment
",50,2,3,"push, pull_request, workflow_dispatch",6
trekhleb/javascript-algorithms,CI.yml,"name: CI

on:
  push:
    branches: [ master ]
  pull_request:
    branches: [ master ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        node-version: [ 16.x ]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v1
        with:
          node-version: ${{ matrix.node-version }}

      - name: Install dependencies
        run: npm i

      - name: Run linting
        run: npm run lint

      - name: Run tests
        run: npm run coverage

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v1
",35,1,2,"push, pull_request",3
tensorflow/tensorflow,arm-cd.yml,"# Copyright 2022 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

name: ARM CD

on:
  push:
    tags:
      - v2.**
    branches:
      - r2.**
  schedule:
    - cron: '0 8 * * *'

permissions:
  contents: read

jobs:
  build:
    if: github.repository == 'tensorflow/tensorflow' # Don't do this in forks
    runs-on: [self-hosted, linux, ARM64]
    strategy:
      fail-fast: false
      matrix:
        pyver: ['3.9', '3.10', '3.11', '3.12']
    steps:
      - name: Stop old running containers (if any)
        shell: bash
        run: |
          running_containers=$(docker ps -q) && \
          if [[ $running_containers == """" ]]; then
            echo ""No running containers"";
          else
            echo ""Running container(s) found"" && \
            docker stop $running_containers;
          fi
          docker container prune -f
      - name: Clean repository
        shell: bash
        run: find /home/ubuntu/actions-runner/_work/tensorflow/tensorflow/. -name . -o -prune -exec sudo rm -rf -- {} + || true
      - name: Checkout repository for nightly (skipped for releases)
        if: ${{ github.event_name == 'schedule' }}
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          ref: 'nightly'
      - name: Checkout repository for releases (skipped for nightly)
        if: ${{ github.event_name == 'push' }}
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - name: Build and test pip wheel
        shell: bash
        run: |
          is_nightly=0 && tf_project_name='tensorflow_cpu_aws' && ${{ github.event_name == 'schedule' }} && is_nightly=1 && tf_project_name='tf_nightly_cpu_aws'
          echo ""PyPI project name:"" $tf_project_name
          CI_DOCKER_BUILD_EXTRA_PARAMS=""--build-arg py_major_minor_version=${{ matrix.pyver }} --build-arg is_nightly=${is_nightly} --build-arg tf_project_name=${tf_project_name}"" \
          ./tensorflow/tools/ci_build/ci_build.sh cpu.arm64 bash tensorflow/tools/ci_build/rel/ubuntu/cpu_arm64_test_build.sh
      - name: Upload pip wheel to PyPI
        if: github.event_name == 'schedule' || (github.event_name == 'push' && startsWith(github.ref, 'refs/tags/v2')) # only if it is a scheduled nightly or tagged
        shell: bash
        run: python3 -m twine upload --verbose /home/ubuntu/actions-runner/_work/tensorflow/tensorflow/whl/* -u ""__token__"" -p ${{ secrets.AWS_PYPI_ACCOUNT_TOKEN }}
",71,1,2,"push, schedule",2
tensorflow/tensorflow,arm-ci-extended-cpp.yml,"# Copyright 2022 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

name: ARM CI Extended C++

on:
  push:
    tags:
      - v2.**
  schedule:
    - cron: '0 2 * * *'

permissions:
  contents: read

jobs:
  build:
    if: github.repository == 'tensorflow/tensorflow' # Don't do this in forks
    runs-on: [self-hosted, linux, ARM64]
    strategy:
      matrix:
        pyver: ['3.10']
    steps:
      - name: Stop old running containers (if any)
        shell: bash
        run: |
          running_containers=$(docker ps -q) && \
          if [[ $running_containers == """" ]]; then
            echo ""No running containers"";
          else
            echo ""Running container(s) found"" && \
            docker stop $running_containers;
          fi
          docker container prune -f
          docker image prune -af
      - name: Clean repository
        shell: bash
        run: find /home/ubuntu/actions-runner/_work/tensorflow/tensorflow/. -name . -o -prune -exec sudo rm -rf -- {} + || true
      - name: Checkout repository for nightly (skipped for releases)
        if: ${{ github.event_name == 'schedule' }}
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          ref: 'nightly'
      - name: Checkout repository
        if: ${{ github.event_name == 'push' }}
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - name: Build binary and run C++ tests
        shell: bash
        run: |
          is_nightly=0 && tf_project_name='tf_ci_ext_c' && ${{ github.event_name == 'schedule' }} && is_nightly=1 && tf_project_name='tf_nightly_ci_ext_c'
          CI_DOCKER_BUILD_EXTRA_PARAMS=""--build-arg py_major_minor_version=${{ matrix.pyver }} --build-arg is_nightly=${is_nightly} --build-arg tf_project_name=${tf_project_name}"" \
          ./tensorflow/tools/ci_build/ci_build.sh cpu.arm64 bash tensorflow/tools/ci_build/rel/ubuntu/cpu_arm64_test_cpp.sh
",64,1,2,"push, schedule",2
tensorflow/tensorflow,arm-ci-extended.yml,"# Copyright 2022 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

name: ARM CI Extended

on:
  push:
    tags:
      - v2.**
  schedule:
    - cron: '0 4 * * *'

permissions:
  contents: read

jobs:
  build:
    if: github.repository == 'tensorflow/tensorflow' # Don't do this in forks
    runs-on: [self-hosted, linux, ARM64]
    strategy:
      fail-fast: false
      matrix:
        pyver: ['3.9', '3.10', '3.11', '3.12']
    steps:
      - name: Stop old running containers (if any)
        shell: bash
        run: |
          running_containers=$(docker ps -q) && \
          if [[ $running_containers == """" ]]; then
            echo ""No running containers"";
          else
            echo ""Running container(s) found"" && \
            docker stop $running_containers;
          fi
          docker container prune -f
          docker image prune -af
      - name: Clean repository
        shell: bash
        run: find /home/ubuntu/actions-runner/_work/tensorflow/tensorflow/. -name . -o -prune -exec sudo rm -rf -- {} + || true
      - name: Checkout repository for nightly (skipped for releases)
        if: ${{ github.event_name == 'schedule' }}
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          ref: 'nightly'
      - name: Checkout repository
        if: ${{ github.event_name == 'push' }}
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - name: Build binary and run python tests on nightly for all python versions
        shell: bash
        run: |
          is_nightly=0 && tf_project_name='tf_ci_ext' && ${{ github.event_name == 'schedule' }} && is_nightly=1 && tf_project_name='tf_nightly_ci_ext'
          CI_DOCKER_BUILD_EXTRA_PARAMS=""--build-arg py_major_minor_version=${{ matrix.pyver }} --build-arg is_nightly=${is_nightly} --build-arg tf_project_name=${tf_project_name}"" \
          ./tensorflow/tools/ci_build/ci_build.sh cpu.arm64 bash tensorflow/tools/ci_build/rel/ubuntu/cpu_arm64_test.sh
",65,1,2,"push, schedule",2
tensorflow/tensorflow,arm-ci.yml,"# Copyright 2022 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

name: ARM CI

on:
  push:
    branches:
      - master
      - r2.**
permissions:
  contents: read

jobs:
  build:
    # Don't do this in forks, and if labeled, only for 'kokoro:force-run'
    if: github.repository == 'tensorflow/tensorflow' && (github.event.action != 'labeled' || (github.event.action == 'labeled' && github.event.label.name == 'kokoro:force-run'))
    runs-on: [self-hosted, linux, ARM64]
    strategy:
      matrix:
        pyver: ['3.10']
    steps:
      - name: Stop old running containers (if any)
        shell: bash
        run: |
          running_containers=$(docker ps -q) && \
          if [[ $running_containers == """" ]]; then
            echo ""No running containers"";
          else
            echo ""Running container(s) found"" && \
            docker stop $running_containers;
          fi
          docker container prune -f
      - name: Clean repository
        shell: bash
        run: find /home/ubuntu/actions-runner/_work/tensorflow/tensorflow/. -name . -o -prune -exec sudo rm -rf -- {} + || true
      - name: Checkout repository
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - name: Build binary and run python tests
        shell: bash
        run: |
          CI_DOCKER_BUILD_EXTRA_PARAMS=""--pull --build-arg py_major_minor_version=${{ matrix.pyver }} --build-arg is_nightly=1 --build-arg tf_project_name=tf_nightly_ci"" \
          ./tensorflow/tools/ci_build/ci_build.sh cpu.arm64 bash tensorflow/tools/ci_build/rel/ubuntu/cpu_arm64_test.sh
",55,1,1,push,1
tensorflow/tensorflow,cffconvert.yml,"# Copyright 2022 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

name: cffconvert

on:
  push:
    paths:
      - CITATION.cff

permissions:
  contents: read

jobs:
  validate:
    if: github.repository == 'tensorflow/tensorflow' # Don't do this in forks
    name: ""validate""
    runs-on: ubuntu-latest
    steps:
      - name: Check out a copy of the repository
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Check whether the citation metadata from CITATION.cff is valid
        uses: citation-file-format/cffconvert-github-action@4cf11baa70a673bfdf9dad0acc7ee33b3f4b6084 # v2.0.0
        with:
          args: ""--validate""
",38,1,1,push,2
tensorflow/tensorflow,issue-on-pr-rollback.yml,"# Copyright 2021 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

name: Creates a GitHub Issue when a PR Rolled back via Commit to Master
on:
  push:
    branches:
      - master
      
permissions: {}

jobs:
  create-issue-on-pr-rollback:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      issues: write
      pull-requests: read
    if: |
      github.repository == 'tensorflow/tensorflow' &&
      startsWith(github.event.head_commit.message, 'Rollback of PR #')
    steps:
      - name: Checkout repo
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - name: Create a new Github Issue
        uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea # v7.0.1
        with:
          github-token: ${{secrets.GITHUB_TOKEN}}
          script: |
            const script = require('./.github/workflows/create_issue.js')
            console.log(await script({github, context}))
",43,1,1,push,2
tensorflow/tensorflow,osv-scanner-scheduled.yml,"# Copyright 2023 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

name: OSV-Scanner Scheduled Scan

on:
  schedule:
    - cron: 0 4 * * 1

permissions:
  # Require writing security events to upload SARIF file to security tab
  security-events: write
  # Only need to read contents
  contents: read

jobs:
  scan-scheduled:
    if: github.repository == 'tensorflow/tensorflow'
    uses: ""google/osv-scanner-action/.github/workflows/osv-scanner-reusable.yml@v2.0.2""
    with:
      scan-args: |-
        --lockfile=requirements.txt:./requirements_lock_3_9.txt
        --lockfile=requirements.txt:./requirements_lock_3_10.txt
        --lockfile=requirements.txt:./requirements_lock_3_11.txt
        --lockfile=requirements.txt:./requirements_lock_3_12.txt
        --lockfile=requirements.txt:./ci/official/containers/linux_arm64/devel.requirements.txt
        --lockfile=requirements.txt:./ci/official/containers/linux_arm64/jax.requirements.txt
        --lockfile=requirements.txt:./ci/official/containers/linux_arm64/devel.usertools/test.requirements.txt
",40,1,1,schedule,1
tensorflow/tensorflow,pylint-presubmit.yml,"# Copyright 2021 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

name: PyLint
on:
  pull_request:
    paths:
      - '**.py'

permissions:
  contents: read

jobs:
  build:
    name: PyLint
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
    - name: Get file changes
      id: get_file_changes
      uses: trilom/file-changes-action@a6ca26c14274c33b15e6499323aac178af06ad4b # v1.2.4
      with:
        output: ' '
    - name: Report list of changed files
      run: |
        echo Changed files: ${{ steps.get_file_changes.outputs.files }}
    - name: Set up Python 3.9
      uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065 # v5.6.0
      with:
        python-version: ""3.9""
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pylint==2.13.9 numpy wheel
    - name: Run PyLint on changed files
      run: |
        echo ""${{ steps.get_file_changes.outputs.files}}"" | tr "" "" ""\n"" | grep "".py$"" | xargs pylint --rcfile=tensorflow/tools/ci_build/pylintrc
",50,1,1,pull_request,3
tensorflow/tensorflow,release-branch-cherrypick.yml,"# Copyright 2022 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

# Usage: Go to
# https://github.com/tensorflow/tensorflow/actions/workflows/release-branch-cherrypick.yml
# and click ""Run Workflow."" Leave ""Use Workflow From"" set to ""master"", then
# input the branch name and paste the cherry-pick commit and click Run. A PR
# will be created.

name: Release Branch Cherrypick
on:
  workflow_dispatch:
    inputs:
      # We use this instead of the ""run on branch"" argument because GitHub looks
      # on that branch for a workflow.yml file, and we'd have to cherry-pick
      # this file into those branches.
      release_branch:
        description: 'Release branch name (e.g. r2.9)'
        required: true
        type: string
      git_commit:
        description: 'Git commit to cherry-pick'
        required: true
        type: string

permissions:
  contents: read

jobs:
  cherrypick:
    name: Cherrypick to ${{ github.event.inputs.release_branch}} - ${{ github.event.inputs.git_commit }}
    runs-on: ubuntu-latest
    if: github.repository == 'tensorflow/tensorflow' # Don't do this in forks
    steps:
    - name: Checkout code
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      with:
        ref: ${{ github.event.inputs.release_branch }}
    - name: Get some helpful info for formatting
      id: cherrypick
      run: |
          git config --global user.name ""TensorFlow Release Automation""
          git config --global user.email ""jenkins@tensorflow.org""
          git fetch origin master
          git cherry-pick ${{ github.event.inputs.git_commit }}
          echo ""SHORTSHA=$(git log -1 ${{ github.event.inputs.git_commit }} --format=""%h"")"" >> ""$GITHUB_OUTPUT""
          echo ""TITLE=$(git log -1 ${{ github.event.inputs.git_commit }} --format=""%s"")"" >> ""$GITHUB_OUTPUT""
    - name: Create Pull Request with changes
      uses: peter-evans/create-pull-request@271a8d0340265f705b14b6d32b9829c1cb33d45e # v7.0.8
      with:
        title: '${{ github.event.inputs.release_branch }} cherry-pick: ${{ steps.cherrypick.outputs.SHORTSHA }} ""${{ steps.cherrypick.outputs.TITLE }}""'
        committer: TensorFlow Release Automation <jenkins@tensorflow.org>
        token: ${{ secrets.JENKINS_TOKEN }}
        base: ${{ github.event.inputs.release_branch }}
        branch: ${{ github.event.inputs.release_branch }}-${{ steps.cherrypick.outputs.SHORTSHA }}
        reviewers: learning-to-play
        body: |
          Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/${{ github.event.inputs.git_commit }}

",71,1,1,workflow_dispatch,2
tensorflow/tensorflow,scorecards-analysis.yml,"# Copyright 2022 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

name: Scorecards supply-chain security
on:
  # For Branch-Protection check. Only the default branch is supported. See
  # https://github.com/ossf/scorecard/blob/main/docs/checks.md#branch-protection
  branch_protection_rule:
  # To guarantee Maintained check is occasionally updated. See
  # https://github.com/ossf/scorecard/blob/main/docs/checks.md#maintained
  schedule:
    - cron: '26 3 * * 2'
  push:
    branches: [ ""master"" ]

# Declare default permissions as read only.
permissions: read-all

jobs:
  analysis:
    if: github.repository == 'tensorflow/tensorflow' # Don't do this in forks
    name: Scorecards analysis
    runs-on: ubuntu-latest
    permissions:
      # Needed to upload the results to code-scanning dashboard.
      security-events: write
      # Needed to publish results and get a badge (see publish_results below).
      id-token: write

    steps:
      - name: ""Checkout code""
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          persist-credentials: false

      - name: ""Run analysis""
        uses: ossf/scorecard-action@05b42c624433fc40578a4040d5cf5e36ddca8cde # v2.4.2
        with:
          results_file: results.sarif
          results_format: sarif
          publish_results: true

      # Upload the results as artifacts (optional). Commenting out will disable uploads of run results in SARIF
      # format to the repository Actions tab.
      - name: ""Upload artifact""
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: SARIF file
          path: results.sarif
          retention-days: 5

      # Upload the results to GitHub's code scanning dashboard (optional).
      # Commenting out will disable upload of results to your repo's Code Scanning dashboard
      - name: ""Upload to code-scanning""
        uses: github/codeql-action/upload-sarif@fca7ace96b7d713c7035871441bd52efbe39e27e # v3.28.19
        with:
          sarif_file: results.sarif
",69,1,3,"branch_protection_rule, schedule, push",4
tensorflow/tensorflow,stale-issues.yml,"# Copyright 2023 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

name: Close inactive issues
on:
  schedule:
    - cron: ""30 1 * * *""

permissions:
  contents: read

jobs:
  close-issues:
    # Don't do this in forks
    if: github.repository == 'tensorflow/tensorflow'
    runs-on: ubuntu-latest
    permissions:
      issues: write
      pull-requests: write
    steps:
      - name: Awaiting response issues
        uses: actions/stale@5bef64f19d7facfb25b37b414482c7164d639639 # v9.1.0
        with:
          #Comma separated list of labels that can be assigned to issues to exclude them from being marked as stale
          exempt-issue-labels: 'override-stale'
          #Comma separated list of labels that can be assigned to PRs to exclude them from being marked as stale
          exempt-pr-labels: ""override-stale""
          #Limit the No. of API calls in one run default value is 30.
          operations-per-run: 1000
          days-before-issue-stale: 7
          days-before-issue-close: 7
          stale-issue-label: ""stale""
          # reason for closed the issue default value is not_planned
          close-issue-reason: completed
          # List of labels to remove when issues/PRs unstale. 
          labels-to-remove-when-unstale: 'stat:awaiting response'
          only-labels: ""stat:awaiting response""
          stale-issue-message: > 
            This issue is stale because it has been open for 7 days with no activity.
            It will be closed if no further activity occurs. Thank you.
          close-issue-message: >
            This issue was closed because it has been inactive for 7 days since being marked as stale.
            Please reopen if you'd like to work on this further.
          days-before-pr-stale: 14
          days-before-pr-close: 14
          stale-pr-message: ""This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.""
          close-pr-message: ""This PR was closed because it has been inactive for 14 days since being marked as stale. Please reopen if you'd like to work on this further.""
          repo-token: ${{ secrets.GITHUB_TOKEN }}
      - name: Contribution issues
        uses: actions/stale@5bef64f19d7facfb25b37b414482c7164d639639 # v9.1.0
        with:
          #Comma separated list of labels that can be assigned to issues to exclude them from being marked as stale
          exempt-issue-labels: 'override-stale'
          #Comma separated list of labels that can be assigned to PRs to exclude them from being marked as stale
          exempt-pr-labels: ""override-stale""
          #Limit the No. of API calls in one run default value is 30.
          operations-per-run: 1000
          days-before-issue-stale: 180
          days-before-issue-close: 365
          stale-issue-label: ""stale""
          # reason for closed the issue default value is not_planned
          close-issue-reason: completed
          # List of labels to remove when issues/PRs unstale. 
          labels-to-remove-when-unstale: ""stat:contribution welcome,stat:good first issue""
          any-of-labels: ""stat:contribution welcome,stat:good first issue""
          stale-issue-message: > 
            This issue is stale because it has been open for 180 days with no activity.
            It will be closed if no further activity occurs. Thank you.
          close-issue-message: >
            This issue was closed because it has been inactive for 1 year.
          repo-token: ${{ secrets.GITHUB_TOKEN }}
",83,1,1,schedule,2
tensorflow/tensorflow,update-nightly.yml,"# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

on:
  workflow_dispatch:  # Allow manual triggers
  schedule:
    - cron: 0 4 * * *  # 4am UTC is 9pm PDT and 8pm PST
name: Set nightly branch to master HEAD

permissions: {}

jobs:
  master-to-nightly:
    if: github.repository == 'tensorflow/tensorflow' # Don't do this in forks
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
    - uses: zofrex/mirror-branch@0be56f4c8077a288a635a491b306ba0bb1c810e6 # v1.0.4
      name: Set nightly branch to master HEAD
      with:
        target-branch: 'nightly'
",34,1,2,"workflow_dispatch, schedule",1
tensorflow/tensorflow,update-rbe.yml,"# Copyright 2022 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================


# This Workflow updates tensorflow/tools/toolchains/remote_config/configs.bzl
# to reference the most recent versions of the SIG Build Docker images.
name: Update RBE Configs
on:
  workflow_dispatch:

permissions:
  contents: read

jobs:
  rbe:
    name: Update RBE Configs
    runs-on: ubuntu-latest
    if: github.repository == 'tensorflow/tensorflow' # Don't do this in forks
    steps:
    - name: Checkout code
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
    - name: Update the RBE Configs
      run: |
        function map() {
          # The ""digest"" that allows us to pull an image is not the digest as
          # returned by the API, but a sha256sum of the entire chunk of image
          # metadata. gcr.io helpfully includes it in the header of the response
          # as docker-content-digest: sha256:[digest]. Note we use egrep to
          # match exactly sha256:<hash> because curl may include a ^M symbol at
          # the end of the line.
          # See https://cloud.google.com/architecture/using-container-images#exploring_image_manifests_digests_and_tags
          echo -n ""Trying to map name $1 to tag $2... ""
          digest=$(curl -s --head ""https://gcr.io/v2/tensorflow-sigs/build/manifests/$2"" | egrep -o ""sha256:[[:alnum:]]*"")
          # Find the line matching the regex ""sigbuild-r2.9"" (with quotes) and
          # replace just the digest portion in it
          sed -i"""" ""/\""$1\""/ s/sha256:[[:alnum:]]*/$digest/g"" tensorflow/tools/toolchains/remote_config/configs.bzl
          echo ""success.""
        }
        # See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/toolchains/remote_config/configs.bzl
        # This is a mapping of name_container_map keys under sigbuild_tf_configs
        # to tag names on gcr.io/tensorflow-sigs/build.
        # TF 2.9
        map sigbuild-r2.9 2.9-python3.9
        map sigbuild-r2.9-python3.8 2.9-python3.8
        map sigbuild-r2.9-python3.9 2.9-python3.9
        map sigbuild-r2.9-python3.10 2.9-python3.10
        # TF 2.10
        map sigbuild-r2.10 2.10-python3.9
        map sigbuild-r2.10-python3.8 2.10-python3.8
        map sigbuild-r2.10-python3.9 2.10-python3.9
        map sigbuild-r2.10-python3.10 2.10-python3.10
        # TF 2.11
        map sigbuild-r2.11 2.11-python3.9
        map sigbuild-r2.11-python3.8 2.11-python3.8
        map sigbuild-r2.11-python3.9 2.11-python3.9
        map sigbuild-r2.11-python3.10 2.11-python3.10
        # WIP Clang Containers, used by TVCs
        map sigbuild-57469 57469-python3.9
        map sigbuild-57469-python3.8 57469-python3.8
        map sigbuild-57469-python3.9 57469-python3.9
        map sigbuild-57469-python3.10 57469-python3.10
        # TF 2.12
        map sigbuild-r2.12 2.12-python3.9
        map sigbuild-r2.12-python3.8 2.12-python3.8
        map sigbuild-r2.12-python3.9 2.12-python3.9
        map sigbuild-r2.12-python3.10 2.12-python3.10
        map sigbuild-r2.12-python3.11 2.12-python3.11
        # TF 2.12 + Clang (containers are the same, but env vars in configs.bzl are different)
        map sigbuild-r2.12-clang 2.12-python3.9
        map sigbuild-r2.12-clang-python3.8 2.12-python3.8
        map sigbuild-r2.12-clang-python3.9 2.12-python3.9
        map sigbuild-r2.12-clang-python3.10 2.12-python3.10
        map sigbuild-r2.12-clang-python3.11 2.12-python3.11
        # TF 2.13
        map sigbuild-r2.13 2.13-python3.9
        map sigbuild-r2.13-python3.8 2.13-python3.8
        map sigbuild-r2.13-python3.9 2.13-python3.9
        map sigbuild-r2.13-python3.10 2.13-python3.10
        map sigbuild-r2.13-python3.11 2.13-python3.11
        # TF 2.13 + Clang (containers are the same, but env vars in configs.bzl are different)
        map sigbuild-r2.13-clang 2.13-python3.9
        map sigbuild-r2.13-clang-python3.8 2.13-python3.8
        map sigbuild-r2.13-clang-python3.9 2.13-python3.9
        map sigbuild-r2.13-clang-python3.10 2.13-python3.10
        map sigbuild-r2.13-clang-python3.11 2.13-python3.11
        # TF 2.14
        map sigbuild-r2.14 2.14-python3.9
        map sigbuild-r2.14-python3.9 2.14-python3.9
        map sigbuild-r2.14-python3.10 2.14-python3.10
        map sigbuild-r2.14-python3.11 2.14-python3.11
        # TF 2.14 + Clang (containers are the same, but env vars in configs.bzl are different)
        map sigbuild-r2.14-clang 2.14-python3.9
        map sigbuild-r2.14-clang-python3.9 2.14-python3.9
        map sigbuild-r2.14-clang-python3.10 2.14-python3.10
        map sigbuild-r2.14-clang-python3.11 2.14-python3.11
        # TF 2.16
        map sigbuild-r2.16 2.16-python3.11
        map sigbuild-r2.16-python3.9 2.16-python3.9
        map sigbuild-r2.16-python3.10 2.16-python3.10
        map sigbuild-r2.16-python3.11 2.16-python3.11
        map sigbuild-r2.16-python3.12 2.16-python3.12
        # TF 2.16 + Clang (containers are the same, but env vars in configs.bzl are different)
        map sigbuild-r2.16-clang 2.16-python3.11
        map sigbuild-r2.16-clang-python3.9 2.16-python3.9
        map sigbuild-r2.16-clang-python3.10 2.16-python3.10
        map sigbuild-r2.16-clang-python3.11 2.16-python3.11
        map sigbuild-r2.16-clang-python3.12 2.16-python3.12
        # TF 2.17
        map sigbuild-r2.17 2.17-python3.11
        map sigbuild-r2.17-python3.9 2.17-python3.9
        map sigbuild-r2.17-python3.10 2.17-python3.10
        map sigbuild-r2.17-python3.11 2.17-python3.11
        map sigbuild-r2.17-python3.12 2.17-python3.12
        # TF 2.17 + Clang (containers are the same, but env vars in configs.bzl are different)
        map sigbuild-r2.17-clang 2.17-python3.11
        map sigbuild-r2.17-clang-python3.9 2.17-python3.9
        map sigbuild-r2.17-clang-python3.10 2.17-python3.10
        map sigbuild-r2.17-clang-python3.11 2.17-python3.11
        map sigbuild-r2.17-clang-python3.12 2.17-python3.12
    - name: Create Pull Request with changes
      uses: peter-evans/create-pull-request@271a8d0340265f705b14b6d32b9829c1cb33d45e # v7.0.8
      with:
        title: Update the RBE images to the latest container versions
        committer: TensorFlow Release Automation <jenkins@tensorflow.org>
        token: ${{ secrets.JENKINS_TOKEN }}
        reviewers: mihaimaruseac,learning-to-play,nitins17
        body: |
          This PR was created by a GitHub Actions workflow to update all the SIG Build-based RBE containers to the most recent containers. See:

          - https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/toolchains/remote_config/configs.bzl
          - https://github.com/tensorflow/tensorflow/blob/master/.github/workflows/update-rbe.yml
",143,1,1,workflow_dispatch,2
ossu/computer-science,delete-empty-issues.yml,"name: Delete empty issues
on:
  issues:
    types:
      - opened
jobs:
  label_issues:
    runs-on: ubuntu-latest
    permissions:
      issues: write

    if: github.event.issue.body == '' || contains(github.event.issue.body, 'Give a 1 sentence description of a problem with the current OSSU Curriculum. Successful critiques of the curriculum will point out ways that OSSU is failing to uphold')
    steps:
      - name: Create comment
        uses: actions-cool/issues-helper@v3
        with:
          actions: 'create-comment'
          token: ${{ secrets.GITHUB_TOKEN }}
          issue-number: ${{ github.event.issue.number }}
          body: |
            Hello @${{ github.event.issue.user.login }}.
            It looks like you've opened an empty issue or one without a unique problem description.
            Please understand that this is a popular project, useful to many learners, and empty issues distract maintainers that are trying to help others.
            If you would like practice with issues, you can follow github documentation to create your own repo:
            https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-new-repository
            And then in that repo practice creating and editing issues:
            https://docs.github.com/en/issues/tracking-your-work-with-issues/configuring-issues/quickstart

            We look forward to your future contributions to OSSU, when you are contributing to improve computer science education for learners all over the world!
      - name: Close issue
        uses: actions-cool/issues-helper@v3
        with:
          actions: 'close-issue'
          token: ${{ secrets.GITHUB_TOKEN }}
          issue-number: ${{ github.event.issue.number }}
",35,1,1,issues,2
ohmyzsh/ohmyzsh,dependencies.yml,"name: Update dependencies
on:
  workflow_dispatch: {}
  schedule:
    - cron: ""0 6 * * 0""

jobs:
  check:
    name: Check for updates
    runs-on: ubuntu-latest
    if: github.repository == 'ohmyzsh/ohmyzsh'
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Authenticate as @ohmyzsh
        id: generate_token
        uses: ohmyzsh/github-app-token@v2
        with:
          app_id: ${{ secrets.OHMYZSH_APP_ID }}
          private_key: ${{ secrets.OHMYZSH_APP_PRIVATE_KEY }}
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ""3.12""
          cache: ""pip""
      - name: Process dependencies
        env:
          GH_TOKEN: ${{ steps.generate_token.outputs.token }}
          GIT_APP_NAME: ohmyzsh[bot]
          GIT_APP_EMAIL: 54982679+ohmyzsh[bot]@users.noreply.github.com
          TMP_DIR: ${{ runner.temp }}
        run: |
          pip install -r .github/workflows/dependencies/requirements.txt
          python3 .github/workflows/dependencies/updater.py
",36,1,2,"workflow_dispatch, schedule",3
ohmyzsh/ohmyzsh,installer.yml,"name: Test and Deploy installer
on:
  workflow_dispatch: {}
  push:
    paths:
      - 'tools/install.sh'
      - '.github/workflows/installer/**'
      - '.github/workflows/installer.yml'

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: false

permissions:
  contents: read # to checkout

jobs:
  test:
    name: Test installer
    if: github.repository == 'ohmyzsh/ohmyzsh'
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os:
          - ubuntu-latest
          - macos-latest
    steps:
      - name: Set up git repository
        uses: actions/checkout@v4
      - name: Install zsh
        if: runner.os == 'Linux'
        run: sudo apt-get update; sudo apt-get install zsh
      - name: Test installer
        run: sh ./tools/install.sh

  deploy:
    name: Deploy installer in install.ohmyz.sh
    if: github.ref == 'refs/heads/master'
    runs-on: ubuntu-latest
    environment: vercel
    needs:
      - test
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Install Vercel CLI
        run: npm install -g vercel
      - name: Setup project and deploy
        env:
          VERCEL_ORG_ID: ${{ secrets.VERCEL_ORG_ID }}
          VERCEL_PROJECT_ID: ${{ secrets.VERCEL_PROJECT_ID }}
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
        run: |
          cp tools/install.sh .github/workflows/installer/install.sh
          cd .github/workflows/installer
          vc deploy --prod -t ""$VERCEL_TOKEN""
",56,2,2,"workflow_dispatch, push",2
ohmyzsh/ohmyzsh,main.yml,"name: CI
on:
  pull_request:
    types:
      - opened
      - synchronize
    branches:
      - master
  push:
    branches:
      - master

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

permissions:
  contents: read # to fetch code (actions/checkout)

jobs:
  tests:
    name: Run tests
    runs-on: ubuntu-latest
    if: github.repository == 'ohmyzsh/ohmyzsh'
    steps:
      - name: Set up git repository
        uses: actions/checkout@v4
      - name: Install zsh
        run: sudo apt-get update; sudo apt-get install zsh
      - name: Check syntax
        run: |
          for file in ./oh-my-zsh.sh \
                      ./lib/*.zsh \
                      ./plugins/*/*.plugin.zsh \
                      ./plugins/*/_* \
                      ./themes/*.zsh-theme; do
            zsh -n ""$file"" || return 1
          done
",38,1,2,"pull_request, push",1
ohmyzsh/ohmyzsh,project.yml,"name: Project tracking
on:
  issues:
    types: [opened, reopened]
  pull_request_target:
    types: [opened, reopened, synchronize]

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

permissions: {}
jobs:
  add-to-project:
    name: Add to project
    runs-on: ubuntu-latest
    if: github.repository == 'ohmyzsh/ohmyzsh'
    steps:
      - name: Authenticate as @ohmyzsh
        id: generate_token
        uses: ohmyzsh/github-app-token@v2
        with:
          app_id: ${{ secrets.OHMYZSH_APP_ID }}
          private_key: ${{ secrets.OHMYZSH_APP_PRIVATE_KEY }}
      - name: Store app token
        run: echo ""GH_TOKEN=${{ steps.generate_token.outputs.token }}"" >> ""$GITHUB_ENV""
      - name: Read project data
        env:
          ORGANIZATION: ohmyzsh
          PROJECT_NUMBER: ""1""
        run: |
          # Get Project data
          gh api graphql -f query='
            query($org: String!, $number: Int!) {
              organization(login: $org){
                projectV2(number: $number) {
                  id
                  fields(first:20) {
                    nodes {
                      ... on ProjectV2Field {
                        id
                        name
                      }
                    }
                  }
                }
              }
            }' -f org=$ORGANIZATION -F number=$PROJECT_NUMBER > project_data.json

          # Parse project data
          cat >> $GITHUB_ENV <<EOF
          PROJECT_ID=$(jq '.data.organization.projectV2.id' project_data.json)
          PLUGIN_FIELD_ID=$(jq '.data.organization.projectV2.fields.nodes[] | select(.name == ""Plugin"") | .id' project_data.json)
          THEME_FIELD_ID=$(jq '.data.organization.projectV2.fields.nodes[] | select(.name == ""Theme"") | .id' project_data.json)
          EOF

      - name: Add to project
        env:
          ISSUE_OR_PR_ID: ${{ github.event.issue.node_id || github.event.pull_request.node_id }}
        run: |
          item_id=""$(gh api graphql -f query='
            mutation($project: ID!, $content: ID!) {
              addProjectV2ItemById(input: {projectId: $project, contentId: $content}) {
                item {
                  id
                }
              }
            }
          ' -f project=$PROJECT_ID -f content=$ISSUE_OR_PR_ID --jq '.data.addProjectV2ItemById.item.id')""

          echo ""ITEM_ID=$item_id"" >> $GITHUB_ENV

      - name: Classify Pull Request
        if: github.event_name == 'pull_request_target'
        run: |
          touch plugins.list themes.list

          gh pr view ${{ github.event.pull_request.number }} \
            --repo ${{ github.repository }} \
            --json files --jq '.files.[].path' | awk -F/ '
            /^plugins\// {
              plugins[$2] = 1
            }
            /^themes\// {
              gsub(/\.zsh-theme$/, """", $2)
              themes[$2] = 1
            }
            END {
              for (plugin in plugins) {
                print plugin >> ""plugins.list""
              }
              for (theme in themes) {
                print theme >> ""themes.list""
              }
            }
          '
          # If only one plugin is modified, add it to the plugin field
          if [[ $(wc -l < plugins.list) = 1 ]]; then
            echo ""PLUGIN=$(cat plugins.list)"" >> $GITHUB_ENV
          fi
          # If only one theme is modified, add it to the theme field
          if [[ $(wc -l < themes.list) = 1 ]]; then
            echo ""THEME=$(cat themes.list)"" >> $GITHUB_ENV
          fi

      - name: Fill Pull Request fields in project
        if: github.event_name == 'pull_request_target'
        run: |
          gh api graphql -f query='
            mutation (
              $project: ID!
              $item: ID!
              $plugin_field: ID!
              $plugin_value: String!
              $theme_field: ID!
              $theme_value: String!
            ) {
              set_plugin: updateProjectV2ItemFieldValue(input: {
                projectId: $project
                itemId: $item
                fieldId: $plugin_field
                value: {
                  text: $plugin_value
                }
              }) {
                projectV2Item {
                  id
                }
              }
              set_theme: updateProjectV2ItemFieldValue(input: {
                projectId: $project
                itemId: $item
                fieldId: $theme_field
                value: {
                  text: $theme_value
                }
              }) {
                projectV2Item {
                  id
                }
              }
            }
          ' -f project=$PROJECT_ID -f item=$ITEM_ID \
            -f plugin_field=$PLUGIN_FIELD_ID -f plugin_value=$PLUGIN \
            -f theme_field=$THEME_FIELD_ID -f theme_value=$THEME \
            --silent
",146,1,2,"issues, pull_request_target",1
Significant-Gravitas/AutoGPT,classic-autogpt-ci.yml,"name: Classic - AutoGPT CI

on:
  push:
    branches: [ master, dev, ci-test* ]
    paths:
      - '.github/workflows/classic-autogpt-ci.yml'
      - 'classic/original_autogpt/**'
  pull_request:
    branches: [ master, dev, release-* ]
    paths:
      - '.github/workflows/classic-autogpt-ci.yml'
      - 'classic/original_autogpt/**'

concurrency:
  group: ${{ format('classic-autogpt-ci-{0}', github.head_ref && format('{0}-{1}', github.event_name, github.event.pull_request.number) || github.sha) }}
  cancel-in-progress: ${{ startsWith(github.event_name, 'pull_request') }}

defaults:
  run:
    shell: bash
    working-directory: classic/original_autogpt

jobs:
  test:
    permissions:
      contents: read
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        python-version: [""3.10""]
        platform-os: [ubuntu, macos, macos-arm64, windows]
    runs-on: ${{ matrix.platform-os != 'macos-arm64' && format('{0}-latest', matrix.platform-os) || 'macos-14' }}

    steps:
      # Quite slow on macOS (2~4 minutes to set up Docker)
      # - name: Set up Docker (macOS)
      #   if: runner.os == 'macOS'
      #   uses: crazy-max/ghaction-setup-docker@v3

      - name: Start MinIO service (Linux)
        if: runner.os == 'Linux'
        working-directory: '.'
        run: |
          docker pull minio/minio:edge-cicd
          docker run -d -p 9000:9000 minio/minio:edge-cicd

      - name: Start MinIO service (macOS)
        if: runner.os == 'macOS'
        working-directory: ${{ runner.temp }}
        run: |
          brew install minio/stable/minio
          mkdir data
          minio server ./data &

      # No MinIO on Windows:
      # - Windows doesn't support running Linux Docker containers
      # - It doesn't seem possible to start background processes on Windows. They are
      #   killed after the step returns.
      #   See: https://github.com/actions/runner/issues/598#issuecomment-2011890429

      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          submodules: true

      - name: Configure git user Auto-GPT-Bot
        run: |
          git config --global user.name ""Auto-GPT-Bot""
          git config --global user.email ""github-bot@agpt.co""

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - id: get_date
        name: Get date
        run: echo ""date=$(date +'%Y-%m-%d')"" >> $GITHUB_OUTPUT

      - name: Set up Python dependency cache
        # On Windows, unpacking cached dependencies takes longer than just installing them
        if: runner.os != 'Windows'
        uses: actions/cache@v4
        with:
          path: ${{ runner.os == 'macOS' && '~/Library/Caches/pypoetry' || '~/.cache/pypoetry' }}
          key: poetry-${{ runner.os }}-${{ hashFiles('classic/original_autogpt/poetry.lock') }}

      - name: Install Poetry (Unix)
        if: runner.os != 'Windows'
        run: |
          curl -sSL https://install.python-poetry.org | python3 -

          if [ ""${{ runner.os }}"" = ""macOS"" ]; then
            PATH=""$HOME/.local/bin:$PATH""
            echo ""$HOME/.local/bin"" >> $GITHUB_PATH
          fi

      - name: Install Poetry (Windows)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          (Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -

          $env:PATH += "";$env:APPDATA\Python\Scripts""
          echo ""$env:APPDATA\Python\Scripts"" >> $env:GITHUB_PATH

      - name: Install Python dependencies
        run: poetry install

      - name: Run pytest with coverage
        run: |
          poetry run pytest -vv \
            --cov=autogpt --cov-branch --cov-report term-missing --cov-report xml \
            --numprocesses=logical --durations=10 \
            --junitxml=junit.xml -o junit_family=legacy \
            tests/unit tests/integration
        env:
          CI: true
          PLAIN_OUTPUT: True
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          S3_ENDPOINT_URL: ${{ runner.os != 'Windows' && 'http://127.0.0.1:9000' || '' }}
          AWS_ACCESS_KEY_ID: minioadmin
          AWS_SECRET_ACCESS_KEY: minioadmin

      - name: Upload test results to Codecov
        if: ${{ !cancelled() }}  # Run even if tests fail
        uses: codecov/test-results-action@v1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload coverage reports to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          flags: autogpt-agent,${{ runner.os }}

      - name: Upload logs to artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-logs
          path: classic/original_autogpt/logs/
",145,1,2,"push, pull_request",7
Significant-Gravitas/AutoGPT,classic-autogpt-docker-cache-clean.yml,"name: Classic - Purge Auto-GPT Docker CI cache

on:
  schedule:
    - cron: 20 4 * * 1,4

env:
  BASE_BRANCH: dev
  IMAGE_NAME: auto-gpt

jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        build-type: [release, dev]
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - id: build
        name: Build image
        uses: docker/build-push-action@v6
        with:
          context: classic/
          file: classic/Dockerfile.autogpt
          build-args: BUILD_TYPE=${{ matrix.build-type }}
          load: true # save to docker images
          # use GHA cache as read-only
          cache-to: type=gha,scope=autogpt-docker-${{ matrix.build-type }},mode=max

      - name: Generate build report
        env:
          event_name: ${{ github.event_name }}
          event_ref: ${{ github.event.schedule }}

          build_type: ${{ matrix.build-type }}

          prod_branch: master
          dev_branch: dev
          repository: ${{ github.repository }}
          base_branch: ${{ github.ref_name != 'master' && github.ref_name != 'dev' && 'dev' || 'master' }}

          current_ref: ${{ github.ref_name }}
          commit_hash: ${{ github.sha }}
          source_url: ${{ format('{0}/tree/{1}', github.event.repository.url, github.sha) }}
          push_forced_label:

          new_commits_json: ${{ null }}
          compare_url_template: ${{ format('/{0}/compare/{{base}}...{{head}}', github.repository) }}

          github_context_json: ${{ toJSON(github) }}
          job_env_json: ${{ toJSON(env) }}
          vars_json: ${{ toJSON(vars) }}

        run: .github/workflows/scripts/docker-ci-summary.sh >> $GITHUB_STEP_SUMMARY
        continue-on-error: true
",60,1,1,schedule,3
Significant-Gravitas/AutoGPT,classic-autogpt-docker-ci.yml,"name: Classic - AutoGPT Docker CI

on:
  push:
    branches: [master, dev]
    paths:
      - '.github/workflows/classic-autogpt-docker-ci.yml'
      - 'classic/original_autogpt/**'
      - 'classic/forge/**'
  pull_request:
    branches: [ master, dev, release-* ]
    paths:
      - '.github/workflows/classic-autogpt-docker-ci.yml'
      - 'classic/original_autogpt/**'
      - 'classic/forge/**'

concurrency:
  group: ${{ format('classic-autogpt-docker-ci-{0}', github.head_ref && format('pr-{0}', github.event.pull_request.number) || github.sha) }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}

defaults:
  run:
    working-directory: classic/original_autogpt

env:
  IMAGE_NAME: auto-gpt
  DEPLOY_IMAGE_NAME: ${{ secrets.DOCKER_USER && format('{0}/', secrets.DOCKER_USER) || '' }}auto-gpt
  DEV_IMAGE_TAG: latest-dev

jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        build-type: [release, dev]
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - if: runner.debug
        run: |
          ls -al
          du -hs *

      - id: build
        name: Build image
        uses: docker/build-push-action@v6
        with:
          context: classic/
          file: classic/Dockerfile.autogpt
          build-args: BUILD_TYPE=${{ matrix.build-type }}
          tags: ${{ env.IMAGE_NAME }}
          labels: GIT_REVISION=${{ github.sha }}
          load: true # save to docker images
          # cache layers in GitHub Actions cache to speed up builds
          cache-from: type=gha,scope=autogpt-docker-${{ matrix.build-type }}
          cache-to: type=gha,scope=autogpt-docker-${{ matrix.build-type }},mode=max

      - name: Generate build report
        env:
          event_name: ${{ github.event_name }}
          event_ref: ${{ github.event.ref }}
          event_ref_type: ${{ github.event.ref}}

          build_type: ${{ matrix.build-type }}

          prod_branch: master
          dev_branch: dev
          repository: ${{ github.repository }}
          base_branch: ${{ github.ref_name != 'master' && github.ref_name != 'dev' && 'dev' || 'master' }}

          current_ref: ${{ github.ref_name }}
          commit_hash: ${{ github.event.after }}
          source_url: ${{ format('{0}/tree/{1}', github.event.repository.url, github.event.release && github.event.release.tag_name || github.sha) }}
          push_forced_label: ${{ github.event.forced && '☢️ forced' || '' }}

          new_commits_json: ${{ toJSON(github.event.commits) }}
          compare_url_template: ${{ format('/{0}/compare/{{base}}...{{head}}', github.repository) }}

          github_context_json: ${{ toJSON(github) }}
          job_env_json: ${{ toJSON(env) }}
          vars_json: ${{ toJSON(vars) }}

        run: .github/workflows/scripts/docker-ci-summary.sh >> $GITHUB_STEP_SUMMARY
        continue-on-error: true

  test:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    services:
      minio:
        image: minio/minio:edge-cicd
        options: >
          --name=minio
          --health-interval=10s --health-timeout=5s --health-retries=3
          --health-cmd=""curl -f http://localhost:9000/minio/health/live""

    steps:
      - name: Check out repository
        uses: actions/checkout@v4
        with:
          submodules: true

      - if: github.event_name == 'push'
        name: Log in to Docker hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USER }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - id: build
        name: Build image
        uses: docker/build-push-action@v6
        with:
          context: classic/
          file: classic/Dockerfile.autogpt
          build-args: BUILD_TYPE=dev # include pytest
          tags: >
            ${{ env.IMAGE_NAME }},
            ${{ env.DEPLOY_IMAGE_NAME }}:${{ env.DEV_IMAGE_TAG }}
          labels: GIT_REVISION=${{ github.sha }}
          load: true # save to docker images
          # cache layers in GitHub Actions cache to speed up builds
          cache-from: type=gha,scope=autogpt-docker-dev
          cache-to: type=gha,scope=autogpt-docker-dev,mode=max

      - id: test
        name: Run tests
        env:
          CI: true
          PLAIN_OUTPUT: True
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          S3_ENDPOINT_URL: http://minio:9000
          AWS_ACCESS_KEY_ID: minioadmin
          AWS_SECRET_ACCESS_KEY: minioadmin
        run: |
          set +e
          docker run --env CI --env OPENAI_API_KEY \
            --network container:minio \
            --env S3_ENDPOINT_URL --env AWS_ACCESS_KEY_ID --env AWS_SECRET_ACCESS_KEY \
            --entrypoint poetry ${{ env.IMAGE_NAME }} run \
            pytest -v --cov=autogpt --cov-branch --cov-report term-missing \
            --numprocesses=4 --durations=10 \
            tests/unit tests/integration 2>&1 | tee test_output.txt

          test_failure=${PIPESTATUS[0]}

          cat << $EOF >> $GITHUB_STEP_SUMMARY
          # Tests $([ $test_failure = 0 ] && echo '✅' || echo '❌')
          \`\`\`
          $(cat test_output.txt)
          \`\`\`
          $EOF

          exit $test_failure

      - if: github.event_name == 'push' && github.ref_name == 'master'
        name: Push image to Docker Hub
        run: docker push ${{ env.DEPLOY_IMAGE_NAME }}:${{ env.DEV_IMAGE_TAG }}
",166,2,2,"push, pull_request",7
Significant-Gravitas/AutoGPT,classic-autogpt-docker-release.yml,"name: Classic - AutoGPT Docker Release

on:
  release:
    types: [published, edited]

  workflow_dispatch:
    inputs:
      no_cache:
        type: boolean
        description: 'Build from scratch, without using cached layers'

env:
  IMAGE_NAME: auto-gpt
  DEPLOY_IMAGE_NAME: ${{ secrets.DOCKER_USER }}/auto-gpt

jobs:
  build:
    if: startsWith(github.ref, 'refs/tags/autogpt-')
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Log in to Docker hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USER }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

        # slashes are not allowed in image tags, but can appear in git branch or tag names
      - id: sanitize_tag
        name: Sanitize image tag
        run: |
          tag=${raw_tag//\//-}
          echo tag=${tag#autogpt-} >> $GITHUB_OUTPUT
        env:
          raw_tag: ${{ github.ref_name }}

      - id: build
        name: Build image
        uses: docker/build-push-action@v6
        with:
          context: classic/
          file: Dockerfile.autogpt
          build-args: BUILD_TYPE=release
          load: true # save to docker images
          # push: true  # TODO: uncomment when this issue is fixed: https://github.com/moby/buildkit/issues/1555
          tags: >
            ${{ env.IMAGE_NAME }},
            ${{ env.DEPLOY_IMAGE_NAME }}:latest,
            ${{ env.DEPLOY_IMAGE_NAME }}:${{ steps.sanitize_tag.outputs.tag }}
          labels: GIT_REVISION=${{ github.sha }}

          # cache layers in GitHub Actions cache to speed up builds
          cache-from: ${{ !inputs.no_cache && 'type=gha' || '' }},scope=autogpt-docker-release
          cache-to: type=gha,scope=autogpt-docker-release,mode=max

      - name: Push image to Docker Hub
        run: docker push --all-tags ${{ env.DEPLOY_IMAGE_NAME }}

      - name: Generate build report
        env:
          event_name: ${{ github.event_name }}
          event_ref: ${{ github.event.ref }}
          event_ref_type: ${{ github.event.ref}}
          inputs_no_cache: ${{ inputs.no_cache }}

          prod_branch: master
          dev_branch: dev
          repository: ${{ github.repository }}
          base_branch: ${{ github.ref_name != 'master' && github.ref_name != 'dev' && 'dev' || 'master' }}

          ref_type: ${{ github.ref_type }}
          current_ref: ${{ github.ref_name }}
          commit_hash: ${{ github.sha }}
          source_url: ${{ format('{0}/tree/{1}', github.event.repository.url, github.event.release && github.event.release.tag_name || github.sha) }}

          github_context_json: ${{ toJSON(github) }}
          job_env_json: ${{ toJSON(env) }}
          vars_json: ${{ toJSON(vars) }}

        run: .github/workflows/scripts/docker-release-summary.sh >> $GITHUB_STEP_SUMMARY
        continue-on-error: true
",87,1,2,"release, workflow_dispatch",4
Significant-Gravitas/AutoGPT,classic-autogpts-ci.yml,"name: Classic - Agent smoke tests

on:
  workflow_dispatch:
  schedule:
    - cron: '0 8 * * *'
  push:
    branches: [ master, dev, ci-test* ]
    paths:
      - '.github/workflows/classic-autogpts-ci.yml'
      - 'classic/original_autogpt/**'
      - 'classic/forge/**'
      - 'classic/benchmark/**'
      - 'classic/run'
      - 'classic/cli.py'
      - 'classic/setup.py'
      - '!**/*.md'
  pull_request:
    branches: [ master, dev, release-* ]
    paths:
      - '.github/workflows/classic-autogpts-ci.yml'
      - 'classic/original_autogpt/**'
      - 'classic/forge/**'
      - 'classic/benchmark/**'
      - 'classic/run'
      - 'classic/cli.py'
      - 'classic/setup.py'
      - '!**/*.md'

defaults:
  run:
    shell: bash
    working-directory: classic

jobs:
  serve-agent-protocol:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        agent-name: [ original_autogpt ]
      fail-fast: false
    timeout-minutes: 20
    env:
      min-python-version: '3.10'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          submodules: true

      - name: Set up Python ${{ env.min-python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.min-python-version }}

      - name: Install Poetry
        working-directory: ./classic/${{ matrix.agent-name }}/
        run: |
          curl -sSL https://install.python-poetry.org | python -

      - name: Run regression tests
        run: |
          ./run agent start ${{ matrix.agent-name }}
          cd ${{ matrix.agent-name }}
          poetry run agbenchmark --mock --test=BasicRetrieval --test=Battleship --test=WebArenaTask_0
          poetry run agbenchmark --test=WriteFile
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          AGENT_NAME: ${{ matrix.agent-name }}
          REQUESTS_CA_BUNDLE: /etc/ssl/certs/ca-certificates.crt
          HELICONE_CACHE_ENABLED: false
          HELICONE_PROPERTY_AGENT: ${{ matrix.agent-name }}
          REPORTS_FOLDER: ${{ format('../../reports/{0}', matrix.agent-name) }}
          TELEMETRY_ENVIRONMENT: autogpt-ci
          TELEMETRY_OPT_IN: ${{ github.ref_name == 'master' }}
",76,1,4,"workflow_dispatch, schedule, push, pull_request",2
Significant-Gravitas/AutoGPT,classic-benchmark-ci.yml,"name: Classic - AGBenchmark CI

on:
  push:
    branches: [ master, dev, ci-test* ]
    paths:
      - 'classic/benchmark/**'
      - '!classic/benchmark/reports/**'
      - .github/workflows/classic-benchmark-ci.yml
  pull_request:
    branches: [ master, dev, release-* ]
    paths:
      - 'classic/benchmark/**'
      - '!classic/benchmark/reports/**'
      - .github/workflows/classic-benchmark-ci.yml

concurrency:
  group: ${{ format('benchmark-ci-{0}', github.head_ref && format('{0}-{1}', github.event_name, github.event.pull_request.number) || github.sha) }}
  cancel-in-progress: ${{ startsWith(github.event_name, 'pull_request') }}

defaults:
  run:
    shell: bash

env:
  min-python-version: '3.10'

jobs:
  test:
    permissions:
      contents: read
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        python-version: [""3.10""]
        platform-os: [ubuntu, macos, macos-arm64, windows]
    runs-on: ${{ matrix.platform-os != 'macos-arm64' && format('{0}-latest', matrix.platform-os) || 'macos-14' }}
    defaults:
      run:
        shell: bash
        working-directory: classic/benchmark
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          submodules: true

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Set up Python dependency cache
        # On Windows, unpacking cached dependencies takes longer than just installing them
        if: runner.os != 'Windows'
        uses: actions/cache@v4
        with:
          path: ${{ runner.os == 'macOS' && '~/Library/Caches/pypoetry' || '~/.cache/pypoetry' }}
          key: poetry-${{ runner.os }}-${{ hashFiles('classic/benchmark/poetry.lock') }}

      - name: Install Poetry (Unix)
        if: runner.os != 'Windows'
        run: |
          curl -sSL https://install.python-poetry.org | python3 -

          if [ ""${{ runner.os }}"" = ""macOS"" ]; then
            PATH=""$HOME/.local/bin:$PATH""
            echo ""$HOME/.local/bin"" >> $GITHUB_PATH
          fi

      - name: Install Poetry (Windows)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          (Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -

          $env:PATH += "";$env:APPDATA\Python\Scripts""
          echo ""$env:APPDATA\Python\Scripts"" >> $env:GITHUB_PATH

      - name: Install Python dependencies
        run: poetry install

      - name: Run pytest with coverage
        run: |
          poetry run pytest -vv \
            --cov=agbenchmark --cov-branch --cov-report term-missing --cov-report xml \
            --durations=10 \
            --junitxml=junit.xml -o junit_family=legacy \
            tests
        env:
          CI: true
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

      - name: Upload test results to Codecov
        if: ${{ !cancelled() }}  # Run even if tests fail
        uses: codecov/test-results-action@v1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload coverage reports to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          flags: agbenchmark,${{ runner.os }}

  self-test-with-agent:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        agent-name: [forge]
      fail-fast: false
    timeout-minutes: 20
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          submodules: true

      - name: Set up Python ${{ env.min-python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.min-python-version }}

      - name: Install Poetry
        run: |
          curl -sSL https://install.python-poetry.org | python -

      - name: Run regression tests
        working-directory: classic
        run: |
          ./run agent start ${{ matrix.agent-name }}
          cd ${{ matrix.agent-name }}

          set +e # Ignore non-zero exit codes and continue execution
          echo ""Running the following command: poetry run agbenchmark --maintain --mock""
          poetry run agbenchmark --maintain --mock
          EXIT_CODE=$?
          set -e  # Stop ignoring non-zero exit codes
          # Check if the exit code was 5, and if so, exit with 0 instead
          if [ $EXIT_CODE -eq 5 ]; then
            echo ""regression_tests.json is empty.""
          fi

          echo ""Running the following command: poetry run agbenchmark --mock""
          poetry run agbenchmark --mock

          echo ""Running the following command: poetry run agbenchmark --mock --category=data""
          poetry run agbenchmark --mock --category=data

          echo ""Running the following command: poetry run agbenchmark --mock --category=coding""
          poetry run agbenchmark --mock --category=coding

          # echo ""Running the following command: poetry run agbenchmark --test=WriteFile""
          # poetry run agbenchmark --test=WriteFile
          cd ../benchmark
          poetry install
          echo ""Adding the BUILD_SKILL_TREE environment variable. This will attempt to add new elements in the skill tree. If new elements are added, the CI fails because they should have been pushed""
          export BUILD_SKILL_TREE=true

          # poetry run agbenchmark --mock

          # CHANGED=$(git diff --name-only | grep -E '(agbenchmark/challenges)|(../classic/frontend/assets)') || echo ""No diffs""
          # if [ ! -z ""$CHANGED"" ]; then
          #   echo ""There are unstaged changes please run agbenchmark and commit those changes since they are needed.""
          #   echo ""$CHANGED""
          #   exit 1
          # else
          #   echo ""No unstaged changes.""
          # fi
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          TELEMETRY_ENVIRONMENT: autogpt-benchmark-ci
          TELEMETRY_OPT_IN: ${{ github.ref_name == 'master' }}
",176,2,2,"push, pull_request",7
Significant-Gravitas/AutoGPT,classic-benchmark_publish_package.yml,"name: Classic - Publish to PyPI

on:
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        submodules: true
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: 3.8

    - name: Install Poetry
      working-directory: ./classic/benchmark/
      run: |
        curl -sSL https://install.python-poetry.org | python3 -
        echo ""$HOME/.poetry/bin"" >> $GITHUB_PATH

    - name: Build project for distribution
      working-directory: ./classic/benchmark/
      run: poetry build

    - name: Install dependencies
      working-directory: ./classic/benchmark/
      run: poetry install

    - name: Check Version
      working-directory: ./classic/benchmark/
      id: check-version
      run: |
        echo version=$(poetry version --short) >> $GITHUB_OUTPUT

    - name: Create Release
      uses: ncipollo/release-action@v1
      with:
        artifacts: ""classic/benchmark/dist/*""
        token: ${{ secrets.GITHUB_TOKEN }}
        draft: false
        generateReleaseNotes: false
        tag: agbenchmark-v${{ steps.check-version.outputs.version }}
        commit: master

    - name: Build and publish
      working-directory: ./classic/benchmark/
      run: poetry publish -u __token__ -p ${{ secrets.PYPI_API_TOKEN }}
",55,1,1,workflow_dispatch,3
Significant-Gravitas/AutoGPT,classic-forge-ci.yml,"name: Classic - Forge CI

on:
  push:
    branches: [ master, dev, ci-test* ]
    paths:
      - '.github/workflows/classic-forge-ci.yml'
      - 'classic/forge/**'
      - '!classic/forge/tests/vcr_cassettes'
  pull_request:
    branches: [ master, dev, release-* ]
    paths:
      - '.github/workflows/classic-forge-ci.yml'
      - 'classic/forge/**'
      - '!classic/forge/tests/vcr_cassettes'

concurrency:
  group: ${{ format('forge-ci-{0}', github.head_ref && format('{0}-{1}', github.event_name, github.event.pull_request.number) || github.sha) }}
  cancel-in-progress: ${{ startsWith(github.event_name, 'pull_request') }}

defaults:
  run:
    shell: bash
    working-directory: classic/forge

jobs:
  test:
    permissions:
      contents: read
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        python-version: [""3.10""]
        platform-os: [ubuntu, macos, macos-arm64, windows]
    runs-on: ${{ matrix.platform-os != 'macos-arm64' && format('{0}-latest', matrix.platform-os) || 'macos-14' }}

    steps:
      # Quite slow on macOS (2~4 minutes to set up Docker)
      # - name: Set up Docker (macOS)
      #   if: runner.os == 'macOS'
      #   uses: crazy-max/ghaction-setup-docker@v3

      - name: Start MinIO service (Linux)
        if: runner.os == 'Linux'
        working-directory: '.'
        run: |
          docker pull minio/minio:edge-cicd
          docker run -d -p 9000:9000 minio/minio:edge-cicd

      - name: Start MinIO service (macOS)
        if: runner.os == 'macOS'
        working-directory: ${{ runner.temp }}
        run: |
          brew install minio/stable/minio
          mkdir data
          minio server ./data &

      # No MinIO on Windows:
      # - Windows doesn't support running Linux Docker containers
      # - It doesn't seem possible to start background processes on Windows. They are
      #   killed after the step returns.
      #   See: https://github.com/actions/runner/issues/598#issuecomment-2011890429

      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          submodules: true

      - name: Checkout cassettes
        if: ${{ startsWith(github.event_name, 'pull_request') }}
        env:
          PR_BASE: ${{ github.event.pull_request.base.ref }}
          PR_BRANCH: ${{ github.event.pull_request.head.ref }}
          PR_AUTHOR: ${{ github.event.pull_request.user.login }}
        run: |
          cassette_branch=""${PR_AUTHOR}-${PR_BRANCH}""
          cassette_base_branch=""${PR_BASE}""
          cd tests/vcr_cassettes

          if ! git ls-remote --exit-code --heads origin $cassette_base_branch ; then
            cassette_base_branch=""master""
          fi

          if git ls-remote --exit-code --heads origin $cassette_branch ; then
            git fetch origin $cassette_branch
            git fetch origin $cassette_base_branch

            git checkout $cassette_branch

            # Pick non-conflicting cassette updates from the base branch
            git merge --no-commit --strategy-option=ours origin/$cassette_base_branch
            echo ""Using cassettes from mirror branch '$cassette_branch',"" \
              ""synced to upstream branch '$cassette_base_branch'.""
          else
            git checkout -b $cassette_branch
            echo ""Branch '$cassette_branch' does not exist in cassette submodule."" \
              ""Using cassettes from '$cassette_base_branch'.""
          fi

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Set up Python dependency cache
        # On Windows, unpacking cached dependencies takes longer than just installing them
        if: runner.os != 'Windows'
        uses: actions/cache@v4
        with:
          path: ${{ runner.os == 'macOS' && '~/Library/Caches/pypoetry' || '~/.cache/pypoetry' }}
          key: poetry-${{ runner.os }}-${{ hashFiles('classic/forge/poetry.lock') }}

      - name: Install Poetry (Unix)
        if: runner.os != 'Windows'
        run: |
          curl -sSL https://install.python-poetry.org | python3 -

          if [ ""${{ runner.os }}"" = ""macOS"" ]; then
            PATH=""$HOME/.local/bin:$PATH""
            echo ""$HOME/.local/bin"" >> $GITHUB_PATH
          fi

      - name: Install Poetry (Windows)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          (Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -

          $env:PATH += "";$env:APPDATA\Python\Scripts""
          echo ""$env:APPDATA\Python\Scripts"" >> $env:GITHUB_PATH

      - name: Install Python dependencies
        run: poetry install

      - name: Run pytest with coverage
        run: |
          poetry run pytest -vv \
            --cov=forge --cov-branch --cov-report term-missing --cov-report xml \
            --durations=10 \
            --junitxml=junit.xml -o junit_family=legacy \
            forge
        env:
          CI: true
          PLAIN_OUTPUT: True
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          S3_ENDPOINT_URL: ${{ runner.os != 'Windows' && 'http://127.0.0.1:9000' || '' }}
          AWS_ACCESS_KEY_ID: minioadmin
          AWS_SECRET_ACCESS_KEY: minioadmin

      - name: Upload test results to Codecov
        if: ${{ !cancelled() }}  # Run even if tests fail
        uses: codecov/test-results-action@v1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload coverage reports to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          flags: forge,${{ runner.os }}

      - id: setup_git_auth
        name: Set up git token authentication
        # Cassettes may be pushed even when tests fail
        if: success() || failure()
        run: |
          config_key=""http.${{ github.server_url }}/.extraheader""
          if [ ""${{ runner.os }}"" = 'macOS' ]; then
            base64_pat=$(echo -n ""pat:${{ secrets.PAT_REVIEW }}"" | base64)
          else
            base64_pat=$(echo -n ""pat:${{ secrets.PAT_REVIEW }}"" | base64 -w0)
          fi

          git config ""$config_key"" \
            ""Authorization: Basic $base64_pat""

          cd tests/vcr_cassettes
          git config ""$config_key"" \
            ""Authorization: Basic $base64_pat""

          echo ""config_key=$config_key"" >> $GITHUB_OUTPUT

      - id: push_cassettes
        name: Push updated cassettes
        # For pull requests, push updated cassettes even when tests fail
        if: github.event_name == 'push' || (! github.event.pull_request.head.repo.fork && (success() || failure()))
        env:
          PR_BRANCH: ${{ github.event.pull_request.head.ref }}
          PR_AUTHOR: ${{ github.event.pull_request.user.login }}
        run: |
          if [ ""${{ startsWith(github.event_name, 'pull_request') }}"" = ""true"" ]; then
            is_pull_request=true
            cassette_branch=""${PR_AUTHOR}-${PR_BRANCH}""
          else
            cassette_branch=""${{ github.ref_name }}""
          fi

          cd tests/vcr_cassettes
          # Commit & push changes to cassettes if any
          if ! git diff --quiet; then
            git add .
            git commit -m ""Auto-update cassettes""
            git push origin HEAD:$cassette_branch
            if [ ! $is_pull_request ]; then
              cd ../..
              git add tests/vcr_cassettes
              git commit -m ""Update cassette submodule""
              git push origin HEAD:$cassette_branch
            fi
            echo ""updated=true"" >> $GITHUB_OUTPUT
          else
            echo ""updated=false"" >> $GITHUB_OUTPUT
            echo ""No cassette changes to commit""
          fi

      - name: Post Set up git token auth
        if: steps.setup_git_auth.outcome == 'success'
        run: |
          git config --unset-all '${{ steps.setup_git_auth.outputs.config_key }}'
          git submodule foreach git config --unset-all '${{ steps.setup_git_auth.outputs.config_key }}'

      - name: Apply ""behaviour change"" label and comment on PR
        if: ${{ startsWith(github.event_name, 'pull_request') }}
        run: |
          PR_NUMBER=""${{ github.event.pull_request.number }}""
          TOKEN=""${{ secrets.PAT_REVIEW }}""
          REPO=""${{ github.repository }}""

          if [[ ""${{ steps.push_cassettes.outputs.updated }}"" == ""true"" ]]; then
            echo ""Adding label and comment...""
            echo $TOKEN | gh auth login --with-token
            gh issue edit $PR_NUMBER --add-label ""behaviour change""
            gh issue comment $PR_NUMBER --body ""You changed AutoGPT's behaviour on ${{ runner.os }}. The cassettes have been updated and will be merged to the submodule when this Pull Request gets merged.""
          fi

      - name: Upload logs to artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-logs
          path: classic/forge/logs/
",243,1,2,"push, pull_request",7
Significant-Gravitas/AutoGPT,classic-frontend-ci.yml,"name: Classic - Frontend CI/CD

on:
  push:
    branches:
      - master
      - dev
      - 'ci-test*' # This will match any branch that starts with ""ci-test""
    paths:
      - 'classic/frontend/**'
      - '.github/workflows/classic-frontend-ci.yml'
  pull_request:
    paths:
      - 'classic/frontend/**'
      - '.github/workflows/classic-frontend-ci.yml'

jobs:
  build:
    permissions:
      contents: write
      pull-requests: write
    runs-on: ubuntu-latest
    env:
      BUILD_BRANCH: ${{ format('classic-frontend-build/{0}', github.ref_name) }}

    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Setup Flutter
        uses: subosito/flutter-action@v2
        with:
          flutter-version: '3.13.2'

      - name: Build Flutter to Web
        run: |
          cd classic/frontend
          flutter build web --base-href /app/

      # - name: Commit and Push to ${{ env.BUILD_BRANCH }}
      #   if: github.event_name == 'push'
      #   run: |
      #     git config --local user.email ""action@github.com""
      #     git config --local user.name ""GitHub Action""
      #     git add classic/frontend/build/web
      #     git checkout -B ${{ env.BUILD_BRANCH }}
      #     git commit -m ""Update frontend build to ${GITHUB_SHA:0:7}"" -a
      #     git push -f origin ${{ env.BUILD_BRANCH }}

      - name: Create PR ${{ env.BUILD_BRANCH }} -> ${{ github.ref_name }}
        if: github.event_name == 'push'
        uses: peter-evans/create-pull-request@v7
        with:
          add-paths: classic/frontend/build/web
          base: ${{ github.ref_name }}
          branch: ${{ env.BUILD_BRANCH }}
          delete-branch: true
          title: ""Update frontend build in `${{ github.ref_name }}`""
          body: ""This PR updates the frontend build based on commit ${{ github.sha }}.""
          commit-message: ""Update frontend build based on commit ${{ github.sha }}""
",60,1,2,"push, pull_request",3
Significant-Gravitas/AutoGPT,classic-python-checks.yml,"name: Classic - Python checks

on:
  push:
    branches: [ master, dev, ci-test* ]
    paths:
      - '.github/workflows/classic-python-checks-ci.yml'
      - 'classic/original_autogpt/**'
      - 'classic/forge/**'
      - 'classic/benchmark/**'
      - '**.py'
      - '!classic/forge/tests/vcr_cassettes'
  pull_request:
    branches: [ master, dev, release-* ]
    paths:
      - '.github/workflows/classic-python-checks-ci.yml'
      - 'classic/original_autogpt/**'
      - 'classic/forge/**'
      - 'classic/benchmark/**'
      - '**.py'
      - '!classic/forge/tests/vcr_cassettes'

concurrency:
  group: ${{ format('classic-python-checks-ci-{0}', github.head_ref && format('{0}-{1}', github.event_name, github.event.pull_request.number) || github.sha) }}
  cancel-in-progress: ${{ startsWith(github.event_name, 'pull_request') }}

defaults:
  run:
    shell: bash

jobs:
  get-changed-parts:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - id: changes-in
        name: Determine affected subprojects
        uses: dorny/paths-filter@v3
        with:
          filters: |
            original_autogpt:
              - classic/original_autogpt/autogpt/**
              - classic/original_autogpt/tests/**
              - classic/original_autogpt/poetry.lock
            forge:
              - classic/forge/forge/**
              - classic/forge/tests/**
              - classic/forge/poetry.lock
            benchmark:
              - classic/benchmark/agbenchmark/**
              - classic/benchmark/tests/**
              - classic/benchmark/poetry.lock
    outputs:
      changed-parts: ${{ steps.changes-in.outputs.changes }}

  lint:
    needs: get-changed-parts
    runs-on: ubuntu-latest
    env:
      min-python-version: ""3.10""

    strategy:
      matrix:
        sub-package: ${{ fromJson(needs.get-changed-parts.outputs.changed-parts) }}
      fail-fast: false

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python ${{ env.min-python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.min-python-version }}

      - name: Set up Python dependency cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/pypoetry
          key: ${{ runner.os }}-poetry-${{ hashFiles(format('{0}/poetry.lock', matrix.sub-package)) }}

      - name: Install Poetry
        run: curl -sSL https://install.python-poetry.org | python3 -

      # Install dependencies

      - name: Install Python dependencies
        run: poetry -C classic/${{ matrix.sub-package }} install

      # Lint

      - name: Lint (isort)
        run: poetry run isort --check .
        working-directory: classic/${{ matrix.sub-package }}

      - name: Lint (Black)
        if: success() || failure()
        run: poetry run black --check .
        working-directory: classic/${{ matrix.sub-package }}

      - name: Lint (Flake8)
        if: success() || failure()
        run: poetry run flake8 .
        working-directory: classic/${{ matrix.sub-package }}

  types:
    needs: get-changed-parts
    runs-on: ubuntu-latest
    env:
      min-python-version: ""3.10""

    strategy:
      matrix:
        sub-package: ${{ fromJson(needs.get-changed-parts.outputs.changed-parts) }}
      fail-fast: false

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python ${{ env.min-python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.min-python-version }}

      - name: Set up Python dependency cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/pypoetry
          key: ${{ runner.os }}-poetry-${{ hashFiles(format('{0}/poetry.lock', matrix.sub-package)) }}

      - name: Install Poetry
        run: curl -sSL https://install.python-poetry.org | python3 -

      # Install dependencies

      - name: Install Python dependencies
        run: poetry -C classic/${{ matrix.sub-package }} install

      # Typecheck

      - name: Typecheck
        if: success() || failure()
        run: poetry run pyright
        working-directory: classic/${{ matrix.sub-package }}
",151,3,2,"push, pull_request",8
Significant-Gravitas/AutoGPT,claude.yml,"name: Claude Code

on:
  issue_comment:
    types: [created]
  pull_request_review_comment:
    types: [created]
  issues:
    types: [opened, assigned]
  pull_request_review:
    types: [submitted]

jobs:
  claude:
    if: |
      (
        (github.event_name == 'issue_comment' && contains(github.event.comment.body, '@claude')) ||
        (github.event_name == 'pull_request_review_comment' && contains(github.event.comment.body, '@claude')) ||
        (github.event_name == 'pull_request_review' && contains(github.event.review.body, '@claude')) ||
        (github.event_name == 'issues' && (contains(github.event.issue.body, '@claude') || contains(github.event.issue.title, '@claude')))
      ) && (
        github.event.comment.author_association == 'OWNER' ||
        github.event.comment.author_association == 'MEMBER' ||
        github.event.comment.author_association == 'COLLABORATOR' ||
        github.event.review.author_association == 'OWNER' ||
        github.event.review.author_association == 'MEMBER' ||
        github.event.review.author_association == 'COLLABORATOR' ||
        github.event.issue.author_association == 'OWNER' ||
        github.event.issue.author_association == 'MEMBER' ||
        github.event.issue.author_association == 'COLLABORATOR'
      )
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: read
      issues: read
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
      - name: Run Claude Code
        id: claude
        uses: anthropics/claude-code-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
",47,1,4,"issue_comment, pull_request_review_comment, issues, pull_request_review",2
Significant-Gravitas/AutoGPT,codeql.yml,"# For most projects, this workflow file will not need changing; you simply need
# to commit it to your repository.
#
# You may wish to alter this file to override the set of languages analyzed,
# or to provide custom queries or build logic.
#
# ******** NOTE ********
# We have attempted to detect the languages in your repository. Please check
# the `language` matrix defined below to confirm you have the correct set of
# supported CodeQL languages.
#
name: ""CodeQL""

on:
  push:
    branches: [ ""master"", ""release-*"", ""dev"" ]
  pull_request:
    branches: [ ""master"", ""release-*"", ""dev"" ]
  merge_group:
  schedule:
    - cron: '15 4 * * 0'

jobs:
  analyze:
    name: Analyze (${{ matrix.language }})
    # Runner size impacts CodeQL analysis time. To learn more, please see:
    #   - https://gh.io/recommended-hardware-resources-for-running-codeql
    #   - https://gh.io/supported-runners-and-hardware-resources
    #   - https://gh.io/using-larger-runners (GitHub.com only)
    # Consider using larger runners or machines with greater resources for possible analysis time improvements.
    runs-on: ${{ (matrix.language == 'swift' && 'macos-latest') || 'ubuntu-latest' }}
    permissions:
      # required for all workflows
      security-events: write

      # required to fetch internal or private CodeQL packs
      packages: read

      # only required for workflows in private repositories
      actions: read
      contents: read

    strategy:
      fail-fast: false
      matrix:
        include:
        - language: typescript
          build-mode: none
        - language: python
          build-mode: none
        # CodeQL supports the following values keywords for 'language': 'c-cpp', 'csharp', 'go', 'java-kotlin', 'javascript-typescript', 'python', 'ruby', 'swift'
        # Use `c-cpp` to analyze code written in C, C++ or both
        # Use 'java-kotlin' to analyze code written in Java, Kotlin or both
        # Use 'javascript-typescript' to analyze code written in JavaScript, TypeScript or both
        # To learn more about changing the languages that are analyzed or customizing the build mode for your analysis,
        # see https://docs.github.com/en/code-security/code-scanning/creating-an-advanced-setup-for-code-scanning/customizing-your-advanced-setup-for-code-scanning.
        # If you are analyzing a compiled language, you can modify the 'build-mode' for that language to customize how
        # your codebase is analyzed, see https://docs.github.com/en/code-security/code-scanning/creating-an-advanced-setup-for-code-scanning/codeql-code-scanning-for-compiled-languages
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    # Initializes the CodeQL tools for scanning.
    - name: Initialize CodeQL
      uses: github/codeql-action/init@v3
      with:
        languages: ${{ matrix.language }}
        build-mode: ${{ matrix.build-mode }}
        # If you wish to specify custom queries, you can do so here or in a config file.
        # By default, queries listed here will override any specified in a config file.
        # Prefix the list here with ""+"" to use these queries and those in the config file.
        config: |
          paths-ignore:
            - classic/frontend/build/**

        # For more details on CodeQL's query packs, refer to: https://docs.github.com/en/code-security/code-scanning/automatically-scanning-your-code-for-vulnerabilities-and-errors/configuring-code-scanning#using-queries-in-ql-packs
        # queries: security-extended,security-and-quality

    # If the analyze step fails for one of the languages you are analyzing with
    # ""We were unable to automatically build your code"", modify the matrix above
    # to set the build mode to ""manual"" for that language. Then modify this step
    # to build your code.
    # ℹ️ Command-line programs to run using the OS shell.
    # 📚 See https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#jobsjob_idstepsrun
    - if: matrix.build-mode == 'manual'
      shell: bash
      run: |
        echo 'If you are using a ""manual"" build mode for one or more of the' \
          'languages you are analyzing, replace this with the commands to build' \
          'your code, for example:'
        echo '  make bootstrap'
        echo '  make release'
        exit 1

    - name: Perform CodeQL Analysis
      uses: github/codeql-action/analyze@v3
      with:
        category: ""/language:${{matrix.language}}""
",98,1,4,"push, pull_request, merge_group, schedule",3
Significant-Gravitas/AutoGPT,platform-autgpt-deploy-prod.yml,"name: AutoGPT Platform - Deploy Prod Environment

on:
  release:
    types: [published]

permissions:
  contents: 'read'
  id-token: 'write'

jobs:
  migrate:
    environment: production
    name: Run migrations for AutoGPT Platform
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install prisma

      - name: Run Backend Migrations
        working-directory: ./autogpt_platform/backend
        run: |
          python -m prisma migrate deploy
        env:
          DATABASE_URL: ${{ secrets.BACKEND_DATABASE_URL }}
          DIRECT_URL: ${{ secrets.BACKEND_DATABASE_URL }}

          
  trigger:
    needs: migrate
    runs-on: ubuntu-latest
    steps:
      - name: Trigger deploy workflow
        uses: peter-evans/repository-dispatch@v3
        with:
          token: ${{ secrets.DEPLOY_TOKEN }}
          repository: Significant-Gravitas/AutoGPT_cloud_infrastructure
          event-type: build_deploy_prod
          client-payload: '{""ref"": ""${{ github.ref }}"", ""sha"": ""${{ github.sha }}"", ""repository"": ""${{ github.repository }}""}'",50,2,1,release,3
Significant-Gravitas/AutoGPT,platform-autogpt-deploy-dev.yaml,"name: AutoGPT Platform - Deploy Dev Environment

on:
  push:
    branches: [ dev ]
    paths:
      - 'autogpt_platform/**'

permissions:
  contents: 'read'
  id-token: 'write'

jobs:
  migrate:
    environment: develop
    name: Run migrations for AutoGPT Platform
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install prisma

      - name: Run Backend Migrations
        working-directory: ./autogpt_platform/backend
        run: |
          python -m prisma migrate deploy
        env:
          DATABASE_URL: ${{ secrets.BACKEND_DATABASE_URL }}
          DIRECT_URL: ${{ secrets.BACKEND_DATABASE_URL }}

  trigger:
    needs: migrate
    runs-on: ubuntu-latest
    steps:
      - name: Trigger deploy workflow
        uses: peter-evans/repository-dispatch@v3
        with:
          token: ${{ secrets.DEPLOY_TOKEN }}
          repository: Significant-Gravitas/AutoGPT_cloud_infrastructure
          event-type: build_deploy_dev
          client-payload: '{""ref"": ""${{ github.ref }}"", ""sha"": ""${{ github.sha }}"", ""repository"": ""${{ github.repository }}""}'
",51,2,1,push,3
Significant-Gravitas/AutoGPT,platform-backend-ci.yml,"name: AutoGPT Platform - Backend CI

on:
  push:
    branches: [master, dev, ci-test*]
    paths:
      - "".github/workflows/platform-backend-ci.yml""
      - ""autogpt_platform/backend/**""
      - ""autogpt_platform/autogpt_libs/**""
  pull_request:
    branches: [master, dev, release-*]
    paths:
      - "".github/workflows/platform-backend-ci.yml""
      - ""autogpt_platform/backend/**""
      - ""autogpt_platform/autogpt_libs/**""
  merge_group:

concurrency:
  group: ${{ format('backend-ci-{0}', github.head_ref && format('{0}-{1}', github.event_name, github.event.pull_request.number) || github.sha) }}
  cancel-in-progress: ${{ startsWith(github.event_name, 'pull_request') }}

defaults:
  run:
    shell: bash
    working-directory: autogpt_platform/backend

jobs:
  test:
    permissions:
      contents: read
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        python-version: [""3.11""]
    runs-on: ubuntu-latest

    services:
      redis:
        image: bitnami/redis:6.2
        env:
          REDIS_PASSWORD: testpassword
        ports:
          - 6379:6379
      rabbitmq:
        image: rabbitmq:3.12-management
        ports:
          - 5672:5672
          - 15672:15672
        env:
          RABBITMQ_DEFAULT_USER: ${{ env.RABBITMQ_DEFAULT_USER }}
          RABBITMQ_DEFAULT_PASS: ${{ env.RABBITMQ_DEFAULT_PASS }}
      clamav:
        image: clamav/clamav-debian:latest
        ports:
          - 3310:3310
        env:
          CLAMAV_NO_FRESHCLAMD: false
          CLAMD_CONF_StreamMaxLength: 50M
          CLAMD_CONF_MaxFileSize: 100M
          CLAMD_CONF_MaxScanSize: 100M
          CLAMD_CONF_MaxThreads: 4
          CLAMD_CONF_ReadTimeout: 300
        options: >-
          --health-cmd ""clamdscan --version || exit 1""
          --health-interval 30s
          --health-timeout 10s
          --health-retries 5
          --health-start-period 180s

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          submodules: true

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Setup Supabase
        uses: supabase/setup-cli@v1
        with:
          version: 1.178.1

      - id: get_date
        name: Get date
        run: echo ""date=$(date +'%Y-%m-%d')"" >> $GITHUB_OUTPUT

      - name: Set up Python dependency cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/pypoetry
          key: poetry-${{ runner.os }}-${{ hashFiles('autogpt_platform/backend/poetry.lock') }}

      - name: Install Poetry (Unix)
        run: |
          # Extract Poetry version from backend/poetry.lock
          HEAD_POETRY_VERSION=$(python ../../.github/workflows/scripts/get_package_version_from_lockfile.py poetry)
          echo ""Found Poetry version ${HEAD_POETRY_VERSION} in backend/poetry.lock""

          if [ -n ""$BASE_REF"" ]; then
            BASE_BRANCH=${BASE_REF/refs\/heads\//}
            BASE_POETRY_VERSION=$((git show ""origin/$BASE_BRANCH"":./poetry.lock; true) | python ../../.github/workflows/scripts/get_package_version_from_lockfile.py poetry -)
            echo ""Found Poetry version ${BASE_POETRY_VERSION} in backend/poetry.lock on ${BASE_REF}""
            POETRY_VERSION=$(printf '%s\n' ""$HEAD_POETRY_VERSION"" ""$BASE_POETRY_VERSION"" | sort -V | tail -n1)
          else
            POETRY_VERSION=$HEAD_POETRY_VERSION
          fi
          echo ""Using Poetry version ${POETRY_VERSION}""

          # Install Poetry
          curl -sSL https://install.python-poetry.org | POETRY_VERSION=$POETRY_VERSION python3 -

          if [ ""${{ runner.os }}"" = ""macOS"" ]; then
            PATH=""$HOME/.local/bin:$PATH""
            echo ""$HOME/.local/bin"" >> $GITHUB_PATH
          fi
        env:
          BASE_REF: ${{ github.base_ref || github.event.merge_group.base_ref }}

      - name: Check poetry.lock
        run: |
          poetry lock

          if ! git diff --quiet --ignore-matching-lines=""^# "" poetry.lock; then
            echo ""Error: poetry.lock not up to date.""
            echo
            git diff poetry.lock
            exit 1
          fi

      - name: Install Python dependencies
        run: poetry install

      - name: Generate Prisma Client
        run: poetry run prisma generate

      - id: supabase
        name: Start Supabase
        working-directory: .
        run: |
          supabase init
          supabase start --exclude postgres-meta,realtime,storage-api,imgproxy,inbucket,studio,edge-runtime,logflare,vector,supavisor
          supabase status -o env | sed 's/=""/=/; s/""$//' >> $GITHUB_OUTPUT
        # outputs:
        # DB_URL, API_URL, GRAPHQL_URL, ANON_KEY, SERVICE_ROLE_KEY, JWT_SECRET

      - name: Wait for ClamAV to be ready
        run: |
          echo ""Waiting for ClamAV daemon to start...""
          max_attempts=60
          attempt=0
          
          until nc -z localhost 3310 || [ $attempt -eq $max_attempts ]; do
            echo ""ClamAV is unavailable - sleeping (attempt $((attempt+1))/$max_attempts)""
            sleep 5
            attempt=$((attempt+1))
          done
          
          if [ $attempt -eq $max_attempts ]; then
            echo ""ClamAV failed to start after $((max_attempts*5)) seconds""
            echo ""Checking ClamAV service logs...""
            docker logs $(docker ps -q --filter ""ancestor=clamav/clamav-debian:latest"") 2>&1 | tail -50 || echo ""No ClamAV container found""
            exit 1
          fi
          
          echo ""ClamAV is ready!""
          
          # Verify ClamAV is responsive
          echo ""Testing ClamAV connection...""
          timeout 10 bash -c 'echo ""PING"" | nc localhost 3310' || {
            echo ""ClamAV is not responding to PING""
            docker logs $(docker ps -q --filter ""ancestor=clamav/clamav-debian:latest"") 2>&1 | tail -50 || echo ""No ClamAV container found""
            exit 1
          }

      - name: Run Database Migrations
        run: poetry run prisma migrate dev --name updates
        env:
          DATABASE_URL: ${{ steps.supabase.outputs.DB_URL }}
          DIRECT_URL: ${{ steps.supabase.outputs.DB_URL }}

      - id: lint
        name: Run Linter
        run: poetry run lint

      - name: Run pytest with coverage
        run: |
          if [[ ""${{ runner.debug }}"" == ""1"" ]]; then
            poetry run pytest -s -vv -o log_cli=true -o log_cli_level=DEBUG test
          else
            poetry run pytest -s -vv test
          fi
        if: success() || (failure() && steps.lint.outcome == 'failure')
        env:
          LOG_LEVEL: ${{ runner.debug && 'DEBUG' || 'INFO' }}
          DATABASE_URL: ${{ steps.supabase.outputs.DB_URL }}
          DIRECT_URL: ${{ steps.supabase.outputs.DB_URL }}
          SUPABASE_URL: ${{ steps.supabase.outputs.API_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ steps.supabase.outputs.SERVICE_ROLE_KEY }}
          SUPABASE_JWT_SECRET: ${{ steps.supabase.outputs.JWT_SECRET }}
          REDIS_HOST: ""localhost""
          REDIS_PORT: ""6379""
          REDIS_PASSWORD: ""testpassword""

    env:
      CI: true
      PLAIN_OUTPUT: True
      RUN_ENV: local
      PORT: 8080
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      # We know these are here, don't report this as a security vulnerability
      # This is used as the default credential for the entire system's RabbitMQ instance
      # If you want to replace this, you can do so by making our entire system generate
      # new credentials for each local user and update the environment variables in
      # the backend service, docker composes, and examples
      RABBITMQ_DEFAULT_USER: ""rabbitmq_user_default""
      RABBITMQ_DEFAULT_PASS: ""k0VMxyIJF9S35f3x2uaw5IWAl6Y536O7""

      # - name: Upload coverage reports to Codecov
      #   uses: codecov/codecov-action@v4
      #   with:
      #     token: ${{ secrets.CODECOV_TOKEN }}
      #     flags: backend,${{ runner.os }}
",227,1,3,"push, pull_request, merge_group",5
Significant-Gravitas/AutoGPT,platform-dev-deploy-event-dispatcher.yml,"name: AutoGPT Platform - Dev Deploy PR Event Dispatcher

on:
  pull_request:
    types: [closed]
  issue_comment:
    types: [created]

permissions:
  issues: write
  pull-requests: write

jobs:
  dispatch:
    runs-on: ubuntu-latest
    steps:
      - name: Check comment permissions and deployment status
        id: check_status
        if: github.event_name == 'issue_comment' && github.event.issue.pull_request
        uses: actions/github-script@v7
        with:
          script: |
            const commentBody = context.payload.comment.body.trim();
            const commentUser = context.payload.comment.user.login;
            const prAuthor = context.payload.issue.user.login;
            const authorAssociation = context.payload.comment.author_association;
            
            // Check permissions
            const hasPermission = (
              authorAssociation === 'OWNER' ||
              authorAssociation === 'MEMBER' ||
              authorAssociation === 'COLLABORATOR'
            );
            
            core.setOutput('comment_body', commentBody);
            core.setOutput('has_permission', hasPermission);
            
            if (!hasPermission && (commentBody === '!deploy' || commentBody === '!undeploy')) {
              core.setOutput('permission_denied', 'true');
              return;
            }
            
            if (commentBody !== '!deploy' && commentBody !== '!undeploy') {
              return;
            }
            
            // Process deploy command
            if (commentBody === '!deploy') {
              core.setOutput('should_deploy', 'true');
            }
            // Process undeploy command
            else if (commentBody === '!undeploy') {
              core.setOutput('should_undeploy', 'true');
            }

      - name: Post permission denied comment
        if: steps.check_status.outputs.permission_denied == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: `❌ **Permission denied**: Only the repository owners, members, or collaborators can use deployment commands.`
            });

      - name: Get PR details for deployment
        id: pr_details
        if: steps.check_status.outputs.should_deploy == 'true' || steps.check_status.outputs.should_undeploy == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const pr = await github.rest.pulls.get({
              owner: context.repo.owner,
              repo: context.repo.repo,
              pull_number: context.issue.number
            });
            core.setOutput('pr_number', pr.data.number);
            core.setOutput('pr_title', pr.data.title);
            core.setOutput('pr_state', pr.data.state);
          
      - name: Dispatch Deploy Event
        if: steps.check_status.outputs.should_deploy == 'true'
        uses: peter-evans/repository-dispatch@v3
        with:
          token: ${{ secrets.DISPATCH_TOKEN }}
          repository: Significant-Gravitas/AutoGPT_cloud_infrastructure
          event-type: pr-event
          client-payload: |
            {
              ""action"": ""deploy"",
              ""pr_number"": ""${{ steps.pr_details.outputs.pr_number }}"",
              ""pr_title"": ""${{ steps.pr_details.outputs.pr_title }}"",
              ""pr_state"": ""${{ steps.pr_details.outputs.pr_state }}"",
              ""repo"": ""${{ github.repository }}""
            }

      - name: Post deploy success comment
        if: steps.check_status.outputs.should_deploy == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: `🚀 **Deploying PR #${{ steps.pr_details.outputs.pr_number }}** to development environment...`
            });

      - name: Dispatch Undeploy Event (from comment)
        if: steps.check_status.outputs.should_undeploy == 'true'
        uses: peter-evans/repository-dispatch@v3
        with:
          token: ${{ secrets.DISPATCH_TOKEN }}
          repository: Significant-Gravitas/AutoGPT_cloud_infrastructure
          event-type: pr-event
          client-payload: |
            {
              ""action"": ""undeploy"",
              ""pr_number"": ""${{ steps.pr_details.outputs.pr_number }}"",
              ""pr_title"": ""${{ steps.pr_details.outputs.pr_title }}"",
              ""pr_state"": ""${{ steps.pr_details.outputs.pr_state }}"",
              ""repo"": ""${{ github.repository }}""
            }

      - name: Post undeploy success comment
        if: steps.check_status.outputs.should_undeploy == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: `🗑️ **Undeploying PR #${{ steps.pr_details.outputs.pr_number }}** from development environment...`
            });

      - name: Check deployment status on PR close
        id: check_pr_close
        if: github.event_name == 'pull_request' && github.event.action == 'closed'
        uses: actions/github-script@v7
        with:
          script: |
            const comments = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number
            });
            
            let lastDeployIndex = -1;
            let lastUndeployIndex = -1;
            
            comments.data.forEach((comment, index) => {
              if (comment.body.trim() === '!deploy') {
                lastDeployIndex = index;
              } else if (comment.body.trim() === '!undeploy') {
                lastUndeployIndex = index;
              }
            });
            
            // Should undeploy if there's a !deploy without a subsequent !undeploy
            const shouldUndeploy = lastDeployIndex !== -1 && lastDeployIndex > lastUndeployIndex;
            core.setOutput('should_undeploy', shouldUndeploy);
            
      - name: Dispatch Undeploy Event (PR closed with active deployment)
        if: >-
          github.event_name == 'pull_request' &&
          github.event.action == 'closed' &&
          steps.check_pr_close.outputs.should_undeploy == 'true'
        uses: peter-evans/repository-dispatch@v3
        with:
          token: ${{ secrets.DISPATCH_TOKEN }}
          repository: Significant-Gravitas/AutoGPT_cloud_infrastructure
          event-type: pr-event
          client-payload: |
            {
              ""action"": ""undeploy"",
              ""pr_number"": ""${{ github.event.pull_request.number }}"",
              ""pr_title"": ""${{ github.event.pull_request.title }}"",
              ""pr_state"": ""${{ github.event.pull_request.state }}"",
              ""repo"": ""${{ github.repository }}""
            }

      - name: Post PR close undeploy comment
        if: >-
          github.event_name == 'pull_request' &&
          github.event.action == 'closed' &&
          steps.check_pr_close.outputs.should_undeploy == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: `🧹 **Auto-undeploying**: PR closed with active deployment. Cleaning up development environment for PR #${{ github.event.pull_request.number }}.`
            });",198,1,2,"pull_request, issue_comment",10
Significant-Gravitas/AutoGPT,platform-frontend-ci.yml,"name: AutoGPT Platform - Frontend CI

on:
  push:
    branches: [master, dev]
    paths:
      - "".github/workflows/platform-frontend-ci.yml""
      - ""autogpt_platform/frontend/**""
  pull_request:
    paths:
      - "".github/workflows/platform-frontend-ci.yml""
      - ""autogpt_platform/frontend/**""
  merge_group:

defaults:
  run:
    shell: bash
    working-directory: autogpt_platform/frontend

jobs:
  lint:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ""21""

      - name: Enable corepack
        run: corepack enable

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Run lint
        run: pnpm lint

  type-check:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ""21""

      - name: Enable corepack
        run: corepack enable

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Generate API client
        run: pnpm generate:api-client

      - name: Run tsc check
        run: pnpm type-check

  chromatic:
    runs-on: ubuntu-latest
    # Only run on dev branch pushes or PRs targeting dev
    if: github.ref == 'refs/heads/dev' || github.base_ref == 'dev'
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ""21""

      - name: Enable corepack
        run: corepack enable

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Run Chromatic
        uses: chromaui/action@latest
        with:
          projectToken: chpt_9e7c1a76478c9c8
          onlyChanged: true
          workingDir: autogpt_platform/frontend
          token: ${{ secrets.GITHUB_TOKEN }}

  test:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        browser: [chromium, webkit]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ""21""

      - name: Enable corepack
        run: corepack enable

      - name: Free Disk Space (Ubuntu)
        uses: jlumbroso/free-disk-space@main
        with:
          large-packages: false # slow
          docker-images: false # limited benefit

      - name: Copy default supabase .env
        run: |
          cp ../.env.example ../.env

      - name: Copy backend .env
        run: |
          cp ../backend/.env.example ../backend/.env

      - name: Run docker compose
        run: |
          docker compose -f ../docker-compose.yml up -d

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Setup .env
        run: cp .env.example .env

      - name: Build frontend
        run: pnpm build --turbo
        # uses Turbopack, much faster and safe enough for a test pipeline

      - name: Install Browser '${{ matrix.browser }}'
        run: pnpm playwright install --with-deps ${{ matrix.browser }}

      - name: Run Playwright tests
        run: pnpm test:no-build --project=${{ matrix.browser }}

      - name: Print Final Docker Compose logs
        if: always()
        run: docker compose -f ../docker-compose.yml logs

      - uses: actions/upload-artifact@v4
        if: ${{ !cancelled() }}
        with:
          name: playwright-report-${{ matrix.browser }}
          path: playwright-report/
          retention-days: 30
",156,4,3,"push, pull_request, merge_group",11
Significant-Gravitas/AutoGPT,repo-close-stale-issues.yml,"name: Repo - Close stale issues
on:
  schedule:
    - cron: '30 1 * * *'
  workflow_dispatch:

permissions:
  issues: write

jobs:
  stale:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/stale@v9
        with:
          # operations-per-run: 5000
          stale-issue-message: >
            This issue has automatically been marked as _stale_ because it has not had
            any activity in the last 170 days. You can _unstale_ it by commenting or
            removing the label. Otherwise, this issue will be closed in 10 days.
          stale-pr-message: >
            This pull request has automatically been marked as _stale_ because it has
            not had any activity in the last 50 days. You can _unstale_ it by commenting
            or removing the label.
          close-issue-message: >
            This issue was closed automatically because it has been stale for 10 days
            with no activity.
          days-before-stale: 170
          days-before-close: 10
          # Do not touch meta issues:
          exempt-issue-labels: meta,fridge,project management
          # Do not affect pull requests:
          days-before-pr-stale: -1
          days-before-pr-close: -1
",34,1,2,"schedule, workflow_dispatch",1
Significant-Gravitas/AutoGPT,repo-pr-enforce-base-branch.yml,"name: Repo - Enforce dev as base branch
on:
  pull_request_target:
    branches: [ master ]
    types: [ opened ]

jobs:
  check_pr_target:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
    steps:
      - name: Check if PR is from dev or hotfix
        if: ${{ !(startsWith(github.event.pull_request.head.ref, 'hotfix/') || github.event.pull_request.head.ref == 'dev') }}
        run: |
          gh pr comment ${{ github.event.number }} --repo ""$REPO"" \
            --body $'This PR targets the `master` branch but does not come from `dev` or a `hotfix/*` branch.\n\nAutomatically setting the base branch to `dev`.'
          gh pr edit ${{ github.event.number }} --base dev --repo ""$REPO""
        env:
          GITHUB_TOKEN: ${{ github.token }}
          REPO: ${{ github.repository }}
",21,1,1,pull_request_target,0
Significant-Gravitas/AutoGPT,repo-pr-label.yml,"name: Repo - Pull Request auto-label

on:
  # So that PRs touching the same files as the push are updated
  push:
    branches: [ master, dev, release-* ]
    paths-ignore:
      - 'classic/forge/tests/vcr_cassettes'
      - 'classic/benchmark/reports/**'
  # So that the `dirtyLabel` is removed if conflicts are resolve
  # We recommend `pull_request_target` so that github secrets are available.
  # In `pull_request` we wouldn't be able to change labels of fork PRs
  pull_request_target:
    types: [ opened, synchronize ]

concurrency:
  group: ${{ format('pr-label-{0}', github.event.pull_request.number || github.sha) }}
  cancel-in-progress: true

jobs:
  conflicts:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
    steps:
      - name: Update PRs with conflict labels
        uses: eps1lon/actions-label-merge-conflict@releases/2.x
        with:
          dirtyLabel: ""conflicts""
          #removeOnDirtyLabel: ""PR: ready to ship""
          repoToken: ""${{ secrets.GITHUB_TOKEN }}""
          commentOnDirty: ""This pull request has conflicts with the base branch, please resolve those so we can evaluate the pull request.""
          commentOnClean: ""Conflicts have been resolved! 🎉 A maintainer will review the pull request shortly.""

  size:
    if: ${{ github.event_name == 'pull_request_target' }}
    permissions:
      issues: write
      pull-requests: write
    runs-on: ubuntu-latest
    steps:
      - uses: codelytv/pr-size-labeler@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          xs_label: 'size/xs'
          xs_max_size: 2
          s_label: 'size/s'
          s_max_size: 10
          m_label: 'size/m'
          m_max_size: 100
          l_label: 'size/l'
          l_max_size: 500
          xl_label: 'size/xl'
          message_if_xl:

  scope:
    if: ${{ github.event_name == 'pull_request_target' }}
    permissions:
      contents: read
      pull-requests: write
    runs-on: ubuntu-latest
    steps:
      - uses: actions/labeler@v5
        with:
          sync-labels: true
",66,3,2,"push, pull_request_target",3
Significant-Gravitas/AutoGPT,repo-stats.yml,"name: Repo - Github Stats

on:
  schedule:
    # Run this once per day, towards the end of the day for keeping the most
    # recent data point most meaningful (hours are interpreted in UTC).
    - cron: ""0 23 * * *""
  workflow_dispatch: # Allow for running this manually.

jobs:
  j1:
    name: github-repo-stats
    runs-on: ubuntu-latest
    steps:
      - name: run-ghrs
        # Use latest release.
        uses: jgehrcke/github-repo-stats@HEAD
        with:
          ghtoken: ${{ secrets.ghrs_github_api_token }}

",20,1,2,"schedule, workflow_dispatch",1
Significant-Gravitas/AutoGPT,repo-workflow-checker.yml,"name: Repo - PR Status Checker
on:
  pull_request:
    types: [opened, synchronize, reopened]
  merge_group:

jobs:
  status-check:
    name: Check PR Status
    runs-on: ubuntu-latest
    steps:
      # - name: Wait some time for all actions to start
      #   run: sleep 30
      - uses: actions/checkout@v4
        # with:
          # fetch-depth: 0
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ""3.10""
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests
      - name: Check PR Status
        run: |
          echo ""Current directory before running Python script:""
          pwd
          echo ""Attempting to run Python script:""
          python .github/workflows/scripts/check_actions_status.py
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
",32,1,2,"pull_request, merge_group",2
microsoft/vscode,basic.yml,"name: Basic checks

on: workflow_dispatch
permissions: {}

# on:
#   push:
#     branches:
#       - main
#   pull_request:
#     branches:
#       - main

jobs:
  main:
    if: github.ref != 'refs/heads/main'
    name: Compilation, Unit and Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 40
    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false

      # TODO: rename azure-pipelines/linux/xvfb.init to github-actions
      - name: Setup Build Environment
        run: |
          sudo cp build/azure-pipelines/linux/xvfb.init /etc/init.d/xvfb
          sudo chmod +x /etc/init.d/xvfb
          sudo update-rc.d xvfb defaults
          sudo service xvfb start

      - uses: actions/setup-node@v4
        with:
          node-version-file: .nvmrc

      - name: Compute node modules cache key
        id: nodeModulesCacheKey
        run: echo ""value=$(node build/azure-pipelines/common/computeNodeModulesCacheKey.js)"" >> $GITHUB_OUTPUT
      - name: Cache node modules
        id: cacheNodeModules
        uses: actions/cache@v4
        with:
          path: ""**/node_modules""
          key: ${{ runner.os }}-cacheNodeModulesLinux-${{ steps.nodeModulesCacheKey.outputs.value }}
      - name: Get npm cache directory path
        id: npmCacheDirPath
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        run: echo ""dir=$(npm config get cache)"" >> $GITHUB_OUTPUT
      - name: Cache npm directory
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        uses: actions/cache@v4
        with:
          path: ${{ steps.npmCacheDirPath.outputs.dir }}
          key: ${{ runner.os }}-npmCacheDir-${{ steps.nodeModulesCacheKey.outputs.value }}
          restore-keys: ${{ runner.os }}-npmCacheDir-
      - name: Execute npm
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        env:
          PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: 1
          ELECTRON_SKIP_BINARY_DOWNLOAD: 1
        run: npm ci

      - name: Compile and Download
        run: npm exec -- npm-run-all -lp compile ""electron x64""

      - name: Run Unit Tests
        id: electron-unit-tests
        run: DISPLAY=:10 ./scripts/test.sh

      - name: Run Integration Tests (Electron)
        id: electron-integration-tests
        run: DISPLAY=:10 ./scripts/test-integration.sh

  hygiene:
    if: github.ref != 'refs/heads/main'
    name: Hygiene and Layering
    runs-on: ubuntu-latest
    timeout-minutes: 40
    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false

      - uses: actions/setup-node@v4
        with:
          node-version-file: .nvmrc

      - name: Compute node modules cache key
        id: nodeModulesCacheKey
        run: echo ""value=$(node build/azure-pipelines/common/computeNodeModulesCacheKey.js)"" >> $GITHUB_OUTPUT
      - name: Cache node modules
        id: cacheNodeModules
        uses: actions/cache@v4
        with:
          path: ""**/node_modules""
          key: ${{ runner.os }}-cacheNodeModulesLinux-${{ steps.nodeModulesCacheKey.outputs.value }}
      - name: Get npm cache directory path
        id: npmCacheDirPath
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        run: echo ""dir=$(npm config get cache)"" >> $GITHUB_OUTPUT
      - name: Cache npm directory
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        uses: actions/cache@v4
        with:
          path: ${{ steps.npmCacheDirPath.outputs.dir }}
          key: ${{ runner.os }}-npmCacheDir-${{ steps.nodeModulesCacheKey.outputs.value }}
          restore-keys: ${{ runner.os }}-npmCacheDir-
      - name: Execute npm
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        env:
          PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: 1
          ELECTRON_SKIP_BINARY_DOWNLOAD: 1
        run: npm ci

      - name: Run Hygiene Checks
        run: npm run gulp hygiene

      - name: Run Valid Layers Checks
        run: npm run valid-layers-check

      - name: Run Define Class Fields Checks
        run: npm run define-class-fields-check

      - name: Compile /build/
        run: npm run compile
        working-directory: build

      - name: Check clean git state
        run: ./.github/workflows/check-clean-git-state.sh

      - name: Run eslint
        run: npm run eslint

      - name: Run vscode-dts Compile Checks
        run: npm run vscode-dts-compile-check

      - name: Run Trusted Types Checks
        run: npm run tsec-compile-check

  warm-cache:
    name: Warm up node modules cache
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    timeout-minutes: 40
    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false

      - uses: actions/setup-node@v4
        with:
          node-version-file: .nvmrc

      - name: Compute node modules cache key
        id: nodeModulesCacheKey
        run: echo ""value=$(node build/azure-pipelines/common/computeNodeModulesCacheKey.js)"" >> $GITHUB_OUTPUT
      - name: Cache node modules
        id: cacheNodeModules
        uses: actions/cache@v4
        with:
          path: ""**/node_modules""
          key: ${{ runner.os }}-cacheNodeModulesLinux-${{ steps.nodeModulesCacheKey.outputs.value }}
      - name: Get npm cache directory path
        id: npmCacheDirPath
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        run: echo ""dir=$(npm config get cache)"" >> $GITHUB_OUTPUT
      - name: Cache npm directory
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        uses: actions/cache@v4
        with:
          path: ${{ steps.npmCacheDirPath.outputs.dir }}
          key: ${{ runner.os }}-npmCacheDir-${{ steps.nodeModulesCacheKey.outputs.value }}
          restore-keys: ${{ runner.os }}-npmCacheDir-
      - name: Execute npm
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        env:
          PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: 1
          ELECTRON_SKIP_BINARY_DOWNLOAD: 1
        run: npm ci
",186,3,1,workflow_dispatch,12
microsoft/vscode,ci.yml,"name: CI

on: workflow_dispatch
permissions: {}

# on:
#   push:
#     branches:
#       - main
#       - release/*
#   pull_request:
#     branches:
#       - main
#       - release/*

jobs:
  windows:
    name: Windows
    runs-on: windows-2022
    timeout-minutes: 60
    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false

      - uses: actions/setup-node@v4
        with:
          node-version-file: .nvmrc

      - uses: actions/setup-python@v5
        with:
          python-version: ""3.x""

      - name: Compute node modules cache key
        id: nodeModulesCacheKey
        run: echo ""value=$(node build/azure-pipelines/common/computeNodeModulesCacheKey.js)"" >> $GITHUB_OUTPUT
      - name: Cache node_modules archive
        id: cacheNodeModules
        uses: actions/cache@v4
        with:
          path: "".build/node_modules_cache""
          key: ""${{ runner.os }}-cacheNodeModulesArchive-${{ steps.nodeModulesCacheKey.outputs.value }}""
      - name: Extract node_modules archive
        if: ${{ steps.cacheNodeModules.outputs.cache-hit == 'true' }}
        run: 7z.exe x .build/node_modules_cache/cache.7z -aos
      - name: Get npm cache directory path
        id: npmCacheDirPath
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        run: echo ""dir=$(npm config get cache)"" >> $GITHUB_OUTPUT
      - name: Cache npm directory
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        uses: actions/cache@v4
        with:
          path: ${{ steps.npmCacheDirPath.outputs.dir }}
          key: ${{ runner.os }}-npmCacheDir-${{ steps.nodeModulesCacheKey.outputs.value }}
          restore-keys: ${{ runner.os }}-npmCacheDir-
      - name: Execute npm
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        env:
          npm_config_foreground_scripts: ""true""
          PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: 1
          ELECTRON_SKIP_BINARY_DOWNLOAD: 1
        run: npm ci
      - name: Create node_modules archive
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        run: |
          mkdir -Force .build
          node build/azure-pipelines/common/listNodeModules.js .build/node_modules_list.txt
          mkdir -Force .build/node_modules_cache
          7z.exe a .build/node_modules_cache/cache.7z -mx3 `@.build/node_modules_list.txt

      - name: Compile and Download
        run: npm exec -- npm-run-all -lp compile ""electron x64"" playwright-install download-builtin-extensions

      - name: Compile Integration Tests
        run: npm run compile
        working-directory: test/integration/browser

      - name: Run Unit Tests (Electron)
        run: .\scripts\test.bat

      - name: Run Unit Tests (node.js)
        run: npm run test-node

      - name: Run Unit Tests (Browser, Chromium)
        run: npm run test-browser-no-install -- --browser chromium

      - name: Run Integration Tests (Electron)
        run: .\scripts\test-integration.bat

      - name: Run Integration Tests (Browser, Firefox)
        timeout-minutes: 20
        run: .\scripts\test-web-integration.bat --browser firefox

      - name: Run Integration Tests (Remote)
        timeout-minutes: 20
        run: .\scripts\test-remote-integration.bat

  linux:
    name: Linux
    runs-on: ubuntu-latest
    timeout-minutes: 40
    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false

      # TODO: rename azure-pipelines/linux/xvfb.init to github-actions
      - name: Setup Build Environment
        run: |
          sudo apt-get update
          sudo apt-get install -y libxkbfile-dev pkg-config libkrb5-dev libxss1 xvfb libgtk-3-0 libgbm1
          sudo cp build/azure-pipelines/linux/xvfb.init /etc/init.d/xvfb
          sudo chmod +x /etc/init.d/xvfb
          sudo update-rc.d xvfb defaults
          sudo service xvfb start

      - uses: actions/setup-node@v4
        with:
          node-version-file: .nvmrc

      - name: Compute node modules cache key
        id: nodeModulesCacheKey
        run: echo ""value=$(node build/azure-pipelines/common/computeNodeModulesCacheKey.js)"" >> $GITHUB_OUTPUT
      - name: Cache node modules
        id: cacheNodeModules
        uses: actions/cache@v4
        with:
          path: ""**/node_modules""
          key: ${{ runner.os }}-cacheNodeModulesLinux-${{ steps.nodeModulesCacheKey.outputs.value }}
      - name: Get npm cache directory path
        id: npmCacheDirPath
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        run: echo ""dir=$(npm config get cache)"" >> $GITHUB_OUTPUT
      - name: Cache npm directory
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        uses: actions/cache@v4
        with:
          path: ${{ steps.npmCacheDirPath.outputs.dir }}
          key: ${{ runner.os }}-npmCacheDir-${{ steps.nodeModulesCacheKey.outputs.value }}
          restore-keys: ${{ runner.os }}-npmCacheDir-
      - name: Execute npm
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        env:
          PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: 1
          ELECTRON_SKIP_BINARY_DOWNLOAD: 1
        run: npm ci

      - name: Compile and Download
        run: npm exec -- npm-run-all -lp compile ""electron x64"" playwright-install download-builtin-extensions

      - name: Compile Integration Tests
        run: npm run compile
        working-directory: test/integration/browser

      - name: Run Unit Tests (Electron)
        id: electron-unit-tests
        run: DISPLAY=:10 ./scripts/test.sh

      - name: Run Unit Tests (node.js)
        id: nodejs-unit-tests
        run: npm run test-node

      - name: Run Unit Tests (Browser, Chromium)
        id: browser-unit-tests
        run: DISPLAY=:10 npm run test-browser-no-install -- --browser chromium

      - name: Run Integration Tests (Electron)
        id: electron-integration-tests
        run: DISPLAY=:10 ./scripts/test-integration.sh

      - name: Run Integration Tests (Browser, Chromium)
        id: browser-integration-tests
        run: DISPLAY=:10 ./scripts/test-web-integration.sh --browser chromium

      - name: Run Integration Tests (Remote)
        id: electron-remote-integration-tests
        timeout-minutes: 15
        run: DISPLAY=:10 ./scripts/test-remote-integration.sh

  darwin:
    name: macOS
    runs-on: macos-latest
    timeout-minutes: 40
    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false

      - uses: actions/setup-node@v4
        with:
          node-version-file: .nvmrc

      - name: Compute node modules cache key
        id: nodeModulesCacheKey
        run: echo ""value=$(node build/azure-pipelines/common/computeNodeModulesCacheKey.js)"" >> $GITHUB_OUTPUT
      - name: Cache node modules
        id: cacheNodeModules
        uses: actions/cache@v4
        with:
          path: ""**/node_modules""
          key: ${{ runner.os }}-cacheNodeModulesMacOS-${{ steps.nodeModulesCacheKey.outputs.value }}
      - name: Get npm cache directory path
        id: npmCacheDirPath
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        run: echo ""dir=$(npm config get cache)"" >> $GITHUB_OUTPUT
      - name: Cache npm directory
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        uses: actions/cache@v4
        with:
          path: ${{ steps.npmCacheDirPath.outputs.dir }}
          key: ${{ runner.os }}-npmCacheDir-${{ steps.nodeModulesCacheKey.outputs.value }}
          restore-keys: ${{ runner.os }}-npmCacheDir-
      - name: Execute npm
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        env:
          PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: 1
          ELECTRON_SKIP_BINARY_DOWNLOAD: 1
        run: npm ci

      - name: Compile and Download
        run: npm exec -- npm-run-all -lp compile ""electron x64"" playwright-install download-builtin-extensions

      - name: Compile Integration Tests
        run: npm run compile
        working-directory: test/integration/browser

      # This is required for SecretStorage unittests
      - name: Create temporary keychain
        run: |
          security create-keychain -p pwd $RUNNER_TEMP/buildagent.keychain
          security default-keychain -s $RUNNER_TEMP/buildagent.keychain
          security unlock-keychain -p pwd $RUNNER_TEMP/buildagent.keychain

      - name: Run Unit Tests (Electron)
        run: DISPLAY=:10 ./scripts/test.sh

      - name: Run Unit Tests (node.js)
        run: npm run test-node

      - name: Run Unit Tests (Browser, Chromium)
        run: DISPLAY=:10 npm run test-browser-no-install -- --browser chromium

      - name: Run Integration Tests (Electron)
        run: DISPLAY=:10 ./scripts/test-integration.sh

      - name: Run Integration Tests (Browser, Webkit)
        run: DISPLAY=:10 ./scripts/test-web-integration.sh --browser webkit

      - name: Run Integration Tests (Remote)
        timeout-minutes: 15
        run: DISPLAY=:10 ./scripts/test-remote-integration.sh

  hygiene:
    name: Hygiene and Layering
    runs-on: ubuntu-latest
    timeout-minutes: 40
    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false

      - uses: actions/setup-node@v4
        with:
          node-version-file: .nvmrc

      - name: Compute node modules cache key
        id: nodeModulesCacheKey
        run: echo ""value=$(node build/azure-pipelines/common/computeNodeModulesCacheKey.js)"" >> $GITHUB_OUTPUT
      - name: Cache node modules
        id: cacheNodeModules
        uses: actions/cache@v4
        with:
          path: ""**/node_modules""
          key: ${{ runner.os }}-cacheNodeModulesLinux-${{ steps.nodeModulesCacheKey.outputs.value }}
      - name: Get npm cache directory path
        id: npmCacheDirPath
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        run: echo ""dir=$(npm config get cache)"" >> $GITHUB_OUTPUT
      - name: Cache npm directory
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        uses: actions/cache@v4
        with:
          path: ${{ steps.npmCacheDirPath.outputs.dir }}
          key: ${{ runner.os }}-npmCacheDir-${{ steps.nodeModulesCacheKey.outputs.value }}
          restore-keys: ${{ runner.os }}-npmCacheDir-
      - name: Execute npm
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        env:
          PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: 1
          ELECTRON_SKIP_BINARY_DOWNLOAD: 1
        run: npm ci

      - name: Download Playwright
        run: npm run playwright-install

      - name: Run Hygiene Checks
        run: npm run gulp hygiene

      - name: Run Valid Layers Checks
        run: npm run valid-layers-check

      - name: Run Define Class Fields Checks
        run: npm run define-class-fields-check

      - name: Compile /build/
        run: npm run compile
        working-directory: build

      - name: Check clean git state
        run: ./.github/workflows/check-clean-git-state.sh

      - name: Run eslint
        run: npm run eslint

      - name: Run vscode-dts Compile Checks
        run: npm run vscode-dts-compile-check

      - name: Run Trusted Types Checks
        run: npm run tsec-compile-check
",328,4,1,workflow_dispatch,17
microsoft/vscode,monaco-editor.yml,"name: Monaco Editor checks

on:
  push:
    branches:
      - main
      - release/*
  pull_request:
    branches:
      - main
      - release/*
permissions: {}

jobs:
  main:
    name: Monaco Editor checks
    runs-on: ubuntu-latest
    timeout-minutes: 40
    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false

      - uses: actions/setup-node@v4
        with:
          node-version-file: .nvmrc

      - name: Compute node modules cache key
        id: nodeModulesCacheKey
        run: echo ""value=$(node build/azure-pipelines/common/computeNodeModulesCacheKey.js)"" >> $GITHUB_OUTPUT
      - name: Cache node modules
        id: cacheNodeModules
        uses: actions/cache@v4
        with:
          path: ""**/node_modules""
          key: ${{ runner.os }}-cacheNodeModules20-${{ steps.nodeModulesCacheKey.outputs.value }}
          restore-keys: ${{ runner.os }}-cacheNodeModules20-
      - name: Get npm cache directory path
        id: npmCacheDirPath
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        run: echo ""dir=$(npm config get cache)"" >> $GITHUB_OUTPUT
      - name: Cache npm directory
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        uses: actions/cache@v4
        with:
          path: ${{ steps.npmCacheDirPath.outputs.dir }}
          key: ${{ runner.os }}-npmCacheDir-${{ steps.nodeModulesCacheKey.outputs.value }}
          restore-keys: ${{ runner.os }}-npmCacheDir-
      - name: Install system dependencies
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        run: |
          sudo apt update
          sudo apt install -y libxkbfile-dev pkg-config libkrb5-dev libxss1
      - name: Execute npm
        if: ${{ steps.cacheNodeModules.outputs.cache-hit != 'true' }}
        env:
          PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: 1
          ELECTRON_SKIP_BINARY_DOWNLOAD: 1
        run: |
          npm ci

      - name: Download Playwright
        run: npm run playwright-install

      - name: Run Monaco Editor Checks
        run: npm run monaco-compile-check

      - name: Editor Distro & ESM
        run: npm run gulp editor-distro

      - name: Editor ESM sources check
        working-directory: ./test/monaco
        run: npm run esm-check

      - name: Typings validation prep
        run: |
          mkdir typings-test

      - name: Typings validation
        working-directory: ./typings-test
        run: |
          npm init -yp
          ../node_modules/.bin/tsc --init
          echo ""import '../out-monaco-editor-core';"" > a.ts
          ../node_modules/.bin/tsc --noEmit

      - name: Package Editor with Webpack
        working-directory: ./test/monaco
        run: npm run bundle-webpack

      - name: Compile Editor Tests
        working-directory: ./test/monaco
        run: npm run compile

      - name: Run Editor Tests
        timeout-minutes: 5
        working-directory: ./test/monaco
        run: npm run test
",100,1,2,"push, pull_request",4
microsoft/vscode,no-package-lock-changes.yml,"name: Prevent package-lock.json changes in PRs

on: pull_request
permissions: {}

jobs:
  main:
    name: Prevent package-lock.json changes in PRs
    runs-on: ubuntu-latest
    steps:
      - name: Get file changes
        uses: trilom/file-changes-action@ce38c8ce2459ca3c303415eec8cb0409857b4272
        id: file_changes
      - name: Check if lockfiles were modified
        id: lockfile_check
        run: |
          if cat $HOME/files.json | jq -e 'any(test(""package-lock\\.json$|Cargo\\.lock$""))' > /dev/null; then
            echo ""lockfiles_modified=true"" >> $GITHUB_OUTPUT
            echo ""Lockfiles were modified in this PR""
          else
            echo ""lockfiles_modified=false"" >> $GITHUB_OUTPUT
            echo ""No lockfiles were modified in this PR""
          fi
      - name: Prevent Copilot from modifying lockfiles
        if: ${{ steps.lockfile_check.outputs.lockfiles_modified == 'true' && github.event.pull_request.user.login == 'Copilot' }}
        run: |
          echo ""Copilot is not allowed to modify package-lock.json or Cargo.lock files.""
          echo ""If you need to update dependencies, please do so manually or through authorized means.""
          exit 1
      - uses: octokit/request-action@dad4362715b7fb2ddedf9772c8670824af564f0d # v2.4.0
        id: get_permissions
        if: ${{ steps.lockfile_check.outputs.lockfiles_modified == 'true' && github.event.pull_request.user.login != 'Copilot' }}
        with:
          route: GET /repos/microsoft/vscode/collaborators/{username}/permission
          username: ${{ github.event.pull_request.user.login }}
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Set control output variable
        id: control
        if: ${{ steps.lockfile_check.outputs.lockfiles_modified == 'true' && github.event.pull_request.user.login != 'Copilot' }}
        run: |
          echo ""user: ${{ github.event.pull_request.user.login }}""
          echo ""role: ${{ fromJson(steps.get_permissions.outputs.data).permission }}""
          echo ""is dependabot: ${{ github.event.pull_request.user.login == 'dependabot[bot]' }}""
          echo ""should_run: ${{ !contains(fromJson('[""admin"", ""maintain"", ""write""]'), fromJson(steps.get_permissions.outputs.data).permission) }}""
          echo ""should_run=${{ !contains(fromJson('[""admin"", ""maintain"", ""write""]'), fromJson(steps.get_permissions.outputs.data).permission) && github.event.pull_request.user.login != 'dependabot[bot]' }}"" >> $GITHUB_OUTPUT
      - name: Check for lockfile changes
        if: ${{ steps.lockfile_check.outputs.lockfiles_modified == 'true' && steps.control.outputs.should_run == 'true' }}
        run: |
          echo ""Changes to package-lock.json/Cargo.lock files aren't allowed in PRs.""
          exit 1
",51,1,1,pull_request,2
microsoft/vscode,no-yarn-lock-changes.yml,"name: Prevent yarn.lock changes in PRs

on: pull_request
permissions: {}

jobs:
  main:
    name: Prevent yarn.lock changes in PRs
    runs-on: ubuntu-latest
    steps:
      - name: Get file changes
        uses: trilom/file-changes-action@a6ca26c14274c33b15e6499323aac178af06ad4b # v1.2.4
        id: file_changes
      - name: Check if lockfiles were modified
        id: lockfile_check
        run: |
          if cat $HOME/files.json | jq -e 'any(test(""yarn\\.lock$|Cargo\\.lock$""))' > /dev/null; then
            echo ""lockfiles_modified=true"" >> $GITHUB_OUTPUT
            echo ""Lockfiles were modified in this PR""
          else
            echo ""lockfiles_modified=false"" >> $GITHUB_OUTPUT
            echo ""No lockfiles were modified in this PR""
          fi
      - name: Prevent Copilot from modifying lockfiles
        if: ${{ steps.lockfile_check.outputs.lockfiles_modified == 'true' && github.event.pull_request.user.login == 'Copilot' }}
        run: |
          echo ""Copilot is not allowed to modify yarn.lock or Cargo.lock files.""
          echo ""If you need to update dependencies, please do so manually or through authorized means.""
          exit 1
      - uses: octokit/request-action@dad4362715b7fb2ddedf9772c8670824af564f0d # v2.4.0
        id: get_permissions
        if: ${{ steps.lockfile_check.outputs.lockfiles_modified == 'true' && github.event.pull_request.user.login != 'Copilot' }}
        with:
          route: GET /repos/microsoft/vscode/collaborators/{username}/permission
          username: ${{ github.event.pull_request.user.login }}
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Set control output variable
        id: control
        if: ${{ steps.lockfile_check.outputs.lockfiles_modified == 'true' && github.event.pull_request.user.login != 'Copilot' }}
        run: |
          echo ""user: ${{ github.event.pull_request.user.login }}""
          echo ""role: ${{ fromJson(steps.get_permissions.outputs.data).permission }}""
          echo ""is dependabot: ${{ github.event.pull_request.user.login == 'dependabot[bot]' }}""
          echo ""should_run: ${{ !contains(fromJson('[""admin"", ""maintain"", ""write""]'), fromJson(steps.get_permissions.outputs.data).permission) }}""
          echo ""should_run=${{ !contains(fromJson('[""admin"", ""maintain"", ""write""]'), fromJson(steps.get_permissions.outputs.data).permission) && github.event.pull_request.user.login != 'dependabot[bot]' }}"" >> $GITHUB_OUTPUT
      - name: Check for lockfile changes
        if: ${{ steps.lockfile_check.outputs.lockfiles_modified == 'true' && steps.control.outputs.should_run == 'true' }}
        run: |
          echo ""Changes to yarn.lock/Cargo.lock files aren't allowed in PRs.""
          exit 1
",51,1,1,pull_request,2
microsoft/vscode,telemetry.yml,"name: 'Telemetry'
on: pull_request
permissions: {}
jobs:
  check-metadata:
    name: 'Check metadata'
    runs-on: 'ubuntu-latest'

    steps:
      - uses: 'actions/checkout@v4'
        with:
          persist-credentials: false

      - uses: 'actions/setup-node@v4'
        with:
          node-version: 'lts/*'

      - name: 'Run vscode-telemetry-extractor'
        run: 'npx --package=@vscode/telemetry-extractor@1.14.0 --yes vscode-telemetry-extractor -s .'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
",21,1,1,pull_request,2
twbs/bootstrap,browserstack.yml,"name: BrowserStack

on:
  push:
    branches:
      - ""**""
      - ""!dependabot/**""
  workflow_dispatch:

env:
  FORCE_COLOR: 2
  NODE: 22

permissions:
  contents: read

jobs:
  browserstack:
    runs-on: ubuntu-latest
    if: github.repository == 'twbs/bootstrap'
    timeout-minutes: 30

    steps:
      - name: Clone repository
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ""${{ env.NODE }}""
          cache: npm

      - name: Install npm dependencies
        run: npm ci

      - name: Run dist
        run: npm run dist

      - name: Run BrowserStack tests
        run: npm run js-test-cloud
        env:
          BROWSER_STACK_ACCESS_KEY: ""${{ secrets.BROWSER_STACK_ACCESS_KEY }}""
          BROWSER_STACK_USERNAME: ""${{ secrets.BROWSER_STACK_USERNAME }}""
          GITHUB_SHA: ""${{ github.sha }}""
",46,1,2,"push, workflow_dispatch",2
twbs/bootstrap,bundlewatch.yml,"name: Bundlewatch

on:
  push:
    branches:
      - main
  pull_request:
  workflow_dispatch:

env:
  FORCE_COLOR: 2
  NODE: 22

permissions:
  contents: read

jobs:
  bundlewatch:
    runs-on: ubuntu-latest

    steps:
      - name: Clone repository
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ""${{ env.NODE }}""
          cache: npm

      - name: Install npm dependencies
        run: npm ci

      - name: Run dist
        run: npm run dist

      - name: Run bundlewatch
        run: npm run bundlewatch
        env:
          BUNDLEWATCH_GITHUB_TOKEN: ""${{ secrets.BUNDLEWATCH_GITHUB_TOKEN }}""
          CI_BRANCH_BASE: main
",43,1,3,"push, pull_request, workflow_dispatch",2
twbs/bootstrap,calibreapp-image-actions.yml,"name: Compress Images

on:
  pull_request:
    paths:
      - '**.jpg'
      - '**.jpeg'
      - '**.png'
      - '**.webp'

permissions:
  contents: read

jobs:
  build:
    # Only run on Pull Requests within the same repository, and not from forks.
    if: github.event.pull_request.head.repo.full_name == github.repository
    name: calibreapp/image-actions
    runs-on: ubuntu-latest
    permissions:
      # allow calibreapp/image-actions to update PRs
      pull-requests: write
    steps:
      - name: Clone repository
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Compress Images
        uses: calibreapp/image-actions@1.1.0
        with:
          githubToken: ${{ secrets.GITHUB_TOKEN }}
",32,1,1,pull_request,2
twbs/bootstrap,codeql.yml,"name: ""CodeQL""

on:
  push:
    branches:
      - main
      - v4-dev
      - ""!dependabot/**""
  pull_request:
    branches:
      - main
      - v4-dev
      - ""!dependabot/**""
  schedule:
    - cron: ""0 2 * * 4""
  workflow_dispatch:

jobs:
  analyze:
    name: Analyze
    runs-on: ubuntu-latest
    permissions:
      security-events: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          config-file: ./.github/codeql/codeql-config.yml
          languages: ""javascript""
          queries: +security-and-quality

      - name: Autobuild
        uses: github/codeql-action/autobuild@v3

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3
        with:
          category: ""/language:javascript""
",44,1,4,"push, pull_request, schedule, workflow_dispatch",4
twbs/bootstrap,cspell.yml,"name: cspell

on:
  push:
    branches:
      - main
  pull_request:
  workflow_dispatch:

env:
  FORCE_COLOR: 2

permissions:
  contents: read

jobs:
  cspell:
    permissions:
      # allow streetsidesoftware/cspell-action to fetch files for commits and PRs
      contents: read
      pull-requests: read
    runs-on: ubuntu-latest

    steps:
      - name: Clone repository
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Run cspell
        uses: streetsidesoftware/cspell-action@v7
        with:
          config: "".cspell.json""
          files: ""**/*.{md,mdx}""
          inline: error
          incremental_files_only: false
",36,1,3,"push, pull_request, workflow_dispatch",2
twbs/bootstrap,css.yml,"name: CSS

on:
  push:
    branches:
      - main
  pull_request:
  workflow_dispatch:

env:
  FORCE_COLOR: 2
  NODE: 22

permissions:
  contents: read

jobs:
  css:
    runs-on: ubuntu-latest

    steps:
      - name: Clone repository
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ""${{ env.NODE }}""
          cache: npm

      - name: Install npm dependencies
        run: npm ci

      - name: Build CSS
        run: npm run css

      - name: Run CSS tests
        run: npm run css-test
",40,1,3,"push, pull_request, workflow_dispatch",2
twbs/bootstrap,docs.yml,"name: Docs

on:
  push:
    branches:
      - main
  pull_request:
  workflow_dispatch:

env:
  FORCE_COLOR: 2
  NODE: 22

permissions:
  contents: read

jobs:
  docs:
    runs-on: ubuntu-latest

    steps:
      - name: Clone repository
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ""${{ env.NODE }}""
          cache: npm

      - run: java -version

      - name: Install npm dependencies
        run: npm ci

      - name: Build docs
        run: npm run docs-build

      - name: Validate HTML
        run: npm run docs-vnu

      - name: Run linkinator
        uses: JustinBeckwith/linkinator-action@v1
        with:
          paths: _site
          recurse: true
          verbosity: error
          skip: ""^http://localhost""
",50,1,3,"push, pull_request, workflow_dispatch",3
twbs/bootstrap,issue-close-require.yml,"name: Close Issue Awaiting Reply

on:
  schedule:
    - cron: ""0 0 * * *""

permissions:
  contents: read

jobs:
  issue-close-require:
    permissions:
      # allow actions-cool/issues-helper to update issues and PRs
      issues: write
      pull-requests: write
    runs-on: ubuntu-latest
    if: github.repository == 'twbs/bootstrap'
    steps:
      - name: awaiting reply
        uses: actions-cool/issues-helper@v3
        with:
          actions: ""close-issues""
          labels: ""awaiting-reply""
          inactive-day: 14
          body: |
            As the issue was labeled with `awaiting-reply`, but there has been no response in 14 days, this issue will be closed. If you have any questions, you can comment/reply.
",26,1,1,schedule,1
twbs/bootstrap,issue-labeled.yml,"name: Issue Labeled

on:
  issues:
    types: [labeled]

permissions:
  contents: read

jobs:
  issue-labeled:
    permissions:
      # allow actions-cool/issues-helper to update issues and PRs
      issues: write
      pull-requests: write
    if: github.repository == 'twbs/bootstrap'
    runs-on: ubuntu-latest
    steps:
      - name: awaiting reply
        if: github.event.label.name == 'needs-example'
        uses: actions-cool/issues-helper@v3
        with:
          actions: ""create-comment""
          token: ${{ secrets.GITHUB_TOKEN }}
          body: |
            Hello @${{ github.event.issue.user.login }}. Bug reports must include a **live demo** of the issue. Per our [contributing guidelines](https://github.com/twbs/bootstrap/blob/main/.github/CONTRIBUTING.md), please create a reduced test case on [CodePen](https://codepen.io/) or [StackBlitz](https://stackblitz.com/) and report back with your link, Bootstrap version, and specific browser and Operating System details.
",26,1,1,issues,1
twbs/bootstrap,js.yml,"name: JS Tests

on:
  push:
    branches:
      - main
  pull_request:
  workflow_dispatch:

env:
  FORCE_COLOR: 2
  NODE: 22

permissions:
  contents: read

jobs:
  run:
    permissions:
      # allow coverallsapp/github-action to create new checks issues and fetch code
      checks: write
      contents: read
    name: JS Tests
    runs-on: ubuntu-latest

    steps:
      - name: Clone repository
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE }}
          cache: npm

      - name: Install npm dependencies
        run: npm ci

      - name: Run dist
        run: npm run js

      - name: Run JS tests
        run: npm run js-test

      - name: Run Coveralls
        uses: coverallsapp/github-action@v2
        if: ${{ !github.event.repository.fork }}
        with:
          github-token: ""${{ secrets.GITHUB_TOKEN }}""
          path-to-lcov: ""./js/coverage/lcov.info""
",52,1,3,"push, pull_request, workflow_dispatch",3
twbs/bootstrap,lint.yml,"name: Lint

on:
  push:
    branches:
      - main
  pull_request:
  workflow_dispatch:

env:
  FORCE_COLOR: 2
  NODE: 22

permissions:
  contents: read

jobs:
  lint:
    runs-on: ubuntu-latest

    steps:
      - name: Clone repository
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ""${{ env.NODE }}""
          cache: npm

      - name: Install npm dependencies
        run: npm ci

      - name: Lint
        run: npm run lint
",37,1,3,"push, pull_request, workflow_dispatch",2
twbs/bootstrap,node-sass.yml,"name: CSS (node-sass)

on:
  push:
    branches:
      - main
  pull_request:
  workflow_dispatch:

env:
  FORCE_COLOR: 2
  NODE: 22

permissions:
  contents: read

jobs:
  css:
    runs-on: ubuntu-latest

    steps:
      - name: Clone repository
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ""${{ env.NODE }}""

      - name: Build CSS with node-sass
        run: |
          npx --package node-sass@latest node-sass --version
          npx --package node-sass@latest node-sass --output-style expanded --source-map true --source-map-contents true --precision 6 scss/ -o dist-sass/css/
          ls -Al dist-sass/css

      - name: Check built CSS files for Sass variables
        shell: bash
        run: |
          SASS_VARS_FOUND=$(find ""dist-sass/css/"" -type f -name ""*.css"" -print0 | xargs -0 --no-run-if-empty grep -F ""\$"" || true)
          if [[ -z ""$SASS_VARS_FOUND"" ]]; then
            echo ""All good, no Sass variables found!""
            exit 0
          else
            echo ""Found $(echo ""$SASS_VARS_FOUND"" | wc -l | bc) Sass variables:""
            echo ""$SASS_VARS_FOUND""
            exit 1
          fi
",49,1,3,"push, pull_request, workflow_dispatch",2
twbs/bootstrap,release-notes.yml,"name: Release notes

on:
  push:
    branches:
      - main
  workflow_dispatch:

permissions:
  contents: read

jobs:
  update_release_draft:
    permissions:
      # allow release-drafter/release-drafter to create GitHub releases and add labels to PRs
      contents: write
      pull-requests: write
    runs-on: ubuntu-latest
    if: github.repository == 'twbs/bootstrap'
    steps:
      - uses: release-drafter/release-drafter@v6
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
",23,1,2,"push, workflow_dispatch",1
flutter/flutter,content-aware-hash.yml,"# Copyright 2013 The Flutter Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

name: Generate a content aware hash for the Flutter Engine

on: workflow_dispatch

jobs:
  generate-engine-content-hash:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Fetch base commit and origin/master
        run: |
          git fetch --no-tags --prune --depth=1 origin ${{ github.event.pull_request.base.sha }}

      - name: Generate Hash
        run: |
          engine_content_hash=$(bin/internal/content_aware_hash.sh)
          # test notice annotation for retrival from api
          echo ""::notice ::{\""engine_content_hash\"": \""${engine_content_hash}\""}""
          # test summary writing
          echo ""{\""engine_content_hash\"": \""${engine_content_hash}\"""" >> $GITHUB_STEP_SUMMARY
",26,1,1,workflow_dispatch,1
flutter/flutter,coverage.yml,"# Copyright 2013 The Flutter Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

name: coverage

on:
  push:
    branches:
      - master
    paths:
      - 'packages/flutter/**'

permissions: read-all

jobs:
  build:
    name: coverage
    runs-on: ubuntu-latest
    if: ${{ github.repository == 'flutter/flutter' }}
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      - name: ./bin/flutter test --coverage
        run: pushd packages/flutter;../../bin/flutter test --coverage -j 1;popd
      - name: upload coverage
        uses: codecov/codecov-action@0565863a31f2c772f9f0395002a31e3f06189574
        with:
          files: packages/flutter/coverage/lcov.info
          verbose: true
",29,1,1,push,2
flutter/flutter,easy-cp.yml,"# Copyright 2023 The Flutter Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

name: Cherry-pick Labeled PR to Release Branch

on:
  pull_request_target:
    branches: master
    types: [labeled]

permissions: write-all

jobs:
  cherrypick_to_release:
    name: cherrypick_to_release
    runs-on: ubuntu-latest
    if: |
      (github.event.label.name == format('cp{0} beta', ':') || github.event.label.name == format('cp{0} stable', ':')) &&
      (github.event.pull_request.merged == true)
    steps:
      - name: Get Release Channel
        run: |
          echo ""CHANNEL=$(echo ${{ github.event.label.name }} | cut -d ':' -f 2 | xargs)"" >> $GITHUB_ENV
      - name: Get Release Candidate Branch
        run: |
          RELEASE_BRANCH=$(curl https://raw.githubusercontent.com/flutter/flutter/$CHANNEL/bin/internal/release-candidate-branch.version)
          echo ""RELEASE_BRANCH=$(echo $RELEASE_BRANCH | tr -d '\n')"" >> $GITHUB_ENV
      - name: Get Cherry Pick PR
        run: |
          echo ""COMMIT_SHA=$(echo ${{ github.event.pull_request.merge_commit_sha }})"" >> $GITHUB_ENV
      - name: Checkout Flutter Repo
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
        with:
          repository: flutteractionsbot/flutter
          path: flutter
          ref: master
          persist-credentials: false
          # Checkout all history commits on master branch, so that the cp commit is a known object
          fetch-depth: 0
      # use same name when checking out branch, since the marketplace action does a hard reset.
      - name: Attempt CP
        id: attempt-cp
        working-directory: ./flutter
        run: |
          git config user.name ""GitHub Actions Bot""
          git config user.email ""<>""
          git remote add upstream https://github.com/flutter/flutter.git
          git fetch upstream $RELEASE_BRANCH
          git fetch upstream master
          git checkout -b cp-${CHANNEL}-${COMMIT_SHA} --track upstream/$RELEASE_BRANCH
          git cherry-pick $COMMIT_SHA
      # TODO(xilaizhang): remove this step once the template is available on release branches.
      - name: Get CP Template
        run: |
          curl -o PULL_REQUEST_CP_TEMPLATE.md https://raw.githubusercontent.com/flutter/flutter/main/.github/PR_TEMPLATE/PULL_REQUEST_CP_TEMPLATE.md
      - name: Create PR on CP success
        if: ${{ steps.attempt-cp.conclusion == 'success' }}
        working-directory: ./flutter
        id: create-pr
        run: |
          git push https://${{ env.GITHUB_TOKEN }}@github.com/flutteractionsbot/flutter cp-${CHANNEL}-${COMMIT_SHA}
          {
            echo 'PR_URL<<EOF'
            gh pr create --title ""[CP-${CHANNEL}]${PR_TITLE}"" --body-file ../PULL_REQUEST_CP_TEMPLATE.md --base ${RELEASE_BRANCH} --label ""cp: review"" --repo flutter/flutter --head flutteractionsbot:cp-${CHANNEL}-${COMMIT_SHA}
            echo EOF
          } >> ""$GITHUB_ENV""
        env:
          GITHUB_TOKEN: ${{ secrets.FLUTTERACTIONSBOT_CP_TOKEN }}
          PR_TITLE: ${{ github.event.pull_request.title }}
      - name: Leave Comment on CP success
        if: ${{ steps.create-pr.conclusion == 'success' }}
        run: |
          echo $PR_URL
          NEW_PR_NUMBER=""${PR_URL##*/}""
          SUCCESS_MSG="" @${{ github.actor }} please fill out the PR description above, afterwards the release team will review this request.""
          gh pr comment $NEW_PR_NUMBER -R flutter/flutter -b ""${SUCCESS_MSG}""
        env:
          GITHUB_TOKEN: ${{ secrets.FLUTTERACTIONSBOT_CP_TOKEN }}
      - name: Leave Comment on CP failure
        if: ${{ failure() && steps.attempt-cp.conclusion == 'failure' }}
        run: |
          FAILURE_MSG=""Failed to create CP due to merge conflicts.<br>""
          FAILURE_MSG+=""You will need to create the PR manually. See [the cherrypick wiki](https://github.com/flutter/flutter/blob/main/docs/releases/Flutter-Cherrypick-Process.md) for more info.""
          gh pr comment ${{ github.event.pull_request.number }} -R flutter/flutter -b ""${FAILURE_MSG}""
        env:
          GITHUB_TOKEN: ${{ secrets.FLUTTERACTIONSBOT_CP_TOKEN }}
",87,1,1,pull_request_target,1
flutter/flutter,files-changed.yml,"# Copyright 2013 The Flutter Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

name: Generate Changed Files JSON

on:
  pull_request:
    types:
      - opened
      - synchronize

jobs:
  generate-json:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Fetch base commit and origin/master
        # Fetch what to compare the commit against
        run: |
          git fetch --no-tags --prune --depth=1 origin ${{ github.event.pull_request.base.sha }}
          git fetch --no-tags --prune --depth=1 origin master
          echo ""FLUTTER_PREBUILT_ENGINE_VERSION=${{ github.event.pull_request.base.sha }}"" >> ""$GITHUB_ENV""

      - name: Initialize Dart SDK
        # This downloads the version of the Dart SDK for the current platform.
        run: |
          ./bin/dart --version
          cd dev/tools && ../../bin/dart pub get

      - name: Write changed files to a JSON file
        run: |
          ./bin/dart run dev/tools/bin/get_files_changed.dart --since=""${{ github.event.pull_request.base.sha }}"" > changed_files.json

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: changed-files
          path: changed_files.json
",41,1,1,pull_request,2
flutter/flutter,labeler.yml,"# Copyright 2013 The Flutter Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

name: ""Pull Request Labeler""
on:
- pull_request_target

# Declare default permissions as read only.
permissions: read-all

jobs:
  triage:
    if: ${{ github.repository == 'flutter/flutter' }}
    permissions:
      pull-requests: write
    runs-on: ubuntu-latest
    steps:
    # Source available at https://github.com/actions/labeler/blob/main/README.md
    - uses: actions/labeler@8558fd74291d67161a8a78ce36a881fa63b766a9
      with:
        sync-labels: true
",22,1,1,pull_request_target,1
flutter/flutter,lock.yaml,"# Copyright 2013 The Flutter Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

# Configuration for Lock Threads - https://github.com/dessant/lock-threads

name: 'Lock Threads'

# By specifying the access of one of the scopes, all of those that are not
# specified are set to 'none'.
permissions:
  issues: write

on:
  schedule:
    - cron: '0 * * * *'

jobs:
  lock:
    permissions:
      issues: write
    runs-on: ubuntu-latest
    if: ${{ github.repository == 'flutter/flutter' }}
    steps:
      - uses: dessant/lock-threads@1bf7ec25051fe7c00bdd17e6a7cf3d7bfb7dc771
        with:
          process-only: 'issues'
          github-token: ${{ github.token }}
          # Number of days of inactivity before a closed issue is locked.
          issue-inactive-days: '14'
          issue-comment: >
            This thread has been automatically locked since there has not been
            any recent activity after it was closed. If you are still experiencing a
            similar issue, please open a new bug, including the output of
            `flutter doctor -v` and a minimal reproduction of the issue.
",35,1,1,schedule,1
flutter/flutter,merge-changelog.yml,"name: Sync stable CHANGELOG to master

on:
  push:
    branches: [stable]
    paths:
      - 'CHANGELOG.md'

jobs:
  merge-changelog:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write

    steps:
      - name: Setup Repository
        uses: actions/checkout@v4
        with:
          repository: ${{ github.repository }}
          ref: master

      - name: Configure git
        run: |
          git config user.name ""[bot] Merge Changelog""
          git config user.email ""<>""

      - name: Read CHANGELOG.md from the stable branch
        id: read_stable_changelog
        run: |
          CHANGELOG_CONTENT=$(git show origin/stable:CHANGELOG.md)
          echo ""CHANGELOG_CONTENT<<EOF"" >> $GITHUB_ENV
          echo ""$CHANGELOG_CONTENT"" >> $GITHUB_ENV
          echo ""EOF"" >> $GITHUB_ENV

      - name: Prepare PR branch and commit changes
        id: prepare_pr_branch
        run: |
          PR_BRANCH=""sync-changelog-stable-to-master-$(date +%s)""
          echo ""pr_branch_name=$PR_BRANCH"" >> ""$GITHUB_OUTPUT""
          git checkout -b ""$PR_BRANCH"" master
          echo ""${{ env.CHANGELOG_CONTENT }}"" > CHANGELOG.md
          git add CHANGELOG.md
          git commit -m ""Sync CHANGELOG.md from stable""
          git push origin ""$PR_BRANCH""

      - name: Create Pull Request
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          PR_HEAD_BRANCH=${{ steps.prepare_pr_branch.outputs.pr_branch_name }}

          gh pr create \
            --base master \
            --head ""$PR_HEAD_BRANCH"" \
            --title ""Sync CHANGELOG.md from stable"" \
            --body ""This PR automates the synchronization of \`CHANGELOG.md\` from the \`stable\` branch to the \`master\` branch."" \
            --label autosubmit
",58,1,1,push,1
flutter/flutter,mirror.yml,"# Copyright 2013 The Flutter Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

# Mirror master to main branches in the flutter repository.
on:
  push:
    branches:
      - 'master'

# Declare default permissions as read only.
permissions: read-all

jobs:
  mirror_job:
    permissions:
      pull-requests: write
    runs-on: ubuntu-latest
    if: ${{ github.repository == 'flutter/flutter' }}
    name: Mirror master branch to main branch
    steps:
      - name: Mirror action step
        id: mirror
        uses: google/mirror-branch-action@30c52ee21f5d3bd7fb28b95501c11aae7f17eebb
        with:
          github-token: ${{ secrets.FLUTTERMIRRORINGBOT_TOKEN }}
          source: 'master'
          dest: 'main'
",28,1,1,push,1
flutter/flutter,no-response.yaml,"# Copyright 2013 The Flutter Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

name: No Response

# Both `issue_comment` and `scheduled` event types are required for this Action
# to work properly.
on:
  issue_comment:
    types: [created]
  schedule:
    # Schedule for five minutes after the hour, every hour
    - cron: '5 * * * *'

# By specifying the access of one of the scopes, all of those that are not
# specified are set to 'none'.
permissions:
  issues: write

jobs:
  noResponse:
    runs-on: ubuntu-latest
    if: ${{ github.repository == 'flutter/flutter' }}
    steps:
      - uses: godofredoc/no-response@0ce2dc0e63e1c7d2b87752ceed091f6d32c9df09
        with:
          token: ${{ github.token }}
          # Comment to post when closing an Issue for lack of response. Set to `false` to disable
          closeComment: >
            Without additional information, we are unfortunately not sure how to
            resolve this issue. We are therefore reluctantly going to close this
            bug for now.

            If you find this problem please file a new issue with the same description,
            what happens, logs and the output of 'flutter doctor -v'. All system setups
            can be slightly different so it's always better to open new issues and reference
            the related ones.

            Thanks for your contribution.
          # Number of days of inactivity before an issue is closed for lack of response.
          daysUntilClose: 21
          # Label requiring a response.
          responseRequiredLabel: ""waiting for customer response""
",44,1,2,"issue_comment, schedule",1
flutter/flutter,tool-test-general.yml,"name: Tool tests general - experiment

on:
  pull_request:
    branches: [master]
    paths:
      - '.github/workflows/tool-test-general.yml'
      - 'dev/**'
      - 'packages/flutter_tools/**'
      - 'bin/**'
      - '.ci.yaml'
      - 'engine/**'
      - 'DEPS'
  push:
    branches: [master]

jobs:
  Linux_tool-tests-general:
    permissions:
      contents: read
    runs-on: ubuntu-latest

    steps:
      # Note: we must check out the tree for the composite action to be available
      - uses: actions/checkout@v4
      - uses: ./.github/actions/composite-flutter-setup

      - name: Tool Test
        run: |
          SHARD=tool_tests SUBSHARD=general dart --enable-asserts dev/bots/test.dart
",30,1,2,"pull_request, push",2
github/gitignore,stale.yml,"name: Stale

# **What it does**: Close pull requests after no updates for 180 days.
# **Why we have it**: This repository gets a lot of PRs, and the maintainers team is small.
#                     This helps reduce the open PRs to ones that are most desired by the community.
# **Who does it impact**: Contributors and maintainers of github/gitignore.

on:
  schedule:
    - cron: '20 16 * * *' # Run every day at 16:20 UTC / 8:20 PST

permissions:
  actions: write
  contents: write # only for delete-branch option
  issues: write
  pull-requests: write

jobs:
  stale:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/stale@5bef64f19d7facfb25b37b414482c7164d639639 # v9.1.0
        with:
          stale-pr-message: 'This PR is stale because there have been no updates in 90 days. It will close after 180 days of inactivity. Leave a comment if you want to keep it open :smile:'
          close-pr-message: 'This PR has been closed because it was inactive for 180 days. If you want to continue working on it, please open a new PR.'
          days-before-stale: 90
          days-before-close: 180
          stale-pr-label: 'stale'
          exempt-pr-labels: 'keep'
          close-issue-reason: not_planned
          ascending: true # Sort PRs by last updated date in ascending order
          operations-per-run: 300
",32,1,1,schedule,1
AUTOMATIC1111/stable-diffusion-webui,on_pull_request.yaml,"name: Linter

on:
  - push
  - pull_request

jobs:
  lint-python:
    name: ruff
    runs-on: ubuntu-latest
    if: github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name != github.event.pull_request.base.repo.full_name
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: 3.11
          # NB: there's no cache: pip here since we're not installing anything
          #     from the requirements.txt file(s) in the repository; it's faster
          #     not to have GHA download an (at the time of writing) 4 GB cache
          #     of PyTorch and other dependencies.
      - name: Install Ruff
        run: pip install ruff==0.3.3
      - name: Run Ruff
        run: ruff .
  lint-js:
    name: eslint
    runs-on: ubuntu-latest
    if: github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name != github.event.pull_request.base.repo.full_name
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      - name: Install Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 18
      - run: npm i --ci
      - run: npm run lint
",38,2,2,"push, pull_request",4
AUTOMATIC1111/stable-diffusion-webui,run_tests.yaml,"name: Tests

on:
  - push
  - pull_request

jobs:
  test:
    name: tests on CPU with empty model
    runs-on: ubuntu-latest
    if: github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name != github.event.pull_request.base.repo.full_name
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: 3.10.6
          cache: pip
          cache-dependency-path: |
            **/requirements*txt
            launch.py
      - name: Cache models
        id: cache-models
        uses: actions/cache@v4
        with:
          path: models
          key: ""2023-12-30""
      - name: Install test dependencies
        run: pip install wait-for-it -r requirements-test.txt
        env:
          PIP_DISABLE_PIP_VERSION_CHECK: ""1""
          PIP_PROGRESS_BAR: ""off""
      - name: Setup environment
        run: python launch.py --skip-torch-cuda-test --exit
        env:
          PIP_DISABLE_PIP_VERSION_CHECK: ""1""
          PIP_PROGRESS_BAR: ""off""
          TORCH_INDEX_URL: https://download.pytorch.org/whl/cpu
          WEBUI_LAUNCH_LIVE_OUTPUT: ""1""
          PYTHONUNBUFFERED: ""1""
      - name: Print installed packages
        run: pip freeze
      - name: Start test server
        run: >
          python -m coverage run
          --data-file=.coverage.server
          launch.py
          --skip-prepare-environment
          --skip-torch-cuda-test
          --test-server
          --do-not-download-clip
          --no-half
          --disable-opt-split-attention
          --use-cpu all
          --api-server-stop
          2>&1 | tee output.txt &
      - name: Run tests
        run: |
          wait-for-it --service 127.0.0.1:7860 -t 20
          python -m pytest -vv --junitxml=test/results.xml --cov . --cov-report=xml --verify-base-url test
      - name: Kill test server
        if: always()
        run: curl -vv -XPOST http://127.0.0.1:7860/sdapi/v1/server-stop && sleep 10
      - name: Show coverage
        run: |
          python -m coverage combine .coverage*
          python -m coverage report -i
          python -m coverage html -i
      - name: Upload main app output
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: output
          path: output.txt
      - name: Upload coverage HTML
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: htmlcov
          path: htmlcov
",81,1,2,"push, pull_request",5
AUTOMATIC1111/stable-diffusion-webui,warns_merge_master.yml,"name: Pull requests can't target master branch

""on"":
  pull_request:
    types:
      - opened
      - synchronize
      - reopened
    branches:
      - master

jobs:
  check:
    runs-on: ubuntu-latest
    steps:
      - name: Warning marge into master
        run: |
          echo -e ""::warning::This pull request directly merge into \""master\"" branch, normally development happens on \""dev\"" branch.""
          exit 1
",19,1,1,pull_request,0
Snailclimb/JavaGuide,test.yml,"name: Docs Test

on:
  - push
  - pull_request

jobs:
  test-docs:
    name: Test docs
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install pnpm
        uses: pnpm/action-setup@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 22
          cache: pnpm

      - name: Install deps
        run: pnpm install --frozen-lockfile

      - name: Build test
        env:
          NODE_OPTIONS: --max_old_space_size=4096
        run: pnpm docs:build
",30,1,2,"push, pull_request",3
airbnb/javascript,node-pretest.yml,"name: 'Tests: pretest/posttest'

on: [pull_request, push]

jobs:
  pretest:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        package:
          - '..'
          - eslint-config-airbnb
          - eslint-config-airbnb-base

    defaults:
      run:
        working-directory: ""packages/${{ matrix.package }}""

    steps:
      - uses: actions/checkout@v2
      - uses: ljharb/actions/node/install@main
        name: 'nvm install lts/* && npm install'
        with:
          node-version: 'lts/*'
      - run: npm run pretest
",26,1,2,"pull_request, push",2
airbnb/javascript,node.yml,"name: 'Tests: node.js'

on: [pull_request, push]

jobs:
  matrix:
    runs-on: ubuntu-latest
    outputs:
      latest: ${{ steps.set-matrix.outputs.requireds }}
    steps:
      - uses: ljharb/actions/node/matrix@main
        id: set-matrix
        with:
          versionsAsRoot: true
          type: 'majors'
          preset: '^12 || ^14 || ^16 || >= 17'

  base:
    needs: [matrix]
    name: 'base config'
    runs-on: ubuntu-latest

    strategy:
      fail-fast: false
      matrix:
        node-version: ${{ fromJson(needs.matrix.outputs.latest) }}
        eslint:
          - 8
          - 7
        package:
          - eslint-config-airbnb-base
        exclude:
          - node-version: 10
            eslint: 8
            package: eslint-config-airbnb-base

    defaults:
      run:
        working-directory: ""packages/${{ matrix.package }}""

    steps:
      - uses: actions/checkout@v2
      - uses: ljharb/actions/node/install@main
        name: 'nvm install ${{ matrix.node-version }} && npm install'
        with:
          before_install: cd ""packages/${{ matrix.package }}""
          node-version: ${{ matrix.node-version }}
          after_install: |
            npm install --no-save ""eslint@${{ matrix.eslint }}""
      - run: node -pe ""require('eslint/package.json').version""
        name: 'eslint version'
      - run: npm run travis
      - uses: codecov/codecov-action@v2

  react:
    needs: [matrix]
    name: 'react config'
    runs-on: ubuntu-latest

    strategy:
      fail-fast: false
      matrix:
        node-version: ${{ fromJson(needs.matrix.outputs.latest) }}
        eslint:
          - 8
          - 7
        package:
          - eslint-config-airbnb
        react-hooks:
          - 4

    defaults:
      run:
        working-directory: ""packages/${{ matrix.package }}""

    steps:
      - uses: actions/checkout@v2
      - uses: ljharb/actions/node/install@main
        name: 'nvm install ${{ matrix.node-version }} && npm install'
        with:
          before_install: cd ""packages/${{ matrix.package }}""
          node-version: ${{ matrix.node-version }}
          after_install: |
            npm install --no-save ""eslint@${{ matrix.eslint }}""
      - run: node -pe ""require('eslint/package.json').version""
        name: 'eslint version'
      - run: npm install --no-save ""eslint-plugin-react-hooks@${{ matrix.react-hooks }}""
        if: ${{ matrix.react-hooks > 0}}
      - run: npm run travis
      - uses: codecov/codecov-action@v2

  prepublish-base:
    name: 'prepublish tests (base config)'
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        eslint:
          - 8
          - 7
        package:
          - eslint-config-airbnb-base

    defaults:
      run:
        working-directory: ""packages/${{ matrix.package }}""

    steps:
      - uses: actions/checkout@v2
      - uses: ljharb/actions/node/install@main
        name: 'nvm install lts/* && npm install'
        with:
          before_install: cd ""packages/${{ matrix.package }}""
          node-version: lts/*
          after_install: |
            npm install --no-save ""eslint@${{ matrix.eslint }}""
      - run: node -pe ""require('eslint/package.json').version""
        name: 'eslint version'
      - run: npm run pretravis
      - run: npm run prepublishOnly
      - run: npm run posttravis

  prepublish-react:
    name: 'prepublish tests (react config)'
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        eslint:
          - 8
          - 7
        package:
          - eslint-config-airbnb
        react-hooks:
          - 4

    defaults:
      run:
        working-directory: ""packages/${{ matrix.package }}""

    steps:
      - uses: actions/checkout@v2
      - uses: ljharb/actions/node/install@main
        name: 'nvm install lts/* && npm install'
        with:
          before_install: cd ""packages/${{ matrix.package }}""
          node-version: lts/*
          after_install: |
            npm install --no-save ""eslint@${{ matrix.eslint }}""
      - run: npm install --no-save ""eslint-plugin-react-hooks@${{ matrix.react-hooks }}""
        if: ${{ matrix.react-hooks > 0}}
      - run: node -pe ""require('eslint/package.json').version""
        name: 'eslint version'
      - run: npm run pretravis
      - run: npm run prepublishOnly
      - run: npm run posttravis

  node:
    name: 'node 10+'
    needs: [base, prepublish-base, react, prepublish-react]
    runs-on: ubuntu-latest
    steps:
      - run: 'echo tests completed'
",163,6,2,"pull_request, push",11
airbnb/javascript,rebase.yml,"name: Automatic Rebase

on: [pull_request_target]

jobs:
  _:
    name: ""Automatic Rebase""

    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v2
    - uses: ljharb/rebase@master
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
",15,1,1,pull_request_target,2
airbnb/javascript,require-allow-edits.yml,"name: Require “Allow Edits”

on: [pull_request_target]

jobs:
  _:
    name: ""Require “Allow Edits”""

    runs-on: ubuntu-latest

    steps:
    - uses: ljharb/require-allow-edits@main
",12,1,1,pull_request_target,1
avelino/awesome-go,check-for-spammy-issues.yml,"name: Issues spammy check
on:
  issues:
    types: [opened]

jobs:
  mark-as-spam:
    name: Remove issues with spammy
    runs-on: ubuntu-latest
    steps:
      - name: close issue
        uses: balevine/mark-as-spam@v1.0
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
",14,1,1,issues,1
avelino/awesome-go,pull-request-first-comment.yaml,"name: First comment in new pull request

on:
  pull_request_target:
    types: [opened]

jobs:
  commentCreated:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
      issues: write
    environment: action
    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - name: first comment
        uses: peter-evans/create-or-update-comment@v3
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          issue-number: ${{ github.event.pull_request.number }}
          body: |
            Thank you for contributing to [awesome-go](https://awesome-go.com/). We will review your contribution as soon as possible.
            
            Make sure you add the links in the body of the pull request that are requested in the [contribution guide](https://github.com/avelino/awesome-go/blob/main/CONTRIBUTING.md):
            - repo link
            - pkg.go.dev
            - goreportcard.com
            - coverage
            
            > Your project is under review. It may take a few days to be approved.
",31,1,1,pull_request_target,1
avelino/awesome-go,run-check.yaml,"name: Check For Stale Repositories
on:
  workflow_dispatch:
  schedule:
    - cron: '0 0 * * 0'

permissions:
  contents: read  #  to fetch code (actions/checkout)

jobs:
  build:
    name: Running test
    runs-on: ubuntu-latest
    container: golang:latest
    steps:
    - uses: actions/checkout@v4
    - name: Get dependencies
      run: go get -v -t -d ./...
    - name: run script
      run: go test -v -run ^TestStaleRepository$
      env:
        OAUTH_TOKEN: ${{secrets.OAUTH_TOKEN}}
",22,1,2,"workflow_dispatch, schedule",1
avelino/awesome-go,site-deploy.yaml,"name: site-deploy

on:
  push:
    branches:
      - ""main""

permissions:
  contents: read #  to fetch code (actions/checkout)

jobs:
  build:
    name: Make and Deploy site
    runs-on: ubuntu-latest
    environment: netlify
    container: golang:latest
    steps:
      - uses: actions/checkout@v4
      - name: Get dependencies
        run: go get -v -t -d ./...
      - name: Make awesome-go.com
        run: go run .
      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: 20
      - name: deploy awesome-go.com
        uses: nwtgck/actions-netlify@v3.0
        with:
          publish-dir: ""./out""
          production-branch: main
          production-deploy: true
          enable-pull-request-comment: false
          enable-commit-comment: false
          enable-commit-status: false
          overwrites-pull-request-comment: false
        env:
          NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}
          NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}
        timeout-minutes: 1
",40,1,1,push,3
avelino/awesome-go,tests.yaml,"name: tests

on:
  push:
    branches:
      - 'main'
  pull_request:

permissions:
  contents: read  #  to fetch code (actions/checkout)

jobs:
  build:
    name: Running test
    runs-on: ubuntu-latest
    container: golang:latest
    steps:
    - uses: actions/checkout@v4
    - name: Get dependencies
      run: go get -v -t -d ./...
    - name: Run tests
      run: go test main_test.go main.go
",22,1,2,"push, pull_request",1
huggingface/transformers,add-model-like.yml,"name: Add model like runner

on:
  push:
    branches:
      - none # put main here when this is fixed
  #pull_request:
  #  paths:
  #    - ""src/**""
  #    - ""tests/**""
  #    - "".github/**""
  #  types: [opened, synchronize, reopened]

jobs:
  run_tests_templates_like:
    name: ""Add new model like template tests""
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt -y update && sudo apt install -y libsndfile1-dev

      - name: Load cached virtual environment
        uses: actions/cache@v4
        id: cache
        with:
          path: ~/venv/
          key: v4-tests_model_like-${{ hashFiles('setup.py') }}

      - name: Create virtual environment on cache miss
        if: steps.cache.outputs.cache-hit != 'true'
        run: |
          python -m venv ~/venv && . ~/venv/bin/activate
          pip install --upgrade pip!=21.3
          pip install -e .[dev]

      - name: Check transformers location
        # make `transformers` available as package (required since we use `-e` flag) and check it's indeed from the repo.
        run: |
          . ~/venv/bin/activate
          python setup.py develop
          transformers_install=$(pip list -e | grep transformers)
          transformers_install_array=($transformers_install)
          transformers_loc=${transformers_install_array[-1]}
          transformers_repo_loc=$(pwd .)
          if [ ""$transformers_loc"" != ""$transformers_repo_loc"" ]; then
              echo ""transformers is from $transformers_loc but it shoud be from $transformers_repo_loc/src.""
              echo ""A fix is required. Stop testing.""
              exit 1
          fi

      - name: Create model files
        run: |
          . ~/venv/bin/activate
          transformers add-new-model-like --config_file tests/fixtures/add_distilbert_like_config.json --path_to_repo .
          make style
          make fix-copies

      - name: Run all PyTorch modeling test
        run: |
          . ~/venv/bin/activate
          python -m pytest -n 2 --dist=loadfile -s --make-reports=tests_new_models tests/bert_new/test_modeling_bert_new.py

      - name: Run style changes
        run: |
          . ~/venv/bin/activate
          make style && make quality && make repo-consistency

      - name: Failure short reports
        if: ${{ always() }}
        run: cat reports/tests_new_models/failures_short.txt

      - name: Test suite reports artifacts
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: run_all_tests_new_models_test_reports
          path: reports/tests_new_models
",80,1,1,push,3
huggingface/transformers,assign-reviewers.yml,"name: Assign PR Reviewers
on:
  pull_request_target:
    branches:
      - main
    types: [ready_for_review]

jobs:
  assign_reviewers:
    permissions:
       pull-requests: write
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install PyGithub
      - name: Run assignment script
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: python .github/scripts/assign_reviewers.py",26,1,1,pull_request_target,2
huggingface/transformers,benchmark.yml,"name: Self-hosted runner (benchmark)

on:
  push:
    branches: [main]
  pull_request:
    types: [ opened, labeled, reopened, synchronize ]

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

env:
  HF_HOME: /mnt/cache

jobs:
  benchmark:
    name: Benchmark
    strategy:
      matrix:
        # group: [aws-g5-4xlarge-cache, aws-p4d-24xlarge-plus] (A100 runner is not enabled)
        group: [aws-g5-4xlarge-cache]
    runs-on:
      group: ${{ matrix.group }}
    if: |
      (github.event_name == 'pull_request' && contains( github.event.pull_request.labels.*.name, 'run-benchmark') )||
      (github.event_name == 'push' && github.ref == 'refs/heads/main')
    container:
      image: huggingface/transformers-pytorch-gpu
      options: --gpus all --privileged --ipc host
    steps:
      - name: Get repo
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}

      - name: Install libpq-dev & psql
        run: |
          apt update
          apt install -y libpq-dev postgresql-client

      - name: Install benchmark script dependencies
        run: python3 -m pip install -r benchmark/requirements.txt

      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)
        working-directory: /transformers
        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e "".[torch]""

      - name: Run database init script
        run: |
          psql -f benchmark/init_db.sql
        env:
          PGDATABASE: metrics
          PGHOST: ${{ secrets.TRANSFORMERS_BENCHMARKS_PGHOST }}
          PGUSER: transformers_benchmarks
          PGPASSWORD: ${{ secrets.TRANSFORMERS_BENCHMARKS_PGPASSWORD }}

      - name: Run benchmark
        run: |
          git config --global --add safe.directory /__w/transformers/transformers
          if [ ""$GITHUB_EVENT_NAME"" = ""pull_request"" ]; then
            commit_id=$(echo ""${{ github.event.pull_request.head.sha }}"")
          elif [ ""$GITHUB_EVENT_NAME"" = ""push"" ]; then
            commit_id=$GITHUB_SHA
          fi
          commit_msg=$(git show -s --format=%s | cut -c1-70)
          python3 benchmark/benchmarks_entrypoint.py ""huggingface/transformers"" ""$BRANCH_NAME"" ""$commit_id"" ""$commit_msg""
        env:
          HF_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}
          # Enable this to see debug logs
          # HF_HUB_VERBOSITY: debug
          # TRANSFORMERS_VERBOSITY: debug
          PGHOST: ${{ secrets.TRANSFORMERS_BENCHMARKS_PGHOST }}
          PGUSER: transformers_benchmarks
          PGPASSWORD: ${{ secrets.TRANSFORMERS_BENCHMARKS_PGPASSWORD }}
          BRANCH_NAME: ${{ github.head_ref || github.ref_name }}
",76,1,2,"push, pull_request",1
huggingface/transformers,build-ci-docker-images.yml,"name: Build pr ci-docker

on:
  push:
    branches:
      - push-ci-image # for now let's only build on this branch
  repository_dispatch:
  workflow_call:
    inputs:
      image_postfix:
        required: true
        type: string
  schedule:
    - cron: ""6 0 * * *""


concurrency:
  group: ${{ github.workflow }}
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-22.04

    if: ${{ contains(github.event.head_commit.message, '[build-ci-image]') || contains(github.event.head_commit.message, '[push-ci-image]') && '!cancelled()' || github.event_name == 'schedule' }}

    strategy:
      matrix:
        file: [""quality"", ""consistency"", ""custom-tokenizers"", ""torch-light"", ""tf-light"", ""exotic-models"", ""torch-tf-light"", ""jax-light"", ""examples-torch"",  ""examples-tf""]
    continue-on-error: true

    steps:
      -
        name: Set tag
        run: |
              if ${{contains(github.event.head_commit.message, '[build-ci-image]')}}; then
                  echo ""TAG=huggingface/transformers-${{ matrix.file }}:dev"" >> ""$GITHUB_ENV""
                  echo ""setting it to DEV!""
              else
                  echo ""TAG=huggingface/transformers-${{ matrix.file }}"" >> ""$GITHUB_ENV""

              fi
      -
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      -
        name: Check out code
        uses: actions/checkout@v4
      -
        name: Login to DockerHub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_PASSWORD }}
      -
        name: Build ${{ matrix.file }}.dockerfile
        uses: docker/build-push-action@v5
        with:
          context: ./docker
          build-args: |
            REF=${{ github.sha }}
          file: ""./docker/${{ matrix.file }}.dockerfile""
          push: ${{ contains(github.event.head_commit.message, 'ci-image]') ||  github.event_name == 'schedule' }}
          tags: ${{ env.TAG }}

  notify:
    runs-on: ubuntu-22.04
    if: ${{ contains(github.event.head_commit.message, '[build-ci-image]') || contains(github.event.head_commit.message, '[push-ci-image]') && '!cancelled()' || github.event_name == 'schedule' }}
    steps:
      - name: Post to Slack
        if: ${{ contains(github.event.head_commit.message, '[push-ci-image]') && github.event_name != 'schedule' }}
        uses: huggingface/hf-workflows/.github/actions/post-slack@main
        with:
          slack_channel: ""#transformers-ci-circleci-images""
          title: 🤗 New docker images for CircleCI are pushed.
          status: ${{ job.status }}
          slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}
",77,2,4,"push, repository_dispatch, workflow_call, schedule",5
huggingface/transformers,build-docker-images.yml,"name: Build docker images (scheduled)

on:
  push:
    branches:
      - build_ci_docker_image*
  repository_dispatch:
  workflow_call:
    inputs:
      image_postfix:
        required: true
        type: string
  schedule:
    - cron: ""17 0 * * *""

concurrency:
  group: docker-images-builds
  cancel-in-progress: false

jobs:
  latest-docker:
    name: ""Latest PyTorch [dev]""
    runs-on:
      group: aws-general-8-plus
    steps:
      -
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      -
        name: Check out code
        uses: actions/checkout@v4
      -
        name: Login to DockerHub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_PASSWORD }}
      -
        name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: ./docker/transformers-all-latest-gpu
          build-args: |
            REF=main
          push: true
          tags: huggingface/transformers-all-latest-gpu${{ inputs.image_postfix }}
      # Push CI images still need to be re-built daily
      -
        name: Build and push (for Push CI) in a daily basis
        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.
        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!
        if: inputs.image_postfix != '-push-ci'
        uses: docker/build-push-action@v5
        with:
          context: ./docker/transformers-all-latest-gpu
          build-args: |
            REF=main
          push: true
          tags: huggingface/transformers-all-latest-gpu-push-ci

      - name: Post to Slack
        if: always()
        uses: huggingface/hf-workflows/.github/actions/post-slack@main
        with:
          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}
          title: 🤗 Results of the transformers-all-latest-gpu-push-ci docker build
          status: ${{ job.status }}
          slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}

  latest-torch-deepspeed-docker:
    name: ""Latest PyTorch + DeepSpeed""
    runs-on:
      group: aws-g4dn-2xlarge-cache
    steps:
      -
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      -
        name: Check out code
        uses: actions/checkout@v4
      -
        name: Login to DockerHub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_PASSWORD }}
      -
        name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: ./docker/transformers-pytorch-deepspeed-latest-gpu
          build-args: |
            REF=main
          push: true
          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu${{ inputs.image_postfix }}

      - name: Post to Slack
        if: always()
        uses: huggingface/hf-workflows/.github/actions/post-slack@main
        with:
          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER}}
          title: 🤗 Results of the transformers-pytorch-deepspeed-latest-gpu docker build
          status: ${{ job.status }}
          slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}

  # Can't build 2 images in a single job `latest-torch-deepspeed-docker` (for `nvcr.io/nvidia`)
  latest-torch-deepspeed-docker-for-push-ci-daily-build:
    name: ""Latest PyTorch + DeepSpeed (Push CI - Daily Build)""
    runs-on:
      group: aws-general-8-plus
    steps:
      -
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      -
        name: Check out code
        uses: actions/checkout@v4
      -
        name: Login to DockerHub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_PASSWORD }}
      # Push CI images still need to be re-built daily
      -
        name: Build and push (for Push CI) in a daily basis
        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.
        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!
        if: inputs.image_postfix != '-push-ci'
        uses: docker/build-push-action@v5
        with:
          context: ./docker/transformers-pytorch-deepspeed-latest-gpu
          build-args: |
            REF=main
          push: true
          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci

      - name: Post to Slack
        if: always()
        uses: huggingface/hf-workflows/.github/actions/post-slack@main
        with:
          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}
          title: 🤗 Results of the transformers-pytorch-deepspeed-latest-gpu-push-ci docker build
          status: ${{ job.status }}
          slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}

  doc-builder:
    name: ""Doc builder""
    # Push CI doesn't need this image
    if: inputs.image_postfix != '-push-ci'
    runs-on:
      group: aws-general-8-plus
    steps:
      -
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      -
        name: Check out code
        uses: actions/checkout@v4
      -
        name: Login to DockerHub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_PASSWORD }}
      -
        name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: ./docker/transformers-doc-builder
          push: true
          tags: huggingface/transformers-doc-builder

      - name: Post to Slack
        if: always()
        uses: huggingface/hf-workflows/.github/actions/post-slack@main
        with:
          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}
          title: 🤗 Results of the huggingface/transformers-doc-builder docker build
          status: ${{ job.status }}
          slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}

  latest-pytorch:
    name: ""Latest PyTorch [dev]""
    # Push CI doesn't need this image
    if: inputs.image_postfix != '-push-ci'
    runs-on:
      group: aws-general-8-plus
    steps:
      -
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      -
        name: Check out code
        uses: actions/checkout@v4
      -
        name: Login to DockerHub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_PASSWORD }}
      -
        name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: ./docker/transformers-pytorch-gpu
          build-args: |
            REF=main
          push: true
          tags: huggingface/transformers-pytorch-gpu

      - name: Post to Slack
        if: always()
        uses: huggingface/hf-workflows/.github/actions/post-slack@main
        with:
          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}
          title: 🤗 Results of the huggingface/transformers-pytorch-gpudocker build
          status: ${{ job.status }}
          slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}

  latest-pytorch-amd:
    name: ""Latest PyTorch (AMD) [dev]""
    runs-on:
      group: aws-general-8-plus
    steps:
      -
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      -
        name: Check out code
        uses: actions/checkout@v4
      -
        name: Login to DockerHub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_PASSWORD }}
      -
        name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: ./docker/transformers-pytorch-amd-gpu
          build-args: |
            REF=main
          push: true
          tags: huggingface/transformers-pytorch-amd-gpu${{ inputs.image_postfix }}
      # Push CI images still need to be re-built daily
      -
        name: Build and push (for Push CI) in a daily basis
        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.
        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!
        if: inputs.image_postfix != '-push-ci'
        uses: docker/build-push-action@v5
        with:
          context: ./docker/transformers-pytorch-amd-gpu
          build-args: |
            REF=main
          push: true
          tags: huggingface/transformers-pytorch-amd-gpu-push-ci

      - name: Post to Slack
        if: always()
        uses: huggingface/hf-workflows/.github/actions/post-slack@main
        with:
          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}
          title: 🤗 Results of the huggingface/transformers-pytorch-amd-gpu-push-ci build
          status: ${{ job.status }}
          slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}

  latest-pytorch-deepspeed-amd:
    name: ""PyTorch + DeepSpeed (AMD) [dev]""
    runs-on:
      group: aws-general-8-plus
    steps:
      -
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      -
        name: Check out code
        uses: actions/checkout@v4
      -
        name: Login to DockerHub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_PASSWORD }}
      -
        name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: ./docker/transformers-pytorch-deepspeed-amd-gpu
          build-args: |
            REF=main
          push: true
          tags: huggingface/transformers-pytorch-deepspeed-amd-gpu${{ inputs.image_postfix }}
      # Push CI images still need to be re-built daily
      -
        name: Build and push (for Push CI) in a daily basis
        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.
        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!
        if: inputs.image_postfix != '-push-ci'
        uses: docker/build-push-action@v5
        with:
          context: ./docker/transformers-pytorch-deepspeed-amd-gpu
          build-args: |
            REF=main
          push: true
          tags: huggingface/transformers-pytorch-deepspeed-amd-gpu-push-ci

      - name: Post to Slack
        if: always()
        uses: huggingface/hf-workflows/.github/actions/post-slack@main
        with:
          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}
          title: 🤗 Results of the transformers-pytorch-deepspeed-amd-gpu build
          status: ${{ job.status }}
          slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}

  latest-quantization-torch-docker:
    name: ""Latest Pytorch + Quantization [dev]""
     # Push CI doesn't need this image
    if: inputs.image_postfix != '-push-ci'
    runs-on:
      group: aws-general-8-plus
    steps:
      -
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      -
        name: Check out code
        uses: actions/checkout@v4
      -
        name: Login to DockerHub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_PASSWORD }}
      -
        name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: ./docker/transformers-quantization-latest-gpu
          build-args: |
            REF=main
          push: true
          tags: huggingface/transformers-quantization-latest-gpu${{ inputs.image_postfix }}

      - name: Post to Slack
        if: always()
        uses: huggingface/hf-workflows/.github/actions/post-slack@main
        with:
          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}
          title: 🤗 Results of the transformers-quantization-latest-gpu build
          status: ${{ job.status }}
          slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}
",355,8,4,"push, repository_dispatch, workflow_call, schedule",43
huggingface/transformers,build-nightly-ci-docker-images.yml,"name: Build docker images (Nightly CI)

on:
  workflow_call:
  push:
    branches:
      - build_nightly_ci_docker_image*

concurrency:
  group: docker-images-builds
  cancel-in-progress: false

jobs:
  latest-with-torch-nightly-docker:
    name: ""Nightly PyTorch + Stable TensorFlow""
    runs-on:
      group: aws-general-8-plus
    steps:
      -
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      -
        name: Check out code
        uses: actions/checkout@v4
      -
        name: Login to DockerHub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_PASSWORD }}
      -
        name: Build and push
        uses: docker/build-push-action@v3
        with:
          context: ./docker/transformers-all-latest-gpu
          build-args: |
            REF=main
            PYTORCH=pre
          push: true
          tags: huggingface/transformers-all-latest-torch-nightly-gpu

  nightly-torch-deepspeed-docker:
    name: ""Nightly PyTorch + DeepSpeed""
    runs-on:
      group: aws-g4dn-2xlarge-cache
    steps:
      -
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      -
        name: Check out code
        uses: actions/checkout@v4
      -
        name: Login to DockerHub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_PASSWORD }}
      -
        name: Build and push
        uses: docker/build-push-action@v3
        with:
          context: ./docker/transformers-pytorch-deepspeed-nightly-gpu
          build-args: |
            REF=main
          push: true
          tags: huggingface/transformers-pytorch-deepspeed-nightly-gpu
",67,2,2,"workflow_call, push",8
huggingface/transformers,build-past-ci-docker-images.yml,"name: Build docker images (Past CI)

on:
  push:
    branches:
      - build_past_ci_docker_image*

concurrency:
  group: docker-images-builds
  cancel-in-progress: false

jobs:
  past-pytorch-docker:
    name: ""Past PyTorch Docker""
    strategy:
      fail-fast: false
      matrix:
        version: [""1.13"", ""1.12"", ""1.11""]
    runs-on:
      group: aws-general-8-plus
    steps:
      -
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      -
        name: Check out code
        uses: actions/checkout@v4
      -
        id: get-base-image
        name: Get Base Image
        env:
          framework_version: ${{ matrix.version }}
        run: |
          echo ""base_image=$(python3 -c 'import os; from utils.past_ci_versions import past_versions_testing; base_image = past_versions_testing[""pytorch""][os.environ[""framework_version""]][""base_image""]; print(base_image)')"" >> $GITHUB_OUTPUT
      -
        name: Print Base Image
        run: |
          echo ${{ steps.get-base-image.outputs.base_image }}
      -
        name: Login to DockerHub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_PASSWORD }}
      -
        name: Build and push
        uses: docker/build-push-action@v3
        with:
          context: ./docker/transformers-past-gpu
          build-args: |
            REF=main
            BASE_DOCKER_IMAGE=${{ steps.get-base-image.outputs.base_image }}
            FRAMEWORK=pytorch
            VERSION=${{ matrix.version }}
          push: true
          tags: huggingface/transformers-pytorch-past-${{ matrix.version }}-gpu

  past-tensorflow-docker:
    name: ""Past TensorFlow Docker""
    strategy:
      fail-fast: false
      matrix:
        version: [""2.11"", ""2.10"", ""2.9"", ""2.8"", ""2.7"", ""2.6"", ""2.5""]
    runs-on:
      group: aws-general-8-plus
    steps:
      -
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      -
        name: Check out code
        uses: actions/checkout@v4
      -
        id: get-base-image
        name: Get Base Image
        env:
          framework_version: ${{ matrix.version }}
        run: |
          echo ""base_image=$(python3 -c 'import os; from utils.past_ci_versions import past_versions_testing; base_image = past_versions_testing[""tensorflow""][os.environ[""framework_version""]][""base_image""]; print(base_image)')"" >> $GITHUB_OUTPUT
      -
        name: Print Base Image
        run: |
          echo ${{ steps.get-base-image.outputs.base_image }}
      -
        name: Login to DockerHub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_PASSWORD }}
      -
        name: Build and push
        uses: docker/build-push-action@v3
        with:
          context: ./docker/transformers-past-gpu
          build-args: |
            REF=main
            BASE_DOCKER_IMAGE=${{ steps.get-base-image.outputs.base_image }}
            FRAMEWORK=tensorflow
            VERSION=${{ matrix.version }}
          push: true
          tags: huggingface/transformers-tensorflow-past-${{ matrix.version }}-gpu
",101,2,1,push,8
huggingface/transformers,build_documentation.yml,"name: Build documentation

on:
  workflow_dispatch:
  push:
    branches:
      - main
      - doc-builder*
      - v*-release
      - use_templates

jobs:
   build:
    uses: huggingface/doc-builder/.github/workflows/build_main_documentation.yml@main
    with:
      commit_sha: ${{ github.sha }}
      package: transformers
      notebook_folder: transformers_doc
      languages: ar de en es fr hi it ko pt tr zh ja te
      custom_container: huggingface/transformers-doc-builder
    secrets:
      token: ${{ secrets.HUGGINGFACE_PUSH }}
      hf_token: ${{ secrets.HF_DOC_BUILD_PUSH }}
",23,1,2,"workflow_dispatch, push",1
huggingface/transformers,build_pr_documentation.yml,"name: Build PR Documentation

on:
  pull_request:

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  build:
    uses: huggingface/doc-builder/.github/workflows/build_pr_documentation.yml@main
    with:
      commit_sha: ${{ github.event.pull_request.head.sha }}
      pr_number: ${{ github.event.number }}
      package: transformers
      languages: en
",17,1,1,pull_request,1
huggingface/transformers,check_failed_tests.yml,"name: Process failed tests

on:
  workflow_call:
    inputs:
      docker:
        required: true
        type: string
      start_sha:
        required: true
        type: string
      job:
        required: true
        type: string
      slack_report_channel:
        required: true
        type: string
      ci_event:
        required: true
        type: string
      report_repo_id:
        required: true
        type: string


env:
  HF_HOME: /mnt/cache
  TRANSFORMERS_IS_CI: yes
  OMP_NUM_THREADS: 8
  MKL_NUM_THREADS: 8
  RUN_SLOW: yes
  # For gated repositories, we still need to agree to share information on the Hub repo. page in order to get access.
  # This token is created under the bot `hf-transformers-bot`.
  HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}
  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}
  TF_FORCE_GPU_ALLOW_GROWTH: true
  CUDA_VISIBLE_DEVICES: 0,1


jobs:
  check_new_failures:
    name: "" ""
    runs-on:
      group: aws-g4dn-4xlarge-cache
    container:
      image: ${{ inputs.docker }}
      options: --gpus all --shm-size ""16gb"" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    steps:
      - uses: actions/download-artifact@v4
        with:
          name: ci_results_${{ inputs.job }}
          path: /transformers/ci_results_${{ inputs.job }}

      - name: Check file
        working-directory: /transformers
        run: |
          if [ -f ci_results_${{ inputs.job }}/new_failures.json ]; then
            echo ""`ci_results_${{ inputs.job }}/new_failures.json` exists, continue ...""
            echo ""process=true"" >> $GITHUB_ENV
          else
            echo ""`ci_results_${{ inputs.job }}/new_failures.json` doesn't exist, abort.""
            echo ""process=false"" >> $GITHUB_ENV
          fi

      - uses: actions/download-artifact@v4
        if: ${{ env.process == 'true' }}
        with:
          pattern: setup_values*
          path: setup_values
          merge-multiple: true

      - name: Prepare some setup values
        if: ${{ env.process == 'true' }}
        run: |
          if [ -f setup_values/prev_workflow_run_id.txt ]; then
            echo ""PREV_WORKFLOW_RUN_ID=$(cat setup_values/prev_workflow_run_id.txt)"" >> $GITHUB_ENV
          else
            echo ""PREV_WORKFLOW_RUN_ID="" >> $GITHUB_ENV
          fi

          if [ -f setup_values/other_workflow_run_id.txt ]; then
            echo ""OTHER_WORKFLOW_RUN_ID=$(cat setup_values/other_workflow_run_id.txt)"" >> $GITHUB_ENV
          else
            echo ""OTHER_WORKFLOW_RUN_ID="" >> $GITHUB_ENV
          fi

      - name: Update clone
        working-directory: /transformers
        if: ${{ env.process == 'true' }}
        run: git fetch && git checkout ${{ github.sha }}

      - name: Get target commit
        working-directory: /transformers/utils
        if: ${{ env.process == 'true' }}
        run: |
          echo ""END_SHA=$(TOKEN=${{ secrets.ACCESS_REPO_INFO_TOKEN }} python3 -c 'import os; from get_previous_daily_ci import get_last_daily_ci_run_commit; commit=get_last_daily_ci_run_commit(token=os.environ[""TOKEN""], workflow_run_id=os.environ[""PREV_WORKFLOW_RUN_ID""]); print(commit)')"" >> $GITHUB_ENV

      - name: Checkout to `start_sha`
        working-directory: /transformers
        if: ${{ env.process == 'true' }}
        run: git fetch && git checkout ${{ inputs.start_sha }}

      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)
        working-directory: /transformers
        if: ${{ env.process == 'true' }}
        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .

      - name: NVIDIA-SMI
        if: ${{ env.process == 'true' }}
        run: |
          nvidia-smi

      - name: Environment
        working-directory: /transformers
        if: ${{ env.process == 'true' }}
        run: |
          python3 utils/print_env.py

      - name: Show installed libraries and their versions
        working-directory: /transformers
        if: ${{ env.process == 'true' }}
        run: pip freeze

      - name: Check failed tests
        working-directory: /transformers
        if: ${{ env.process == 'true' }}
        run: python3 utils/check_bad_commit.py --start_commit ${{ inputs.start_sha }} --end_commit ${{ env.END_SHA }} --file ci_results_${{ inputs.job }}/new_failures.json --output_file new_failures_with_bad_commit.json

      - name: Show results
        working-directory: /transformers
        if: ${{ env.process == 'true' }}
        run: |
          ls -l new_failures_with_bad_commit.json
          cat new_failures_with_bad_commit.json

      - name: Checkout back
        working-directory: /transformers
        if: ${{ env.process == 'true' }}
        run: |
          git checkout ${{ inputs.start_sha }}

      - name: Process report
        shell: bash
        working-directory: /transformers
        if: ${{ env.process == 'true' }}
        env:
          ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}
          TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN: ${{ secrets.TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN }}
          JOB_NAME: ${{ inputs.job }}
          REPORT_REPO_ID: ${{ inputs.report_repo_id }}
        run: |
          python3 utils/process_bad_commit_report.py

      - name: Process report
        shell: bash
        working-directory: /transformers
        if: ${{ env.process == 'true' }}
        env:
          ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}
          TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN: ${{ secrets.TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN }}
          JOB_NAME: ${{ inputs.job }}
          REPORT_REPO_ID: ${{ inputs.report_repo_id }}
        run: |
          {
            echo 'REPORT_TEXT<<EOF'
            python3 utils/process_bad_commit_report.py
            echo EOF
          } >> ""$GITHUB_ENV""

      - name: Prepare Slack report title
        working-directory: /transformers
        if: ${{ env.process == 'true' }}
        run: |
          pip install slack_sdk
          echo ""title=$(python3 -c 'import sys; sys.path.append(""utils""); from utils.notification_service import job_to_test_map; ci_event = ""${{ inputs.ci_event }}""; job = ""${{ inputs.job }}""; test_name = job_to_test_map[job]; title = f""New failed tests of {ci_event}"" + "":"" + f"" {test_name}""; print(title)')"" >> $GITHUB_ENV

      - name: Send processed report
        if: ${{ env.process == 'true' && !endsWith(env.REPORT_TEXT, '{}') }}
        uses: slackapi/slack-github-action@6c661ce58804a1a20f6dc5fbee7f0381b469e001
        with:
          # Slack channel id, channel name, or user id to post message.
          # See also: https://api.slack.com/methods/chat.postMessage#channels
          channel-id: '#${{ inputs.slack_report_channel }}'
          # For posting a rich message using Block Kit
          payload: |
            {
              ""blocks"": [
                {
                  ""type"": ""header"",
                  ""text"": {
                    ""type"": ""plain_text"",
                    ""text"": ""${{ env.title }}""
                  }
                },
                {
                  ""type"": ""section"",
                  ""text"": {
                    ""type"": ""mrkdwn"",
                    ""text"": ""${{ env.REPORT_TEXT }}""
                  }
                }
              ]
            }
        env:
          SLACK_BOT_TOKEN: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}
",205,1,1,workflow_call,3
huggingface/transformers,check_tiny_models.yml,"name: Check Tiny Models

on:
  push:
    branches:
      - check_tiny_models*
  repository_dispatch:
  schedule:
    - cron: ""0 2 * * *""

env:
  TOKEN: ${{ secrets.TRANSFORMERS_HUB_BOT_HF_TOKEN }}

jobs:
  check_tiny_models:
    name: Check tiny models
    runs-on: ubuntu-22.04
    steps:
      - name: Checkout transformers
        uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - uses: actions/checkout@v4
      - name: Set up Python 3.8
        uses: actions/setup-python@v5
        with:
          # Semantic version range syntax or exact version of a Python version
          python-version: '3.8'
          # Optional - x64 or x86 architecture, defaults to x64
          architecture: 'x64'

      - name: Install
        run: |
          sudo apt-get -y update && sudo apt-get install -y libsndfile1-dev espeak-ng cmake
          pip install --upgrade pip
          python -m pip install -U .[sklearn,torch,testing,sentencepiece,torch-speech,vision,timm,video,tf-cpu]
          pip install tensorflow_probability
          python -m pip install -U 'natten<0.15.0'

      - name: Create all tiny models (locally)
        run: |
          python utils/create_dummy_models.py tiny_local_models --all --num_workers 2

      - name: Local tiny model reports artifacts
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: tiny_local_model_creation_reports
          path: tiny_local_models/reports

      # GitHub-hosted runners have 2-core CPUs
      - name: Run pipeline tests against all new (local) tiny models
        run: |
          OMP_NUM_THREADS=1 TRANSFORMERS_TINY_MODEL_PATH=tiny_local_models python -m pytest --max-worker-restart=0 -n 2 --dist=loadfile -s -rA --make-reports=tests_pipelines tests/models -m is_pipeline_test -k ""test_pipeline_"" | tee tests_output.txt

      - name: Test suite reports artifacts
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: tiny_local_model_creation_reports
          path: reports/tests_pipelines

      - name: Create + Upload tiny models for new model architecture(s)
        run: |
          python utils/update_tiny_models.py --num_workers 2

      - name: Full report
        run: cat tiny_models/reports/tiny_model_creation_report.json

      - name: Failure report
        run: cat tiny_models/reports/simple_failed_report.txt

      - name: Summary report
        run: cat tiny_models/reports/tiny_model_summary.json

      - name: New tiny model creation reports artifacts
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: tiny_model_creation_reports
          path: tiny_models/reports
",82,1,3,"push, repository_dispatch, schedule",6
huggingface/transformers,doctest_job.yml,"name: Doctest job

on:
  workflow_call:
    inputs:
      job_splits:
        required: true
        type: string
      split_keys:
        required: true
        type: string

env:
  HF_HOME: /mnt/cache
  TRANSFORMERS_IS_CI: yes
  RUN_SLOW: yes
  OMP_NUM_THREADS: 16
  MKL_NUM_THREADS: 16
  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}
  TF_FORCE_GPU_ALLOW_GROWTH: true

jobs:
  run_doctests:
    name: "" ""
    strategy:
      max-parallel: 8  # 8 jobs at a time
      fail-fast: false
      matrix:
        split_keys: ${{ fromJson(inputs.split_keys) }}
    runs-on: 
      group: aws-g4dn-4xlarge-cache
    container:
      image: huggingface/transformers-all-latest-gpu
      options: --gpus 0 --shm-size ""16gb"" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    steps:
      - name: Update clone
        working-directory: /transformers
        run: git fetch && git checkout ${{ github.sha }}

      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)
        working-directory: /transformers
        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .[flax]

      - name: GPU visibility
        working-directory: /transformers
        run: |
          python3 utils/print_env.py

      - name: Show installed libraries and their versions
        run: pip freeze

      - name: Get doctest files
        working-directory: /transformers
        run: |
          echo ""${{ toJson(fromJson(inputs.job_splits)[matrix.split_keys]) }}"" > doc_tests.txt
          cat doc_tests.txt

      - name: Set `split_keys`
        shell: bash
        run: |
          echo ""${{ matrix.split_keys }}""
          split_keys=${{ matrix.split_keys }}
          split_keys=${split_keys//'/'/'_'}
          echo ""split_keys""
          echo ""split_keys=$split_keys"" >> $GITHUB_ENV

      - name: Run doctests
        working-directory: /transformers
        run: |
          cat doc_tests.txt
          python3 -m pytest -v --make-reports doc_tests_gpu_${{ env.split_keys }} --doctest-modules $(cat doc_tests.txt) -sv --doctest-continue-on-failure --doctest-glob=""*.md""

      - name: Failure short reports
        if: ${{ failure() }}
        continue-on-error: true
        run: cat /transformers/reports/doc_tests_gpu_${{ env.split_keys }}/failures_short.txt

      - name: ""Test suite reports artifacts: doc_tests_gpu_test_reports_${{ env.split_keys }}""
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: doc_tests_gpu_test_reports_${{ env.split_keys }}
          path: /transformers/reports/doc_tests_gpu_${{ env.split_keys }}
",83,1,1,workflow_call,1
huggingface/transformers,doctests.yml,"name: Doctests

on:
  push:
    branches:
      - run_doctest*
  repository_dispatch:
  schedule:
    - cron: ""17 2 * * *""

env:
  NUM_SLICES: 3

jobs:
  setup:
    name: Setup
    runs-on: 
      group: aws-g4dn-4xlarge-cache
    container:
      image: huggingface/transformers-all-latest-gpu
      options: --gpus 0 --shm-size ""16gb"" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    outputs:
      job_splits: ${{ steps.set-matrix.outputs.job_splits }}
      split_keys: ${{ steps.set-matrix.outputs.split_keys }}
    steps:
      - name: Update clone
        working-directory: /transformers
        run: |
          git fetch && git checkout ${{ github.sha }}

      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)
        working-directory: /transformers
        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .

      - name: Show installed libraries and their versions
        working-directory: /transformers
        run: pip freeze

      - name: Check values for matrix
        working-directory: /transformers
        run: |
          python3 utils/split_doctest_jobs.py
          python3 utils/split_doctest_jobs.py --only_return_keys --num_splits ${{ env.NUM_SLICES }}

      - id: set-matrix
        working-directory: /transformers
        name: Set values for matrix
        run: |
          echo ""job_splits=$(python3 utils/split_doctest_jobs.py)"" >> $GITHUB_OUTPUT
          echo ""split_keys=$(python3 utils/split_doctest_jobs.py --only_return_keys --num_splits ${{ env.NUM_SLICES }})"" >> $GITHUB_OUTPUT

  call_doctest_job:
    name: ""Call doctest jobs""
    needs: setup
    strategy:
      max-parallel: 1  # 1 split at a time (in `doctest_job.yml`, we set `8` to run 8 jobs at the same time)
      fail-fast: false
      matrix:
        split_keys: ${{ fromJson(needs.setup.outputs.split_keys) }}
    uses: ./.github/workflows/doctest_job.yml
    with:
      job_splits: ${{ needs.setup.outputs.job_splits }}
      split_keys: ${{ toJson(matrix.split_keys) }}
    secrets: inherit

  send_results:
    name: Send results to webhook
    runs-on: ubuntu-22.04
    if: always()
    needs: [call_doctest_job]
    steps:
      - uses: actions/checkout@v4
      - uses: actions/download-artifact@v4
      - name: Send message to Slack
        env:
          CI_SLACK_BOT_TOKEN: ${{ secrets.CI_SLACK_BOT_TOKEN }}
          ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}
          # Use `CI_SLACK_CHANNEL_DUMMY_TESTS` when doing experimentation
          SLACK_REPORT_CHANNEL: ${{ secrets.CI_SLACK_CHANNEL_ID_DAILY_DOCS }}
        run: |
          pip install slack_sdk
          python utils/notification_service_doc_tests.py

      - name: ""Upload results""
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: doc_test_results
          path: doc_test_results
",89,3,3,"push, repository_dispatch, schedule",4
huggingface/transformers,model_jobs.yml,"name: model jobs

on:
  workflow_call:
    inputs:
      folder_slices:
        required: true
        type: string
      machine_type:
        required: true
        type: string
      slice_id:
        required: true
        type: number
      runner_map:
        required: false
        type: string
      docker:
        required: true
        type: string
      report_name_prefix:
        required: false
        default: run_models_gpu
        type: string

env:
  HF_HOME: /mnt/cache
  TRANSFORMERS_IS_CI: yes
  OMP_NUM_THREADS: 8
  MKL_NUM_THREADS: 8
  RUN_SLOW: yes
  # For gated repositories, we still need to agree to share information on the Hub repo. page in order to get access.
  # This token is created under the bot `hf-transformers-bot`.
  HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}
  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}
  TF_FORCE_GPU_ALLOW_GROWTH: true
  CUDA_VISIBLE_DEVICES: 0,1

jobs:
  run_models_gpu:
    name: "" ""
    strategy:
      max-parallel: 8
      fail-fast: false
      matrix:
        folders: ${{ fromJson(inputs.folder_slices)[inputs.slice_id] }}
    runs-on:
      group: ${{ fromJson(inputs.runner_map)[matrix.folders][inputs.machine_type] }}
    container:
      image: ${{ inputs.docker }}
      options: --gpus all --shm-size ""16gb"" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    steps:
      - name: Echo input and matrix info
        shell: bash
        run: |
          echo ""${{ inputs.folder_slices }}""
          echo ""${{ matrix.folders }}""
          echo ""${{ toJson(fromJson(inputs.folder_slices)[inputs.slice_id]) }}""

      - name: Echo folder ${{ matrix.folders }}
        shell: bash
        # For folders like `models/bert`, set an env. var. (`matrix_folders`) to `models_bert`, which will be used to
        # set the artifact folder names (because the character `/` is not allowed).
        run: |
          echo ""${{ matrix.folders }}""
          matrix_folders=${{ matrix.folders }}
          matrix_folders=${matrix_folders/'models/'/'models_'}
          echo ""$matrix_folders""
          echo ""matrix_folders=$matrix_folders"" >> $GITHUB_ENV

      - name: Update clone
        working-directory: /transformers
        run: git fetch && git checkout ${{ github.sha }}

      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)
        working-directory: /transformers
        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .

      - name: Update / Install some packages (for Past CI)
        if: ${{ contains(inputs.docker, '-past-') }}
        working-directory: /transformers
        run: |
          python3 -m pip install -U datasets

      - name: Update / Install some packages (for Past CI)
        if: ${{ contains(inputs.docker, '-past-') && contains(inputs.docker, '-pytorch-') }}
        working-directory: /transformers
        run: |
          python3 -m pip install --no-cache-dir git+https://github.com/huggingface/accelerate@main#egg=accelerate

      - name: NVIDIA-SMI
        run: |
          nvidia-smi

      - name: Environment
        working-directory: /transformers
        run: |
          python3 utils/print_env.py

      - name: Show installed libraries and their versions
        working-directory: /transformers
        run: pip freeze

      - name: Set `machine_type` for report and artifact names
        working-directory: /transformers
        shell: bash
        run: |
          echo ""${{ inputs.machine_type }}""

          if [ ""${{ inputs.machine_type }}"" = ""aws-g4dn-4xlarge-cache"" ]; then
            machine_type=single-gpu
          elif [ ""${{ inputs.machine_type }}"" = ""aws-g4dn-12xlarge-cache"" ]; then
            machine_type=multi-gpu
          else
            machine_type=${{ inputs.machine_type }}
          fi

          echo ""$machine_type""
          echo ""machine_type=$machine_type"" >> $GITHUB_ENV

      - name: Run all tests on GPU
        working-directory: /transformers
        run: python3 -m pytest -rsfE -v --make-reports=${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}

      - name: Failure short reports
        if: ${{ failure() }}
        continue-on-error: true
        run: cat /transformers/reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports/failures_short.txt

      - name: Run test
        shell: bash
        run: |
          mkdir -p /transformers/reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports
          echo ""hello"" > /transformers/reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports/hello.txt
          echo ""${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports""

      - name: ""Test suite reports artifacts: ${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports""
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports
          path: /transformers/reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports
",142,1,1,workflow_call,1
huggingface/transformers,model_jobs_intel_gaudi.yml,"name: model jobs

on:
  workflow_call:
    inputs:
      folder_slices:
        required: true
        type: string
      slice_id:
        required: true
        type: number
      runner:
        required: true
        type: string
      machine_type:
        required: true
        type: string
      report_name_prefix:
        required: false
        default: run_models_gpu
        type: string

env:
  RUN_SLOW: yes
  PT_HPU_LAZY_MODE: 0
  TRANSFORMERS_IS_CI: yes
  PT_ENABLE_INT64_SUPPORT: 1
  HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}
  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}
  HF_HOME: /mnt/cache/.cache/huggingface

jobs:
  run_models_gpu:
    name: "" ""
    strategy:
      max-parallel: 8
      fail-fast: false
      matrix:
        folders: ${{ fromJson(inputs.folder_slices)[inputs.slice_id] }}
    runs-on:
      group: ${{ inputs.runner }}
    container:
      image: vault.habana.ai/gaudi-docker/1.21.1/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:latest
      options: --runtime=habana
        -v /mnt/cache/.cache/huggingface:/mnt/cache/.cache/huggingface
        --env OMPI_MCA_btl_vader_single_copy_mechanism=none
        --env HABANA_VISIBLE_DEVICES
        --env HABANA_VISIBLE_MODULES
        --cap-add=sys_nice
        --shm-size=64G
    steps:
      - name: Echo input and matrix info
        shell: bash
        run: |
          echo ""${{ inputs.folder_slices }}""
          echo ""${{ matrix.folders }}""
          echo ""${{ toJson(fromJson(inputs.folder_slices)[inputs.slice_id]) }}""

      - name: Echo folder ${{ matrix.folders }}
        shell: bash
        run: |
          echo ""${{ matrix.folders }}""
          matrix_folders=${{ matrix.folders }}
          matrix_folders=${matrix_folders/'models/'/'models_'}
          echo ""$matrix_folders""
          echo ""matrix_folders=$matrix_folders"" >> $GITHUB_ENV

      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install dependencies
        run: |
          pip install -e .[testing,torch] ""numpy<2.0.0"" scipy scikit-learn

      - name: HL-SMI
        run: |
          hl-smi
          echo ""HABANA_VISIBLE_DEVICES=${HABANA_VISIBLE_DEVICES}""
          echo ""HABANA_VISIBLE_MODULES=${HABANA_VISIBLE_MODULES}""

      - name: Environment
        run: python3 utils/print_env.py

      - name: Show installed libraries and their versions
        run: pip freeze

      - name: Set `machine_type` for report and artifact names
        shell: bash
        run: |
          if [ ""${{ inputs.machine_type }}"" = ""1gaudi"" ]; then
            machine_type=single-gpu
          elif [ ""${{ inputs.machine_type }}"" = ""2gaudi"" ]; then
            machine_type=multi-gpu
          else
            machine_type=${{ inputs.machine_type }}
          fi
          echo ""machine_type=$machine_type"" >> $GITHUB_ENV

      - name: Run all tests on Gaudi
        run: python3 -m pytest -v --make-reports=${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}

      - name: Failure short reports
        if: ${{ failure() }}
        continue-on-error: true
        run: cat reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports/failures_short.txt

      - name: Run test
        shell: bash
        run: |
          mkdir -p reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports
          echo ""hello"" > reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports/hello.txt
          echo ""${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports""

      - name: ""Test suite reports artifacts: ${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports""
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports
          path: reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports
",121,1,1,workflow_call,2
huggingface/transformers,new_model_pr_merged_notification.yml,"# Used to notify core maintainers about new model PR being merged
name: New model PR merged notification

on:
  push:
    branches:
      - main
    paths:
      - 'src/transformers/models/*/modeling_*'

jobs:
  notify_new_model:
    name: Notify new model
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Check new model
        shell: bash
        run: |
          python -m pip install gitpython
          python -c 'from utils.pr_slow_ci_models import get_new_model; new_model = get_new_model(diff_with_last_commit=True); print(new_model)' | tee output.txt
          echo ""NEW_MODEL=$(tail -n 1 output.txt)"" >> $GITHUB_ENV
          echo ""COMMIT_SHA=$(git log -1 --format=%H)"" >> $GITHUB_ENV

      - name: print commit sha
        if: ${{ env.NEW_MODEL != ''}}
        shell: bash
        run: |
          echo ""$COMMIT_SHA""

      - name: print new model
        if: ${{ env.NEW_MODEL != ''}}
        shell: bash
        run: |
          echo ""$NEW_MODEL""

      - name: Notify
        if: ${{ env.NEW_MODEL != ''}}
        uses: slackapi/slack-github-action@6c661ce58804a1a20f6dc5fbee7f0381b469e001
        with:
          # Slack channel id, channel name, or user id to post message.
          # See also: https://api.slack.com/methods/chat.postMessage#channels
          channel-id: transformers-new-model-notification
          # For posting a rich message using Block Kit
          payload: |
            {
              ""blocks"": [
                {
                  ""type"": ""header"",
                  ""text"": {
                    ""type"": ""plain_text"",
                    ""text"": ""New model!"",
                    ""emoji"": true
                  }
                },
                {
                  ""type"": ""section"",
                  ""text"": {
                    ""type"": ""mrkdwn"",
                    ""text"": ""<https://github.com/huggingface/transformers/commit/${{ env.COMMIT_SHA }}|New model: ${{ env.NEW_MODEL }}> GH_ArthurZucker, GH_lysandrejik, GH_ydshieh\ncommit SHA: ${{ env.COMMIT_SHA }}""
                  }
                }
              ]
            }
        env:
          SLACK_BOT_TOKEN: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}",68,1,1,push,2
huggingface/transformers,pr-style-bot.yml,"# To run this bot, comment ""@bot /style"" on a PR
name: Style Bot

on:
  issue_comment:
    types: [created]

permissions:
  pull-requests: write

jobs:
  style:
    uses: huggingface/huggingface_hub/.github/workflows/style-bot-action.yml@main
    with:
      python_quality_dependencies: ""[quality]""
      style_command_type: ""default""
    secrets:
      bot_token: ${{ secrets.HF_STYLE_BOT_ACTION }}
",18,1,1,issue_comment,1
huggingface/transformers,push-important-models.yml,"name: Slow tests on important models (on Push - A10)

on:
  push:
    branches: [ main ]

env:
  OUTPUT_SLACK_CHANNEL_ID: ""C06L2SGMEEA""
  HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}
  HF_HOME: /mnt/cache
  TRANSFORMERS_IS_CI: yes
  OMP_NUM_THREADS: 8
  MKL_NUM_THREADS: 8
  RUN_SLOW: yes # For gated repositories, we still need to agree to share information on the Hub repo. page in order to get access. # This token is created under the bot `hf-transformers-bot`.
  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}
  TF_FORCE_GPU_ALLOW_GROWTH: true

jobs:
  get_modified_models:
    name: ""Get all modified files""
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Check out code
        uses: actions/checkout@v4

      - name: Get changed files
        id: changed-files
        uses: tj-actions/changed-files@1c8e6069583811afb28f97afeaf8e7da80c6be5c
        with:
          files: src/transformers/models/**

      - name: Run step if only the files listed above change
        if: steps.changed-files.outputs.any_changed == 'true'
        id: set-matrix
        env:
          ALL_CHANGED_FILES: ${{ steps.changed-files.outputs.all_changed_files }}
        run: |
            model_arrays=()
            for file in $ALL_CHANGED_FILES; do
                model_path=""${file#*models/}""
                model_path=""models/${model_path%%/*}""
                if grep -qFx ""$model_path"" utils/important_models.txt; then
                    # Append the file to the matrix string
                    model_arrays+=(""$model_path"")
                fi
            done
            matrix_string=$(printf '""%s"", ' ""${model_arrays[@]}"" | sed 's/, $//')
            echo ""matrix=[$matrix_string]"" >> $GITHUB_OUTPUT
  test_modified_files:
    needs: get_modified_models
    name: Slow & FA2 tests
    runs-on:
      group: aws-g5-4xlarge-cache
    container:
      image: huggingface/transformers-all-latest-gpu
      options: --gpus all --privileged --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    if: ${{ needs.get_modified_models.outputs.matrix != '[]' && needs.get_modified_models.outputs.matrix != '' && fromJson(needs.get_modified_models.outputs.matrix)[0] != null }}
    strategy:
      fail-fast: false
      matrix:
        model-name: ${{ fromJson(needs.get_modified_models.outputs.matrix) }}

    steps:
      - name: Check out code
        uses: actions/checkout@v4

      - name: Install locally transformers & other libs
        run: |
          apt install sudo
          sudo -H pip install --upgrade pip
          sudo -H pip uninstall -y transformers
          sudo -H pip install -U -e "".[testing]""
          MAX_JOBS=4 pip install flash-attn --no-build-isolation
          pip install bitsandbytes

      - name: NVIDIA-SMI
        run: |
          nvidia-smi

      - name: Show installed libraries and their versions
        run: pip freeze

      - name: Run FA2 tests
        id: run_fa2_tests
        run:
          pytest -rsfE -m ""flash_attn_test"" --make-reports=${{ matrix.model-name }}_fa2_tests/ tests/${{ matrix.model-name }}/test_modeling_*

      - name: ""Test suite reports artifacts: ${{ matrix.model-name }}_fa2_tests""
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.model-name }}_fa2_tests
          path: /transformers/reports/${{ matrix.model-name }}_fa2_tests

      - name: Post to Slack
        if: always()
        uses: huggingface/hf-workflows/.github/actions/post-slack@main
        with:
          slack_channel: ${{ env.OUTPUT_SLACK_CHANNEL_ID }}
          title: 🤗 Results of the FA2 tests - ${{ matrix.model-name }}
          status: ${{ steps.run_fa2_tests.conclusion}}
          slack_token: ${{ secrets.CI_SLACK_BOT_TOKEN }}

      - name: Run integration tests
        id: run_integration_tests
        if: always()
        run:
          pytest -rsfE -k ""IntegrationTest""  --make-reports=tests_integration_${{ matrix.model-name }} tests/${{ matrix.model-name }}/test_modeling_*

      - name: ""Test suite reports artifacts: tests_integration_${{ matrix.model-name }}""
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: tests_integration_${{ matrix.model-name }}
          path: /transformers/reports/tests_integration_${{ matrix.model-name }}

      - name: Post to Slack
        if: always()
        uses: huggingface/hf-workflows/.github/actions/post-slack@main
        with:
          slack_channel: ${{ env.OUTPUT_SLACK_CHANNEL_ID }}
          title: 🤗 Results of the Integration tests - ${{ matrix.model-name }}
          status: ${{ steps.run_integration_tests.conclusion}}
          slack_token: ${{ secrets.CI_SLACK_BOT_TOKEN }}

      - name: Tailscale # In order to be able to SSH when a test fails
        if: ${{ runner.debug == '1'}}
        uses: huggingface/tailscale-action@v1
        with:
          authkey: ${{ secrets.TAILSCALE_SSH_AUTHKEY }}
          slackChannel: ${{ secrets.SLACK_CIFEEDBACK_CHANNEL }}
          slackToken: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}
          waitForSSH: true
",135,2,1,push,8
huggingface/transformers,release-conda.yml,"name: Release - Conda

on:
  push:
    tags:
      - v*
    branches:
      - conda_*

env:
  ANACONDA_API_TOKEN: ${{ secrets.ANACONDA_API_TOKEN }}

jobs:
  build_and_package:
    runs-on: ubuntu-22.04
    defaults:
      run:
        shell: bash -l {0}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install miniconda
        uses: conda-incubator/setup-miniconda@v2
        with:
          auto-update-conda: true
          auto-activate-base: false
          python-version: 3.8
          activate-environment: ""build-transformers""
          channels: huggingface

      - name: Setup conda env
        run: |
          conda install -c defaults anaconda-client conda-build

      - name: Extract version
        run: echo ""TRANSFORMERS_VERSION=`python setup.py --version`"" >> $GITHUB_ENV

      - name: Build conda packages
        run: |
          conda info
          conda list
          conda-build .github/conda

      - name: Upload to Anaconda
        run: anaconda upload `conda-build .github/conda --output` --force
",47,1,1,push,2
huggingface/transformers,self-comment-ci.yml,"name: PR comment GitHub CI

on:
  issue_comment:
    types:
      - created
    branches-ignore:
      - main
concurrency:
  group: ${{ github.workflow }}-${{ github.event.issue.number }}-${{ startsWith(github.event.comment.body, 'run-slow') || startsWith(github.event.comment.body, 'run slow') || startsWith(github.event.comment.body, 'run_slow') }}
  cancel-in-progress: true
permissions: read-all

env:
  HF_HOME: /mnt/cache
  TRANSFORMERS_IS_CI: yes
  OMP_NUM_THREADS: 8
  MKL_NUM_THREADS: 8
  RUN_SLOW: yes
  # For gated repositories, we still need to agree to share information on the Hub repo. page in order to get access.
  # This token is created under the bot `hf-transformers-bot`.
  HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}
  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}
  TF_FORCE_GPU_ALLOW_GROWTH: true
  CUDA_VISIBLE_DEVICES: 0,1

jobs:
  get-pr-number:
    runs-on: ubuntu-22.04
    name: Get PR number
    # For security: only allow team members to run
    if: ${{ github.event.issue.state == 'open' && contains(fromJSON('[""ydshieh"", ""ArthurZucker"", ""zucchini-nlp"", ""qubvel"", ""molbap"", ""gante"", ""LysandreJik"", ""Cyrilvallez"", ""Rocketknight1"", ""SunMarc"", ""muellerzr"", ""eustlb"", ""MekkCyber"", ""manueldeprada"", ""vasqu"", ""ivarflakstad""]'), github.actor) && (startsWith(github.event.comment.body, 'run-slow') || startsWith(github.event.comment.body, 'run slow') || startsWith(github.event.comment.body, 'run_slow')) }}
    outputs:
      PR_NUMBER: ${{ steps.set_pr_number.outputs.PR_NUMBER }}
    steps:
      - name: Get PR number
        shell: bash
        run: |
          if [[ ""${{ github.event.issue.number }}"" != """" && ""${{ github.event.issue.pull_request }}"" != """" ]]; then
            echo ""PR_NUMBER=${{ github.event.issue.number }}"" >> $GITHUB_ENV
          else
            echo ""PR_NUMBER="" >> $GITHUB_ENV
          fi

      - name: Check PR number
        shell: bash
        run: |
          echo ""${{ env.PR_NUMBER }}""

      - name: Set PR number
        id: set_pr_number
        run: echo ""PR_NUMBER=${{ env.PR_NUMBER }}"" >> ""$GITHUB_OUTPUT""

  get-sha:
    runs-on: ubuntu-22.04
    needs: get-pr-number
    if: ${{ needs.get-pr-number.outputs.PR_NUMBER != ''}}
    outputs:
      PR_HEAD_SHA: ${{ steps.get_sha.outputs.PR_HEAD_SHA }}
      PR_MERGE_SHA: ${{ steps.get_sha.outputs.PR_MERGE_SHA }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: ""0""
          ref: ""refs/pull/${{needs.get-pr-number.outputs.PR_NUMBER}}/merge""

      - name: Get SHA (and verify timestamps against the issue comment date)
        id: get_sha
        env:
          PR_NUMBER: ${{ needs.get-pr-number.outputs.PR_NUMBER }}
          COMMENT_DATE: ${{ github.event.comment.created_at }}
        run: |
            git fetch origin refs/pull/$PR_NUMBER/head:refs/remotes/pull/$PR_NUMBER/head
            git checkout refs/remotes/pull/$PR_NUMBER/head
            echo ""PR_HEAD_SHA: $(git log -1 --format=%H)""
            echo ""PR_HEAD_SHA=$(git log -1 --format=%H)"" >> ""$GITHUB_OUTPUT""
            git fetch origin refs/pull/$PR_NUMBER/merge:refs/remotes/pull/$PR_NUMBER/merge
            git checkout refs/remotes/pull/$PR_NUMBER/merge
            echo ""PR_MERGE_SHA: $(git log -1 --format=%H)""
            echo ""PR_MERGE_SHA=$(git log -1 --format=%H)"" >> ""$GITHUB_OUTPUT""
            PR_MERGE_COMMIT_TIMESTAMP=$(git log -1 --date=unix --format=%cd)
            echo ""PR_MERGE_COMMIT_TIMESTAMP: $PR_MERGE_COMMIT_TIMESTAMP""
            COMMENT_TIMESTAMP=$(date -d ""${COMMENT_DATE}"" +""%s"")
            echo ""COMMENT_DATE: $COMMENT_DATE""
            echo ""COMMENT_TIMESTAMP: $COMMENT_TIMESTAMP""
            if [ $COMMENT_TIMESTAMP -le $PR_MERGE_COMMIT_TIMESTAMP ]; then
              echo ""Last commit on the pull request is newer than the issue comment triggering this run! Abort!"";
              exit -1;
            fi

  # use a python script to handle this complex logic
  # case 1: `run-slow` (auto. infer with limited number of models, but in particular, new model)
  # case 2: `run-slow model_1, model_2`
  get-tests:
    runs-on: ubuntu-22.04
    needs: [get-pr-number, get-sha]
    if: ${{ needs.get-pr-number.outputs.PR_NUMBER != ''}}
    outputs:
      models: ${{ steps.models_to_run.outputs.models }}
      quantizations: ${{ steps.models_to_run.outputs.quantizations }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: ""0""
          ref: ""refs/pull/${{needs.get-pr-number.outputs.PR_NUMBER}}/merge""

      - name: Verify merge commit SHA
        env:
          VERIFIED_PR_MERGE_SHA: ${{ needs.get-sha.outputs.PR_MERGE_SHA }}
        run: |
            PR_MERGE_SHA=$(git log -1 --format=%H)
            if [ $PR_MERGE_SHA != $VERIFIED_PR_MERGE_SHA ]; then
              echo ""The merged commit SHA is not the same as the verified one! Security issue detected, abort the workflow!"";
              exit -1;
            fi

      - name: Get models to test
        env:
          PR_COMMENT: ${{ github.event.comment.body }}
        run: |
          python -m pip install GitPython
          python utils/pr_slow_ci_models.py --message ""$PR_COMMENT"" | tee output.txt
          echo ""models=$(tail -n 1 output.txt)"" >> $GITHUB_ENV
          python utils/pr_slow_ci_models.py --message ""$PR_COMMENT"" --quantization | tee output2.txt
          echo ""quantizations=$(tail -n 1 output2.txt)"" >> $GITHUB_ENV

      - name: Show models to test
        id: models_to_run
        run: |
          echo ""${{ env.models }}""
          echo ""models=${{ env.models }}"" >> $GITHUB_ENV
          echo ""models=${{ env.models }}"" >> $GITHUB_OUTPUT
          echo ""${{ env.quantizations }}""
          echo ""quantizations=${{ env.quantizations }}"" >> $GITHUB_OUTPUT

  reply_to_comment:
    name: Reply to the comment
    if: ${{ needs.get-tests.outputs.models != '[]'  || needs.get-tests.outputs.quantizations != '[]' }}
    needs: [get-pr-number, get-tests]
    permissions:
      pull-requests: write
    runs-on: ubuntu-22.04
    steps:
      - name: Reply to the comment
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          MODELS: ${{ needs.get-tests.outputs.models }}
          BODY: ""\n\nmodels: ${{ needs.get-tests.outputs.models }}\nquantizations: ${{ needs.get-tests.outputs.quantizations }}""
        run: |
          gh api \
            --method POST \
            -H ""Accept: application/vnd.github+json"" \
            -H ""X-GitHub-Api-Version: 2022-11-28"" \
            repos/${{ github.repository }}/issues/${{ needs.get-pr-number.outputs.PR_NUMBER }}/comments \
            -f ""body=This comment contains run-slow, running the specified jobs: ${{ env.BODY }} ...""

  create_run:
    name: Create run
    if: ${{ needs.get-tests.outputs.models != '[]' || needs.get-tests.outputs.quantizations != '[]' }}
    needs: [get-sha, get-tests, reply_to_comment]
    permissions:
      statuses: write
    runs-on: ubuntu-22.04
    steps:
      - name: Create Run
        id: create_run
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          # Create a commit status (pending) for a run of this workflow. The status has to be updated later in `update_run_status`.
          # See https://docs.github.com/en/rest/commits/statuses?apiVersion=2022-11-28#create-a-commit-status
          GITHUB_RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
        run: |
          gh api \
            --method POST \
            -H ""Accept: application/vnd.github+json"" \
            -H ""X-GitHub-Api-Version: 2022-11-28"" \
            repos/${{ github.repository }}/statuses/${{ needs.get-sha.outputs.PR_HEAD_SHA }} \
            -f ""target_url=$GITHUB_RUN_URL"" -f ""state=pending"" -f ""description=Slow CI job"" -f ""context=pytest/custom-tests""

  run_models_gpu:
    name: Run all tests for the model
    if: ${{ needs.get-tests.outputs.models != '[]' }}
    needs: [get-pr-number, get-sha, get-tests, create_run]
    strategy:
      fail-fast: false
      matrix:
        folders: ${{ fromJson(needs.get-tests.outputs.models) }}
        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]
    runs-on:
       group: '${{ matrix.machine_type }}'
    container:
      image: huggingface/transformers-all-latest-gpu
      options: --gpus all --shm-size ""16gb"" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    steps:
      - name: Echo input and matrix info
        shell: bash
        run: |
          echo ""${{ matrix.folders }}""

      - name: Echo folder ${{ matrix.folders }}
        shell: bash
        # For folders like `models/bert`, set an env. var. (`matrix_folders`) to `models_bert`, which will be used to
        # set the artifact folder names (because the character `/` is not allowed).
        run: |
          echo ""${{ matrix.folders }}""
          matrix_folders=${{ matrix.folders }}
          matrix_folders=${matrix_folders/'models/'/'models_'}
          echo ""$matrix_folders""
          echo ""matrix_folders=$matrix_folders"" >> $GITHUB_ENV

      - name: Checkout to PR merge commit
        working-directory: /transformers
        run: |
          git fetch origin refs/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge:refs/remotes/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge
          git checkout refs/remotes/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge
          git log -1 --format=%H

      - name: Verify merge commit SHA
        env:
          VERIFIED_PR_MERGE_SHA: ${{ needs.get-sha.outputs.PR_MERGE_SHA }}
        working-directory: /transformers
        run: |
          PR_MERGE_SHA=$(git log -1 --format=%H)
          if [ $PR_MERGE_SHA != $VERIFIED_PR_MERGE_SHA ]; then
            echo ""The merged commit SHA is not the same as the verified one! Security issue detected, abort the workflow!"";
            exit -1;
          fi

      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)
        working-directory: /transformers
        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .

      - name: NVIDIA-SMI
        run: |
          nvidia-smi

      - name: Set `machine_type` for report and artifact names
        working-directory: /transformers
        shell: bash
        run: |
          echo ""${{ matrix.machine_type }}""
          if [ ""${{ matrix.machine_type }}"" = ""aws-g4dn-4xlarge-cache"" ]; then
            machine_type=single-gpu
          elif [ ""${{ matrix.machine_type }}"" = ""aws-g4dn-12xlarge-cache"" ]; then
            machine_type=multi-gpu
          else
            machine_type=${{ matrix.machine_type }}
          fi
          echo ""$machine_type""
          echo ""machine_type=$machine_type"" >> $GITHUB_ENV

      - name: Environment
        working-directory: /transformers
        run: |
          python3 utils/print_env.py

      - name: Show installed libraries and their versions
        working-directory: /transformers
        run: pip freeze

      - name: Run all tests on GPU
        working-directory: /transformers
        run: |
          export CUDA_VISIBLE_DEVICES=""$(python3 utils/set_cuda_devices_for_ci.py --test_folder ${{ matrix.folders }})""
          echo $CUDA_VISIBLE_DEVICES
          python3 -m pytest -v -rsfE --make-reports=${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}

      - name: Failure short reports
        if: ${{ failure() }}
        continue-on-error: true
        run: cat /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/failures_short.txt

      - name: Make sure report directory exists
        shell: bash
        run: |
          mkdir -p /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports
          echo ""hello"" > /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/hello.txt
          echo ""${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports""

      - name: ""Test suite reports artifacts: ${{ env.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports""
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports
          path: /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports

  run_quantization_torch_gpu:
    name: Run all tests for a quantization
    if: ${{ needs.get-tests.outputs.quantizations != '[]' }}
    needs: [get-pr-number, get-sha, get-tests, create_run]
    strategy:
      fail-fast: false
      matrix:
        folders: ${{ fromJson(needs.get-tests.outputs.quantizations) }}
        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]
    runs-on:
      group: '${{ matrix.machine_type }}'
    container:
      image: huggingface/transformers-quantization-latest-gpu
      options: --gpus all --shm-size ""16gb"" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    steps:
      - name: Echo folder ${{ matrix.folders }}
        shell: bash
        run: |
          echo ""${{ matrix.folders }}""
          matrix_folders=${{ matrix.folders }}
          matrix_folders=${matrix_folders/'quantization/'/'quantization_'}
          echo ""$matrix_folders""
          echo ""matrix_folders=$matrix_folders"" >> $GITHUB_ENV

      - name: Checkout to PR merge commit
        working-directory: /transformers
        run: |
          git fetch origin refs/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge:refs/remotes/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge
          git checkout refs/remotes/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge
          git log -1 --format=%H

      - name: Verify merge commit SHA
        env:
          VERIFIED_PR_MERGE_SHA: ${{ needs.get-sha.outputs.PR_MERGE_SHA }}
        working-directory: /transformers
        run: |
          PR_MERGE_SHA=$(git log -1 --format=%H)
          if [ $PR_MERGE_SHA != $VERIFIED_PR_MERGE_SHA ]; then
            echo ""The merged commit SHA is not the same as the verified one! Security issue detected, abort the workflow!"";
            exit -1;
          fi

      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)
        working-directory: /transformers
        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .
      - name: NVIDIA-SMI
        run: |
          nvidia-smi

      - name: Set `machine_type` for report and artifact names
        working-directory: /transformers
        shell: bash
        run: |
          echo ""${{ matrix.machine_type }}""
          if [ ""${{ matrix.machine_type }}"" = ""aws-g4dn-4xlarge-cache"" ]; then
            machine_type=single-gpu
          elif [ ""${{ matrix.machine_type }}"" = ""aws-g4dn-12xlarge-cache"" ]; then
            machine_type=multi-gpu
          else
            machine_type=${{ matrix.machine_type }}
          fi
          echo ""$machine_type""
          echo ""machine_type=$machine_type"" >> $GITHUB_ENV

      - name: Environment
        working-directory: /transformers
        run: |
          python3 utils/print_env.py

      - name: Show installed libraries and their versions
        working-directory: /transformers
        run: pip freeze

      - name: Run quantization tests on GPU
        working-directory: /transformers
        run: |
          python3 -m pytest -v --make-reports=${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}

      - name: Failure short reports
        if: ${{ failure() }}
        continue-on-error: true
        run: cat /transformers/reports/${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports/failures_short.txt

      - name: Make sure report directory exists
        shell: bash
        run: |
          mkdir -p /transformers/reports/${{ env.machine_type }}_run_quantization_gpu_${{ matrix.folders }}_test_reports
          echo ""hello"" > /transformers/reports/${{ env.machine_type }}_run_quantization_gpu_${{ matrix.folders }}_test_reports/hello.txt
          echo ""${{ env.machine_type }}_run_quantization_gpu_${{ matrix.folders }}_test_reports""

      - name: ""Test suite reports artifacts: ${{ env.machine_type }}_run_quantization_torch_gpu_${{ env.matrix_folders }}_test_reports""
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.machine_type }}_run_quantization_torch_gpu_${{ env.matrix_folders }}_test_reports
          path: /transformers/reports/${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports

  update_run_status:
    name: Update Check Run Status
    needs: [get-sha, create_run, run_models_gpu, run_quantization_torch_gpu]
    permissions:
      statuses: write
    if: ${{ always() && needs.create_run.result == 'success' }}
    runs-on: ubuntu-22.04
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      GITHUB_RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
      STATUS_OK: ${{ contains(fromJSON('[""skipped"", ""success""]'), needs.run_models_gpu.result) && contains(fromJSON('[""skipped"", ""success""]'), needs.run_quantization_torch_gpu.result) }}
    steps:
      - name: Get `run_models_gpu` job status
        run: |
          echo ""${{ needs.run_models_gpu.result }}""
          echo ""${{ needs.run_quantization_torch_gpu.result }}""
          echo $STATUS_OK
          if [ ""$STATUS_OK"" = ""true"" ]; then
            echo ""STATUS=success"" >> $GITHUB_ENV
          else
            echo ""STATUS=failure"" >> $GITHUB_ENV
          fi

      - name: Update PR commit statuses
        run: |
          echo ""${{ needs.run_models_gpu.result }}""
          echo ""${{ env.STATUS }}""
          gh api \
            --method POST \
            -H ""Accept: application/vnd.github+json"" \
            -H ""X-GitHub-Api-Version: 2022-11-28"" \
            repos/${{ github.repository }}/statuses/${{ needs.get-sha.outputs.PR_HEAD_SHA }} \
            -f ""target_url=$GITHUB_RUN_URL"" -f ""state=${{ env.STATUS }}"" -f ""description=Slow CI job"" -f ""context=pytest/custom-tests""
",416,8,1,issue_comment,6
huggingface/transformers,self-nightly-caller.yml,"name: Self-hosted runner (nightly-ci)


on:
  repository_dispatch:
  schedule:
    - cron: ""17 2 * * *""
  push:
    branches:
      - run_nightly_ci*

jobs:
  build_nightly_ci_images:
    name: Build Nightly CI Docker Images
    if: (github.event_name == 'schedule') || ((github.event_name == 'push') && startsWith(github.ref_name, 'run_nightly_ci'))
    uses: ./.github/workflows/build-nightly-ci-docker-images.yml
    secrets: inherit

  model-ci:
    name: Model CI
    needs: [build_nightly_ci_images]
    uses: ./.github/workflows/self-scheduled.yml
    with:
      job: run_models_gpu
      slack_report_channel: ""#transformers-ci-past-future""
      runner: ci
      docker: huggingface/transformers-all-latest-torch-nightly-gpu
      ci_event: Nightly CI
    secrets: inherit

  deepspeed-ci:
    name: DeepSpeed CI
    needs: [build_nightly_ci_images]
    uses: ./.github/workflows/self-scheduled.yml
    with:
      job: run_torch_cuda_extensions_gpu
      slack_report_channel: ""#transformers-ci-past-future""
      runner: ci
      # test deepspeed nightly build with the latest release torch
      docker: huggingface/transformers-pytorch-deepspeed-latest-gpu
      ci_event: Nightly CI
      working-directory-prefix: /workspace
    secrets: inherit
",43,3,3,"repository_dispatch, schedule, push",3
huggingface/transformers,self-nightly-past-ci-caller.yml,"name: Self-hosted runner (nightly-past-ci-caller)

on:
  schedule:
    - cron: ""17 2,14 * * *""
  push:
    branches:
      - run_past_ci*

jobs:
  get_number:
    name: Get number
    runs-on: ubuntu-22.04
    outputs:
      run_number: ${{ steps.get_number.outputs.run_number }}
    steps:
      - name: Get number
        id: get_number
        run: |
          echo ""${{ github.run_number }}""
          echo ""$(python3 -c 'print(int(${{ github.run_number }}) % 10)')""
          echo ""run_number=$(python3 -c 'print(int(${{ github.run_number }}) % 10)')"" >> $GITHUB_OUTPUT

  run_past_ci_tensorflow_2-11:
    name: TensorFlow 2.11
    needs: get_number
    if: needs.get_number.outputs.run_number == 3 && (cancelled() != true) && ((github.event_name == 'push') && startsWith(github.ref_name, 'run_past_ci'))
    uses: ./.github/workflows/self-past-caller.yml
    with:
      framework: tensorflow
      version: ""2.11""
      sha: ${{ github.sha }}
    secrets: inherit

  run_past_ci_tensorflow_2-10:
    name: TensorFlow 2.10
    needs: get_number
    if: needs.get_number.outputs.run_number == 4 && (cancelled() != true) && ((github.event_name == 'push') && startsWith(github.ref_name, 'run_past_ci'))
    uses: ./.github/workflows/self-past-caller.yml
    with:
      framework: tensorflow
      version: ""2.10""
      sha: ${{ github.sha }}
    secrets: inherit

  run_past_ci_tensorflow_2-9:
    name: TensorFlow 2.9
    needs: get_number
    if: needs.get_number.outputs.run_number == 5 && (cancelled() != true) && ((github.event_name == 'push') && startsWith(github.ref_name, 'run_past_ci'))
    uses: ./.github/workflows/self-past-caller.yml
    with:
      framework: tensorflow
      version: ""2.9""
      sha: ${{ github.sha }}
    secrets: inherit

  run_past_ci_tensorflow_2-8:
    name: TensorFlow 2.8
    needs: get_number
    if: needs.get_number.outputs.run_number == 6 && (cancelled() != true) && ((github.event_name == 'push') && startsWith(github.ref_name, 'run_past_ci'))
    uses: ./.github/workflows/self-past-caller.yml
    with:
      framework: tensorflow
      version: ""2.8""
      sha: ${{ github.sha }}
    secrets: inherit

  run_past_ci_tensorflow_2-7:
    name: TensorFlow 2.7
    needs: get_number
    if: needs.get_number.outputs.run_number == 7 && (cancelled() != true) && ((github.event_name == 'push') && startsWith(github.ref_name, 'run_past_ci'))
    uses: ./.github/workflows/self-past-caller.yml
    with:
      framework: tensorflow
      version: ""2.7""
      sha: ${{ github.sha }}
    secrets: inherit

  run_past_ci_tensorflow_2-6:
    name: TensorFlow 2.6
    needs: get_number
    if: needs.get_number.outputs.run_number == 8 && (cancelled() != true) && ((github.event_name == 'push') && startsWith(github.ref_name, 'run_past_ci'))
    uses: ./.github/workflows/self-past-caller.yml
    with:
      framework: tensorflow
      version: ""2.6""
      sha: ${{ github.sha }}
    secrets: inherit

  run_past_ci_tensorflow_2-5:
    name: TensorFlow 2.5
    needs: get_number
    if: needs.get_number.outputs.run_number == 9 &&  (cancelled() != true) && ((github.event_name == 'push') && startsWith(github.ref_name, 'run_past_ci'))
    uses: ./.github/workflows/self-past-caller.yml
    with:
      framework: tensorflow
      version: ""2.5""
      sha: ${{ github.sha }}
    secrets: inherit
",99,8,2,"schedule, push",7
huggingface/transformers,self-past-caller.yml,"name: Self-hosted runner (past-ci)


on:
  workflow_call:
    inputs:
      framework:
        required: true
        type: string
      version:
        required: true
        type: string
      # Use this to control the commit to test against
      sha:
        default: 'main'
        required: false
        type: string

jobs:
  model-ci:
    name: Model CI
    uses: ./.github/workflows/self-scheduled.yml
    with:
      job: run_models_gpu
      slack_report_channel: ""#transformers-ci-past-future""
      runner: past-ci
      docker: huggingface/transformers-${{ inputs.framework }}-past-${{ inputs.version }}-gpu
      ci_event: Past CI - ${{ inputs.framework }}-${{ inputs.version }}
    secrets: inherit

  deepspeed-ci:
    name: DeepSpeed CI
    uses: ./.github/workflows/self-scheduled.yml
    with:
      job: run_torch_cuda_extensions_gpu
      slack_report_channel: ""#transformers-ci-past-future""
      runner: past-ci
      docker: huggingface/transformers-${{ inputs.framework }}-past-${{ inputs.version }}-gpu
      ci_event: Past CI - ${{ inputs.framework }}-${{ inputs.version }}
    secrets: inherit
",40,2,1,workflow_call,2
huggingface/transformers,self-push-amd-mi210-caller.yml,"name: Self-hosted runner (AMD mi210 CI caller)

on:
  #workflow_run:
  #  workflows: [""Self-hosted runner (push-caller)""]
  #  branches: [""main""]
  #  types: [completed]
  push:
    branches:
      - run_amd_push_ci_caller*
    paths:
      - ""src/**""
      - ""tests/**""
      - "".github/**""
      - ""templates/**""
      - ""utils/**""

jobs:
  run_amd_ci:
    name: AMD mi210
    if: (cancelled() != true) && ((github.event_name == 'workflow_run') || ((github.event_name == 'push') && startsWith(github.ref_name, 'run_amd_push_ci_caller')))
    uses: ./.github/workflows/self-push-amd.yml
    with:
      gpu_flavor: mi210
    secrets: inherit
",25,1,1,push,1
huggingface/transformers,self-push-amd-mi250-caller.yml,"name: Self-hosted runner (AMD mi250 CI caller)

on:
  #workflow_run:
  #  workflows: [""Self-hosted runner (push-caller)""]
  #  branches: [""main""]
  #  types: [completed]
  push:
    branches:
      - run_amd_push_ci_caller*
    paths:
      - ""src/**""
      - ""tests/**""
      - "".github/**""
      - ""templates/**""
      - ""utils/**""

jobs:
  run_amd_ci:
    name: AMD mi250
    if: (cancelled() != true) && ((github.event_name == 'workflow_run') || ((github.event_name == 'push') && startsWith(github.ref_name, 'run_amd_push_ci_caller')))
    uses: ./.github/workflows/self-push-amd.yml
    with:
      gpu_flavor: mi250
    secrets: inherit
",25,1,1,push,1
huggingface/transformers,self-push-amd-mi300-caller.yml,"name: Self-hosted runner (AMD mi300 CI caller)

on:
  #workflow_run:
  #  workflows: [""Self-hosted runner (push-caller)""]
  #  branches: [""main""]
  #  types: [completed]
  push:
    branches:
      - run_amd_push_ci_caller*
    paths:
      - ""src/**""
      - ""tests/**""
      - "".github/**""
      - ""templates/**""
      - ""utils/**""

jobs:
  run_amd_ci:
    name: AMD mi300
    if: (cancelled() != true) && ((github.event_name == 'workflow_run') || ((github.event_name == 'push') && (startsWith(github.ref_name, 'run_amd_push_ci_caller') || startsWith(github.ref_name, 'mi300-ci'))))
    uses: ./.github/workflows/self-push-amd.yml
    with:
      gpu_flavor: mi300
    secrets: inherit
",25,1,1,push,1
huggingface/transformers,self-push-amd.yml,"name: Self-hosted runner AMD GPU (push)

on:
  workflow_call:
    inputs:
      gpu_flavor:
        required: true
        type: string

env:
  HF_HOME: /mnt/cache
  TRANSFORMERS_IS_CI: yes
  OMP_NUM_THREADS: 8
  MKL_NUM_THREADS: 8
  PYTEST_TIMEOUT: 60
  TF_FORCE_GPU_ALLOW_GROWTH: true
  HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}

jobs:
  check_runner_status:
    name: Check Runner Status
    runs-on: ubuntu-22.04
    steps:
      - name: Checkout transformers
        uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - name: Check Runner Status
        run: python utils/check_self_hosted_runner.py --target_runners amd-mi210-single-gpu-ci-runner-docker --token ${{ secrets.ACCESS_REPO_INFO_TOKEN }}

  check_runners:
    name: Check Runners
    needs: check_runner_status
    strategy:
      matrix:
        machine_type: [single-gpu, multi-gpu]
    runs-on: [self-hosted, amd-gpu, '${{ matrix.machine_type }}', '${{ inputs.gpu_flavor }}']
    container:
      image: huggingface/transformers-pytorch-amd-gpu-push-ci  # <--- We test only for PyTorch for now
      options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size ""16gb"" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    steps:
      - name: ROCM-SMI
        run: |
          rocm-smi
      - name: ROCM-INFO
        run: |
          rocminfo  | grep ""Agent"" -A 14
      - name: Show ROCR environment
        run: |
          echo ""ROCR: $ROCR_VISIBLE_DEVICES""

  setup_gpu:
    name: Setup
    needs: check_runners
    strategy:
      matrix:
        machine_type: [single-gpu, multi-gpu]
    runs-on: [self-hosted, amd-gpu, '${{ matrix.machine_type }}', '${{ inputs.gpu_flavor }}']
    container:
      image: huggingface/transformers-pytorch-amd-gpu-push-ci  # <--- We test only for PyTorch for now
      options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size ""16gb"" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      test_map: ${{ steps.set-matrix.outputs.test_map }}
    env:
      # `CI_BRANCH_PUSH`: The branch name from the push event
      # `CI_BRANCH_WORKFLOW_RUN`: The name of the branch on which this workflow is triggered by `workflow_run` event
      # `CI_SHA_PUSH`: The commit SHA from the push event
      # `CI_SHA_WORKFLOW_RUN`: The commit SHA that triggers this workflow by `workflow_run` event
      CI_BRANCH_PUSH: ${{ github.event.ref }}
      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}
      CI_SHA_PUSH: ${{ github.event.head_commit.id }}
      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}
    steps:
      # Necessary to get the correct branch name and commit SHA for `workflow_run` event
      # We also take into account the `push` event (we might want to test some changes in a branch)
      - name: Prepare custom environment variables
        shell: bash
        # `CI_BRANCH`: The non-empty branch name from the above two (one and only one of them is empty)
        # `CI_SHA`: The non-empty commit SHA from the above two (one and only one of them is empty)
        run: |
          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}
          echo $CI_BRANCH_PUSH
          echo $CI_BRANCH_WORKFLOW_RUN
          echo $CI_SHA_PUSH
          echo $CI_SHA_WORKFLOW_RUN
          [[ ! -z ""$CI_BRANCH_PUSH"" ]] && echo ""CI_BRANCH=$CI_BRANCH_PUSH"" >> $GITHUB_ENV || echo ""CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN"" >> $GITHUB_ENV
          [[ ! -z ""$CI_SHA_PUSH"" ]] && echo ""CI_SHA=$CI_SHA_PUSH"" >> $GITHUB_ENV || echo ""CI_SHA=$CI_SHA_WORKFLOW_RUN"" >> $GITHUB_ENV

      - name: print environment variables
        run: |
          echo ""env.CI_BRANCH = ${{ env.CI_BRANCH }}""
          echo ""env.CI_SHA = ${{ env.CI_SHA }}""

      - name: Update clone using environment variables
        working-directory: /transformers
        run: |
          echo ""original branch = $(git branch --show-current)""
          git fetch && git checkout ${{ env.CI_BRANCH }}
          echo ""updated branch = $(git branch --show-current)""
          git checkout ${{ env.CI_SHA }}
          echo ""log = $(git log -n 1)""

      - name: Cleanup
        working-directory: /transformers
        run: |
          rm -rf tests/__pycache__
          rm -rf tests/models/__pycache__
          rm -rf reports

      - name: Show installed libraries and their versions
        working-directory: /transformers
        run: pip freeze

      - name: Fetch the tests to run
        working-directory: /transformers
        # TODO: add `git-python` in the docker images
        run: |
          pip install --upgrade git-python
          python3 utils/tests_fetcher.py --diff_with_last_commit | tee test_preparation.txt

      - name: Report fetched tests
        uses: actions/upload-artifact@v4
        with:
          name: test_fetched
          path: /transformers/test_preparation.txt

      - id: set-matrix
        name: Organize tests into models
        working-directory: /transformers
        # The `keys` is used as GitHub actions matrix for jobs, i.e. `models/bert`, `tokenization`, `pipeline`, etc.
        # The `test_map` is used to get the actual identified test files under each key.
        # If no test to run (so no `test_map.json` file), create a dummy map (empty matrix will fail)
        run: |
          if [ -f test_map.json ]; then
              keys=$(python3 -c 'import json; fp = open(""test_map.json""); test_map = json.load(fp); fp.close(); d = list(test_map.keys()); print(d)')
              test_map=$(python3 -c 'import json; fp = open(""test_map.json""); test_map = json.load(fp); fp.close(); print(test_map)')
          else
              keys=$(python3 -c 'keys = [""dummy""]; print(keys)')
              test_map=$(python3 -c 'test_map = {""dummy"": []}; print(test_map)')
          fi
          echo $keys
          echo $test_map
          echo ""matrix=$keys"" >> $GITHUB_OUTPUT
          echo ""test_map=$test_map"" >> $GITHUB_OUTPUT

  run_models_gpu:
    name: Model tests
    needs: setup_gpu
    # `dummy` means there is no test to run
    if: contains(fromJson(needs.setup_gpu.outputs.matrix), 'dummy') != true
    strategy:
      fail-fast: false
      matrix:
        folders: ${{ fromJson(needs.setup_gpu.outputs.matrix) }}
        machine_type: [single-gpu, multi-gpu]
    runs-on: [self-hosted, amd-gpu, '${{ matrix.machine_type }}', '${{ inputs.gpu_flavor }}']
    container:
      image: huggingface/transformers-pytorch-amd-gpu-push-ci  # <--- We test only for PyTorch for now
      options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size ""16gb"" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    env:
      # For the meaning of these environment variables, see the job `Setup`
      CI_BRANCH_PUSH: ${{ github.event.ref }}
      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}
      CI_SHA_PUSH: ${{ github.event.head_commit.id }}
      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}
    steps:
      # Necessary to get the correct branch name and commit SHA for `workflow_run` event
      # We also take into account the `push` event (we might want to test some changes in a branch)
      - name: Prepare custom environment variables
        shell: bash
        # For the meaning of these environment variables, see the job `Setup`
        run: |
          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}
          echo $CI_BRANCH_PUSH
          echo $CI_BRANCH_WORKFLOW_RUN
          echo $CI_SHA_PUSH
          echo $CI_SHA_WORKFLOW_RUN
          [[ ! -z ""$CI_BRANCH_PUSH"" ]] && echo ""CI_BRANCH=$CI_BRANCH_PUSH"" >> $GITHUB_ENV || echo ""CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN"" >> $GITHUB_ENV
          [[ ! -z ""$CI_SHA_PUSH"" ]] && echo ""CI_SHA=$CI_SHA_PUSH"" >> $GITHUB_ENV || echo ""CI_SHA=$CI_SHA_WORKFLOW_RUN"" >> $GITHUB_ENV

      - name: print environment variables
        run: |
          echo ""env.CI_BRANCH = ${{ env.CI_BRANCH }}""
          echo ""env.CI_SHA = ${{ env.CI_SHA }}""

      - name: Update clone using environment variables
        working-directory: /transformers
        run: |
          echo ""original branch = $(git branch --show-current)""
          git fetch && git checkout ${{ env.CI_BRANCH }}
          echo ""updated branch = $(git branch --show-current)""
          git checkout ${{ env.CI_SHA }}
          echo ""log = $(git log -n 1)""

      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)
        working-directory: /transformers
        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .

      - name: Echo folder ${{ matrix.folders }}
        shell: bash
        # For folders like `models/bert`, set an env. var. (`matrix_folders`) to `models_bert`, which will be used to
        # set the artifact folder names (because the character `/` is not allowed).
        run: |
          echo ""${{ matrix.folders }}""
          echo ""${{ fromJson(needs.setup_gpu.outputs.test_map)[matrix.folders] }}""
          matrix_folders=${{ matrix.folders }}
          matrix_folders=${matrix_folders/'models/'/'models_'}
          echo ""$matrix_folders""
          echo ""matrix_folders=$matrix_folders"" >> $GITHUB_ENV

      - name: ROCM-SMI
        run: |
          rocm-smi
      - name: ROCM-INFO
        run: |
          rocminfo  | grep ""Agent"" -A 14
      - name: Show ROCR environment
        run: |
          echo ""ROCR: $ROCR_VISIBLE_DEVICES""

      - name: Environment
        working-directory: /transformers
        run: |
          python3 utils/print_env.py

      - name: Show installed libraries and their versions
        working-directory: /transformers
        run: pip freeze

      - name: Run all non-slow selected tests on GPU
        working-directory: /transformers
        run: |
          python3 -m pytest -n 2 --dist=loadfile -v --make-reports=${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports ${{ fromJson(needs.setup_gpu.outputs.test_map)[matrix.folders] }} -m ""not not_device_test""

      - name: Failure short reports
        if: ${{ failure() }}
        continue-on-error: true
        run: cat /transformers/reports/${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/failures_short.txt

      - name: ""Test suite reports artifacts: ${{ matrix.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports""
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports
          path: /transformers/reports/${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports

  send_results:
    name: Send results to webhook
    runs-on: ubuntu-22.04
    if: always()
    needs: [
        check_runner_status,
        check_runners,
        setup_gpu,
        run_models_gpu,
#        run_tests_torch_cuda_extensions_single_gpu,
#        run_tests_torch_cuda_extensions_multi_gpu
    ]
    env:
      # For the meaning of these environment variables, see the job `Setup`
      CI_BRANCH_PUSH: ${{ github.event.ref }}
      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}
      CI_SHA_PUSH: ${{ github.event.head_commit.id }}
      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}
    steps:
      - name: Preliminary job status
        shell: bash
        # For the meaning of these environment variables, see the job `Setup`
        run: |
          echo ""Runner availability: ${{ needs.check_runner_status.result }}""
          echo ""Setup status: ${{ needs.setup_gpu.result }}""
          echo ""Runner status: ${{ needs.check_runners.result }}""

      # Necessary to get the correct branch name and commit SHA for `workflow_run` event
      # We also take into account the `push` event (we might want to test some changes in a branch)
      - name: Prepare custom environment variables
        shell: bash
        # For the meaning of these environment variables, see the job `Setup`
        run: |
          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}
          echo $CI_BRANCH_PUSH
          echo $CI_BRANCH_WORKFLOW_RUN
          echo $CI_SHA_PUSH
          echo $CI_SHA_WORKFLOW_RUN
          [[ ! -z ""$CI_BRANCH_PUSH"" ]] && echo ""CI_BRANCH=$CI_BRANCH_PUSH"" >> $GITHUB_ENV || echo ""CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN"" >> $GITHUB_ENV
          [[ ! -z ""$CI_SHA_PUSH"" ]] && echo ""CI_SHA=$CI_SHA_PUSH"" >> $GITHUB_ENV || echo ""CI_SHA=$CI_SHA_WORKFLOW_RUN"" >> $GITHUB_ENV

      - name: print environment variables
        run: |
          echo ""env.CI_BRANCH = ${{ env.CI_BRANCH }}""
          echo ""env.CI_SHA = ${{ env.CI_SHA }}""

      - uses: actions/checkout@v4
        # To avoid failure when multiple commits are merged into `main` in a short period of time.
        # Checking out to an old commit beyond the fetch depth will get an error `fatal: reference is not a tree: ...
        # (Only required for `workflow_run` event, where we get the latest HEAD on `main` instead of the event commit)
        with:
          fetch-depth: 20

      - name: Update clone using environment variables
        run: |
          echo ""original branch = $(git branch --show-current)""
          git fetch && git checkout ${{ env.CI_BRANCH }}
          echo ""updated branch = $(git branch --show-current)""
          git checkout ${{ env.CI_SHA }}
          echo ""log = $(git log -n 1)""

      - uses: actions/download-artifact@v4
      - name: Send message to Slack
        env:
          CI_SLACK_BOT_TOKEN: ${{ secrets.CI_SLACK_BOT_TOKEN }}
          CI_SLACK_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID }}
          CI_SLACK_CHANNEL_ID_DAILY: ${{ secrets.CI_SLACK_CHANNEL_ID_DAILY }}
          CI_SLACK_CHANNEL_ID_AMD: ${{ secrets.CI_SLACK_CHANNEL_ID_AMD }}
          CI_SLACK_CHANNEL_DUMMY_TESTS: ${{ secrets.CI_SLACK_CHANNEL_DUMMY_TESTS }}
          CI_SLACK_REPORT_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID_AMD }}
          ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}
          CI_EVENT: Push CI (AMD) - ${{ inputs.gpu_flavor }}
          CI_TITLE_PUSH: ${{ github.event.head_commit.message }}
          CI_TITLE_WORKFLOW_RUN: ${{ github.event.workflow_run.head_commit.message }}
          CI_SHA: ${{ env.CI_SHA }}
          RUNNER_STATUS: ${{ needs.check_runner_status.result }}
          RUNNER_ENV_STATUS: ${{ needs.check_runners.result }}
          SETUP_STATUS: ${{ needs.setup_gpu.result }}

        # We pass `needs.setup_gpu.outputs.matrix` as the argument. A processing in `notification_service.py` to change
        # `models/bert` to `models_bert` is required, as the artifact names use `_` instead of `/`.
        run: |
          pip install huggingface_hub
          pip install slack_sdk
          pip show slack_sdk
          python utils/notification_service.py ""${{ needs.setup_gpu.outputs.matrix }}""
",334,5,1,workflow_call,5
huggingface/transformers,self-push-caller.yml,"# Used to trigger self-push CI
name: Self-hosted runner (push-caller)

on:
  push:
    branches:
      - main
    paths:
      - ""src/**""
      - ""tests/**""
      - "".github/**""
      - ""templates/**""
      - ""utils/**""

jobs:
  check-for-setup:
      runs-on: ubuntu-22.04
      name: Check if setup was changed
      outputs:
        changed: ${{ steps.was_changed.outputs.changed }}
      steps:
        - uses: actions/checkout@v4
          with: 
            fetch-depth: ""2""
        
        - name: Get changed files
          id: changed-files
          uses: tj-actions/changed-files@1c8e6069583811afb28f97afeaf8e7da80c6be5c
        
        - name: Was setup changed 
          id: was_changed
          run: |
            for file in ${{ steps.changed-files.outputs.all_changed_files }}; do
              if [ `basename ""${file}""` = ""setup.py"" ]; then
                echo ""changed=1"" >> $GITHUB_OUTPUT
              fi
            done

  build-docker-containers:
    needs: check-for-setup
    if: (github.event_name == 'push') && (needs.check-for-setup.outputs.changed == '1')
    uses: ./.github/workflows/build-docker-images.yml
    with:
      image_postfix: ""-push-ci""
    secrets: inherit

  run_push_ci:
    name: Trigger Push CI
    runs-on: ubuntu-22.04
    if: ${{ always() }}
    needs: build-docker-containers
    steps:
      - name: Trigger push CI via workflow_run
        run: echo ""Trigger push CI via workflow_run""
",54,3,1,push,3
huggingface/transformers,self-push.yml,"name: Self-hosted runner (push)

on:
  workflow_run:
    workflows: [""Self-hosted runner (push-caller)""]
    branches: [""main""]
    types: [completed]
  push:
    branches:
      - ci_*
      - ci-*
    paths:
      - ""src/**""
      - ""tests/**""
      - "".github/**""
      - ""templates/**""
      - ""utils/**""
  repository_dispatch:

env:
  HF_HOME: /mnt/cache
  TRANSFORMERS_IS_CI: yes
  OMP_NUM_THREADS: 8
  MKL_NUM_THREADS: 8
  PYTEST_TIMEOUT: 60
  TF_FORCE_GPU_ALLOW_GROWTH: true
  CUDA_VISIBLE_DEVICES: 0,1

jobs:
  setup:
    name: Setup
    strategy:
      matrix:
        machine_type: [aws-g4dn-2xlarge-cache, aws-g4dn-12xlarge-cache]
    runs-on:
      group: '${{ matrix.machine_type }}'
    container:
      image: huggingface/transformers-all-latest-gpu-push-ci
      options: --gpus 0 --shm-size ""16gb"" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      test_map: ${{ steps.set-matrix.outputs.test_map }}
    env:
      # `CI_BRANCH_PUSH`: The branch name from the push event
      # `CI_BRANCH_WORKFLOW_RUN`: The name of the branch on which this workflow is triggered by `workflow_run` event
      # `CI_SHA_PUSH`: The commit SHA from the push event
      # `CI_SHA_WORKFLOW_RUN`: The commit SHA that triggers this workflow by `workflow_run` event
      CI_BRANCH_PUSH: ${{ github.event.ref }}
      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}
      CI_SHA_PUSH: ${{ github.event.head_commit.id }}
      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}
    steps:
      # Necessary to get the correct branch name and commit SHA for `workflow_run` event
      # We also take into account the `push` event (we might want to test some changes in a branch)
      - name: Prepare custom environment variables
        shell: bash
        # `CI_BRANCH`: The non-empty branch name from the above two (one and only one of them is empty)
        # `CI_SHA`: The non-empty commit SHA from the above two (one and only one of them is empty)
        run: |
          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}
          echo $CI_BRANCH_PUSH
          echo $CI_BRANCH_WORKFLOW_RUN
          echo $CI_SHA_PUSH
          echo $CI_SHA_WORKFLOW_RUN
          [[ ! -z ""$CI_BRANCH_PUSH"" ]] && echo ""CI_BRANCH=$CI_BRANCH_PUSH"" >> $GITHUB_ENV || echo ""CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN"" >> $GITHUB_ENV
          [[ ! -z ""$CI_SHA_PUSH"" ]] && echo ""CI_SHA=$CI_SHA_PUSH"" >> $GITHUB_ENV || echo ""CI_SHA=$CI_SHA_WORKFLOW_RUN"" >> $GITHUB_ENV

      - name: print environment variables
        run: |
          echo ""env.CI_BRANCH = ${{ env.CI_BRANCH }}""
          echo ""env.CI_SHA = ${{ env.CI_SHA }}""

      - name: Update clone using environment variables
        working-directory: /transformers
        run: |
          echo ""original branch = $(git branch --show-current)""
          git fetch && git checkout ${{ env.CI_BRANCH }}
          echo ""updated branch = $(git branch --show-current)""
          git checkout ${{ env.CI_SHA }}
          echo ""log = $(git log -n 1)""

      - name: Cleanup
        working-directory: /transformers
        run: |
          rm -rf tests/__pycache__
          rm -rf tests/models/__pycache__
          rm -rf reports

      - name: Show installed libraries and their versions
        working-directory: /transformers
        run: pip freeze

      - name: Fetch the tests to run
        working-directory: /transformers
        # TODO: add `git-python` in the docker images
        run: |
          pip install --upgrade git-python
          python3 utils/tests_fetcher.py --diff_with_last_commit | tee test_preparation.txt

      - name: Report fetched tests
        uses: actions/upload-artifact@v4
        with:
          name: test_fetched
          path: /transformers/test_preparation.txt

      - id: set-matrix
        name: Organize tests into models
        working-directory: /transformers
        # The `keys` is used as GitHub actions matrix for jobs, i.e. `models/bert`, `tokenization`, `pipeline`, etc.
        # The `test_map` is used to get the actual identified test files under each key.
        # If no test to run (so no `test_map.json` file), create a dummy map (empty matrix will fail)
        run: |
          if [ -f test_map.json ]; then
              keys=$(python3 -c 'import json; fp = open(""test_map.json""); test_map = json.load(fp); fp.close(); d = list(test_map.keys()); print(d)')
              test_map=$(python3 -c 'import json; fp = open(""test_map.json""); test_map = json.load(fp); fp.close(); print(test_map)')
          else
              keys=$(python3 -c 'keys = [""dummy""]; print(keys)')
              test_map=$(python3 -c 'test_map = {""dummy"": []}; print(test_map)')
          fi
          echo $keys
          echo $test_map
          echo ""matrix=$keys"" >> $GITHUB_OUTPUT
          echo ""test_map=$test_map"" >> $GITHUB_OUTPUT

  run_tests_single_gpu:
    name: Model tests
    needs: setup
    # `dummy` means there is no test to run
    if: contains(fromJson(needs.setup.outputs.matrix), 'dummy') != true
    strategy:
      fail-fast: false
      matrix:
        folders: ${{ fromJson(needs.setup.outputs.matrix) }}
        machine_type: [aws-g4dn-2xlarge-cache]
    runs-on:
      group: '${{ matrix.machine_type }}'
    container:
      image: huggingface/transformers-all-latest-gpu-push-ci
      options: --gpus 0 --shm-size ""16gb"" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    env:
      # For the meaning of these environment variables, see the job `Setup`
      CI_BRANCH_PUSH: ${{ github.event.ref }}
      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}
      CI_SHA_PUSH: ${{ github.event.head_commit.id }}
      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}
    steps:
      # Necessary to get the correct branch name and commit SHA for `workflow_run` event
      # We also take into account the `push` event (we might want to test some changes in a branch)
      - name: Prepare custom environment variables
        shell: bash
        # For the meaning of these environment variables, see the job `Setup`
        run: |
          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}
          echo $CI_BRANCH_PUSH
          echo $CI_BRANCH_WORKFLOW_RUN
          echo $CI_SHA_PUSH
          echo $CI_SHA_WORKFLOW_RUN
          [[ ! -z ""$CI_BRANCH_PUSH"" ]] && echo ""CI_BRANCH=$CI_BRANCH_PUSH"" >> $GITHUB_ENV || echo ""CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN"" >> $GITHUB_ENV
          [[ ! -z ""$CI_SHA_PUSH"" ]] && echo ""CI_SHA=$CI_SHA_PUSH"" >> $GITHUB_ENV || echo ""CI_SHA=$CI_SHA_WORKFLOW_RUN"" >> $GITHUB_ENV

      - name: print environment variables
        run: |
          echo ""env.CI_BRANCH = ${{ env.CI_BRANCH }}""
          echo ""env.CI_SHA = ${{ env.CI_SHA }}""

      - name: Set `machine_type` for report and artifact names
        working-directory: /transformers
        shell: bash
        run: |
          echo ""${{ matrix.machine_type }}""

          if [ ""${{ matrix.machine_type }}"" = ""aws-g4dn-2xlarge-cache"" ]; then
            machine_type=single-gpu
          elif [ ""${{ matrix.machine_type }}"" = ""aws-g4dn-12xlarge-cache"" ]; then
            machine_type=multi-gpu
          else
            machine_type=${{ matrix.machine_type }}
          fi

          echo ""$machine_type""
          echo ""machine_type=$machine_type"" >> $GITHUB_ENV

      - name: Update clone using environment variables
        working-directory: /transformers
        run: |
          echo ""original branch = $(git branch --show-current)""
          git fetch && git checkout ${{ env.CI_BRANCH }}
          echo ""updated branch = $(git branch --show-current)""
          git checkout ${{ env.CI_SHA }}
          echo ""log = $(git log -n 1)""

      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)
        working-directory: /transformers
        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .

      - name: Echo folder ${{ matrix.folders }}
        shell: bash
        # For folders like `models/bert`, set an env. var. (`matrix_folders`) to `models_bert`, which will be used to
        # set the artifact folder names (because the character `/` is not allowed).
        run: |
          echo ""${{ matrix.folders }}""
          echo ""${{ fromJson(needs.setup.outputs.test_map)[matrix.folders] }}""
          matrix_folders=${{ matrix.folders }}
          matrix_folders=${matrix_folders/'models/'/'models_'}
          echo ""$matrix_folders""
          echo ""matrix_folders=$matrix_folders"" >> $GITHUB_ENV

      - name: NVIDIA-SMI
        run: |
          nvidia-smi

      - name: Environment
        working-directory: /transformers
        run: |
          python3 utils/print_env.py

      - name: Show installed libraries and their versions
        working-directory: /transformers
        run: pip freeze

      - name: Run all non-slow selected tests on GPU
        working-directory: /transformers
        run: |
          python3 -m pytest -n 2 --dist=loadfile -v --make-reports=${{ env.machine_type }}_tests_gpu_${{ matrix.folders }} ${{ fromJson(needs.setup.outputs.test_map)[matrix.folders] }}

      - name: Failure short reports
        if: ${{ failure() }}
        continue-on-error: true
        run: cat /transformers/reports/${{ env.machine_type }}_tests_gpu_${{ matrix.folders }}/failures_short.txt

      - name: ""Test suite reports artifacts: ${{ env.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports""
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports
          path: /transformers/reports/${{ env.machine_type }}_tests_gpu_${{ matrix.folders }}

  run_tests_multi_gpu:
    name: Model tests
    needs: setup
    # `dummy` means there is no test to run
    if: contains(fromJson(needs.setup.outputs.matrix), 'dummy') != true
    strategy:
      fail-fast: false
      matrix:
        folders: ${{ fromJson(needs.setup.outputs.matrix) }}
        machine_type: [aws-g4dn-12xlarge-cache]
    runs-on:
      group: '${{ matrix.machine_type }}'
    container:
      image: huggingface/transformers-all-latest-gpu-push-ci
      options: --gpus all --shm-size ""16gb"" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    env:
      # For the meaning of these environment variables, see the job `Setup`
      CI_BRANCH_PUSH: ${{ github.event.ref }}
      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}
      CI_SHA_PUSH: ${{ github.event.head_commit.id }}
      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}
    steps:
      # Necessary to get the correct branch name and commit SHA for `workflow_run` event
      # We also take into account the `push` event (we might want to test some changes in a branch)
      - name: Prepare custom environment variables
        shell: bash
        # For the meaning of these environment variables, see the job `Setup`
        run: |
          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}
          echo $CI_BRANCH_PUSH
          echo $CI_BRANCH_WORKFLOW_RUN
          echo $CI_SHA_PUSH
          echo $CI_SHA_WORKFLOW_RUN
          [[ ! -z ""$CI_BRANCH_PUSH"" ]] && echo ""CI_BRANCH=$CI_BRANCH_PUSH"" >> $GITHUB_ENV || echo ""CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN"" >> $GITHUB_ENV
          [[ ! -z ""$CI_SHA_PUSH"" ]] && echo ""CI_SHA=$CI_SHA_PUSH"" >> $GITHUB_ENV || echo ""CI_SHA=$CI_SHA_WORKFLOW_RUN"" >> $GITHUB_ENV

      - name: print environment variables
        run: |
          echo ""env.CI_BRANCH = ${{ env.CI_BRANCH }}""
          echo ""env.CI_SHA = ${{ env.CI_SHA }}""

      - name: Set `machine_type` for report and artifact names
        working-directory: /transformers
        shell: bash
        run: |
          echo ""${{ matrix.machine_type }}""

          if [ ""${{ matrix.machine_type }}"" = ""aws-g4dn-2xlarge-cache"" ]; then
            machine_type=single-gpu
          elif [ ""${{ matrix.machine_type }}"" = ""aws-g4dn-12xlarge-cache"" ]; then
            machine_type=multi-gpu
          else
            machine_type=${{ matrix.machine_type }}
          fi

          echo ""$machine_type""
          echo ""machine_type=$machine_type"" >> $GITHUB_ENV

      - name: Update clone using environment variables
        working-directory: /transformers
        run: |
          echo ""original branch = $(git branch --show-current)""
          git fetch && git checkout ${{ env.CI_BRANCH }}
          echo ""updated branch = $(git branch --show-current)""
          git checkout ${{ env.CI_SHA }}
          echo ""log = $(git log -n 1)""

      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)
        working-directory: /transformers
        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .

      - name: Echo folder ${{ matrix.folders }}
        shell: bash
        # For folders like `models/bert`, set an env. var. (`matrix_folders`) to `models_bert`, which will be used to
        # set the artifact folder names (because the character `/` is not allowed).
        run: |
          echo ""${{ matrix.folders }}""
          echo ""${{ fromJson(needs.setup.outputs.test_map)[matrix.folders] }}""
          matrix_folders=${{ matrix.folders }}
          matrix_folders=${matrix_folders/'models/'/'models_'}
          echo ""$matrix_folders""
          echo ""matrix_folders=$matrix_folders"" >> $GITHUB_ENV

      - name: NVIDIA-SMI
        run: |
          nvidia-smi

      - name: Environment
        working-directory: /transformers
        run: |
          python3 utils/print_env.py

      - name: Show installed libraries and their versions
        working-directory: /transformers
        run: pip freeze

      - name: Run all non-slow selected tests on GPU
        env:
          MKL_SERVICE_FORCE_INTEL: 1
        working-directory: /transformers
        run: |
          python3 -m pytest -n 2 --dist=loadfile -v --make-reports=${{ env.machine_type }}_tests_gpu_${{ matrix.folders }} ${{ fromJson(needs.setup.outputs.test_map)[matrix.folders] }}

      - name: Failure short reports
        if: ${{ failure() }}
        continue-on-error: true
        run: cat /transformers/reports/${{ env.machine_type }}_tests_gpu_${{ matrix.folders }}/failures_short.txt

      - name: ""Test suite reports artifacts: ${{ env.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports""
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports
          path: /transformers/reports/${{ env.machine_type }}_tests_gpu_${{ matrix.folders }}

  run_tests_torch_cuda_extensions_single_gpu:
    name: Torch CUDA extension tests
    needs: setup
    if: contains(fromJson(needs.setup.outputs.matrix), 'deepspeed') || contains(fromJson(needs.setup.outputs.matrix), 'extended')
    strategy:
      fail-fast: false
      matrix:
        machine_type: [aws-g4dn-2xlarge-cache]
    runs-on:
      group: '${{ matrix.machine_type }}'
    container:
      image: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci
      options: --gpus 0 --shm-size ""16gb"" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    env:
      # For the meaning of these environment variables, see the job `Setup`
      CI_BRANCH_PUSH: ${{ github.event.ref }}
      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}
      CI_SHA_PUSH: ${{ github.event.head_commit.id }}
      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}
    steps:
      # Necessary to get the correct branch name and commit SHA for `workflow_run` event
      # We also take into account the `push` event (we might want to test some changes in a branch)
      - name: Prepare custom environment variables
        shell: bash
        # For the meaning of these environment variables, see the job `Setup`
        run: |
          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}
          echo $CI_BRANCH_PUSH
          echo $CI_BRANCH_WORKFLOW_RUN
          echo $CI_SHA_PUSH
          echo $CI_SHA_WORKFLOW_RUN
          [[ ! -z ""$CI_BRANCH_PUSH"" ]] && echo ""CI_BRANCH=$CI_BRANCH_PUSH"" >> $GITHUB_ENV || echo ""CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN"" >> $GITHUB_ENV
          [[ ! -z ""$CI_SHA_PUSH"" ]] && echo ""CI_SHA=$CI_SHA_PUSH"" >> $GITHUB_ENV || echo ""CI_SHA=$CI_SHA_WORKFLOW_RUN"" >> $GITHUB_ENV

      - name: print environment variables
        run: |
          echo ""env.CI_BRANCH = ${{ env.CI_BRANCH }}""
          echo ""env.CI_SHA = ${{ env.CI_SHA }}""

      - name: Set `machine_type` for report and artifact names
        working-directory: /workspace/transformers
        shell: bash
        run: |
          echo ""${{ matrix.machine_type }}""

          if [ ""${{ matrix.machine_type }}"" = ""aws-g4dn-2xlarge-cache"" ]; then
            machine_type=single-gpu
          elif [ ""${{ matrix.machine_type }}"" = ""aws-g4dn-12xlarge-cache"" ]; then
            machine_type=multi-gpu
          else
            machine_type=${{ matrix.machine_type }}
          fi

          echo ""$machine_type""
          echo ""machine_type=$machine_type"" >> $GITHUB_ENV

      - name: Update clone using environment variables
        working-directory: /workspace/transformers
        run: |
          echo ""original branch = $(git branch --show-current)""
          git fetch && git checkout ${{ env.CI_BRANCH }}
          echo ""updated branch = $(git branch --show-current)""
          git checkout ${{ env.CI_SHA }}
          echo ""log = $(git log -n 1)""

      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)
        working-directory: /workspace/transformers
        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .

      - name: Remove cached torch extensions
        run: rm -rf /github/home/.cache/torch_extensions/

      # To avoid unknown test failures
      - name: Pre build DeepSpeed *again*
        working-directory: /workspace
        run: |
          python3 -m pip uninstall -y deepspeed
          DS_BUILD_CPU_ADAM=1 DS_BUILD_FUSED_ADAM=1 python3 -m pip install deepspeed --global-option=""build_ext"" --global-option=""-j8"" --no-cache -v --disable-pip-version-check

      - name: NVIDIA-SMI
        run: |
          nvidia-smi

      - name: Environment
        working-directory: /workspace/transformers
        run: |
          python utils/print_env.py

      - name: Show installed libraries and their versions
        working-directory: /workspace/transformers
        run: pip freeze

      - name: Run all non-slow selected tests on GPU
        working-directory: /workspace/transformers
        # TODO: Here we pass all tests in the 2 folders for simplicity. It's better to pass only the identified tests.
        run: |
          python -m pytest -n 1 --dist=loadfile -v --make-reports=${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports tests/deepspeed tests/extended

      - name: Failure short reports
        if: ${{ failure() }}
        continue-on-error: true
        run: cat /workspace/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports/failures_short.txt

      - name: ""Test suite reports artifacts: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports""
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports
          path: /workspace/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports

  run_tests_torch_cuda_extensions_multi_gpu:
    name: Torch CUDA extension tests
    needs: setup
    if: contains(fromJson(needs.setup.outputs.matrix), 'deepspeed') || contains(fromJson(needs.setup.outputs.matrix), 'extended')
    strategy:
      fail-fast: false
      matrix:
        machine_type: [aws-g4dn-12xlarge-cache]
    runs-on:
      group: '${{ matrix.machine_type }}'
    container:
      image: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci
      options: --gpus all --shm-size ""16gb"" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    env:
      # For the meaning of these environment variables, see the job `Setup`
      CI_BRANCH_PUSH: ${{ github.event.ref }}
      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}
      CI_SHA_PUSH: ${{ github.event.head_commit.id }}
      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}
    steps:
      # Necessary to get the correct branch name and commit SHA for `workflow_run` event
      # We also take into account the `push` event (we might want to test some changes in a branch)
      - name: Prepare custom environment variables
        shell: bash
        # For the meaning of these environment variables, see the job `Setup`
        run: |
          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}
          echo $CI_BRANCH_PUSH
          echo $CI_BRANCH_WORKFLOW_RUN
          echo $CI_SHA_PUSH
          echo $CI_SHA_WORKFLOW_RUN
          [[ ! -z ""$CI_BRANCH_PUSH"" ]] && echo ""CI_BRANCH=$CI_BRANCH_PUSH"" >> $GITHUB_ENV || echo ""CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN"" >> $GITHUB_ENV
          [[ ! -z ""$CI_SHA_PUSH"" ]] && echo ""CI_SHA=$CI_SHA_PUSH"" >> $GITHUB_ENV || echo ""CI_SHA=$CI_SHA_WORKFLOW_RUN"" >> $GITHUB_ENV

      - name: print environment variables
        run: |
          echo ""env.CI_BRANCH = ${{ env.CI_BRANCH }}""
          echo ""env.CI_SHA = ${{ env.CI_SHA }}""

      - name: Set `machine_type` for report and artifact names
        working-directory: /workspace/transformers
        shell: bash
        run: |
          echo ""${{ matrix.machine_type }}""

          if [ ""${{ matrix.machine_type }}"" = ""aws-g4dn-2xlarge-cache"" ]; then
            machine_type=single-gpu
          elif [ ""${{ matrix.machine_type }}"" = ""aws-g4dn-12xlarge-cache"" ]; then
            machine_type=multi-gpu
          else
            machine_type=${{ matrix.machine_type }}
          fi

          echo ""$machine_type""
          echo ""machine_type=$machine_type"" >> $GITHUB_ENV

      - name: Update clone using environment variables
        working-directory: /workspace/transformers
        run: |
          echo ""original branch = $(git branch --show-current)""
          git fetch && git checkout ${{ env.CI_BRANCH }}
          echo ""updated branch = $(git branch --show-current)""
          git checkout ${{ env.CI_SHA }}
          echo ""log = $(git log -n 1)""

      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)
        working-directory: /workspace/transformers
        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .

      - name: Remove cached torch extensions
        run: rm -rf /github/home/.cache/torch_extensions/

      # To avoid unknown test failures
      - name: Pre build DeepSpeed *again*
        working-directory: /workspace
        run: |
          python3 -m pip uninstall -y deepspeed
          DS_BUILD_CPU_ADAM=1 DS_BUILD_FUSED_ADAM=1 python3 -m pip install deepspeed --global-option=""build_ext"" --global-option=""-j8"" --no-cache -v --disable-pip-version-check

      - name: NVIDIA-SMI
        run: |
          nvidia-smi

      - name: Environment
        working-directory: /workspace/transformers
        run: |
          python utils/print_env.py

      - name: Show installed libraries and their versions
        working-directory: /workspace/transformers
        run: pip freeze

      - name: Run all non-slow selected tests on GPU
        working-directory: /workspace/transformers
        # TODO: Here we pass all tests in the 2 folders for simplicity. It's better to pass only the identified tests.
        run: |
          python -m pytest -n 1 --dist=loadfile -v --make-reports=${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports tests/deepspeed tests/extended

      - name: Failure short reports
        if: ${{ failure() }}
        continue-on-error: true
        run: cat /workspace/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports/failures_short.txt

      - name: ""Test suite reports artifacts: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports""
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports
          path: /workspace/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports

  send_results:
    name: Send results to webhook
    runs-on: ubuntu-22.04
    if: always()
    needs: [
        setup,
        run_tests_single_gpu,
        run_tests_multi_gpu,
        run_tests_torch_cuda_extensions_single_gpu,
        run_tests_torch_cuda_extensions_multi_gpu
    ]
    env:
      # For the meaning of these environment variables, see the job `Setup`
      CI_BRANCH_PUSH: ${{ github.event.ref }}
      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}
      CI_SHA_PUSH: ${{ github.event.head_commit.id }}
      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}
    steps:
      - name: Preliminary job status
        shell: bash
        # For the meaning of these environment variables, see the job `Setup`
        run: |
          echo ""Setup status: ${{ needs.setup.result }}""

      # Necessary to get the correct branch name and commit SHA for `workflow_run` event
      # We also take into account the `push` event (we might want to test some changes in a branch)
      - name: Prepare custom environment variables
        shell: bash
        # For the meaning of these environment variables, see the job `Setup`
        run: |
          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}
          echo $CI_BRANCH_PUSH
          echo $CI_BRANCH_WORKFLOW_RUN
          echo $CI_SHA_PUSH
          echo $CI_SHA_WORKFLOW_RUN
          [[ ! -z ""$CI_BRANCH_PUSH"" ]] && echo ""CI_BRANCH=$CI_BRANCH_PUSH"" >> $GITHUB_ENV || echo ""CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN"" >> $GITHUB_ENV
          [[ ! -z ""$CI_SHA_PUSH"" ]] && echo ""CI_SHA=$CI_SHA_PUSH"" >> $GITHUB_ENV || echo ""CI_SHA=$CI_SHA_WORKFLOW_RUN"" >> $GITHUB_ENV

      - name: print environment variables
        run: |
          echo ""env.CI_BRANCH = ${{ env.CI_BRANCH }}""
          echo ""env.CI_SHA = ${{ env.CI_SHA }}""

      - uses: actions/checkout@v4
        # To avoid failure when multiple commits are merged into `main` in a short period of time.
        # Checking out to an old commit beyond the fetch depth will get an error `fatal: reference is not a tree: ...
        # (Only required for `workflow_run` event, where we get the latest HEAD on `main` instead of the event commit)
        with:
          fetch-depth: 20

      - name: Update clone using environment variables
        run: |
          echo ""original branch = $(git branch --show-current)""
          git fetch && git checkout ${{ env.CI_BRANCH }}
          echo ""updated branch = $(git branch --show-current)""
          git checkout ${{ env.CI_SHA }}
          echo ""log = $(git log -n 1)""

      - uses: actions/download-artifact@v4
      - name: Send message to Slack
        env:
          CI_SLACK_BOT_TOKEN: ${{ secrets.CI_SLACK_BOT_TOKEN }}
          CI_SLACK_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID }}
          CI_SLACK_CHANNEL_ID_DAILY: ${{ secrets.CI_SLACK_CHANNEL_ID_DAILY }}
          CI_SLACK_CHANNEL_DUMMY_TESTS: ${{ secrets.CI_SLACK_CHANNEL_DUMMY_TESTS }}
          CI_SLACK_REPORT_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID }}
          ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}
          CI_EVENT: push
          CI_TITLE_PUSH: ${{ github.event.head_commit.message }}
          CI_TITLE_WORKFLOW_RUN: ${{ github.event.workflow_run.head_commit.message }}
          CI_SHA: ${{ env.CI_SHA }}
          SETUP_STATUS: ${{ needs.setup.result }}

        # We pass `needs.setup.outputs.matrix` as the argument. A processing in `notification_service.py` to change
        # `models/bert` to `models_bert` is required, as the artifact names use `_` instead of `/`.
        run: |
          pip install huggingface_hub
          pip install slack_sdk
          pip show slack_sdk
          python utils/notification_service.py ""${{ needs.setup.outputs.matrix }}""
",652,6,3,"workflow_run, push, repository_dispatch",7
huggingface/transformers,self-scheduled-amd-caller.yml,"name: Self-hosted runner (AMD scheduled CI caller)

on:
  schedule:
    - cron: ""17 2 * * *""

jobs:
  run_scheduled_amd_ci:
    name: Trigger Scheduled AMD CI
    runs-on: ubuntu-22.04
    if: ${{ always() }}
    steps:
      - name: Trigger scheduled AMD CI via workflow_run
        run: echo ""Trigger scheduled AMD CI via workflow_run""
",14,1,1,schedule,0
huggingface/transformers,self-scheduled-amd-mi250-caller.yml,"name: Self-hosted runner (AMD mi250 scheduled CI caller)

on:
  workflow_run:
    workflows: [""Self-hosted runner (AMD scheduled CI caller)""]
    branches: [""main""]
    types: [completed]
  push:
    branches:
      - run_amd_scheduled_ci_caller*

jobs:
  model-ci:
    name: Model CI
    uses: huggingface/hf-workflows/.github/workflows/transformers_amd_ci_scheduled.yaml@main
    with:
      job: run_models_gpu
      slack_report_channel: ""#transformers-ci-daily-amd""
      runner: mi250
      docker: huggingface/transformers-pytorch-amd-gpu
      ci_event: Scheduled CI (AMD) - mi250
      report_repo_id: optimum-amd/transformers_daily_ci
    secrets: inherit

  torch-pipeline:
    name: Torch pipeline CI
    uses: huggingface/hf-workflows/.github/workflows/transformers_amd_ci_scheduled.yaml@main
    with:
      job: run_pipelines_torch_gpu
      slack_report_channel: ""#transformers-ci-daily-amd""
      runner: mi250
      docker: huggingface/transformers-pytorch-amd-gpu
      ci_event: Scheduled CI (AMD) - mi250
      report_repo_id: optimum-amd/transformers_daily_ci
    secrets: inherit

  example-ci:
    name: Example CI
    uses: huggingface/hf-workflows/.github/workflows/transformers_amd_ci_scheduled.yaml@main
    with:
      job: run_examples_gpu
      slack_report_channel: ""#transformers-ci-daily-amd""
      runner: mi250
      docker: huggingface/transformers-pytorch-amd-gpu
      ci_event: Scheduled CI (AMD) - mi250
      report_repo_id: optimum-amd/transformers_daily_ci
    secrets: inherit

  deepspeed-ci:
    name: DeepSpeed CI
    uses: huggingface/hf-workflows/.github/workflows/transformers_amd_ci_scheduled.yaml@main
    with:
      job: run_torch_cuda_extensions_gpu
      slack_report_channel: ""#transformers-ci-daily-amd""
      runner: mi250
      docker: huggingface/transformers-pytorch-deepspeed-amd-gpu
      ci_event: Scheduled CI (AMD) - mi250
      report_repo_id: optimum-amd/transformers_daily_ci
    secrets: inherit
",59,4,2,"workflow_run, push",4
huggingface/transformers,self-scheduled-amd-mi300-caller.yml,"name: Self-hosted runner scale set (AMD mi300 scheduled CI caller)

# Note: For every job in this workflow, the name of the runner scale set is finalized in the runner yaml i.e. huggingface/hf-workflows/.github/workflows/transformers_amd_ci_scheduled_arc_scale_set.yaml
# For example, 1gpu scale set: amd-mi300-ci-1gpu
#              2gpu scale set: amd-mi300-ci-2gpu

on:
  workflow_run:
    workflows: [""Self-hosted runner (AMD scheduled CI caller)""]
    branches: [""main""]
    types: [completed]
  push:
    branches:
      - run_amd_scheduled_ci_caller*

jobs:
  model-ci:
    name: Model CI
    uses: huggingface/hf-workflows/.github/workflows/transformers_amd_ci_scheduled_arc_scale_set.yaml@main
    with:
      job: run_models_gpu
      slack_report_channel: ""#amd-hf-ci""
      runner_scale_set: amd-mi300-ci
      docker: huggingface/transformers-pytorch-amd-gpu
      ci_event: Scheduled CI (AMD) - mi300
      report_repo_id: optimum-amd/transformers_daily_ci
    secrets: inherit

  torch-pipeline:
    name: Torch pipeline CI
    uses: huggingface/hf-workflows/.github/workflows/transformers_amd_ci_scheduled_arc_scale_set.yaml@main
    with:
      job: run_pipelines_torch_gpu
      slack_report_channel: ""#amd-hf-ci""
      runner_scale_set: amd-mi300-ci
      docker: huggingface/transformers-pytorch-amd-gpu
      ci_event: Scheduled CI (AMD) - mi300
      report_repo_id: optimum-amd/transformers_daily_ci
    secrets: inherit

  example-ci:
    name: Example CI
    uses: huggingface/hf-workflows/.github/workflows/transformers_amd_ci_scheduled_arc_scale_set.yaml@main
    with:
      job: run_examples_gpu
      slack_report_channel: ""#amd-hf-ci""
      runner_scale_set: amd-mi300-ci
      docker: huggingface/transformers-pytorch-amd-gpu
      ci_event: Scheduled CI (AMD) - mi300
      report_repo_id: optimum-amd/transformers_daily_ci
    secrets: inherit

  deepspeed-ci:
    name: DeepSpeed CI
    uses: huggingface/hf-workflows/.github/workflows/transformers_amd_ci_scheduled_arc_scale_set.yaml@main
    with:
      job: run_torch_cuda_extensions_gpu
      slack_report_channel: ""#amd-hf-ci""
      runner_scale_set: amd-mi300-ci
      docker: huggingface/transformers-pytorch-deepspeed-amd-gpu
      ci_event: Scheduled CI (AMD) - mi300
      report_repo_id: optimum-amd/transformers_daily_ci
    secrets: inherit
",63,4,2,"workflow_run, push",4
huggingface/transformers,self-scheduled-caller.yml,"name: Self-hosted runner (scheduled)


on:
  repository_dispatch:
  schedule:
    - cron: ""17 2 * * *""
  push:
    branches:
      - run_scheduled_ci*
  workflow_dispatch:
    inputs:
      prev_workflow_run_id:
        description: 'previous workflow run id to compare'
        type: string
        required: false
        default: """"
      other_workflow_run_id:
        description: 'other workflow run id to compare'
        type: string
        required: false
        default: """"


# Used for `push` to easily modify the target workflow runs to compare against
env:
    prev_workflow_run_id: """"
    other_workflow_run_id: """"


jobs:
  setup:
    name: Setup
    runs-on: ubuntu-22.04
    steps:
      - name: Setup
        run: |
          mkdir ""setup_values""
          echo ""${{ inputs.prev_workflow_run_id || env.prev_workflow_run_id }}"" > ""setup_values/prev_workflow_run_id.txt""
          echo ""${{ inputs.other_workflow_run_id || env.other_workflow_run_id }}"" > ""setup_values/other_workflow_run_id.txt""

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: setup_values
          path: setup_values

  model-ci:
    name: Model CI
    uses: ./.github/workflows/self-scheduled.yml
    with:
      job: run_models_gpu
      slack_report_channel: ""#transformers-ci-daily-models""
      docker: huggingface/transformers-all-latest-gpu
      ci_event: Daily CI
      report_repo_id: hf-internal-testing/transformers_daily_ci
    secrets: inherit

  torch-pipeline:
    name: Torch pipeline CI
    uses: ./.github/workflows/self-scheduled.yml
    with:
      job: run_pipelines_torch_gpu
      slack_report_channel: ""#transformers-ci-daily-pipeline-torch""
      docker: huggingface/transformers-pytorch-gpu
      ci_event: Daily CI
      report_repo_id: hf-internal-testing/transformers_daily_ci
    secrets: inherit

  example-ci:
    name: Example CI
    uses: ./.github/workflows/self-scheduled.yml
    with:
      job: run_examples_gpu
      slack_report_channel: ""#transformers-ci-daily-examples""
      docker: huggingface/transformers-all-latest-gpu
      ci_event: Daily CI
      report_repo_id: hf-internal-testing/transformers_daily_ci
    secrets: inherit

  trainer-fsdp-ci:
    name: Trainer/FSDP CI
    uses: ./.github/workflows/self-scheduled.yml
    with:
      job: run_trainer_and_fsdp_gpu
      slack_report_channel: ""#transformers-ci-daily-training""
      docker: huggingface/transformers-all-latest-gpu
      ci_event: Daily CI
      report_repo_id: hf-internal-testing/transformers_daily_ci
    secrets: inherit

  deepspeed-ci:
    name: DeepSpeed CI
    uses: ./.github/workflows/self-scheduled.yml
    with:
      job: run_torch_cuda_extensions_gpu
      slack_report_channel: ""#transformers-ci-daily-training""
      docker: huggingface/transformers-pytorch-deepspeed-latest-gpu
      ci_event: Daily CI
      working-directory-prefix: /workspace
      report_repo_id: hf-internal-testing/transformers_daily_ci
    secrets: inherit

  quantization-ci:
    name: Quantization CI
    uses: ./.github/workflows/self-scheduled.yml
    with:
      job: run_quantization_torch_gpu
      slack_report_channel: ""#transformers-ci-daily-quantization""
      docker: huggingface/transformers-quantization-latest-gpu
      ci_event: Daily CI
      report_repo_id: hf-internal-testing/transformers_daily_ci
    secrets: inherit
",113,7,4,"repository_dispatch, schedule, push, workflow_dispatch",7
huggingface/transformers,self-scheduled-intel-gaudi.yml,"name: Self-hosted runner (scheduled-intel-gaudi)

on:
  workflow_call:
    inputs:
      job:
        required: true
        type: string
      slack_report_channel:
        required: true
        type: string
      runner_scale_set:
        required: true
        type: string
      ci_event:
        required: true
        type: string
      report_repo_id:
        required: true
        type: string

env:
  NUM_SLICES: 2
  RUN_SLOW: yes
  PT_HPU_LAZY_MODE: 0
  TRANSFORMERS_IS_CI: yes
  PT_ENABLE_INT64_SUPPORT: 1
  HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}
  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}
  HF_HOME: /mnt/cache/.cache/huggingface

jobs:
  setup:
    if: contains(fromJSON('[""run_models_gpu"", ""run_trainer_and_fsdp_gpu""]'), inputs.job)
    name: Setup
    runs-on: ubuntu-latest
    outputs:
      slice_ids: ${{ steps.set-matrix.outputs.slice_ids }}
      folder_slices: ${{ steps.set-matrix.outputs.folder_slices }}
      quantization_matrix: ${{ steps.set-matrix.outputs.quantization_matrix }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ""3.10""

      - id: set-matrix
        if: contains(fromJSON('[""run_models_gpu"", ""run_trainer_and_fsdp_gpu""]'), inputs.job)
        name: Identify models to test
        working-directory: tests
        run: |
          if [ ""${{ inputs.job }}"" = ""run_models_gpu"" ]; then
            echo ""folder_slices=$(python3 ../utils/split_model_tests.py --num_splits ${{ env.NUM_SLICES }})"" >> $GITHUB_OUTPUT
            echo ""slice_ids=$(python3 -c 'd = list(range(${{ env.NUM_SLICES }})); print(d)')"" >> $GITHUB_OUTPUT
          elif [ ""${{ inputs.job }}"" = ""run_trainer_and_fsdp_gpu"" ]; then
            echo ""folder_slices=[['trainer'], ['fsdp']]"" >> $GITHUB_OUTPUT
            echo ""slice_ids=[0, 1]"" >> $GITHUB_OUTPUT
          fi

      - id: set-matrix-quantization
        if: ${{ inputs.job == 'run_quantization_torch_gpu' }}
        name: Identify quantization method to test
        working-directory: tests
        run: |
          echo ""quantization_matrix=$(python3 -c 'import os; tests = os.getcwd(); quantization_tests = os.listdir(os.path.join(tests, ""quantization"")); d = sorted(list(filter(os.path.isdir, [f""quantization/{x}"" for x in quantization_tests]))) ;  print(d)')"" >> $GITHUB_OUTPUT

  run_models_gpu:
    if: ${{ inputs.job == 'run_models_gpu' }}
    name: "" ""
    needs: setup
    strategy:
      fail-fast: false
      matrix:
        machine_type: [1gaudi, 2gaudi]
        slice_id: ${{ fromJSON(needs.setup.outputs.slice_ids) }}
    uses: ./.github/workflows/model_jobs_intel_gaudi.yml
    with:
      slice_id: ${{ matrix.slice_id }}
      machine_type: ${{ matrix.machine_type }}
      folder_slices: ${{ needs.setup.outputs.folder_slices }}
      runner: ${{ inputs.runner_scale_set }}-${{ matrix.machine_type }}
      report_name_prefix: run_models_gpu

    secrets: inherit

  run_trainer_and_fsdp_gpu:
    if: ${{ inputs.job == 'run_trainer_and_fsdp_gpu' }}
    name: "" ""
    needs: setup
    strategy:
      fail-fast: false
      matrix:
        machine_type: [1gaudi, 2gaudi]
        slice_id: ${{ fromJSON(needs.setup.outputs.slice_ids) }}
    uses: ./.github/workflows/model_jobs_intel_gaudi.yml
    with:
      slice_id: ${{ matrix.slice_id }}
      machine_type: ${{ matrix.machine_type }}
      folder_slices: ${{ needs.setup.outputs.folder_slices }}
      runner: ${{ inputs.runner_scale_set }}-${{ matrix.machine_type }}
      report_name_prefix: run_trainer_and_fsdp_gpu

    secrets: inherit

  run_pipelines_gpu:
    if: ${{ inputs.job == 'run_pipelines_gpu' }}
    name: Pipelines
    strategy:
      fail-fast: false
      matrix:
        machine_type: [1gaudi, 2gaudi]
    runs-on:
      group: ${{ inputs.runner_scale_set }}-${{ matrix.machine_type }}
    container:
      image: vault.habana.ai/gaudi-docker/1.21.1/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:latest
      options: --runtime=habana
        -v /mnt/cache/.cache/huggingface:/mnt/cache/.cache/huggingface
        --env OMPI_MCA_btl_vader_single_copy_mechanism=none
        --env HABANA_VISIBLE_DEVICES
        --env HABANA_VISIBLE_MODULES
        --cap-add=sys_nice
        --shm-size=64G
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install dependencies
        run: |
          pip install -e .[testing,torch] ""numpy<2.0.0"" scipy scikit-learn librosa soundfile

      - name: HL-SMI
        run: |
          hl-smi
          echo ""HABANA_VISIBLE_DEVICES=${HABANA_VISIBLE_DEVICES}""
          echo ""HABANA_VISIBLE_MODULES=${HABANA_VISIBLE_MODULES}""

      - name: Environment
        run: python3 utils/print_env.py

      - name: Show installed libraries and their versions
        run: pip freeze

      - name: Set `machine_type` for report and artifact names
        shell: bash
        run: |
          if [ ""${{ matrix.machine_type }}"" = ""1gaudi"" ]; then
            machine_type=single-gpu
          elif [ ""${{ matrix.machine_type }}"" = ""2gaudi"" ]; then
            machine_type=multi-gpu
          else
            machine_type=${{ matrix.machine_type }}
          fi
          echo ""machine_type=$machine_type"" >> $GITHUB_ENV

      - name: Run all pipeline tests on Intel Gaudi
        run: |
          python3 -m pytest -v --make-reports=${{ env.machine_type }}_run_pipelines_gpu_test_reports tests/pipelines -m ""not not_device_test""

      - name: Failure short reports
        if: ${{ failure() }}
        continue-on-error: true
        run: |
          cat reports/${{ env.machine_type }}_run_pipelines_gpu_test_reports/failures_short.txt

      - name: ""Test suite reports artifacts: ${{ env.machine_type }}_run_pipelines_gpu_test_reports""
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.machine_type }}_run_pipelines_gpu_test_reports
          path: reports/${{ env.machine_type }}_run_pipelines_gpu_test_reports

  run_examples_gpu:
    if: ${{ inputs.job == 'run_examples_gpu' }}
    name: Examples directory
    strategy:
      fail-fast: false
      matrix:
        machine_type: [1gaudi]
    runs-on:
      group: ${{ inputs.runner_scale_set }}-${{ matrix.machine_type }}
    container:
      image: vault.habana.ai/gaudi-docker/1.21.1/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:latest
      options: --runtime=habana
        -v /mnt/cache/.cache/huggingface:/mnt/cache/.cache/huggingface
        --env OMPI_MCA_btl_vader_single_copy_mechanism=none
        --env HABANA_VISIBLE_DEVICES
        --env HABANA_VISIBLE_MODULES
        --cap-add=sys_nice
        --shm-size=64G
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install dependencies
        run: |
          pip install -e .[testing,torch] ""numpy<2.0.0"" scipy scikit-learn librosa soundfile

      - name: HL-SMI
        run: |
          hl-smi
          echo ""HABANA_VISIBLE_DEVICES=${HABANA_VISIBLE_DEVICES}""
          echo ""HABANA_VISIBLE_MODULES=${HABANA_VISIBLE_MODULES}""

      - name: Environment
        run: |
          python3 utils/print_env.py

      - name: Show installed libraries and their versions
        run: |
          pip freeze

      - name: Set `machine_type` for report and artifact names
        shell: bash
        run: |
          if [ ""${{ matrix.machine_type }}"" = ""1gaudi"" ]; then
            machine_type=single-gpu
          elif [ ""${{ matrix.machine_type }}"" = ""2gaudi"" ]; then
            machine_type=multi-gpu
          else
            machine_type=${{ matrix.machine_type }}
          fi
          echo ""machine_type=$machine_type"" >> $GITHUB_ENV

      - name: Run examples tests on Intel Gaudi
        run: |
          pip install -r examples/pytorch/_tests_requirements.txt
          python3 -m pytest -v --make-reports=${{ env.machine_type }}_run_examples_gpu_test_reports examples/pytorch -m ""not not_device_test""

      - name: Failure short reports
        if: ${{ failure() }}
        continue-on-error: true
        run: |
          cat reports/${{ env.machine_type }}_run_examples_gpu_test_reports/failures_short.txt

      - name: ""Test suite reports artifacts: ${{ env.machine_type }}_run_examples_gpu_test_reports""
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.machine_type }}_run_examples_gpu_test_reports
          path: reports/${{ env.machine_type }}_run_examples_gpu_test_reports

  run_deepspeed_gpu:
    if: ${{ inputs.job == 'run_deepspeed_gpu' }}
    name: Intel Gaudi deepspeed tests
    strategy:
      fail-fast: false
      matrix:
        machine_type: [1gaudi, 2gaudi]
    runs-on:
      group: ${{ inputs.runner_scale_set }}-${{ matrix.machine_type }}
    container:
      image: vault.habana.ai/gaudi-docker/1.21.1/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:latest
      options: --runtime=habana
        -v /mnt/cache/.cache/huggingface:/mnt/cache/.cache/huggingface
        --env OMPI_MCA_btl_vader_single_copy_mechanism=none
        --env HABANA_VISIBLE_DEVICES
        --env HABANA_VISIBLE_MODULES
        --cap-add=sys_nice
        --shm-size=64G
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install dependencies
        run: |
          pip install -e .[testing,torch] ""numpy<2.0.0"" scipy scikit-learn librosa soundfile
          pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.20.0

      - name: HL-SMI
        run: |
          hl-smi
          echo ""HABANA_VISIBLE_DEVICES=${HABANA_VISIBLE_DEVICES}""
          echo ""HABANA_VISIBLE_MODULES=${HABANA_VISIBLE_MODULES}""

      - name: Environment
        run: |
          python3 utils/print_env.py

      - name: Show installed libraries and their versions
        run: |
          pip freeze

      - name: Set `machine_type` for report and artifact names
        shell: bash
        run: |
          if [ ""${{ matrix.machine_type }}"" = ""1gaudi"" ]; then
            machine_type=single-gpu
          elif [ ""${{ matrix.machine_type }}"" = ""2gaudi"" ]; then
            machine_type=multi-gpu
          else
            machine_type=${{ matrix.machine_type }}
          fi
          echo ""machine_type=$machine_type"" >> $GITHUB_ENV

      - name: Run all deepspeed tests on intel Gaudi
        run: |
          python3 -m pytest -v --make-reports=${{ env.machine_type }}_run_deepspeed_gpu_test_reports tests/deepspeed -m ""not not_device_test""

      - name: Failure short reports
        if: ${{ failure() }}
        continue-on-error: true
        run: |
          cat reports/${{ env.machine_type }}_run_deepspeed_gpu_test_reports/failures_short.txt

      - name: ""Test suite reports artifacts: ${{ env.machine_type }}_run_deepspeed_gpu_test_reports""
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.machine_type }}_run_deepspeed_gpu_test_reports
          path: reports/${{ env.machine_type }}_run_deepspeed_gpu_test_reports

  send_results:
    name: Slack Report
    needs:
      [
        setup,
        run_models_gpu,
        run_examples_gpu,
        run_pipelines_gpu,
        run_deepspeed_gpu,
        run_trainer_and_fsdp_gpu,
      ]
    if: ${{ always() }}
    uses: ./.github/workflows/slack-report.yml
    with:
      job: ${{ inputs.job }}
      setup_status: ${{ needs.setup.result }}
      slack_report_channel: ${{ inputs.slack_report_channel }}
      quantization_matrix: ${{ needs.setup.outputs.quantization_matrix }}
      folder_slices: ${{ needs.setup.outputs.folder_slices }}
      report_repo_id: ${{ inputs.report_repo_id }}
      ci_event: ${{ inputs.ci_event }}

    secrets: inherit
",345,7,1,workflow_call,11
huggingface/transformers,self-scheduled-intel-gaudi3-caller.yml,"name: Self-hosted runner (Intel Gaudi3 scheduled CI caller)

on:
  repository_dispatch:
  workflow_dispatch:
  schedule:
    - cron: ""17 2 * * *""

jobs:
  model-ci:
    name: Model CI
    uses: ./.github/workflows/self-scheduled-intel-gaudi.yml
    with:
      job: run_models_gpu
      ci_event: Scheduled CI (Intel) - Gaudi3
      runner_scale_set: itac-bm-emr-gaudi3-dell
      slack_report_channel: ""#transformers-ci-daily-intel-gaudi3""
      report_repo_id: optimum-intel/transformers_daily_ci_intel_gaudi3

    secrets: inherit

  pipeline-ci:
    name: Pipeline CI
    uses: ./.github/workflows/self-scheduled-intel-gaudi.yml
    with:
      job: run_pipelines_gpu
      ci_event: Scheduled CI (Intel) - Gaudi3
      runner_scale_set: itac-bm-emr-gaudi3-dell
      slack_report_channel: ""#transformers-ci-daily-intel-gaudi3""
      report_repo_id: optimum-intel/transformers_daily_ci_intel_gaudi3

    secrets: inherit

  example-ci:
    name: Example CI
    uses: ./.github/workflows/self-scheduled-intel-gaudi.yml
    with:
      job: run_examples_gpu
      ci_event: Scheduled CI (Intel) - Gaudi3
      runner_scale_set: itac-bm-emr-gaudi3-dell
      slack_report_channel: ""#transformers-ci-daily-intel-gaudi3""
      report_repo_id: optimum-intel/transformers_daily_ci_intel_gaudi3

    secrets: inherit

  deepspeed-ci:
    name: DeepSpeed CI
    uses: ./.github/workflows/self-scheduled-intel-gaudi.yml
    with:
      job: run_deepspeed_gpu
      ci_event: Scheduled CI (Intel) - Gaudi3
      runner_scale_set: itac-bm-emr-gaudi3-dell
      slack_report_channel: ""#transformers-ci-daily-intel-gaudi3""
      report_repo_id: optimum-intel/transformers_daily_ci_intel_gaudi3

    secrets: inherit

  trainer-fsdp-ci:
    name: Trainer/FSDP CI
    uses: ./.github/workflows/self-scheduled-intel-gaudi.yml
    with:
      job: run_trainer_and_fsdp_gpu
      ci_event: Scheduled CI (Intel) - Gaudi3
      runner_scale_set: itac-bm-emr-gaudi3-dell
      slack_report_channel: ""#transformers-ci-daily-intel-gaudi3""
      report_repo_id: optimum-intel/transformers_daily_ci_intel_gaudi3
    secrets: inherit
",67,5,3,"repository_dispatch, workflow_dispatch, schedule",5
huggingface/transformers,self-scheduled.yml,"name: Self-hosted runner (scheduled)

# Note that each job's dependencies go into a corresponding docker file.
#
# For example for `run_torch_cuda_extensions_gpu` the docker image is
# `huggingface/transformers-pytorch-deepspeed-latest-gpu`, which can be found at
# `docker/transformers-pytorch-deepspeed-latest-gpu/Dockerfile`

on:
  workflow_call:
    inputs:
      job:
        required: true
        type: string
      slack_report_channel:
        required: true
        type: string
      docker:
        required: true
        type: string
      ci_event:
        required: true
        type: string
      working-directory-prefix:
        default: ''
        required: false
        type: string
      report_repo_id:
        required: true
        type: string


env:
  HF_HOME: /mnt/cache
  TRANSFORMERS_IS_CI: yes
  OMP_NUM_THREADS: 8
  MKL_NUM_THREADS: 8
  RUN_SLOW: yes
  # For gated repositories, we still need to agree to share information on the Hub repo. page in order to get access.
  # This token is created under the bot `hf-transformers-bot`.
  HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}
  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}
  TF_FORCE_GPU_ALLOW_GROWTH: true
  CUDA_VISIBLE_DEVICES: 0,1
  NUM_SLICES: 2

jobs:
  setup:
    if: contains(fromJSON('[""run_models_gpu"", ""run_trainer_and_fsdp_gpu"", ""run_quantization_torch_gpu""]'), inputs.job)
    name: Setup
    strategy:
      matrix:
        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]
    runs-on:
      group: '${{ matrix.machine_type }}'
    container:
      image: huggingface/transformers-all-latest-gpu
      options: --gpus 0 --shm-size ""16gb"" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    outputs:
      folder_slices: ${{ steps.set-matrix.outputs.folder_slices }}
      slice_ids: ${{ steps.set-matrix.outputs.slice_ids }}
      runner_map: ${{ steps.set-matrix.outputs.runner_map }}
      quantization_matrix: ${{ steps.set-matrix-quantization.outputs.quantization_matrix }}
    steps:
      - name: Update clone
        working-directory: /transformers
        run: |
          git fetch && git checkout ${{ github.sha }}

      - name: Cleanup
        working-directory: /transformers
        run: |
          rm -rf tests/__pycache__
          rm -rf tests/models/__pycache__
          rm -rf reports

      - name: Show installed libraries and their versions
        working-directory: /transformers
        run: pip freeze

      - id: set-matrix
        if: contains(fromJSON('[""run_models_gpu"", ""run_trainer_and_fsdp_gpu""]'), inputs.job)
        name: Identify models to test
        working-directory: /transformers/tests
        run: |
          if [ ""${{ inputs.job }}"" = ""run_models_gpu"" ]; then
            echo ""folder_slices=$(python3 ../utils/split_model_tests.py --num_splits ${{ env.NUM_SLICES }})"" >> $GITHUB_OUTPUT
            echo ""slice_ids=$(python3 -c 'd = list(range(${{ env.NUM_SLICES }})); print(d)')"" >> $GITHUB_OUTPUT
            echo ""runner_map=$(python3 ../utils/get_runner_map.py)"" >> $GITHUB_OUTPUT
          elif [ ""${{ inputs.job }}"" = ""run_trainer_and_fsdp_gpu"" ]; then
            echo ""folder_slices=[['trainer'], ['fsdp']]"" >> $GITHUB_OUTPUT
            echo ""slice_ids=[0, 1]"" >> $GITHUB_OUTPUT
          fi

      - id: set-matrix-quantization
        if: ${{ inputs.job == 'run_quantization_torch_gpu' }}
        name: Identify quantization method to test
        working-directory: /transformers/tests
        run: |
          echo ""quantization_matrix=$(python3 -c 'import os; tests = os.getcwd(); quantization_tests = os.listdir(os.path.join(tests, ""quantization"")); d = sorted(list(filter(os.path.isdir, [f""quantization/{x}"" for x in quantization_tests]))) ;  print(d)')"" >> $GITHUB_OUTPUT

      - name: NVIDIA-SMI
        run: |
          nvidia-smi

  run_models_gpu:
    if: ${{ inputs.job == 'run_models_gpu' }}
    name: "" ""
    needs: setup
    strategy:
      fail-fast: false
      matrix:
        machine_type: [single-gpu, multi-gpu]
        slice_id: ${{ fromJSON(needs.setup.outputs.slice_ids) }}
    uses: ./.github/workflows/model_jobs.yml
    with:
      folder_slices: ${{ needs.setup.outputs.folder_slices }}
      machine_type: ${{ matrix.machine_type }}
      slice_id: ${{ matrix.slice_id }}
      runner_map: ${{ needs.setup.outputs.runner_map }}
      docker: ${{ inputs.docker }}
    secrets: inherit

  run_trainer_and_fsdp_gpu:
    if: ${{ inputs.job == 'run_trainer_and_fsdp_gpu' }}
    name: "" ""
    needs: setup
    strategy:
      fail-fast: false
      matrix:
        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]
        slice_id: [0, 1]
    uses: ./.github/workflows/model_jobs.yml
    with:
      folder_slices: ${{ needs.setup.outputs.folder_slices }}
      machine_type: ${{ matrix.machine_type }}
      slice_id: ${{ matrix.slice_id }}
      docker: ${{ inputs.docker }}
      report_name_prefix: run_trainer_and_fsdp_gpu
    secrets: inherit

  run_pipelines_torch_gpu:
    if: ${{ inputs.job == 'run_pipelines_torch_gpu' }}
    name: PyTorch pipelines
    strategy:
      fail-fast: false
      matrix:
        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]
    runs-on:
      group: '${{ matrix.machine_type }}'
    container:
      image: huggingface/transformers-pytorch-gpu
      options: --gpus all --shm-size ""16gb"" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    steps:
      - name: Update clone
        working-directory: /transformers
        run: git fetch && git checkout ${{ github.sha }}

      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)
        working-directory: /transformers
        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .

      - name: NVIDIA-SMI
        run: |
          nvidia-smi

      - name: Environment
        working-directory: /transformers
        run: |
          python3 utils/print_env.py

      - name: Show installed libraries and their versions
        working-directory: /transformers
        run: pip freeze

      - name: Set `machine_type` for report and artifact names
        working-directory: /transformers
        shell: bash
        run: |
          echo ""${{ matrix.machine_type }}""

          if [ ""${{ matrix.machine_type }}"" = ""aws-g4dn-4xlarge-cache"" ]; then
            machine_type=single-gpu
          elif [ ""${{ matrix.machine_type }}"" = ""aws-g4dn-12xlarge-cache"" ]; then
            machine_type=multi-gpu
          else
            machine_type=${{ matrix.machine_type }}
          fi

          echo ""$machine_type""
          echo ""machine_type=$machine_type"" >> $GITHUB_ENV

      - name: Run all pipeline tests on GPU
        working-directory: /transformers
        run: |
          python3 -m pytest -n 1 -v --dist=loadfile --make-reports=${{ env.machine_type }}_run_pipelines_torch_gpu_test_reports tests/pipelines

      - name: Failure short reports
        if: ${{ failure() }}
        continue-on-error: true
        run: cat /transformers/reports/${{ env.machine_type }}_run_pipelines_torch_gpu_test_reports/failures_short.txt

      - name: ""Test suite reports artifacts: ${{ env.machine_type }}_run_pipelines_torch_gpu_test_reports""
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.machine_type }}_run_pipelines_torch_gpu_test_reports
          path: /transformers/reports/${{ env.machine_type }}_run_pipelines_torch_gpu_test_reports

  run_examples_gpu:
    if: ${{ inputs.job == 'run_examples_gpu' }}
    name: Examples directory
    strategy:
      fail-fast: false
      matrix:
        machine_type: [aws-g4dn-4xlarge-cache]
    runs-on:
      group: '${{ matrix.machine_type }}'
    container:
      image: huggingface/transformers-all-latest-gpu
      options: --gpus 0 --shm-size ""16gb"" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    steps:
      - name: Update clone
        working-directory: /transformers
        run: git fetch && git checkout ${{ github.sha }}

      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)
        working-directory: /transformers
        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .

      - name: NVIDIA-SMI
        run: |
          nvidia-smi

      - name: Environment
        working-directory: /transformers
        run: |
          python3 utils/print_env.py

      - name: Show installed libraries and their versions
        working-directory: /transformers
        run: pip freeze

      - name: Set `machine_type` for report and artifact names
        working-directory: /transformers
        shell: bash
        run: |
          echo ""${{ matrix.machine_type }}""

          if [ ""${{ matrix.machine_type }}"" = ""aws-g4dn-4xlarge-cache"" ]; then
            machine_type=single-gpu
          elif [ ""${{ matrix.machine_type }}"" = ""aws-g4dn-12xlarge-cache"" ]; then
            machine_type=multi-gpu
          else
            machine_type=${{ matrix.machine_type }}
          fi

          echo ""$machine_type""
          echo ""machine_type=$machine_type"" >> $GITHUB_ENV

      - name: Run examples tests on GPU
        working-directory: /transformers
        run: |
          pip install -r examples/pytorch/_tests_requirements.txt
          python3 -m pytest -v --make-reports=${{ env.machine_type }}_run_examples_gpu_test_reports examples/pytorch

      - name: Failure short reports
        if: ${{ failure() }}
        continue-on-error: true
        run: cat /transformers/reports/${{ env.machine_type }}_run_examples_gpu_test_reports/failures_short.txt

      - name: ""Test suite reports artifacts: ${{ env.machine_type }}_run_examples_gpu_test_reports""
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.machine_type }}_run_examples_gpu_test_reports
          path: /transformers/reports/${{ env.machine_type }}_run_examples_gpu_test_reports

  run_torch_cuda_extensions_gpu:
    if: ${{ inputs.job == 'run_torch_cuda_extensions_gpu' }}
    name: Torch CUDA extension tests
    strategy:
      fail-fast: false
      matrix:
        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]
    runs-on:
      group: '${{ matrix.machine_type }}'
    container:
      image: ${{ inputs.docker }}
      options: --gpus all --shm-size ""16gb"" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    steps:
      - name: Update clone
        working-directory: ${{ inputs.working-directory-prefix }}/transformers
        run: git fetch && git checkout ${{ github.sha }}

      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)
        working-directory: ${{ inputs.working-directory-prefix }}/transformers
        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .

      - name: Update / Install some packages (for Past CI)
        if: ${{ contains(inputs.docker, '-past-') && contains(inputs.docker, '-pytorch-') }}
        working-directory: ${{ inputs.working-directory-prefix }}/transformers
        run: |
          python3 -m pip install -U datasets
          python3 -m pip install --no-cache-dir git+https://github.com/huggingface/accelerate@main#egg=accelerate

      - name: Remove cached torch extensions
        run: rm -rf /github/home/.cache/torch_extensions/

      # To avoid unknown test failures
      - name: Pre build DeepSpeed *again* (for daily CI)
        if: ${{ contains(inputs.ci_event, 'Daily CI') }}
        working-directory: ${{ inputs.working-directory-prefix }}/
        run: |
          python3 -m pip uninstall -y deepspeed
          DS_DISABLE_NINJA=1 DS_BUILD_CPU_ADAM=1 DS_BUILD_FUSED_ADAM=1 python3 -m pip install deepspeed --global-option=""build_ext"" --global-option=""-j8"" --no-cache -v --disable-pip-version-check

      # To avoid unknown test failures
      - name: Pre build DeepSpeed *again* (for nightly & Past CI)
        if: ${{ contains(inputs.ci_event, 'Nightly CI') || contains(inputs.ci_event, 'Past CI') }}
        working-directory: ${{ inputs.working-directory-prefix }}/
        run: |
          python3 -m pip uninstall -y deepspeed
          rm -rf DeepSpeed
          git clone https://github.com/deepspeedai/DeepSpeed && cd DeepSpeed && rm -rf build
          DS_BUILD_CPU_ADAM=1 DS_BUILD_FUSED_ADAM=1 python3 -m pip install . --global-option=""build_ext"" --global-option=""-j8"" --no-cache -v --disable-pip-version-check

      - name: NVIDIA-SMI
        run: |
          nvidia-smi

      - name: Environment
        working-directory: ${{ inputs.working-directory-prefix }}/transformers
        run: |
          python3 utils/print_env.py

      - name: Show installed libraries and their versions
        working-directory: ${{ inputs.working-directory-prefix }}/transformers
        run: pip freeze

      - name: Set `machine_type` for report and artifact names
        working-directory: ${{ inputs.working-directory-prefix }}/transformers
        shell: bash
        run: |
          echo ""${{ matrix.machine_type }}""

          if [ ""${{ matrix.machine_type }}"" = ""aws-g4dn-4xlarge-cache"" ]; then
            machine_type=single-gpu
          elif [ ""${{ matrix.machine_type }}"" = ""aws-g4dn-12xlarge-cache"" ]; then
            machine_type=multi-gpu
          else
            machine_type=${{ matrix.machine_type }}
          fi

          echo ""$machine_type""
          echo ""machine_type=$machine_type"" >> $GITHUB_ENV

      - name: Run all tests on GPU
        working-directory: ${{ inputs.working-directory-prefix }}/transformers
        run: |
          python3 -m pytest -v --make-reports=${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports tests/deepspeed tests/extended

      - name: Failure short reports
        if: ${{ failure() }}
        continue-on-error: true
        run: cat ${{ inputs.working-directory-prefix }}/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports/failures_short.txt

      - name: ""Test suite reports artifacts: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports""
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports
          path: ${{ inputs.working-directory-prefix }}/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports

  run_quantization_torch_gpu:
    if: ${{ inputs.job == 'run_quantization_torch_gpu' }}
    name: "" ""
    needs: setup
    strategy:
      max-parallel: 4
      fail-fast: false
      matrix:
        folders: ${{ fromJson(needs.setup.outputs.quantization_matrix) }}
        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]
    runs-on:
      group: '${{ matrix.machine_type }}'
    container:
      image: huggingface/transformers-quantization-latest-gpu
      options: --gpus all --shm-size ""16gb"" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    steps:
      - name: Echo folder ${{ matrix.folders }}
        shell: bash
        run: |
          echo ""${{ matrix.folders }}""
          matrix_folders=${{ matrix.folders }}
          matrix_folders=${matrix_folders/'quantization/'/'quantization_'}
          echo ""$matrix_folders""
          echo ""matrix_folders=$matrix_folders"" >> $GITHUB_ENV

      - name: Update clone
        working-directory: /transformers
        run: git fetch && git checkout ${{ github.sha }}

      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)
        working-directory: /transformers
        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .

      - name: NVIDIA-SMI
        run: |
          nvidia-smi

      - name: Environment
        working-directory: /transformers
        run: |
          python3 utils/print_env.py

      - name: Show installed libraries and their versions
        working-directory: /transformers
        run: pip freeze

      - name: Set `machine_type` for report and artifact names
        working-directory: /transformers
        shell: bash
        run: |
          echo ""${{ matrix.machine_type }}""

          if [ ""${{ matrix.machine_type }}"" = ""aws-g4dn-4xlarge-cache"" ]; then
            machine_type=single-gpu
          elif [ ""${{ matrix.machine_type }}"" = ""aws-g4dn-12xlarge-cache"" ]; then
            machine_type=multi-gpu
          else
            machine_type=${{ matrix.machine_type }}
          fi

          echo ""$machine_type""
          echo ""machine_type=$machine_type"" >> $GITHUB_ENV

      - name: Run quantization tests on GPU
        working-directory: /transformers
        run: |
          python3 -m pytest -v --make-reports=${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}

      - name: Failure short reports
        if: ${{ failure() }}
        continue-on-error: true
        run: cat /transformers/reports/${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports/failures_short.txt

      - name: ""Test suite reports artifacts: ${{ env.machine_type }}_run_quantization_torch_gpu_${{ env.matrix_folders }}_test_reports""
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.machine_type }}_run_quantization_torch_gpu_${{ env.matrix_folders }}_test_reports
          path: /transformers/reports/${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports

  run_extract_warnings:
    # Let's only do this for the job `run_models_gpu` to simplify the (already complex) logic.
    if: ${{ always() && inputs.job == 'run_models_gpu' }}
    name: Extract warnings in CI artifacts
    runs-on: ubuntu-22.04
    needs: [setup, run_models_gpu]
    steps:
      - name: Checkout transformers
        uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - name: Install transformers
        run: pip install transformers

      - name: Show installed libraries and their versions
        run: pip freeze

      - name: Create output directory
        run: mkdir warnings_in_ci

      - uses: actions/download-artifact@v4
        with:
          path: warnings_in_ci

      - name: Show artifacts
        run: echo ""$(python3 -c 'import os; d = os.listdir(); print(d)')""
        working-directory: warnings_in_ci

      - name: Extract warnings in CI artifacts
        run: |
          python3 utils/extract_warnings.py --workflow_run_id ${{ github.run_id }} --output_dir warnings_in_ci --token ${{ secrets.ACCESS_REPO_INFO_TOKEN }} --from_gh
          echo ""$(python3 -c 'import os; import json; fp = open(""warnings_in_ci/selected_warnings.json""); d = json.load(fp); d = ""\n"".join(d) ;print(d)')""

      - name: Upload artifact
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: warnings_in_ci
          path: warnings_in_ci/selected_warnings.json

  send_results:
    name: Slack Report
    needs: [
      setup,
      run_models_gpu,
      run_trainer_and_fsdp_gpu,
      run_pipelines_torch_gpu,
      run_examples_gpu,
      run_torch_cuda_extensions_gpu,
      run_quantization_torch_gpu,
      run_extract_warnings
    ]
    if: ${{ always() }}
    uses: ./.github/workflows/slack-report.yml
    with:
      job: ${{ inputs.job }}
      # This would be `skipped` if `setup` is skipped.
      setup_status: ${{ needs.setup.result }}
      slack_report_channel: ${{ inputs.slack_report_channel }}
      # This would be an empty string if `setup` is skipped.
      folder_slices: ${{ needs.setup.outputs.folder_slices }}
      quantization_matrix: ${{ needs.setup.outputs.quantization_matrix }}
      ci_event: ${{ inputs.ci_event }}
      report_repo_id: ${{ inputs.report_repo_id }}

    secrets: inherit

  check_new_failures:
    if: ${{ always() && inputs.ci_event == 'Daily CI' && needs.send_results.result == 'success' }}
    name: Check new failures
    needs: send_results
    uses: ./.github/workflows/check_failed_tests.yml
    with:
      docker: ${{ inputs.docker }}
      start_sha: ${{ github.sha }}
      job: ${{ inputs.job }}
      slack_report_channel: ${{ inputs.slack_report_channel }}
      ci_event: ${{ inputs.ci_event }}
      report_repo_id: ${{ inputs.report_repo_id }}

    secrets: inherit
",536,10,1,workflow_call,11
huggingface/transformers,slack-report.yml,"name: CI slack report

on:
  workflow_call:
    inputs:
      job:
        required: true
        type: string
      slack_report_channel:
        required: true
        type: string
      setup_status:
        required: true
        type: string
      folder_slices:
        required: true
        type: string
      quantization_matrix:
        required: true
        type: string
      ci_event:
        required: true
        type: string
      report_repo_id:
        required: true
        type: string

env:
  TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN: ${{ secrets.TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN }}

jobs:
  send_results:
    name: Send results to webhook
    runs-on: ubuntu-22.04
    if: always()
    steps:
      - name: Preliminary job status
        shell: bash
        # For the meaning of these environment variables, see the job `Setup`
        run: |
          echo ""Setup status: ${{ inputs.setup_status }}""

      - uses: actions/checkout@v4
      - uses: actions/download-artifact@v4

      - name: Prepare some setup values
        run: |
          if [ -f setup_values/prev_workflow_run_id.txt ]; then
            echo ""PREV_WORKFLOW_RUN_ID=$(cat setup_values/prev_workflow_run_id.txt)"" >> $GITHUB_ENV
          else
            echo ""PREV_WORKFLOW_RUN_ID="" >> $GITHUB_ENV
          fi

          if [ -f setup_values/other_workflow_run_id.txt ]; then
            echo ""OTHER_WORKFLOW_RUN_ID=$(cat setup_values/other_workflow_run_id.txt)"" >> $GITHUB_ENV
          else
            echo ""OTHER_WORKFLOW_RUN_ID="" >> $GITHUB_ENV
          fi

      - name: Send message to Slack
        shell: bash
        env:
          CI_SLACK_BOT_TOKEN: ${{ secrets.CI_SLACK_BOT_TOKEN }}
          CI_SLACK_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID }}
          CI_SLACK_CHANNEL_ID_DAILY: ${{ secrets.CI_SLACK_CHANNEL_ID_DAILY }}
          CI_SLACK_CHANNEL_DUMMY_TESTS: ${{ secrets.CI_SLACK_CHANNEL_DUMMY_TESTS }}
          SLACK_REPORT_CHANNEL: ${{ inputs.slack_report_channel }}
          ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}
          CI_EVENT: ${{ inputs.ci_event }}
          CI_SHA: ${{ github.sha }}
          CI_TEST_JOB: ${{ inputs.job }}
          SETUP_STATUS: ${{ inputs.setup_status }}
          REPORT_REPO_ID: ${{ inputs.report_repo_id }}
        # We pass `needs.setup.outputs.matrix` as the argument. A processing in `notification_service.py` to change
        # `models/bert` to `models_bert` is required, as the artifact names use `_` instead of `/`.
        # For a job that doesn't depend on (i.e. `needs`) `setup`, the value for `inputs.folder_slices` would be an
        # empty string, and the called script still get one argument (which is the emtpy string).
        run: |
          pip install huggingface_hub
          pip install slack_sdk
          pip show slack_sdk
          if [ ""${{ inputs.quantization_matrix }}"" != """" ]; then
            python utils/notification_service.py ""${{ inputs.quantization_matrix }}""
          else
            python utils/notification_service.py ""${{ inputs.folder_slices }}""
          fi          

      # Upload complete failure tables, as they might be big and only truncated versions could be sent to Slack.
      - name: Failure table artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ci_results_${{ inputs.job }}
          path: ci_results_${{ inputs.job }}
",93,1,1,workflow_call,3
huggingface/transformers,ssh-runner.yml,"name: SSH into our runners

on:
  workflow_dispatch:
    inputs:
      runner_type:
        description: 'Type of runner to test (a10 or t4)'
        required: true
      docker_image:
        description: 'Name of the Docker image'
        required: true
      num_gpus:
        description: 'Type of the number of gpus to use (`single` or `multi`)'
        required: true

env:
  HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}
  HF_HOME: /mnt/cache
  TRANSFORMERS_IS_CI: yes
  OMP_NUM_THREADS: 8
  MKL_NUM_THREADS: 8
  RUN_SLOW: yes # For gated repositories, we still need to agree to share information on the Hub repo. page in order to get access. # This token is created under the bot `hf-transformers-bot`.
  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}
  TF_FORCE_GPU_ALLOW_GROWTH: true
  CUDA_VISIBLE_DEVICES: 0,1

jobs:
  get_runner:
    name: ""Get runner to use""
    runs-on: ubuntu-22.04
    outputs:
      RUNNER: ${{ steps.set_runner.outputs.RUNNER }}
    steps:
      - name: Get runner to use
        shell: bash
        run: |
          if [[ ""${{ github.event.inputs.num_gpus }}"" == ""single"" && ""${{ github.event.inputs.runner_type }}"" == ""t4"" ]]; then
            echo ""RUNNER=aws-g4dn-4xlarge-cache"" >> $GITHUB_ENV
          elif [[ ""${{ github.event.inputs.num_gpus }}"" == ""multi"" && ""${{ github.event.inputs.runner_type }}"" == ""t4"" ]]; then
            echo ""RUNNER=aws-g4dn-12xlarge-cache"" >> $GITHUB_ENV
          elif [[ ""${{ github.event.inputs.num_gpus }}"" == ""single"" && ""${{ github.event.inputs.runner_type }}"" == ""a10"" ]]; then
            echo ""RUNNER=aws-g5-4xlarge-cache"" >> $GITHUB_ENV
          elif [[ ""${{ github.event.inputs.num_gpus }}"" == ""multi"" && ""${{ github.event.inputs.runner_type }}"" == ""a10"" ]]; then
            echo ""RUNNER=aws-g5-12xlarge-cache"" >> $GITHUB_ENV
          else
            echo ""RUNNER="" >> $GITHUB_ENV
          fi

      - name: Set runner to use
        id: set_runner
        run: |
          echo ${{ env.RUNNER }}
          echo ""RUNNER=${{ env.RUNNER }}"" >> $GITHUB_OUTPUT

  ssh_runner:
    name: ""SSH""
    needs: get_runner
    runs-on:
      group: ${{ needs.get_runner.outputs.RUNNER }}
    container:
      image: ${{ github.event.inputs.docker_image }}
      options: --gpus all --privileged --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/

    steps:
      - name: Update clone
        working-directory: /transformers
        run: |
          git fetch && git checkout ${{ github.sha }}

      - name: Cleanup
        working-directory: /transformers
        run: |
          rm -rf tests/__pycache__
          rm -rf tests/models/__pycache__
          rm -rf reports

      - name: Show installed libraries and their versions
        working-directory: /transformers
        run: pip freeze

      - name: NVIDIA-SMI
        run: |
          nvidia-smi

      - name: Store Slack infos
        #because the SSH can be enabled dynamically if the workflow failed, so we need to store slack infos to be able to retrieve them during the waitforssh step
        shell: bash
        run: |
          echo ""${{ github.actor }}""
          github_actor=${{ github.actor }}
          github_actor=${github_actor/'-'/'_'}
          echo ""$github_actor""
          echo ""github_actor=$github_actor"" >> $GITHUB_ENV

      - name: Store Slack infos
        #because the SSH can be enabled dynamically if the workflow failed, so we need to store slack infos to be able to retrieve them during the waitforssh step
        shell: bash
        run: |
          echo ""${{ env.github_actor }}""
          if [ ""${{ secrets[format('{0}_{1}', env.github_actor, 'SLACK_ID')] }}"" != """" ]; then
            echo ""SLACKCHANNEL=${{ secrets[format('{0}_{1}', env.github_actor, 'SLACK_ID')] }}"" >> $GITHUB_ENV
          else
            echo ""SLACKCHANNEL=${{ secrets.SLACK_CIFEEDBACK_CHANNEL }}"" >> $GITHUB_ENV
          fi

      - name: Tailscale # In order to be able to SSH when a test fails
        uses: huggingface/tailscale-action@main
        with:
          authkey: ${{ secrets.TAILSCALE_SSH_AUTHKEY }}
          slackChannel: ${{ env.SLACKCHANNEL }}
          slackToken: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}
          waitForSSH: true
          sshTimeout: 15m
",113,2,1,workflow_dispatch,1
huggingface/transformers,stale.yml,"name: Stale Bot

on:
  schedule:
    - cron: ""0 8 * * *""

jobs:
  close_stale_issues:
    name: Close Stale Issues
    if: github.repository == 'huggingface/transformers'
    runs-on: ubuntu-22.04
    permissions:
      issues: write
    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    steps:
    - uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: 3.8

    - name: Install requirements
      run: |
        pip install PyGithub
    - name: Close stale issues
      run: |
        python scripts/stale.py
",29,1,1,schedule,2
huggingface/transformers,trufflehog.yml,"on:
  push:

name: Secret Leaks

permissions:
  contents: read

jobs:
  trufflehog:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Secret Scanning
        uses: trufflesecurity/trufflehog@main
        with:
          extra_args: --results=verified,unknown
",20,1,1,push,2
huggingface/transformers,update_metdata.yml,"name: Update Transformers metadata

on:
  push:
    branches:
      - main
      - update_transformers_metadata*

jobs:
  build_and_package:
    runs-on: ubuntu-22.04
    defaults:
      run:
        shell: bash -l {0}

    steps:
      - uses: actions/checkout@v4

      - name: Setup environment
        run: |
          pip install --upgrade pip
          pip install datasets pandas
          pip install .[torch,tf,flax]

      - name: Update metadata
        run: |
          python utils/update_metadata.py --token ${{ secrets.LYSANDRE_HF_TOKEN }} --commit_sha ${{ github.sha }}
",27,1,1,push,1
huggingface/transformers,upload_pr_documentation.yml,"name: Upload PR Documentation

on:
  workflow_run:
    workflows: [""Build PR Documentation""]
    types:
      - completed

jobs:
  build:
    uses: huggingface/doc-builder/.github/workflows/upload_pr_documentation.yml@main
    with:
      package_name: transformers
    secrets:
      hf_token: ${{ secrets.HF_DOC_BUILD_PUSH }}
      comment_bot_token: ${{ secrets.COMMENT_BOT_TOKEN }}",16,1,1,workflow_run,1
ollama/ollama,latest.yaml,"name: latest

on:
  release:
    types: [released]

jobs:
  update-latest:
    environment: release
    runs-on: linux
    steps:
      - uses: actions/checkout@v4
      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ vars.DOCKER_USER }}
          password: ${{ secrets.DOCKER_ACCESS_TOKEN }}
      - name: Tag images as latest
        env:
          PUSH: ""1""
        shell: bash
        run: |
          export ""VERSION=${GITHUB_REF_NAME#v}""
          ./scripts/tag_latest.sh
",24,1,1,release,2
ollama/ollama,release.yaml,"name: release

on:
  push:
    tags:
      - 'v*'

env:
  CGO_CFLAGS: '-O3'
  CGO_CXXFLAGS: '-O3'

jobs:
  setup-environment:
    runs-on: ubuntu-latest
    environment: release
    outputs:
      GOFLAGS: ${{ steps.goflags.outputs.GOFLAGS }}
    steps:
      - uses: actions/checkout@v4
      - name: Set environment
        id: goflags
        run: |
          echo GOFLAGS=""'-ldflags=-w -s \""-X=github.com/ollama/ollama/version.Version=${GITHUB_REF_NAME#v}\"" \""-X=github.com/ollama/ollama/server.mode=release\""'"" >>$GITHUB_OUTPUT

  darwin-build:
    runs-on: macos-13
    environment: release
    needs: setup-environment
    strategy:
      matrix:
        os: [darwin]
        arch: [amd64, arm64]
    env:
      GOFLAGS: ${{ needs.setup-environment.outputs.GOFLAGS }}
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-go@v5
        with:
          go-version-file: go.mod
      - run: |
          go build -o dist/ .
        env:
          GOOS: ${{ matrix.os }}
          GOARCH: ${{ matrix.arch }}
          CGO_ENABLED: 1
          CGO_CPPFLAGS: '-mmacosx-version-min=11.3'
      - if: matrix.arch == 'amd64'
        run: |
          cmake --preset CPU -DCMAKE_OSX_DEPLOYMENT_TARGET=11.3 -DCMAKE_SYSTEM_PROCESSOR=x86_64 -DCMAKE_OSX_ARCHITECTURES=x86_64
          cmake --build --parallel --preset CPU
          cmake --install build --component CPU --strip --parallel 8
      - uses: actions/upload-artifact@v4
        with:
          name: build-${{ matrix.os }}-${{ matrix.arch }}
          path: dist/*

  darwin-sign:
    runs-on: macos-13
    environment: release
    needs: darwin-build
    steps:
      - uses: actions/checkout@v4
      - run: |
          echo $MACOS_SIGNING_KEY | base64 --decode > certificate.p12
          security create-keychain -p password build.keychain
          security default-keychain -s build.keychain
          security unlock-keychain -p password build.keychain
          security import certificate.p12 -k build.keychain -P $MACOS_SIGNING_KEY_PASSWORD -T /usr/bin/codesign
          security set-key-partition-list -S apple-tool:,apple:,codesign: -s -k password build.keychain
          security set-keychain-settings -lut 3600 build.keychain
        env:
          MACOS_SIGNING_KEY: ${{ secrets.MACOS_SIGNING_KEY }}
          MACOS_SIGNING_KEY_PASSWORD: ${{ secrets.MACOS_SIGNING_KEY_PASSWORD }}
      - uses: actions/download-artifact@v4
        with:
          name: build-darwin-amd64
          path: dist/darwin-amd64
      - uses: actions/download-artifact@v4
        with:
          name: build-darwin-arm64
          path: dist/darwin-arm64
      - run: |
          export VERSION=${GITHUB_REF_NAME#v}
          ./scripts/build_darwin.sh sign macapp
        env:
          APPLE_IDENTITY: ${{ secrets.APPLE_IDENTITY }}
          APPLE_PASSWORD: ${{ secrets.APPLE_PASSWORD }}
          APPLE_TEAM_ID: ${{ vars.APPLE_TEAM_ID }}
          APPLE_ID: ${{ vars.APPLE_ID }}
          SDKROOT: /Applications/Xcode_14.1.0.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk
          DEVELOPER_DIR: /Applications/Xcode_14.1.0.app/Contents/Developer
      - uses: actions/upload-artifact@v4
        with:
          name: dist-darwin
          path: |
            dist/Ollama-darwin.zip
            dist/ollama-darwin.tgz

  windows-depends:
    strategy:
      matrix:
        os: [windows]
        arch: [amd64]
        preset: ['CPU']
        include:
          - os: windows
            arch: amd64
            preset: 'CUDA 12'
            install: https://developer.download.nvidia.com/compute/cuda/12.8.0/local_installers/cuda_12.8.0_571.96_windows.exe
            cuda-version: '12.8'
            flags: ''
          - os: windows
            arch: amd64
            preset: 'ROCm 6'
            install: https://download.amd.com/developer/eula/rocm-hub/AMD-Software-PRO-Edition-24.Q4-WinSvr2022-For-HIP.exe
            rocm-version: '6.2'
            flags: '-DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ -DCMAKE_C_FLAGS=""-parallel-jobs=4 -Wno-ignored-attributes -Wno-deprecated-pragma"" -DCMAKE_CXX_FLAGS=""-parallel-jobs=4 -Wno-ignored-attributes -Wno-deprecated-pragma""'
    runs-on: ${{ matrix.arch == 'arm64' && format('{0}-{1}', matrix.os, matrix.arch) || matrix.os }}
    environment: release
    env:
      GOFLAGS: ${{ needs.setup-environment.outputs.GOFLAGS }}
    steps:
      - name: Install system dependencies
        run: |
          choco install -y --no-progress ccache ninja
          ccache -o cache_dir=${{ github.workspace }}\.ccache
      - if: startsWith(matrix.preset, 'CUDA ') || startsWith(matrix.preset, 'ROCm ')
        id: cache-install
        uses: actions/cache/restore@v4
        with:
          path: |
            C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA
            C:\Program Files\AMD\ROCm
          key: ${{ matrix.install }}
      - if: startsWith(matrix.preset, 'CUDA ')
        name: Install CUDA ${{ matrix.cuda-version }}
        run: |
          $ErrorActionPreference = ""Stop""
          if (""${{ steps.cache-install.outputs.cache-hit }}"" -ne 'true') {
            Invoke-WebRequest -Uri ""${{ matrix.install }}"" -OutFile ""install.exe""
            $subpackages = @(""cudart"", ""nvcc"", ""cublas"", ""cublas_dev"") | Foreach-Object {""${_}_${{ matrix.cuda-version }}""}
            Start-Process -FilePath .\install.exe -ArgumentList (@(""-s"") + $subpackages) -NoNewWindow -Wait
          }

          $cudaPath = (Resolve-Path ""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\*"").path
          echo ""$cudaPath\bin"" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
      - if: startsWith(matrix.preset, 'ROCm')
        name: Install ROCm ${{ matrix.rocm-version }}
        run: |
          $ErrorActionPreference = ""Stop""
          if (""${{ steps.cache-install.outputs.cache-hit }}"" -ne 'true') {
            Invoke-WebRequest -Uri ""${{ matrix.install }}"" -OutFile ""install.exe""
            Start-Process -FilePath .\install.exe -ArgumentList '-install' -NoNewWindow -Wait
          }

          $hipPath = (Resolve-Path ""C:\Program Files\AMD\ROCm\*"").path
          echo ""$hipPath\bin"" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
          echo ""CC=$hipPath\bin\clang.exe"" | Out-File -FilePath $env:GITHUB_ENV -Append
          echo ""CXX=$hipPath\bin\clang++.exe"" | Out-File -FilePath $env:GITHUB_ENV -Append
          echo ""HIPCXX=$hipPath\bin\clang++.exe"" | Out-File -FilePath $env:GITHUB_ENV -Append
          echo ""HIP_PLATFORM=amd"" | Out-File -FilePath $env:GITHUB_ENV -Append
          echo ""CMAKE_PREFIX_PATH=$hipPath"" | Out-File -FilePath $env:GITHUB_ENV -Append
      - if: matrix.preset == 'CPU'
        run: |
          echo ""CC=clang.exe"" | Out-File -FilePath $env:GITHUB_ENV -Append
          echo ""CXX=clang++.exe"" | Out-File -FilePath $env:GITHUB_ENV -Append
      - if: ${{ !cancelled() && steps.cache-install.outputs.cache-hit != 'true' }}
        uses: actions/cache/save@v4
        with:
          path: |
            C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA
            C:\Program Files\AMD\ROCm
          key: ${{ matrix.install }}
      - uses: actions/checkout@v4
      - uses: actions/cache@v4
        with:
          path: ${{ github.workspace }}\.ccache
          key: ccache-${{ matrix.os }}-${{ matrix.arch }}-${{ matrix.preset }}
      - name: Build target ""${{ matrix.preset }}""
        run: |
          Import-Module 'C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\Tools\Microsoft.VisualStudio.DevShell.dll'
          Enter-VsDevShell -VsInstallPath 'C:\Program Files\Microsoft Visual Studio\2022\Enterprise' -SkipAutomaticLocation  -DevCmdArguments '-arch=x64 -no_logo'
          cmake --preset ""${{ matrix.preset }}"" ${{ matrix.flags }}
          cmake --build --parallel --preset ""${{ matrix.preset }}""
          cmake --install build --component ""${{ startsWith(matrix.preset, 'CUDA ') && 'CUDA' || startsWith(matrix.preset, 'ROCm ') && 'HIP' || 'CPU' }}"" --strip --parallel 8
        env:
          CMAKE_GENERATOR: Ninja
      - uses: actions/upload-artifact@v4
        with:
          name: depends-${{ matrix.os }}-${{ matrix.arch }}-${{ matrix.preset }}
          path: dist\*

  windows-build:
    strategy:
      matrix:
        os: [windows]
        arch: [amd64, arm64]
    runs-on: ${{ matrix.arch == 'arm64' && format('{0}-{1}', matrix.os, matrix.arch) || matrix.os }}
    environment: release
    needs: [setup-environment]
    env:
      GOFLAGS: ${{ needs.setup-environment.outputs.GOFLAGS }}
    steps:
      - name: Install AMD64 system dependencies
        if: matrix.arch == 'amd64'
        run: |
          $ErrorActionPreference = ""Stop""
          Start-Process ""C:\msys64\usr\bin\pacman.exe"" -ArgumentList @(""-S"", ""--noconfirm"", ""mingw-w64-clang-x86_64-gcc-compat"", ""mingw-w64-clang-x86_64-clang"") -NoNewWindow -Wait
          echo ""C:\msys64\usr\bin"" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
          echo ""C:\msys64\clang64\bin"" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
      - name: Install ARM64 system dependencies
        if: matrix.arch == 'arm64'
        run: |
          $ErrorActionPreference = ""Stop""
          Set-ExecutionPolicy Bypass -Scope Process -Force
          [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072
          iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))
          echo ""C:\ProgramData\chocolatey\bin"" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append

          choco install -y --no-progress git gzip
          echo ""C:\Program Files\Git\cmd"" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append

          Invoke-WebRequest -Uri ""https://github.com/mstorsjo/llvm-mingw/releases/download/20240619/llvm-mingw-20240619-ucrt-aarch64.zip"" -OutFile ""${{ runner.temp }}\llvm-mingw-ucrt-aarch64.zip""
          Expand-Archive -Path ${{ runner.temp }}\llvm-mingw-ucrt-aarch64.zip -DestinationPath ""C:\Program Files\""
          $installPath=(Resolve-Path -Path ""C:\Program Files\llvm-mingw-*-ucrt-aarch64"").path
          echo $installPath\bin | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
      - uses: actions/checkout@v4
      - uses: actions/setup-go@v5
        with:
          go-version-file: go.mod
      - run: |
          go build -o dist/${{ matrix.os }}-${{ matrix.arch }}/ .
      - if: matrix.arch == 'arm64'
        run: |
          Invoke-WebRequest -Uri ""https://aka.ms/vs/17/release/vc_redist.arm64.exe"" -OutFile ""dist\windows-arm64\vc_redist.arm64.exe""
      - run: |
          $env:VERSION='${{ github.ref_name }}' -Replace ""v(.*)"", '$1'
          & .\scripts\build_windows.ps1 buildApp
        env:
          VCToolsRedistDir: stub
      - uses: actions/upload-artifact@v4
        with:
          name: build-${{ matrix.os }}-${{ matrix.arch }}
          path: |
            dist\${{ matrix.os }}-${{ matrix.arch }}\*.exe
            dist\${{ matrix.os }}-${{ matrix.arch }}-app.exe

  windows-sign:
    runs-on: windows
    environment: release
    needs: [windows-depends, windows-build]
    steps:
      - uses: actions/checkout@v4
      - uses: google-github-actions/auth@v2
        with:
          project_id: ollama
          credentials_json: ${{ secrets.GOOGLE_SIGNING_CREDENTIALS }}
      - run: |
          $ErrorActionPreference = ""Stop""
          Invoke-WebRequest -Uri ""https://go.microsoft.com/fwlink/p/?LinkId=323507"" -OutFile ""${{ runner.temp }}\sdksetup.exe""
          Start-Process ""${{ runner.temp }}\sdksetup.exe"" -ArgumentList @(""/q"") -NoNewWindow -Wait

          Invoke-WebRequest -Uri ""https://github.com/GoogleCloudPlatform/kms-integrations/releases/download/cng-v1.0/kmscng-1.0-windows-amd64.zip"" -OutFile ""${{ runner.temp }}\plugin.zip""
          Expand-Archive -Path ""${{ runner.temp }}\plugin.zip"" -DestinationPath ""${{ runner.temp }}\plugin\""
          & ""${{ runner.temp }}\plugin\*\kmscng.msi"" /quiet

          echo ""${{ vars.OLLAMA_CERT }}"" >ollama_inc.crt
      - uses: actions/download-artifact@v4
        with:
          pattern: build-windows-*
          path: dist\
          merge-multiple: true
      - uses: actions/download-artifact@v4
        with:
          pattern: depends-windows-amd64-*
          path: dist\windows-amd64\
          merge-multiple: true
      - run: |
          & .\scripts\build_windows.ps1 gatherDependencies sign buildInstaller distZip
        env:
          KEY_CONTAINER: ${{ vars.KEY_CONTAINER }}
      - uses: actions/upload-artifact@v4
        with:
          name: dist-windows
          path: |
            dist\OllamaSetup.exe
            dist\ollama-windows-*.zip

  linux-build:
    strategy:
      matrix:
        include:
          - os: linux
            arch: amd64
            target: archive
          - os: linux
            arch: amd64
            target: rocm
          - os: linux
            arch: arm64
            target: archive
    runs-on: ${{ matrix.arch == 'arm64' && format('{0}-{1}', matrix.os, matrix.arch) || matrix.os }}
    environment: release
    needs: setup-environment
    env:
      GOFLAGS: ${{ needs.setup-environment.outputs.GOFLAGS }}
    steps:
      - uses: actions/checkout@v4
      - uses: docker/setup-buildx-action@v3
      - uses: docker/build-push-action@v6
        with:
          context: .
          platforms: ${{ matrix.os }}/${{ matrix.arch }}
          target: ${{ matrix.target }}
          build-args: |
            GOFLAGS=${{ env.GOFLAGS }}
            CGO_CFLAGS=${{ env.CGO_CFLAGS }}
            CGO_CXXFLAGS=${{ env.CGO_CXXFLAGS }}
          outputs: type=local,dest=dist/${{ matrix.os }}-${{ matrix.arch }}
          cache-from: type=registry,ref=ollama/ollama:latest
          cache-to: type=inline
      - run: |
          for COMPONENT in bin/* lib/ollama/*; do
            case ""$COMPONENT"" in
              bin/ollama)                echo $COMPONENT >>ollama-${{ matrix.os }}-${{ matrix.arch }}.tar.in ;;
              lib/ollama/*.so*)          echo $COMPONENT >>ollama-${{ matrix.os }}-${{ matrix.arch }}.tar.in ;;
              lib/ollama/cuda_sbsa)      echo $COMPONENT >>ollama-${{ matrix.os }}-${{ matrix.arch }}.tar.in ;;
              lib/ollama/cuda_jetpack5)  echo $COMPONENT >>ollama-${{ matrix.os }}-${{ matrix.arch }}-jetpack5.tar.in ;;
              lib/ollama/cuda_jetpack6)  echo $COMPONENT >>ollama-${{ matrix.os }}-${{ matrix.arch }}-jetpack6.tar.in ;;
              lib/ollama/rocm)           echo $COMPONENT >>ollama-${{ matrix.os }}-${{ matrix.arch }}-rocm.tar.in ;;
            esac
          done
        working-directory: dist/${{ matrix.os }}-${{ matrix.arch }}
      - run: |
          echo ""Manifests""
          for ARCHIVE in dist/${{ matrix.os }}-${{ matrix.arch }}/*.tar.in ; do
            echo $ARCHIVE
            cat $ARCHIVE
          done
      - run: |
          for ARCHIVE in dist/${{ matrix.os }}-${{ matrix.arch }}/*.tar.in; do
            tar c -C dist/${{ matrix.os }}-${{ matrix.arch }} -T $ARCHIVE --owner 0 --group 0 | pigz -9vc >$(basename ${ARCHIVE//.*/}.tgz);
          done
      - uses: actions/upload-artifact@v4
        with:
          name: dist-${{ matrix.os }}-${{ matrix.arch }}-${{ matrix.target }}
          path: |
            *.tgz

  # Build each Docker variant (OS, arch, and flavor) separately. Using QEMU is unreliable and slower.
  docker-build-push:
    strategy:
      matrix:
        include:
          - os: linux
            arch: arm64
            build-args: |
              CGO_CFLAGS
              CGO_CXXFLAGS
              GOFLAGS
          - os: linux
            arch: amd64
            build-args: |
              CGO_CFLAGS
              CGO_CXXFLAGS
              GOFLAGS
          - os: linux
            arch: amd64
            suffix: '-rocm'
            build-args: |
              CGO_CFLAGS
              CGO_CXXFLAGS
              GOFLAGS
              FLAVOR=rocm
    runs-on: ${{ matrix.arch == 'arm64' && format('{0}-{1}', matrix.os, matrix.arch) || matrix.os }}
    environment: release
    needs: setup-environment
    env:
      GOFLAGS: ${{ needs.setup-environment.outputs.GOFLAGS }}
    steps:
      - uses: actions/checkout@v4
      - uses: docker/setup-buildx-action@v3
      - uses: docker/login-action@v3
        with:
          username: ${{ vars.DOCKER_USER }}
          password: ${{ secrets.DOCKER_ACCESS_TOKEN }}
      - id: build-push
        uses: docker/build-push-action@v6
        with:
          context: .
          platforms: ${{ matrix.os }}/${{ matrix.arch }}
          build-args: ${{ matrix.build-args }}
          outputs: type=image,name=ollama/ollama,push-by-digest=true,name-canonical=true,push=true
          cache-from: type=registry,ref=ollama/ollama:latest
          cache-to: type=inline
      - run: |
          mkdir -p ${{ matrix.os }}-${{ matrix.arch }}
          echo ""${{ steps.build-push.outputs.digest }}"" >${{ matrix.os }}-${{ matrix.arch }}-${{ matrix.suffix }}.txt
        working-directory: ${{ runner.temp }}
      - uses: actions/upload-artifact@v4
        with:
          name: digest-${{ matrix.os }}-${{ matrix.arch }}-${{ matrix.suffix }}
          path: |
            ${{ runner.temp }}/${{ matrix.os }}-${{ matrix.arch }}-${{ matrix.suffix }}.txt

  # Merge Docker images for the same flavor into a single multi-arch manifest
  docker-merge-push:
    strategy:
      matrix:
        suffix: ['', '-rocm']
    runs-on: linux
    environment: release
    needs: [docker-build-push]
    steps:
      - uses: docker/login-action@v3
        with:
          username: ${{ vars.DOCKER_USER }}
          password: ${{ secrets.DOCKER_ACCESS_TOKEN }}
      - id: metadata
        uses: docker/metadata-action@v4
        with:
          flavor: |
            latest=false
            suffix=${{ matrix.suffix }}
          images: |
            ollama/ollama
          tags: |
            type=ref,enable=true,priority=600,prefix=pr-,event=pr
            type=semver,pattern={{version}}
      - uses: actions/download-artifact@v4
        with:
          pattern: digest-*
          path: ${{ runner.temp }}
          merge-multiple: true
      - run: |
          docker buildx imagetools create $(echo '${{ steps.metadata.outputs.json }}' | jq -cr '.tags | map(""-t"", .) | join("" "")') $(cat *-${{ matrix.suffix }}.txt | xargs printf 'ollama/ollama@%s ')
          docker buildx imagetools inspect ollama/ollama:${{ steps.metadata.outputs.version }}
        working-directory: ${{ runner.temp }}

  # Trigger downstream release process
  trigger:
    runs-on: ubuntu-latest
    environment: release
    needs: [darwin-build, windows-build, windows-depends]
    steps:
      - name: Trigger downstream release process
        run: |
          curl -L \
            -X POST \
            -H ""Accept: application/vnd.github+json"" \
            -H ""Authorization: Bearer ${{ secrets.RELEASE_TOKEN }}"" \
            -H ""X-GitHub-Api-Version: 2022-11-28"" \
            https://api.github.com/repos/ollama/${{ vars.RELEASE_REPO }}/dispatches \
            -d ""{\""event_type\"": \""trigger-workflow\"", \""client_payload\"": {\""run_id\"": \""${GITHUB_RUN_ID}\"", \""version\"": \""${GITHUB_REF_NAME#v}\""}}""

  # Aggregate all the assets and ship a release
  release:
    needs: [darwin-sign, windows-sign, linux-build]
    runs-on: linux
    environment: release
    permissions:
      contents: write
    env:
      GH_TOKEN: ${{ github.token }}
    steps:
      - uses: actions/checkout@v4
      - uses: actions/download-artifact@v4
        with:
          name: dist-darwin
          path: dist
      - uses: actions/download-artifact@v4
        with:
          name: dist-windows
          path: dist
      - uses: actions/download-artifact@v4
        with:
          pattern: dist-linux-*
          path: stage
          merge-multiple: false
      - name: Merge linux amd64 payload
        working-directory: stage/dist-linux-amd64-archive
        run: |
          tar zxf ollama-linux-amd64.tgz
          tar zxf ../dist-linux-amd64-rocm/ollama-linux-amd64.tgz
          rm -f ollama-linux-amd64.tgz ../dist-linux-amd64-rocm/ollama-linux-amd64.tgz
          tar -c -f- --owner 0 --group 0 . | pigz -9vc > ../ollama-linux-amd64.tgz
      - name: Cleanup linux payloads
        run: |
          find stage -name ollama-linux\*.tgz -exec mv {} dist/ \;
      - run: find . -type f -not -name 'sha256sum.txt' | xargs sha256sum | tee sha256sum.txt
        working-directory: dist
      - name: Create or update Release
        run: |
          RELEASE_VERSION=""$(echo ${GITHUB_REF_NAME} | cut -f1 -d-)""

          echo ""Looking for existing release for ${RELEASE_VERSION}""
          OLD_TAG=$(gh release ls --json name,tagName | jq -r "".[] | select(.name == \""${RELEASE_VERSION}\"") | .tagName"")
          if [ -n ""$OLD_TAG"" ]; then
            echo ""Updating release ${RELEASE_VERSION} to point to new tag ${GITHUB_REF_NAME}""
            gh release edit ${OLD_TAG} --tag ${GITHUB_REF_NAME}
          else
            echo ""Creating new release ${RELEASE_VERSION} pointing to tag ${GITHUB_REF_NAME}""
            gh release create ${GITHUB_REF_NAME} \
              --title ${RELEASE_VERSION} \
              --draft \
              --generate-notes \
              --prerelease
          fi
          echo ""Uploading artifacts for tag ${GITHUB_REF_NAME}""
          gh release upload ${GITHUB_REF_NAME} dist/* --clobber
",510,11,1,push,37
ollama/ollama,test.yaml,"name: test

concurrency:
  # For PRs, later CI runs preempt previous ones. e.g. a force push on a PR
  # cancels running CI jobs and starts all new ones.
  #
  # For non-PR pushes, concurrency.group needs to be unique for every distinct
  # CI run we want to have happen. Use run_id, which in practice means all
  # non-PR CI runs will be allowed to run without preempting each other.
  group: ${{ github.workflow }}-$${{ github.pull_request.number || github.run_id }}
  cancel-in-progress: true

on:
  pull_request:
    paths:
      - '**/*'
      - '!docs/**'
      - '!README.md'

jobs:
  changes:
    runs-on: ubuntu-latest
    outputs:
      changed: ${{ steps.changes.outputs.changed }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - id: changes
        run: |
          changed() {
            local BASE=${{ github.event.pull_request.base.sha }}
            local HEAD=${{ github.event.pull_request.head.sha }}
            local MERGE_BASE=$(git merge-base $BASE $HEAD)
            git diff-tree -r --no-commit-id --name-only ""$MERGE_BASE"" ""$HEAD"" \
              | xargs python3 -c ""import sys; from pathlib import Path; print(any(Path(x).match(glob) for x in sys.argv[1:] for glob in '$*'.split(' ')))""
          }

          echo changed=$(changed 'llama/llama.cpp/**/*' 'ml/backend/ggml/ggml/**/*') | tee -a $GITHUB_OUTPUT

  linux:
    needs: [changes]
    if: needs.changes.outputs.changed == 'True'
    strategy:
      matrix:
        include:
          - preset: CPU
          - preset: CUDA
            container: nvidia/cuda:12.8.1-devel-ubuntu22.04
            flags: '-DCMAKE_CUDA_ARCHITECTURES=87'
          - preset: ROCm
            container: rocm/dev-ubuntu-22.04:6.1.2
            extra-packages: rocm-libs
            flags: '-DAMDGPU_TARGETS=gfx1010 -DCMAKE_PREFIX_PATH=/opt/rocm'
    runs-on: linux
    container: ${{ matrix.container }}
    steps:
      - uses: actions/checkout@v4
      - run: |
          [ -n ""${{ matrix.container }}"" ] || sudo=sudo
          $sudo apt-get update
          $sudo apt-get install -y cmake ccache ${{ matrix.extra-packages }}
        env:
          DEBIAN_FRONTEND: noninteractive
      - uses: actions/cache@v4
        with:
          path: /github/home/.cache/ccache
          key: ccache-${{ runner.os }}-${{ runner.arch }}-${{ matrix.preset }}
      - run: |
          cmake --preset ${{ matrix.preset }} ${{ matrix.flags }}
          cmake --build --preset ${{ matrix.preset }} --parallel

  windows:
    needs: [changes]
    if: needs.changes.outputs.changed == 'True'
    strategy:
      matrix:
        include:
          - preset: CPU
          - preset: CUDA
            install: https://developer.download.nvidia.com/compute/cuda/12.8.0/local_installers/cuda_12.8.0_571.96_windows.exe
            flags: '-DCMAKE_CUDA_ARCHITECTURES=80'
          - preset: ROCm
            install: https://download.amd.com/developer/eula/rocm-hub/AMD-Software-PRO-Edition-24.Q4-WinSvr2022-For-HIP.exe
            flags: '-DAMDGPU_TARGETS=gfx1010 -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ -DCMAKE_C_FLAGS=""-parallel-jobs=4 -Wno-ignored-attributes -Wno-deprecated-pragma"" -DCMAKE_CXX_FLAGS=""-parallel-jobs=4 -Wno-ignored-attributes -Wno-deprecated-pragma""'
    runs-on: windows
    steps:
      - run: |
          choco install -y --no-progress ccache ninja
          ccache -o cache_dir=${{ github.workspace }}\.ccache
      - if: matrix.preset == 'CUDA' || matrix.preset == 'ROCm'
        id: cache-install
        uses: actions/cache/restore@v4
        with:
          path: |
            C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA
            C:\Program Files\AMD\ROCm
          key: ${{ matrix.install }}
      - if: matrix.preset == 'CUDA'
        name: Install CUDA ${{ matrix.cuda-version }}
        run: |
          $ErrorActionPreference = ""Stop""
          if (""${{ steps.cache-install.outputs.cache-hit }}"" -ne 'true') {
            Invoke-WebRequest -Uri ""${{ matrix.install }}"" -OutFile ""install.exe""
            Start-Process -FilePath .\install.exe -ArgumentList (@(""-s"", ""cudart_12.8"", ""nvcc_12.8"", ""cublas_12.8"", ""cublas_dev_12.8"")) -NoNewWindow -Wait
          }

          $cudaPath = (Resolve-Path ""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\*"").path
          echo ""$cudaPath\bin"" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
      - if: matrix.preset == 'ROCm'
        name: Install ROCm ${{ matrix.rocm-version }}
        run: |
          $ErrorActionPreference = ""Stop""
          if (""${{ steps.cache-install.outputs.cache-hit }}"" -ne 'true') {
            Invoke-WebRequest -Uri ""${{ matrix.install }}"" -OutFile ""install.exe""
            Start-Process -FilePath .\install.exe -ArgumentList '-install' -NoNewWindow -Wait
          }

          $hipPath = (Resolve-Path ""C:\Program Files\AMD\ROCm\*"").path
          echo ""$hipPath\bin"" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
          echo ""CC=$hipPath\bin\clang.exe"" | Out-File -FilePath $env:GITHUB_ENV -Append
          echo ""CXX=$hipPath\bin\clang++.exe"" | Out-File -FilePath $env:GITHUB_ENV -Append
          echo ""HIPCXX=$hipPath\bin\clang++.exe"" | Out-File -FilePath $env:GITHUB_ENV -Append
          echo ""HIP_PLATFORM=amd"" | Out-File -FilePath $env:GITHUB_ENV -Append
          echo ""CMAKE_PREFIX_PATH=$hipPath"" | Out-File -FilePath $env:GITHUB_ENV -Append
      - if: ${{ !cancelled() && steps.cache-install.outputs.cache-hit != 'true' }}
        uses: actions/cache/save@v4
        with:
          path: |
            C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA
            C:\Program Files\AMD\ROCm
          key: ${{ matrix.install }}
      - uses: actions/checkout@v4
      - uses: actions/cache@v4
        with:
          path: ${{ github.workspace }}\.ccache
          key: ccache-${{ runner.os }}-${{ runner.arch }}-${{ matrix.preset }}
      - run: |
          Import-Module 'C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\Tools\Microsoft.VisualStudio.DevShell.dll'
          Enter-VsDevShell -VsInstallPath 'C:\Program Files\Microsoft Visual Studio\2022\Enterprise' -SkipAutomaticLocation  -DevCmdArguments '-arch=x64 -no_logo'
          cmake --preset ""${{ matrix.preset }}"" ${{ matrix.flags }}
          cmake --build --parallel --preset ""${{ matrix.preset }}""
        env:
          CMAKE_GENERATOR: Ninja

  go_mod_tidy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: check that 'go mod tidy' is clean
        run: go mod tidy --diff || (echo ""Please run 'go mod tidy'."" && exit 1)

  test:
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
    runs-on: ${{ matrix.os }}
    env:
      CGO_ENABLED: '1'
      GOEXPERIMENT: 'synctest'
    steps:
      - name: checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # 4.2.2

      - name: cache restore
        uses: actions/cache/restore@1bd1e32a3bdc45362d1e726936510720a7c30a57 # v4.2.0
        with:
          # Note: unlike the other setups, this is only grabbing the mod download
          # cache, rather than the whole mod directory, as the download cache
          # contains zips that can be unpacked in parallel faster than they can be
          # fetched and extracted by tar
          path: |
            ~/.cache/go-build
            ~/go/pkg/mod/cache
            ~\AppData\Local\go-build
          # NOTE: The -3- here should be incremented when the scheme of data to be
          # cached changes (e.g. path above changes).
          key: ${{ github.job }}-${{ runner.os }}-${{ matrix.goarch }}-${{ matrix.buildflags }}-go-3-${{ hashFiles('**/go.sum') }}-${{ github.run_id }}
          restore-keys: |
            ${{ github.job }}-${{ runner.os }}-${{ matrix.goarch }}-${{ matrix.buildflags }}-go-3-${{ hashFiles('**/go.sum') }}
            ${{ github.job }}-${{ runner.os }}-${{ matrix.goarch }}-${{ matrix.buildflags }}-go-3-

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          # The caching strategy of setup-go is less than ideal, and wastes
          # time by not saving artifacts due to small failures like the linter
          # complaining, etc. This means subsequent have to rebuild their world
          # again until all checks pass. For instance, if you mispell a word,
          # you're punished until you fix it. This is more hostile than
          # helpful.
          cache: false

          go-version-file: go.mod

      # It is tempting to run this in a platform independent way, but the past
      # shows this codebase will see introductions of platform specific code
      # generation, and so we need to check this per platform to ensure we
      # don't abuse go generate on specific platforms.
      - name: check that 'go generate' is clean
        if: always()
        run: |
          go generate ./...
          git diff --name-only --exit-code || (echo ""Please run 'go generate ./...'."" && exit 1)

      - name: go test
        if: always()
        run: go test -count=1 -benchtime=1x ./...

      # TODO(bmizerany): replace this heavy tool with just the
      # tools/checks/binaries we want and then make them all run in parallel
      # across jobs, not on a single tiny vm on Github Actions.
      - uses: golangci/golangci-lint-action@v6
        with:
          args: --timeout 10m0s -v

      - name: cache save
        # Always save the cache, even if the job fails. The artifacts produced
        # during the building of test binaries are not all for naught. They can
        # be used to speed up subsequent runs.
        if: always()

        uses: actions/cache/save@1bd1e32a3bdc45362d1e726936510720a7c30a57 # v4.2.0
        with:
          # Note: unlike the other setups, this is only grabbing the mod download
          # cache, rather than the whole mod directory, as the download cache
          # contains zips that can be unpacked in parallel faster than they can be
          # fetched and extracted by tar
          path: |
            ~/.cache/go-build
            ~/go/pkg/mod/cache
            ~\AppData\Local\go-build
          # NOTE: The -3- here should be incremented when the scheme of data to be
          # cached changes (e.g. path above changes).
          key: ${{ github.job }}-${{ runner.os }}-${{ matrix.goarch }}-${{ matrix.buildflags }}-go-3-${{ hashFiles('**/go.sum') }}-${{ github.run_id }}

  patches:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Verify patches apply cleanly and do not change files
        run: |
          make -f Makefile.sync clean checkout apply-patches sync
          git diff --compact-summary --exit-code",244,6,1,pull_request,14
ytdl-org/youtube-dl,ci.yml,"name: CI

env:
  all-cpython-versions: 2.6, 2.7, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 3.10, 3.11, 3.12
  main-cpython-versions: 2.7, 3.2, 3.5, 3.9, 3.11
  pypy-versions: pypy-2.7, pypy-3.6, pypy-3.7
  cpython-versions: main
  test-set: core
  # Python beta version to be built using pyenv before setup-python support
  # Must also be included in all-cpython-versions 
  next: 3.13

on:
  push:
    # push inputs aren't known to GitHub
    inputs:
      cpython-versions:
        type: string
        default: all
      test-set:
        type: string
        default: core
  pull_request:
    # pull_request inputs aren't known to GitHub
    inputs:
      cpython-versions:
        type: string
        default: main
      test-set:
        type: string
        default: both
  workflow_dispatch:
    inputs:
      cpython-versions:
        type: choice
        description: CPython versions (main = 2.7, 3.2, 3.5, 3.9, 3.11)
        options:
          - all
          - main
        required: true
        default: main
      test-set:
        type: choice
        description: core, download
        options:
          - both
          - core
          - download
        required: true
        default: both

permissions:
  contents: read

jobs:
  select:
    name: Select tests from inputs
    runs-on: ubuntu-latest
    outputs:
      cpython-versions: ${{ steps.run.outputs.cpython-versions }}
      test-set: ${{ steps.run.outputs.test-set }}
      own-pip-versions: ${{ steps.run.outputs.own-pip-versions }}
    steps:
    # push and pull_request inputs aren't known to GitHub (pt3)
    - name: Set push defaults
      if: ${{ github.event_name == 'push' }}
      env:
        cpython-versions: all
        test-set: core
      run: |
        echo ""cpython-versions=${{env.cpython-versions}}"" >> ""$GITHUB_ENV""
        echo ""test_set=${{env.test_set}}"" >> ""$GITHUB_ENV""
    - name: Get pull_request inputs
      if: ${{ github.event_name == 'pull_request' }}
      env:
        cpython-versions: main
        test-set: both
      run: |
        echo ""cpython-versions=${{env.cpython-versions}}"" >> ""$GITHUB_ENV""
        echo ""test_set=${{env.test_set}}"" >> ""$GITHUB_ENV""
    - name: Make version array
      id: run
      run: |
        # Make a JSON Array from comma/space-separated string (no extra escaping)
        json_list() { \
          ret=""""; IFS=""${IFS},""; set -- $*; \
          for a in ""$@""; do \
            ret=$(printf '%s""%s""' ""${ret}${ret:+, }"" ""$a""); \
          done; \
          printf '[%s]' ""$ret""; }
        tests=""${{ inputs.test-set || env.test-set }}""
        [ $tests = both ] && tests=""core download""
        printf 'test-set=%s\n' ""$(json_list $tests)"" >> ""$GITHUB_OUTPUT""
        versions=""${{ inputs.cpython-versions || env.cpython-versions }}""
        if [ ""$versions"" = all ]; then \
          versions=""${{ env.all-cpython-versions }}""; else \
          versions=""${{ env.main-cpython-versions }}""; \
        fi
        printf 'cpython-versions=%s\n' \
          ""$(json_list ${versions}${versions:+, }${{ env.pypy-versions }})"" >> ""$GITHUB_OUTPUT""
        # versions with a special get-pip.py in a per-version subdirectory
        printf 'own-pip-versions=%s\n' \
          ""$(json_list 2.6, 2.7, 3.2, 3.3, 3.4, 3.5, 3.6)"" >> ""$GITHUB_OUTPUT""

  tests:
    name: Run tests
    needs: select
    permissions:
      contents: read
      packages: write
    runs-on: ${{ matrix.os }}
    env:
      PIP: python -m pip
      PIP_DISABLE_PIP_VERSION_CHECK: true
      PIP_NO_PYTHON_VERSION_WARNING: true
    strategy:
      fail-fast: true
      matrix:
        os: [ubuntu-22.04]
        python-version: ${{ fromJSON(needs.select.outputs.cpython-versions) }}
        python-impl: [cpython]
        ytdl-test-set: ${{ fromJSON(needs.select.outputs.test-set) }}
        run-tests-ext: [sh]
        include:
        - os: windows-2019
          python-version: 3.4
          python-impl: cpython
          ytdl-test-set: ${{ contains(needs.select.outputs.test-set, 'core') && 'core' || 'nocore' }}
          run-tests-ext: bat
        - os: windows-2019
          python-version: 3.4
          python-impl: cpython
          ytdl-test-set: ${{ contains(needs.select.outputs.test-set, 'download') && 'download'  || 'nodownload' }}
          run-tests-ext: bat
        # jython
        - os: ubuntu-22.04
          python-version: 2.7
          python-impl: jython
          ytdl-test-set: ${{ contains(needs.select.outputs.test-set, 'core') && 'core' || 'nocore' }}
          run-tests-ext: sh
        - os: ubuntu-22.04
          python-version: 2.7
          python-impl: jython
          ytdl-test-set: ${{ contains(needs.select.outputs.test-set, 'download') && 'download'  || 'nodownload' }}
          run-tests-ext: sh
    steps:
    - name: Prepare Linux
      if: ${{ startswith(matrix.os, 'ubuntu') }}
      shell: bash
      run: |
        # apt in runner, if needed, may not be up-to-date
        sudo apt-get update
    - name: Checkout
      uses: actions/checkout@v3
    #-------- Python 3 -----
    - name: Set up supported Python ${{ matrix.python-version }}
      id: setup-python
      if: ${{ matrix.python-impl == 'cpython' && matrix.python-version != '2.6' && matrix.python-version != '2.7' && matrix.python-version != env.next }}
      # wrap broken actions/setup-python@v4
      # NB may run apt-get install in Linux
      uses: ytdl-org/setup-python@v1
      env:
        # Temporary (?) workaround for Python 3.5 failures - May 2024
        PIP_TRUSTED_HOST: ""pypi.python.org pypi.org files.pythonhosted.org""
      with:
        python-version: ${{ matrix.python-version }}
        cache-build: true
        allow-build: info
    - name: Locate supported Python ${{ matrix.python-version }}
      if: ${{ env.pythonLocation }}
      shell: bash
      run: |
        echo ""PYTHONHOME=${pythonLocation}"" >> ""$GITHUB_ENV""
        export expected=""${{ steps.setup-python.outputs.python-path }}""
        dirname() { printf '%s\n' \
            'import os, sys' \
            'print(os.path.dirname(sys.argv[1]))' \
            | ${expected} - ""$1""; }
        expd=""$(dirname ""$expected"")""
        export python=""$(command -v python)""
        [ ""$expd"" = ""$(dirname ""$python"")"" ] || echo ""PATH=$expd:${PATH}"" >> ""$GITHUB_ENV""
        [ -x ""$python"" ] || printf '%s\n' \
            'import os' \
            'exp = os.environ[""expected""]' \
            'python = os.environ[""python""]' \
            'exps = os.path.split(exp)' \
            'if python and (os.path.dirname(python) == exp[0]):' \
            '    exit(0)' \
            'exps[1] = ""python"" + os.path.splitext(exps[1])[1]' \
            'python = os.path.join(*exps)' \
            'try:' \
            '    os.symlink(exp, python)' \
            'except AttributeError:' \
            '    os.rename(exp, python)' \
            | ${expected} -
        printf '%s\n' \
            'import sys' \
            'print(sys.path)' \
            | ${expected} -
    #-------- Python next (was 3.12) -
    - name: Set up CPython 3.next environment
      if: ${{ matrix.python-impl == 'cpython' && matrix.python-version == env.next }}
      shell: bash
      run: |
        PYENV_ROOT=$HOME/.local/share/pyenv
        echo ""PYENV_ROOT=${PYENV_ROOT}"" >> ""$GITHUB_ENV""
    - name: Cache Python 3.next 
      id: cachenext
      if: ${{ matrix.python-impl == 'cpython' && matrix.python-version == env.next }}
      uses: actions/cache@v3
      with:
        key: python-${{ env.next }}
        path: |
          ${{ env.PYENV_ROOT }}
    - name: Build and set up Python 3.next
      if: ${{ matrix.python-impl == 'cpython' && matrix.python-version == env.next && ! steps.cachenext.outputs.cache-hit }}
      # dl and build locally
      shell: bash
      run: |
        # Install build environment
        sudo apt-get install -y build-essential llvm libssl-dev tk-dev  \
                      libncursesw5-dev libreadline-dev libsqlite3-dev   \
                      libffi-dev xz-utils zlib1g-dev libbz2-dev liblzma-dev
        # Download PyEnv from its GitHub repository.
        export PYENV_ROOT=${{ env.PYENV_ROOT }}
        export PATH=$PYENV_ROOT/bin:$PATH
        git clone ""https://github.com/pyenv/pyenv.git"" ""$PYENV_ROOT""
        pyenv install ${{ env.next }}
    - name: Locate Python 3.next
      if: ${{ matrix.python-impl == 'cpython' && matrix.python-version == env.next }}
      shell: bash
      run: |
        PYTHONHOME=""$(echo ""${{ env.PYENV_ROOT }}/versions/${{ env.next }}.""*)""
        test -n ""$PYTHONHOME""
        echo ""PYTHONHOME=$PYTHONHOME"" >> ""$GITHUB_ENV""
        echo ""PATH=${PYTHONHOME}/bin:$PATH"" >> ""$GITHUB_ENV""
    #-------- Python 2.7 --
    - name: Set up Python 2.7
      if: ${{ matrix.python-impl == 'cpython' && matrix.python-version == '2.7' }}
      # install 2.7
      shell: bash
      run: |
        # Ubuntu 22.04 no longer has python-is-python2: fetch it
        curl -L ""http://launchpadlibrarian.net/474693132/python-is-python2_2.7.17-4_all.deb"" -o python-is-python2.deb
        sudo apt-get install -y python2
        sudo dpkg --force-breaks -i python-is-python2.deb
        echo ""PYTHONHOME=/usr"" >> ""$GITHUB_ENV""
    #-------- Python 2.6 --
    - name: Set up Python 2.6 environment
      if: ${{ matrix.python-impl == 'cpython' && matrix.python-version == '2.6' }}
      shell: bash
      run: |
        openssl_name=openssl-1.0.2u
        echo ""openssl_name=${openssl_name}"" >> ""$GITHUB_ENV""
        openssl_dir=$HOME/.local/opt/$openssl_name
        echo ""openssl_dir=${openssl_dir}"" >> ""$GITHUB_ENV""
        PYENV_ROOT=$HOME/.local/share/pyenv
        echo ""PYENV_ROOT=${PYENV_ROOT}"" >> ""$GITHUB_ENV""
        sudo apt-get install -y openssl ca-certificates
    - name: Cache Python 2.6
      id: cache26
      if: ${{ matrix.python-version == '2.6' }}
      uses: actions/cache@v3
      with:
        key: python-2.6.9
        path: |
          ${{ env.openssl_dir }}
          ${{ env.PYENV_ROOT }}
    - name: Build and set up Python 2.6
      if: ${{ matrix.python-impl == 'cpython' && matrix.python-version == '2.6' && ! steps.cache26.outputs.cache-hit }}
      # dl and build locally
      shell: bash
      run: |
        # Install build environment
        sudo apt-get install -y build-essential llvm libssl-dev tk-dev  \
                      libncursesw5-dev libreadline-dev libsqlite3-dev   \
                      libffi-dev xz-utils zlib1g-dev libbz2-dev liblzma-dev
        # Download and install OpenSSL 1.0.2, back in time
        openssl_name=${{ env.openssl_name }}
        openssl_targz=${openssl_name}.tar.gz
        openssl_dir=${{ env.openssl_dir }}
        openssl_inc=$openssl_dir/include
        openssl_lib=$openssl_dir/lib
        openssl_ssl=$openssl_dir/ssl
        curl -L ""https://www.openssl.org/source/$openssl_targz"" -o $openssl_targz
        tar -xf $openssl_targz
        ( cd $openssl_name; \
          ./config --prefix=$openssl_dir --openssldir=${openssl_dir}/ssl \
            --libdir=lib -Wl,-rpath=${openssl_dir}/lib shared zlib-dynamic && \
          make && \
          make install )
        rm -rf $openssl_name
        rmdir $openssl_ssl/certs && ln -s /etc/ssl/certs $openssl_ssl/certs
        # Download PyEnv from its GitHub repository.
        export PYENV_ROOT=${{ env.PYENV_ROOT }}
        export PATH=$PYENV_ROOT/bin:$PATH
        git clone ""https://github.com/pyenv/pyenv.git"" ""$PYENV_ROOT""
        # Prevent pyenv build trying (and failing) to update pip
        export GET_PIP=get-pip-2.6.py
        echo 'import sys; sys.exit(0)' > ${GET_PIP}
        GET_PIP=$(realpath $GET_PIP)
        # Build and install Python
        export CFLAGS=""-I$openssl_inc""
        export LDFLAGS=""-L$openssl_lib""
        export LD_LIBRARY_PATH=""$openssl_lib""
        pyenv install 2.6.9
    - name: Locate Python 2.6
      if: ${{ matrix.python-impl == 'cpython' && matrix.python-version == '2.6' }}
      shell: bash
      run: |
        PYTHONHOME=""${{ env.PYENV_ROOT }}/versions/2.6.9""
        echo ""PYTHONHOME=$PYTHONHOME"" >> ""$GITHUB_ENV""
        echo ""PATH=${PYTHONHOME}/bin:$PATH"" >> ""$GITHUB_ENV""
        echo ""LD_LIBRARY_PATH=${{ env.openssl_dir }}/lib${LD_LIBRARY_PATH:+:}${LD_LIBRARY_PATH}"" >> ""$GITHUB_ENV""
    #-------- Jython ------
    - name: Set up Java 8
      if: ${{ matrix.python-impl == 'jython' }}
      uses: actions/setup-java@v3
      with:
        java-version: 8
        distribution: 'zulu'
    - name: Setup Jython environment
      if: ${{ matrix.python-impl == 'jython' }}
      shell: bash
      run: |
        echo ""JYTHON_ROOT=${HOME}/jython"" >> ""$GITHUB_ENV""
        echo ""PIP=pip"" >> ""$GITHUB_ENV""
    - name: Cache Jython
      id: cachejy
      if: ${{ matrix.python-impl == 'jython' && matrix.python-version == '2.7' }}
      uses: actions/cache@v3
      with:
        # 2.7.3 now available, may solve SNI issue
        key: jython-2.7.1
        path: |
          ${{ env.JYTHON_ROOT }}
    - name: Install Jython
      if: ${{ matrix.python-impl == 'jython' && matrix.python-version == '2.7' && ! steps.cachejy.outputs.cache-hit }}
      shell: bash
      run: |
        JYTHON_ROOT=""${{ env.JYTHON_ROOT }}""
        curl -L ""https://repo1.maven.org/maven2/org/python/jython-installer/2.7.1/jython-installer-2.7.1.jar"" -o jython-installer.jar
        java -jar jython-installer.jar -s -d ""${JYTHON_ROOT}""
        echo ""${JYTHON_ROOT}/bin"" >> ""$GITHUB_PATH""
    - name: Set up cached Jython
      if: ${{ steps.cachejy.outputs.cache-hit }}
      shell: bash
      run: |
        JYTHON_ROOT=""${{ env.JYTHON_ROOT }}""
        echo ""${JYTHON_ROOT}/bin"" >> $GITHUB_PATH
    - name: Install supporting Python 2.7 if possible
      if: ${{ steps.cachejy.outputs.cache-hit }}
      shell: bash
      run: |
        sudo apt-get install -y python2.7 || true
    #-------- pip ---------
    - name: Set up supported Python ${{ matrix.python-version }} pip
      if: ${{ (matrix.python-version != '3.2' && steps.setup-python.outputs.python-path) || matrix.python-version == '2.7' }}
      # This step may run in either Linux or Windows
      shell: bash
      run: |
        echo ""$PATH""
        echo ""$PYTHONHOME""
        # curl is available on both Windows and Linux, -L follows redirects, -O gets name
        python -m ensurepip || python -m pip --version || { \
          get_pip=""${{ contains(needs.select.outputs.own-pip-versions, matrix.python-version) && format('{0}/', matrix.python-version) || '' }}""; \
          curl -L -O ""https://bootstrap.pypa.io/pip/${get_pip}get-pip.py""; \
          python get-pip.py; }
    - name: Set up Python 2.6 pip
      if: ${{ matrix.python-version == '2.6' }}
      shell: bash
      run: |
        python -m pip --version || { \
          curl -L -O ""https://bootstrap.pypa.io/pip/2.6/get-pip.py""; \
          curl -L -O ""https://files.pythonhosted.org/packages/ac/95/a05b56bb975efa78d3557efa36acaf9cf5d2fd0ee0062060493687432e03/pip-9.0.3-py2.py3-none-any.whl""; \
          python get-pip.py --no-setuptools --no-wheel pip-9.0.3-py2.py3-none-any.whl; }
        # work-around to invoke pip module on 2.6: https://bugs.python.org/issue2751
        echo ""PIP=python -m pip.__main__"" >> ""$GITHUB_ENV""
    - name: Set up other Python ${{ matrix.python-version }} pip
      if: ${{ matrix.python-version == '3.2' && steps.setup-python.outputs.python-path }}
      shell: bash
      run: |
        python -m pip --version || { \
          curl -L -O ""https://bootstrap.pypa.io/pip/3.2/get-pip.py""; \
          curl -L -O ""https://files.pythonhosted.org/packages/b2/d0/cd115fe345dd6f07ec1c780020a7dfe74966fceeb171e0f20d1d4905b0b7/pip-7.1.2-py2.py3-none-any.whl""; \
          python get-pip.py --no-setuptools --no-wheel pip-7.1.2-py2.py3-none-any.whl; }
    #-------- unittest ----
    - name: Upgrade Unittest for Python 2.6
      if: ${{ matrix.python-version == '2.6' }}
      shell: bash
      run: |
        # Work around deprecation of support for non-SNI clients at PyPI CDN (see https://status.python.org/incidents/hzmjhqsdjqgb)
        $PIP -qq show unittest2 || { \
          for u in ""65/26/32b8464df2a97e6dd1b656ed26b2c194606c16fe163c695a992b36c11cdf/six-1.13.0-py2.py3-none-any.whl"" \
              ""f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl"" \
              ""c7/a3/c5da2a44c85bfbb6eebcfc1dde24933f8704441b98fdde6528f4831757a6/linecache2-1.0.0-py2.py3-none-any.whl"" \
              ""17/0a/6ac05a3723017a967193456a2efa0aa9ac4b51456891af1e2353bb9de21e/traceback2-1.4.0-py2.py3-none-any.whl"" \
              ""72/20/7f0f433060a962200b7272b8c12ba90ef5b903e218174301d0abfd523813/unittest2-1.1.0-py2.py3-none-any.whl""; do \
            curl -L -O ""https://files.pythonhosted.org/packages/${u}""; \
            $PIP install ${u##*/}; \
          done; }
        # make tests use unittest2
        for test in ./test/test_*.py ./test/helper.py; do
          sed -r -i -e '/^import unittest$/s/test/test2 as unittest/' ""$test""
        done
    #-------- nose --------
    - name: Install nose for Python ${{ matrix.python-version }}
      if: ${{ (matrix.python-version != '3.2' && steps.setup-python.outputs.python-path) || (matrix.python-impl == 'cpython' && (matrix.python-version == '2.7' || matrix.python-version == env.next)) }}
      shell: bash
      run: |
        echo ""$PATH""
        echo ""$PYTHONHOME""
        # Use PyNose for recent Pythons instead of Nose
        py3ver=""${{ matrix.python-version }}""
        py3ver=${py3ver#3.}
        [ ""$py3ver"" != ""${{ matrix.python-version }}"" ] && py3ver=${py3ver%.*} || py3ver=0
        [ ""$py3ver"" -ge 9 ] && nose=pynose || nose=nose
        $PIP -qq show $nose || $PIP install $nose
    - name: Install nose for other Python 2
      if: ${{ matrix.python-impl == 'jython' || (matrix.python-impl == 'cpython' && matrix.python-version == '2.6') }}
      shell: bash
      run: |
        # Work around deprecation of support for non-SNI clients at PyPI CDN (see https://status.python.org/incidents/hzmjhqsdjqgb)
        $PIP -qq show nose || { \
          curl -L -O ""https://files.pythonhosted.org/packages/99/4f/13fb671119e65c4dce97c60e67d3fd9e6f7f809f2b307e2611f4701205cb/nose-1.3.7-py2-none-any.whl""; \
          $PIP install nose-1.3.7-py2-none-any.whl; }
    - name: Install nose for other Python 3
      if: ${{ matrix.python-version == '3.2' && steps.setup-python.outputs.python-path }}
      shell: bash
      run: |
        $PIP -qq show nose || { \
          curl -L -O ""https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl""; \
          $PIP install nose-1.3.7-py3-none-any.whl; }
    - name: Set up nosetest test
      if: ${{ contains(needs.select.outputs.test-set, matrix.ytdl-test-set ) }}
      shell: bash
      run: |
        # set PYTHON_VER
        PYTHON_VER=${{ matrix.python-version }}
        [ ""${PYTHON_VER#*-}"" != ""$PYTHON_VER"" ] || PYTHON_VER=""${{ matrix.python-impl }}-${PYTHON_VER}""
        echo ""PYTHON_VER=$PYTHON_VER"" >> ""$GITHUB_ENV""
        echo ""PYTHON_IMPL=${{ matrix.python-impl }}"" >> ""$GITHUB_ENV""
        # define a test to validate the Python version used by nosetests
        printf '%s\n' \
          'from __future__ import unicode_literals' \
          'import sys, os, platform' \
          'try:' \
          '    import unittest2 as unittest' \
          'except ImportError:' \
          '    import unittest' \
          'class TestPython(unittest.TestCase):' \
          '    def setUp(self):' \
          '        self.ver = os.environ[""PYTHON_VER""].split(""-"")' \
          '    def test_python_ver(self):' \
          '        self.assertEqual([""%d"" % v for v in sys.version_info[:2]], self.ver[-1].split(""."")[:2])' \
          '        self.assertTrue(sys.version.startswith(self.ver[-1]))' \
          '        self.assertIn(self.ver[0], "","".join((sys.version, platform.python_implementation())).lower())' \
          '    def test_python_impl(self):' \
          '        self.assertIn(platform.python_implementation().lower(), (os.environ[""PYTHON_IMPL""], self.ver[0]))' \
          > test/test_python.py
    #-------- TESTS -------
    - name: Run tests
      if: ${{ contains(needs.select.outputs.test-set, matrix.ytdl-test-set ) }}
      continue-on-error: ${{ matrix.ytdl-test-set == 'download' || matrix.python-impl == 'jython' }}
      env:
        YTDL_TEST_SET: ${{ matrix.ytdl-test-set }}
      run: |
        ./devscripts/run_tests.${{ matrix.run-tests-ext }}
  flake8:
    name: Linter
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
    - name: Install flake8
      run: pip install flake8
    - name: Run flake8
      run: flake8 .

",482,3,3,"push, pull_request, workflow_dispatch",8
vercel/next.js,build_and_deploy.yml,"# Update all mentions of this name in vercel-packages when changing.
name: build-and-deploy

on:
  push:
  # we need the preview tarball for deploy tests
  pull_request:
    types: [opened, synchronize]
  workflow_dispatch:

env:
  NAPI_CLI_VERSION: 2.18.4
  TURBO_VERSION: 2.3.3
  NODE_LTS_VERSION: 20
  CARGO_PROFILE_RELEASE_LTO: 'true'
  TURBO_TEAM: 'vercel'
  TURBO_CACHE: 'remote:rw'
  # Without this environment variable, rust-lld will fail because some dependencies defaults to newer version of macOS by default.
  #
  # See https://doc.rust-lang.org/rustc/platform-support/apple-darwin.html#os-version for more details
  MACOSX_DEPLOYMENT_TARGET: 11.0
  # This will become ""true"" if the latest commit (merged release PR) is either:
  # - ""Version Packages (#<number>)""
  # - ""Version Pacakges (canary/rc) (#<number>)""
  # set from scripts/check-is-release.js
  __NEW_RELEASE: 'false'

jobs:
  deploy-target:
    runs-on: ubuntu-latest
    # Don't trigger this job on `pull_request` events from upstream branches.
    # Those would already run this job on the `push` event
    if: ${{ github.event_name != 'pull_request' || github.event.pull_request.head.repo.fork }}
    outputs:
      value: ${{ steps.deploy-target.outputs.value }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 1
      - run: echo ""${{ github.event.after }}""
      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_LTS_VERSION }}
          check-latest: true
      - name: Setup corepack
        run: |
          npm i -g corepack@0.31
          corepack enable
      - name: Determine deploy target
        # 'force-preview' performs a full preview build but only if acknowledged i.e. workflow_dispatch
        # 'automated-preview' for pushes on branches other than 'canary' for integration testing.
        # 'staging' for canary branch since that will eventually be published i.e. become the production build.
        id: deploy-target
        run: |
          # TODO: Remove the new release check once the new release workflow is fully replaced.
          RELEASE_CHECK=$(node ./scripts/check-is-release.js 2> /dev/null || :)
          if [[ $RELEASE_CHECK == 'new-release' ]];
          then
            echo ""__NEW_RELEASE=true"" >> $GITHUB_ENV
            echo ""value=production"" >> $GITHUB_OUTPUT
          elif [[ $RELEASE_CHECK == v* ]];
          then
            echo ""value=production"" >> $GITHUB_OUTPUT
          elif [ '${{ github.ref }}' == 'refs/heads/canary' ]
          then
            echo ""value=staging"" >> $GITHUB_OUTPUT
          elif [ '${{ github.event_name }}' == 'workflow_dispatch' ]
          then
            echo ""value=force-preview"" >> $GITHUB_OUTPUT
          elif [[ $(node scripts/run-for-change.js --not --type docs --exec echo 'false') != 'false' ]];
          then
            echo ""value=skipped"" >> $GITHUB_OUTPUT
          else
            echo ""value=automated-preview"" >> $GITHUB_OUTPUT
          fi
      - name: Print deploy target
        run: echo ""Deploy target is '${{ steps.deploy-target.outputs.value }}'""

  build:
    if: ${{ needs.deploy-target.outputs.value != 'skipped' }}
    needs:
      - deploy-target
    runs-on: ubuntu-latest
    env:
      NEXT_TELEMETRY_DISABLED: 1
      # we build a dev binary for use in CI so skip downloading
      # canary next-swc binaries in the monorepo
      NEXT_SKIP_NATIVE_POSTINSTALL: 1
    steps:
      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_LTS_VERSION }}
          check-latest: true
      - name: Setup corepack
        run: |
          npm i -g corepack@0.31
          corepack enable

      - uses: actions/checkout@v4
        with:
          fetch-depth: 25

      - id: get-store-path
        run: echo STORE_PATH=$(pnpm store path) >> $GITHUB_OUTPUT

      - uses: actions/cache@v4
        timeout-minutes: 5
        id: cache-pnpm-store
        with:
          path: ${{ steps.get-store-path.outputs.STORE_PATH }}
          key: pnpm-store-${{ hashFiles('pnpm-lock.yaml') }}
          restore-keys: |
            pnpm-store-
            pnpm-store-${{ hashFiles('pnpm-lock.yaml') }}

      - run: pnpm install

      - run: pnpm run build

      - uses: actions/cache@v4
        timeout-minutes: 5
        id: cache-build
        with:
          path: ./*
          key: ${{ github.sha }}-${{ github.run_number }}-${{ github.run_attempt}}

  # Build binaries for publishing
  build-native:
    if: ${{ needs.deploy-target.outputs.value != 'skipped' }}
    needs:
      - deploy-target
    defaults:
      run:
        shell: bash -leo pipefail {0}

    strategy:
      fail-fast: false
      matrix:
        exclude:
          # only build the binaries we run automated tests against
          # linux GNU x64
          - settings:
              target: ${{ needs.deploy-target.outputs.value == 'automated-preview' && 'aarch64-apple-darwin' }}
          - settings:
              target: ${{ needs.deploy-target.outputs.value == 'automated-preview' && 'aarch64-pc-windows-msvc' }}
          - settings:
              target: ${{ needs.deploy-target.outputs.value == 'automated-preview' && 'aarch64-unknown-linux-gnu' }}
          - settings:
              target: ${{ needs.deploy-target.outputs.value == 'automated-preview' && 'aarch64-unknown-linux-musl' }}
          - settings:
              target: ${{ needs.deploy-target.outputs.value == 'automated-preview' && 'x86_64-pc-windows-msvc' }}
          - settings:
              target: ${{ needs.deploy-target.outputs.value == 'automated-preview' && 'x86_64-unknown-linux-musl' }}
          - settings:
              target: ${{ needs.deploy-target.outputs.value == 'automated-preview' && 'x86_64-apple-darwin' }}

        settings:
          - host:
              - 'self-hosted'
              - 'macos'
              - 'arm64'

            target: 'x86_64-apple-darwin'
            # --env-mode loose is a breaking change required with turbo 2.x since Strict mode is now the default
            # TODO: we should add the relevant envs later to to switch to strict mode
            build: |
              npm i -g ""@napi-rs/cli@${NAPI_CLI_VERSION}"" && corepack enable
              pnpm dlx turbo@${TURBO_VERSION} run build-native-release -vvv --env-mode loose --remote-cache-timeout 90 --summarize -- --target x86_64-apple-darwin
              strip -x packages/next-swc/native/next-swc.*.node

          - host:
              - 'self-hosted'
              - 'macos'
              - 'arm64'

            target: 'aarch64-apple-darwin'
            # --env-mode loose is a breaking change required with turbo 2.x since Strict mode is now the default
            # TODO: we should add the relevant envs later to to switch to strict mode
            build: |
              npm i -g ""@napi-rs/cli@${NAPI_CLI_VERSION}"" && corepack enable
              pnpm dlx turbo@${TURBO_VERSION} run build-native-release -vvv --env-mode loose --remote-cache-timeout 90 --summarize -- --target aarch64-apple-darwin
              strip -x packages/next-swc/native/next-swc.*.node

          - host:
              - 'self-hosted'
              - 'windows'
              - 'x64'

            # --env-mode loose is a breaking change required with turbo 2.x since Strict mode is now the default
            # TODO: we should add the relevant envs later to to switch to strict mode
            build: |
              corepack enable
              npm i -g ""@napi-rs/cli@${NAPI_CLI_VERSION}""
              pnpm dlx turbo@${TURBO_VERSION} run build-native-release -vvv --env-mode loose --remote-cache-timeout 90 --summarize -- --target x86_64-pc-windows-msvc
            target: 'x86_64-pc-windows-msvc'

          - host:
              - 'self-hosted'
              - 'windows'
              - 'x64'

            target: 'aarch64-pc-windows-msvc'
            # --env-mode loose is a breaking change required with turbo 2.x since Strict mode is now the default
            # TODO: we should add the relevant envs later to to switch to strict mode
            build: |
              corepack enable
              npm i -g ""@napi-rs/cli@${NAPI_CLI_VERSION}""
              pnpm dlx turbo@${TURBO_VERSION} run build-native-no-plugin-release -vvv --env-mode loose --remote-cache-timeout 90 --summarize -- --target aarch64-pc-windows-msvc

          - host:
              - 'self-hosted'
              - 'linux'
              - 'x64'
              - 'metal'

            target: 'x86_64-unknown-linux-gnu'
            # [NOTE] If you want to update / modify build steps, check these things:
            # - We use docker images to pin the glibc version to link against,
            #   even if host target is identical to the container image (i.e host: x64-linux, image: x64-linux)
            # - After build `objdump -T` prints out the glibc version next-swc is linked against,
            #   to ensure it did not change unexpectedly if docker image, or other dependency changed
            # - zig linker with portable glibc is avoided as it has known issues with static tls + node.js + multi threaded
            #   environment.
            docker: ghcr.io/napi-rs/napi-rs/nodejs-rust:stable-2023-09-17-x64
            build: >-
              set -ex &&
              apt update &&
              apt install -y pkg-config xz-utils dav1d libdav1d-dev &&
              rustup show &&
              rustup target add x86_64-unknown-linux-gnu &&
              npm i -g ""@napi-rs/cli@${NAPI_CLI_VERSION}"" &&
              unset CC_x86_64_unknown_linux_gnu && unset CC &&
              cd packages/next-swc && npm run build-native-release -- --target x86_64-unknown-linux-gnu &&
              strip native/next-swc.*.node &&
              objdump -T native/next-swc.*.node | grep GLIBC_

          - host:
              - 'self-hosted'
              - 'linux'
              - 'x64'
              - 'metal'

            target: 'x86_64-unknown-linux-musl'
            docker: ghcr.io/napi-rs/napi-rs/nodejs-rust:stable-2023-09-17-alpine
            build: >-
              set -ex &&
              apk update &&
              apk add --no-cache libc6-compat pkgconfig dav1d libdav1d dav1d-dev clang-static llvm-dev &&
              rustup show &&
              rustup target add x86_64-unknown-linux-musl &&
              npm i -g ""@napi-rs/cli@${NAPI_CLI_VERSION}"" &&
              export RUSTFLAGS='--cfg tokio_unstable -Zshare-generics=y -Zthreads=8 -Csymbol-mangling-version=v0 -Ctarget-feature=-crt-static' &&
              cd packages/next-swc && npm run build-native-release -- --target x86_64-unknown-linux-musl &&
              strip native/next-swc.*.node

          - host:
              - 'self-hosted'
              - 'linux'
              - 'x64'
              - 'metal'

            target: 'aarch64-unknown-linux-gnu'
            docker: ghcr.io/napi-rs/napi-rs/nodejs-rust:stable-2023-09-17-aarch64
            build: >-
              set -ex &&
              apt update &&
              apt install -y pkg-config xz-utils dav1d libdav1d-dev &&
              export JEMALLOC_SYS_WITH_LG_PAGE=16 &&
              rustup show &&
              rustup target add aarch64-unknown-linux-gnu &&
              npm i -g ""@napi-rs/cli@${NAPI_CLI_VERSION}"" &&
              export CC_aarch64_unknown_linux_gnu=/usr/bin/clang &&
              export CFLAGS_aarch64_unknown_linux_gnu=\""--target=aarch64-unknown-linux-gnu --sysroot=/usr/aarch64-unknown-linux-gnu\"" &&
              cd packages/next-swc && npm run build-native-release -- --target aarch64-unknown-linux-gnu &&
              llvm-strip -x native/next-swc.*.node &&
              objdump -T native/next-swc.*.node | grep GLIBC_

          - host:
              - 'self-hosted'
              - 'linux'
              - 'x64'
              - 'metal'

            target: 'aarch64-unknown-linux-musl'
            docker: ghcr.io/napi-rs/napi-rs/nodejs-rust:stable-2023-09-17-alpine
            build: >-
              set -ex &&
              apk update &&
              apk add --no-cache libc6-compat pkgconfig dav1d libdav1d dav1d-dev clang-static llvm-dev &&
              export JEMALLOC_SYS_WITH_LG_PAGE=16 &&
              npm i -g ""@napi-rs/cli@${NAPI_CLI_VERSION}"" &&
              rustup show &&
              rustup target add aarch64-unknown-linux-musl &&
              export RUSTFLAGS='--cfg tokio_unstable -Zshare-generics=y -Zthreads=8 -Zunstable-options -Csymbol-mangling-version=v0 -Clinker-flavor=gnu-lld-cc -Clink-self-contained=+linker' &&
              cd packages/next-swc && npm run build-native-release -- --target aarch64-unknown-linux-musl &&
              llvm-strip -x native/next-swc.*.node

    name: stable - ${{ matrix.settings.target }} - node@16
    runs-on: ${{ matrix.settings.host }}
    timeout-minutes: 45
    steps:
      # https://github.com/actions/virtual-environments/issues/1187
      - name: tune linux network
        run: sudo ethtool -K eth0 tx off rx off
        if: ${{ matrix.settings.host == 'ubuntu-latest' }}
      - name: tune linux network
        run: sudo ethtool -K eth0 tx off rx off
        if: ${{ matrix.settings.host == 'ubuntu-latest' }}
      - name: tune windows network
        run: Disable-NetAdapterChecksumOffload -Name * -TcpIPv4 -UdpIPv4 -TcpIPv6 -UdpIPv6
        if: ${{ matrix.settings.host == 'windows-latest' }}
      - name: tune mac network
        run: sudo sysctl -w net.link.generic.system.hwcksum_tx=0 && sudo sysctl -w net.link.generic.system.hwcksum_rx=0
        if: ${{ matrix.settings.host == 'macos-latest' }}
      # we use checkout here instead of the build cache since
      # it can fail to restore in different OS'
      - uses: actions/checkout@v4
        with:
          # crates/napi/build.rs uses git-describe to find the most recent git tag. It's okay if
          # this fails, but fetch with enough depth that we're likely to find a recent tag.
          fetch-depth: 100

      - name: Setup node
        uses: actions/setup-node@v4
        if: ${{ !matrix.settings.docker }}
        with:
          node-version: ${{ env.NODE_LTS_VERSION }}
          check-latest: true

      - name: Setup corepack
        run: |
          npm i -g corepack@0.31
          corepack enable

      # we always want to run this to set environment variables
      - name: Install Rust
        uses: ./.github/actions/setup-rust
        with:
          targets: ${{ matrix.settings.target }}

      - name: normalize versions
        run: node scripts/normalize-version-bump.js

      - name: Setup toolchain
        run: ${{ matrix.settings.setup }}
        if: ${{ matrix.settings.setup }}

      - name: Cache on ${{ github.ref_name }}
        uses: ijjk/rust-cache@turbo-cache-v1.0.8
        with:
          save-if: 'true'
          cache-provider: 'turbo'
          shared-key: build-${{ matrix.settings.target }}-${{ hashFiles('.cargo/config.toml') }}

      - name: Clear native build
        run: rm -rf packages/next-swc/native

      # we only need custom caching for docker builds
      # as they are on an older Node.js version and have
      # issues with turbo caching
      - name: pull build cache
        if: ${{ matrix.settings.docker }}
        run: TURBO_VERSION=${TURBO_VERSION} node ./scripts/pull-turbo-cache.js ${{ matrix.settings.target }}

      - name: check build exists
        if: ${{ matrix.settings.docker }}
        run: if [ -f packages/next-swc/native/next-swc.*.node ]; then echo ""BUILD_EXISTS=yes"" >> $GITHUB_OUTPUT; else echo ""BUILD_EXISTS=no"" >> $GITHUB_OUTPUT; fi
        id: build-exists

      - name: Build in docker
        if: ${{ matrix.settings.docker && steps.build-exists.outputs.BUILD_EXISTS == 'no' }}
        run: |
          docker run -v ""/var/run/docker.sock"":""/var/run/docker.sock"" \
            -e CI -e RUST_BACKTRACE -e NAPI_CLI_VERSION -e CARGO_TERM_COLOR -e CARGO_INCREMENTAL \
            -e CARGO_PROFILE_RELEASE_LTO -e CARGO_REGISTRIES_CRATES_IO_PROTOCOL -e TURBO_API \
            -e TURBO_TEAM -e TURBO_TOKEN -e TURBO_VERSION -e TURBO_CACHE=""remote:rw"" \
            -v ${{ env.HOME }}/.cargo/git:/root/.cargo/git \
            -v ${{ env.HOME }}/.cargo/registry:/root/.cargo/registry \
            -v ${{ github.workspace }}:/build \
            -w /build \
            --entrypoint=bash ${{ matrix.settings.docker }} \
            -c ""${{ matrix.settings.build }}""

      - name: cache build
        if: ${{ matrix.settings.docker && steps.build-exists.outputs.BUILD_EXISTS == 'no' }}
        run: pnpm dlx turbo@${TURBO_VERSION} run cache-build-native --force -- ${{ matrix.settings.target }}

      - name: 'Build'
        run: ${{ matrix.settings.build }}
        if: ${{ !matrix.settings.docker }}

      - name: 'check build cache status'
        id: check-did-build
        run: if [[ ! -z $(ls packages/next-swc/native) ]]; then echo ""DID_BUILD=true"" >> $GITHUB_OUTPUT; fi

      # Try to upload metrics for Turbopack to datadog's CI pipeline execution
      - name: 'Collect turbopack build metrics'
        id: check-turbopack-bytesize
        if: ${{ steps.check-did-build.outputs.DID_BUILD == 'true' }}
        continue-on-error: true
        run: |
          mkdir -p ./turbopack-bin-size
          shopt -s nullglob
          for filename in packages/next-swc/native/next-swc.*.node; do
            # Strip out filename to extract target triple
            export FILENAME=$(basename ${filename})
            export FILENAME=${FILENAME#*.}
            export FILENAME=${FILENAME%.node}
            export BYTESIZE=$(wc -c < $filename | xargs)
            echo ""Reporting $FILENAME:$BYTESIZE for Turbopack bytesize""
            echo ""turbopack.bytesize.$FILENAME:$BYTESIZE"" > ./turbopack-bin-size/${{ matrix.settings.target }}
          done

      - name: Upload turbopack bytesize artifact
        if: ${{ steps.check-did-build.outputs.DID_BUILD == 'true' }}
        uses: actions/upload-artifact@v4
        with:
          name: turbopack-bytesize-${{ matrix.settings.target }}
          path: turbopack-bin-size/*

      - name: Upload swc artifact
        uses: actions/upload-artifact@v4
        with:
          name: next-swc-binaries-${{ matrix.settings.target }}
          path: packages/next-swc/native/next-swc.*.node

      - name: Upload turbo summary artifact
        uses: actions/upload-artifact@v4
        with:
          name: turbo-run-summary-${{ matrix.settings.target }}
          path: .turbo/runs

  build-wasm:
    if: ${{ needs.deploy-target.outputs.value != 'skipped' }}
    needs:
      - deploy-target
    strategy:
      matrix:
        target: [web, nodejs]

    runs-on:
      - 'self-hosted'
      - 'linux'
      - 'x64'
      - 'metal'

    steps:
      - uses: actions/checkout@v4

      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_LTS_VERSION }}
          check-latest: true
      - run: corepack enable

      - name: Install Rust
        uses: ./.github/actions/setup-rust
        with:
          targets: wasm32-unknown-unknown

      - name: Install wasm-pack
        run: curl https://rustwasm.github.io/wasm-pack/installer/init.sh -sSf | sh

      - name: normalize versions
        run: node scripts/normalize-version-bump.js

      - name: Build
        # --env-mode loose is a breaking change required with turbo 2.x since Strict mode is now the default
        # TODO: we should add the relevant envs later to to switch to strict mode
        run: pnpm dlx turbo@${TURBO_VERSION} run build-wasm -vvv --env-mode loose --remote-cache-timeout 90 --summarize -- --target ${{ matrix.target }}

      - name: Add target to folder name
        run: '[[ -d ""crates/wasm/pkg"" ]] && mv crates/wasm/pkg crates/wasm/pkg-${{ matrix.target }} || ls crates/wasm'

      - name: Upload turbo summary artifact
        uses: actions/upload-artifact@v4
        with:
          name: turbo-run-summary-wasm-${{matrix.target}}
          path: .turbo/runs

      - name: Upload swc artifact
        uses: actions/upload-artifact@v4
        with:
          name: wasm-binaries-${{matrix.target}}
          path: crates/wasm/pkg-*

  deploy-tarball:
    if: ${{ needs.deploy-target.outputs.value != 'production' }}
    name: Deploy preview tarball
    runs-on: ubuntu-latest
    needs:
      - deploy-target
      - build
      - build-wasm
      - build-native
    steps:
      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_LTS_VERSION }}
          check-latest: true
      - name: Setup corepack
        run: |
          npm i -g corepack@0.31
          corepack enable

      # https://github.com/actions/virtual-environments/issues/1187
      - name: tune linux network
        run: sudo ethtool -K eth0 tx off rx off

      - uses: actions/cache@v4
        timeout-minutes: 5
        id: restore-build
        with:
          path: ./*
          # Cache includes repo checkout which is required for later scripts
          fail-on-cache-miss: true
          key: ${{ github.sha }}-${{ github.run_number }}-${{ github.run_attempt }}
          restore-keys: |
            ${{ github.sha }}-${{ github.run_number }}
            ${{ github.sha }}-${{ github.run_number }}-${{ github.run_attempt}}

      - uses: actions/download-artifact@v4
        with:
          pattern: next-swc-binaries-*
          merge-multiple: true
          path: packages/next-swc/native

      - uses: actions/download-artifact@v4
        with:
          pattern: wasm-binaries-*
          merge-multiple: true
          path: crates/wasm

      - name: Create tarballs
        # github.event.after is available on push and pull_request#synchronize events.
        # For workflow_dispatch events, github.sha is the head commit.
        run: node scripts/create-preview-tarballs.js ""${{ github.sha }}"" ""${{ github.event.after || github.sha }}"" ""${{ runner.temp }}/preview-tarballs""

      - name: Upload tarballs
        uses: actions/upload-artifact@v4
        with:
          # Update all mentions of this name in vercel-packages when changing.
          name: preview-tarballs
          path: ${{ runner.temp }}/preview-tarballs/*

  publishRelease:
    if: ${{ needs.deploy-target.outputs.value == 'production' }}
    name: Potentially publish release
    runs-on: ubuntu-latest
    needs:
      - deploy-target
      - build
      - build-wasm
      - build-native
    permissions:
      contents: write
      id-token: write
    env:
      NPM_TOKEN: ${{ secrets.NPM_TOKEN_ELEVATED }}
    steps:
      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_LTS_VERSION }}
          check-latest: true
      - name: Setup corepack
        run: |
          npm i -g corepack@0.31
          corepack enable

      # https://github.com/actions/virtual-environments/issues/1187
      - name: tune linux network
        run: sudo ethtool -K eth0 tx off rx off

      - uses: actions/cache@v4
        timeout-minutes: 5
        id: restore-build
        with:
          path: ./*
          # Cache includes repo checkout which is required for later scripts
          fail-on-cache-miss: true
          key: ${{ github.sha }}-${{ github.run_number }}-${{ github.run_attempt }}
          restore-keys: |
            ${{ github.sha }}-${{ github.run_number }}
            ${{ github.sha }}-${{ github.run_number }}-${{ github.run_attempt}}

      - uses: actions/download-artifact@v4
        with:
          pattern: next-swc-binaries-*
          merge-multiple: true
          path: packages/next-swc/native

      - uses: actions/download-artifact@v4
        with:
          pattern: wasm-binaries-*
          merge-multiple: true
          path: crates/wasm

      - run: npm i -g npm@10.4.0 # need latest version for provenance (pinning to avoid bugs)
      - run: echo ""//registry.npmjs.org/:_authToken=$NPM_TOKEN"" >> ~/.npmrc
      - run: ./scripts/publish-native.js
      # Legacy release process
      - run: ./scripts/publish-release.js
        if: ${{ env.__NEW_RELEASE == 'false' }}
        env:
          RELEASE_BOT_GITHUB_TOKEN: ${{ secrets.RELEASE_BOT_GITHUB_TOKEN }}

      # New release process
      - name: Publish to NPM
        id: changesets
        # TODO: Change to IS_RELEASE condition when new release becomes stable.
        if: ${{ env.__NEW_RELEASE == 'true' }}
        uses: changesets/action@v1
        with:
          publish: pnpm ci:publish
        env:
          GITHUB_TOKEN: ${{ secrets.RELEASE_BOT_GITHUB_TOKEN }}
          NPM_TOKEN: ${{ secrets.NPM_TOKEN_ELEVATED }}

      - name: Send a Slack notification of the publish status
        # TODO: Change to IS_RELEASE condition when new release becomes stable.
        if: ${{ env.__NEW_RELEASE == 'true' && (steps.changesets.outputs.published == 'true' || steps.changesets.outputs.published == 'false') }}
        run: pnpm tsx scripts/release/slack.ts
        env:
          SLACK_TOKEN: ${{ secrets.SLACK_TOKEN }}
          RELEASE_STATUS: ${{ steps.changesets.outputs.published }}
          WORKFLOW_LINK: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
          WORKFLOW_ACTOR: ${{ github.actor }}

      - name: Upload npm log artifact
        if: steps.changesets.outputs.published == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: npm-publish-logs
          path: /home/runner/.npm/_logs/*

  publish-turbopack-npm-packages:
    # Matches the commit message written by turbopack/xtask/src/publish.rs:377
    if: ""${{(github.ref == 'refs/heads/canary') && startsWith(github.event.head_commit.message, 'chore: release turbopack npm packages')}}""
    runs-on: ubuntu-latest
    permissions:
      contents: write
      id-token: write
    steps:
      - uses: actions/checkout@v3

      - uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_LTS_VERSION }}
          check-latest: true
      - name: Setup corepack
        run: |
          npm i -g corepack@0.31
          corepack enable

      - uses: ./.github/actions/setup-rust
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - run: pnpm install --frozen-lockfile

      - name: Build packages
        run: pnpx turbo@canary run build --only --filter='./turbopack/packages/*'

      - name: Write NPM_TOKEN
        run: echo ""//registry.npmjs.org/:_authToken=${{ secrets.NPM_TOKEN_ELEVATED }}"" > ~/.npmrc

      - name: Publish
        run: cargo xtask workspace --publish

  deployExamples:
    if: ${{ needs.deploy-target.outputs.value != 'automated-preview' }}
    name: Deploy examples
    runs-on: ubuntu-latest
    needs: [build, deploy-target]
    steps:
      - run: echo '${{ needs.deploy-target.outputs.value }}'
      - uses: actions/checkout@v4
        with:
          fetch-depth: 25
      - name: Install Vercel CLI
        run: npm i -g vercel@latest
      - name: Deploy preview examples
        if: ${{ needs.deploy-target.outputs.value != 'production' }}
        run: ./scripts/deploy-examples.sh
        env:
          VERCEL_API_TOKEN: ${{ secrets.VERCEL_API_TOKEN }}
          DEPLOY_ENVIRONMENT: preview
      - name: Deploy production examples
        if: ${{ needs.deploy-target.outputs.value == 'production' }}
        run: ./scripts/deploy-examples.sh
        env:
          VERCEL_API_TOKEN: ${{ secrets.VERCEL_API_TOKEN }}
          DEPLOY_ENVIRONMENT: production

  buildPassed:
    needs: ['deploy-target', 'build', 'build-wasm', 'build-native']
    if: ${{ always() && needs.deploy-target.outputs.value != '' }}
    name: thank you, build
    runs-on: ubuntu-latest
    steps:
      - run: exit 1
        if: ${{ always() && (contains(needs.*.result, 'failure') || contains(needs.*.result, 'cancelled')) }}

  releaseStats:
    name: Release Stats
    runs-on:
      - 'self-hosted'
      - 'linux'
      - 'x64'
      - 'metal'
    timeout-minutes: 25
    needs: [publishRelease]
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 25

      - uses: actions/download-artifact@v4
        with:
          pattern: next-swc-binaries-*
          merge-multiple: true
          path: packages/next-swc/native

      - run: cp -r packages/next-swc/native .github/actions/next-stats-action/native

      - run: ./scripts/release-stats.sh
      - uses: ./.github/actions/next-stats-action
        env:
          PR_STATS_COMMENT_TOKEN: ${{ secrets.PR_STATS_COMMENT_TOKEN }}
          NEXT_SKIP_NATIVE_POSTINSTALL: 1

  upload_turbopack_bytesize:
    if: ${{ needs.deploy-target.outputs.value != 'automated-preview'}}
    name: Upload Turbopack Bytesize metrics to Datadog
    runs-on: ubuntu-latest
    needs: [build-native, deploy-target]
    env:
      DATADOG_API_KEY: ${{ secrets.DATA_DOG_API_KEY }}
    steps:
      - name: Collect bytesize metrics
        uses: actions/download-artifact@v4
        with:
          pattern: turbopack-bytesize-*
          merge-multiple: true
          path: turbopack-bin-size

      - name: Upload to Datadog
        run: |
          ls -al turbopack-bin-size

          for filename in turbopack-bin-size/*; do
            export BYTESIZE+="" --metrics $(cat $filename)""
          done

          echo ""Reporting $BYTESIZE""

          npx @datadog/datadog-ci@2.23.1 metric --no-fail --level pipeline $BYTESIZE
",762,11,3,"push, pull_request, workflow_dispatch",37
vercel/next.js,build_and_test.yml,"name: build-and-test

on:
  push:
    branches: ['canary']
  pull_request:
    types: [opened, synchronize]

# NOTE: anything in `afterBuild` inherits environment variables defined in
# `build_reusable.yml` (not these!) because that job executes within the context
# of that workflow. Environment variables are not automatically passed to
# reusable workflows.
env:
  NODE_MAINTENANCE_VERSION: 18
  NODE_LTS_VERSION: 20

jobs:
  optimize-ci:
    uses: ./.github/workflows/graphite_ci_optimizer.yml
    secrets: inherit

  changes:
    name: Determine changes
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 25

      - name: check for docs only change
        id: docs-change
        run: |
          echo ""DOCS_ONLY<<EOF"" >> $GITHUB_OUTPUT;
          echo ""$(node scripts/run-for-change.js --not --type docs --exec echo 'false')"" >> $GITHUB_OUTPUT;
          echo 'EOF' >> $GITHUB_OUTPUT

      - name: check for release
        id: is-release
        run: |
          RELEASE_CHECK=$(node ./scripts/check-is-release.js 2> /dev/null || :)
          if [[ $RELEASE_CHECK == ""new-release"" || $RELEASE_CHECK == v* ]];
            then
              echo ""IS_RELEASE=true"" >> $GITHUB_OUTPUT
            else
              echo ""IS_RELEASE=false"" >> $GITHUB_OUTPUT
          fi

    outputs:
      docs-only: ${{ steps.docs-change.outputs.DOCS_ONLY != 'false' }}
      is-release: ${{ steps.is-release.outputs.IS_RELEASE == 'true' }}
      rspack: >-
        ${{
          github.event_name == 'pull_request' &&
          contains(github.event.pull_request.labels.*.name, 'Rspack')
        }}

  build-native:
    name: build-native
    uses: ./.github/workflows/build_reusable.yml
    needs: ['changes']
    if: ${{ needs.changes.outputs.docs-only == 'false' }}
    with:
      skipInstallBuild: 'yes'
      stepName: 'build-native'
    secrets: inherit

  build-native-windows:
    name: build-native-windows
    uses: ./.github/workflows/build_reusable.yml
    needs: ['changes']
    if: ${{ needs.changes.outputs.docs-only == 'false' }}
    with:
      skipInstallBuild: 'yes'
      stepName: 'build-native-windows'
      runs_on_labels: '[""windows"",""self-hosted"",""x64""]'
      buildNativeTarget: 'x86_64-pc-windows-msvc'

    secrets: inherit

  build-next:
    name: build-next
    uses: ./.github/workflows/build_reusable.yml
    with:
      skipNativeBuild: 'yes'
      stepName: 'build-next'
    secrets: inherit

  lint:
    name: lint
    needs: ['build-next']
    uses: ./.github/workflows/build_reusable.yml
    with:
      skipNativeBuild: 'yes'
      skipNativeInstall: 'yes'
      afterBuild: |
        pnpm lint-no-typescript
        pnpm check-examples
        pnpm validate-externals-doc
      stepName: 'lint'
    secrets: inherit

  validate-docs-links:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 18
      - name: Setup corepack
        run: |
          npm i -g corepack@0.31
          corepack enable
      - name: 'Run link checker'
        run: node ./.github/actions/validate-docs-links/dist/index.js
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  check-types-precompiled:
    name: types and precompiled
    needs: ['changes', 'build-native', 'build-next']

    uses: ./.github/workflows/build_reusable.yml
    with:
      afterBuild: pnpm types-and-precompiled
      stepName: 'types-and-precompiled'
    secrets: inherit

  test-cargo-unit:
    name: test cargo unit
    needs: ['changes', 'build-next']
    if: ${{ needs.changes.outputs.docs-only == 'false' }}

    uses: ./.github/workflows/build_reusable.yml
    with:
      needsRust: 'yes'
      needsNextest: 'yes'
      skipNativeBuild: 'yes'
      afterBuild: pnpm dlx turbo@${TURBO_VERSION} run test-cargo-unit
      mold: 'yes'
      stepName: 'test-cargo-unit'
    secrets: inherit

  test-bench:
    name: test cargo benches
    needs: ['optimize-ci', 'changes', 'build-next']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' }}

    uses: ./.github/workflows/test-turbopack-rust-bench-test.yml
    secrets: inherit

  rust-check:
    name: rust check
    needs: ['changes', 'build-next']
    if: ${{ needs.changes.outputs.docs-only == 'false' }}

    uses: ./.github/workflows/build_reusable.yml
    with:
      needsRust: 'yes'
      skipInstallBuild: 'yes'
      skipNativeBuild: 'yes'
      afterBuild: pnpm dlx turbo@${TURBO_VERSION} run rust-check
      stepName: 'rust-check'
    secrets: inherit

  rustdoc-check:
    name: rustdoc check
    needs: ['changes', 'build-next']
    if: ${{ needs.changes.outputs.docs-only == 'false' }}

    uses: ./.github/workflows/build_reusable.yml
    with:
      needsRust: 'yes'
      skipInstallBuild: 'yes'
      skipNativeBuild: 'yes'
      afterBuild: ./scripts/deploy-turbopack-docs.sh
      stepName: 'rustdoc-check'
    secrets: inherit

  ast-grep:
    needs: ['changes', 'build-next']
    runs-on: ubuntu-latest
    name: ast-grep lint
    steps:
      - uses: actions/checkout@v4
      - name: ast-grep lint step
        uses: ast-grep/action@v1.5.0
        with:
          # Keep in sync with the next.js repo's root package.json
          version: 0.31.0

  devlow-bench:
    name: Run devlow benchmarks
    needs: ['optimize-ci', 'changes', 'build-next', 'build-native']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' && !github.event.pull_request.head.repo.fork }}

    strategy:
      fail-fast: false
      matrix:
        mode:
          - '--turbopack=false'
          - '--turbopack=true'
        selector:
          - '--scenario=heavy-npm-deps-dev --page=homepage'
          - '--scenario=heavy-npm-deps-build --page=homepage'
          - '--scenario=heavy-npm-deps-build-turbo-cache-enabled --page=homepage'
    uses: ./.github/workflows/build_reusable.yml
    with:
      afterBuild: |
        ./node_modules/.bin/devlow-bench ./scripts/devlow-bench.mjs \
          --datadog=ubuntu-latest-16-core \
          ${{ matrix.mode }} \
          ${{ matrix.selector }}
      stepName: 'devlow-bench-${{ matrix.mode }}-${{ matrix.selector }}'
    secrets: inherit

  test-devlow:
    name: test devlow package
    needs: ['optimize-ci', 'changes']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' }}
    uses: ./.github/workflows/build_reusable.yml
    with:
      skipNativeBuild: 'yes'
      stepName: 'test-devlow'
      afterBuild: |
        pnpm run --filter=devlow-bench test
    secrets: inherit

  test-turbopack-dev:
    name: test turbopack dev
    needs: ['optimize-ci', 'changes', 'build-next', 'build-native']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' }}

    strategy:
      fail-fast: false
      matrix:
        exclude:
          # Excluding React 18 tests unless on `canary` branch until budget is approved.
          - react: ${{ github.event_name == 'pull_request' && !contains(github.event.pull_request.labels.*.name, 'run-react-18-tests') && '18.3.1' }}
        group: [1/5, 2/5, 3/5, 4/5, 5/5]
        # Empty value uses default
        react: ['', '18.3.1']
    uses: ./.github/workflows/build_reusable.yml
    with:
      afterBuild: |
        export NEXT_EXTERNAL_TESTS_FILTERS=""$(pwd)/test/turbopack-dev-tests-manifest.json""
        export IS_TURBOPACK_TEST=1
        export TURBOPACK_DEV=1
        export NEXT_TEST_MODE=dev
        export NEXT_TEST_REACT_VERSION=""${{ matrix.react }}""

        node run-tests.js \
          --test-pattern '^(test\/(development|e2e))/.*\.test\.(js|jsx|ts|tsx)$' \
          --timings \
          -g ${{ matrix.group }}
      stepName: 'test-turbopack-dev-react-${{ matrix.react }}-${{ matrix.group }}'
    secrets: inherit

  test-turbopack-integration:
    name: test turbopack development integration
    needs: ['optimize-ci', 'changes', 'build-next', 'build-native']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' }}

    strategy:
      fail-fast: false
      matrix:
        group: [1/6, 2/6, 3/6, 4/6, 5/6, 6/6]
        # Empty value uses default
        # TODO: Run with React 18.
        # Integration tests use the installed React version in next/package.json.include:
        # We can't easily switch like we do for e2e tests.
        # Skipping this dimensions until we can figure out a way to test multiple React versions.
        react: ['']
    uses: ./.github/workflows/build_reusable.yml
    with:
      nodeVersion: 18.18.2
      afterBuild: |
        export NEXT_EXTERNAL_TESTS_FILTERS=""$(pwd)/test/turbopack-dev-tests-manifest.json""
        export IS_TURBOPACK_TEST=1
        export TURBOPACK_DEV=1
        export NEXT_TEST_REACT_VERSION=""${{ matrix.react }}""

        node run-tests.js \
          --timings \
          -g ${{ matrix.group }} \
          --type integration
      stepName: 'test-turbopack-integration-react-${{ matrix.react }}-${{ matrix.group }}'
    secrets: inherit

  test-turbopack-production:
    name: test turbopack production
    needs: ['optimize-ci', 'changes', 'build-next', 'build-native']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' }}

    strategy:
      fail-fast: false
      matrix:
        exclude:
          # Excluding React 18 tests unless on `canary` branch until budget is approved.
          - react: ${{ github.event_name == 'pull_request' && !contains(github.event.pull_request.labels.*.name, 'run-react-18-tests') && '18.3.1' }}
        group: [1/7, 2/7, 3/7, 4/7, 5/7, 6/7, 7/7]
        # Empty value uses default
        # TODO: Run with React 18.
        # Integration tests use the installed React version in next/package.json.include:
        # We can't easily switch like we do for e2e tests.
        # Skipping this dimensions until we can figure out a way to test multiple React versions.
        react: ['', '18.3.1']
    uses: ./.github/workflows/build_reusable.yml
    with:
      nodeVersion: 18.18.2
      afterBuild: |
        export NEXT_EXTERNAL_TESTS_FILTERS=""$(pwd)/test/turbopack-build-tests-manifest.json""
        export IS_TURBOPACK_TEST=1
        export TURBOPACK_BUILD=1
        export NEXT_TEST_MODE=start
        export NEXT_TEST_REACT_VERSION=""${{ matrix.react }}""
        # TODO(PACK-4578): Remove
        export TURBOPACK_TEMP_DISABLE_DUPLICATE_MODULES_CHECK=1

        node run-tests.js --timings -g ${{ matrix.group }} --type production
      stepName: 'test-turbopack-production-react-${{ matrix.react }}-${{ matrix.group }}'
    secrets: inherit

  test-turbopack-production-integration:
    name: test turbopack production integration
    needs: ['optimize-ci', 'changes', 'build-next', 'build-native']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' }}

    strategy:
      fail-fast: false
      matrix:
        group: [1/7, 2/7, 3/7, 4/7, 5/7, 6/7, 7/7]
    uses: ./.github/workflows/build_reusable.yml
    with:
      nodeVersion: 18.18.2
      afterBuild: |
        export NEXT_EXTERNAL_TESTS_FILTERS=""$(pwd)/test/turbopack-build-tests-manifest.json""
        export IS_TURBOPACK_TEST=1
        export TURBOPACK_BUILD=1

        node run-tests.js \
          --timings \
          -g ${{ matrix.group }} \
          --type integration
      stepName: 'test-turbopack-production-integration-${{ matrix.group }}'
    secrets: inherit

  test-rspack-dev:
    name: test rspack dev
    needs: ['optimize-ci', 'changes', 'build-next', 'build-native']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' && needs.changes.outputs.rspack == 'true' }}
    strategy:
      fail-fast: false
      matrix:
        group: [1/5, 2/5, 3/5, 4/5, 5/5]
    uses: ./.github/workflows/build_reusable.yml
    with:
      afterBuild: |
        export NEXT_EXTERNAL_TESTS_FILTERS=""$(pwd)/test/rspack-dev-tests-manifest.json""
        export NEXT_TEST_MODE=dev

        # rspack flags
        export NEXT_RSPACK=1
        export NEXT_TEST_USE_RSPACK=1

        # HACK: Despite the name, this environment variable is only used to gate
        # tests, so it's applicable to rspack
        export TURBOPACK_DEV=1

        node run-tests.js \
          --test-pattern '^(test\/(development|e2e))/.*\.test\.(js|jsx|ts|tsx)$' \
          --timings \
          -g ${{ matrix.group }}
      stepName: 'test-rspack-dev-react-${{ matrix.react }}-${{ matrix.group }}'
    secrets: inherit

  test-rspack-integration:
    name: test rspack development integration
    needs: ['optimize-ci', 'changes', 'build-next', 'build-native']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' && needs.changes.outputs.rspack == 'true' }}
    strategy:
      fail-fast: false
      matrix:
        group: [1/6, 2/6, 3/6, 4/6, 5/6, 6/6]
    uses: ./.github/workflows/build_reusable.yml
    with:
      nodeVersion: 18.18.2
      afterBuild: |
        export NEXT_EXTERNAL_TESTS_FILTERS=""$(pwd)/test/rspack-dev-tests-manifest.json""

        # rspack flags
        export NEXT_RSPACK=1
        export NEXT_TEST_USE_RSPACK=1

        # HACK: Despite the name, this environment variable is only used to gate
        # tests, so it's applicable to rspack
        export TURBOPACK_DEV=1

        node run-tests.js \
          --timings \
          -g ${{ matrix.group }} \
          --type integration
      stepName: 'test-rspack-integration-react-${{ matrix.react }}-${{ matrix.group }}'
    secrets: inherit

  test-rspack-production:
    name: test rspack production
    needs: ['optimize-ci', 'changes', 'build-next', 'build-native']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' && needs.changes.outputs.rspack == 'true' }}
    strategy:
      fail-fast: false
      matrix:
        exclude:
          # Excluding React 18 tests unless on `canary` branch until budget is approved.
          - react: ${{ github.event_name == 'pull_request' && !contains(github.event.pull_request.labels.*.name, 'run-react-18-tests') && '18.3.1' }}
        group: [1/7, 2/7, 3/7, 4/7, 5/7, 6/7, 7/7]
        # Empty value uses default
    uses: ./.github/workflows/build_reusable.yml
    with:
      nodeVersion: 18.18.2
      afterBuild: |
        export NEXT_EXTERNAL_TESTS_FILTERS=""$(pwd)/test/rspack-build-tests-manifest.json""
        export NEXT_TEST_MODE=start

        # rspack flags
        export NEXT_RSPACK=1
        export NEXT_TEST_USE_RSPACK=1

        # HACK: Despite the name, this environment variable is only used to gate
        # tests, so it's applicable to rspack
        export TURBOPACK_BUILD=1

        node run-tests.js --timings -g ${{ matrix.group }} --type production
      stepName: 'test-rspack-production-react-${{ matrix.react }}-${{ matrix.group }}'
    secrets: inherit

  test-rspack-production-integration:
    name: test rspack production integration
    needs: ['optimize-ci', 'changes', 'build-next', 'build-native']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' && needs.changes.outputs.rspack == 'true' }}
    strategy:
      fail-fast: false
      matrix:
        group: [1/7, 2/7, 3/7, 4/7, 5/7, 6/7, 7/7]
    uses: ./.github/workflows/build_reusable.yml
    with:
      nodeVersion: 18.18.2
      afterBuild: |
        export NEXT_EXTERNAL_TESTS_FILTERS=""$(pwd)/test/rspack-build-tests-manifest.json""

        # rspack flags
        export NEXT_RSPACK=1
        export NEXT_TEST_USE_RSPACK=1

        # HACK: Despite the name, this environment variable is only used to gate
        # tests, so it's applicable to rspack
        export TURBOPACK_BUILD=1

        node run-tests.js \
          --timings \
          -g ${{ matrix.group }} \
          --type integration
      stepName: 'test-rspack-production-integration-${{ matrix.group }}'
    secrets: inherit

  test-next-swc-wasm:
    name: test next-swc wasm
    needs: ['optimize-ci', 'changes', 'build-next']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' }}

    uses: ./.github/workflows/build_reusable.yml
    with:
      skipNativeBuild: 'yes'
      skipNativeInstall: 'yes'
      afterBuild: |
        rustup target add wasm32-unknown-unknown
        node ./scripts/normalize-version-bump.js
        pnpm dlx turbo@${TURBO_VERSION} run build-wasm -- --target nodejs
        git checkout .

        export NEXT_TEST_MODE=start
        export NEXT_TEST_WASM=true
        node run-tests.js \
          test/production/pages-dir/production/test/index.test.ts \
          test/e2e/streaming-ssr/index.test.ts
      stepName: 'test-next-swc-wasm'
    secrets: inherit

  #[NOTE] currently this only checks building wasi target
  test-next-swc-napi-wasi:
    name: test next-swc wasi
    needs: ['optimize-ci', 'changes', 'build-next']
    # TODO: Re-enable this when https://github.com/napi-rs/napi-rs/issues/2009 is addressed.
    # Specifically, the `platform` value is now `threads` in
    # https://github.com/napi-rs/napi-rs/blob/e4ad4767efaf093fdff3dc768856f6100a6e3f72/cli/src/api/build.ts#L530
    if: false
    # if: ${{ needs.changes.outputs.docs-only == 'false' }}

    uses: ./.github/workflows/build_reusable.yml
    with:
      skipNativeBuild: 'yes'
      skipNativeInstall: 'yes'
      afterBuild: |
        rustup target add wasm32-wasip1-threads
        pnpm dlx turbo@${TURBO_VERSION} run build-native-wasi
      stepName: 'test-next-swc-napi-wasi'
    secrets: inherit

  test-unit:
    name: test unit
    needs: ['changes', 'build-next', 'build-native']
    if: ${{ needs.changes.outputs.docs-only == 'false' }}

    strategy:
      fail-fast: false
      matrix:
        node: [18, 20] # TODO: use env var like [env.NODE_MAINTENANCE_VERSION, env.NODE_LTS_VERSION]

    uses: ./.github/workflows/build_reusable.yml
    with:
      nodeVersion: ${{ matrix.node }}
      afterBuild: node run-tests.js --type unit
      stepName: 'test-unit-${{ matrix.node }}'

    secrets: inherit

  test-unit-windows:
    name: test unit windows
    needs: ['changes', 'build-native', 'build-native-windows']
    if: ${{ needs.changes.outputs.docs-only == 'false' }}

    strategy:
      fail-fast: false
      matrix:
        node: [18, 20] # TODO: use env var like [env.NODE_MAINTENANCE_VERSION, env.NODE_LTS_VERSION]

    uses: ./.github/workflows/build_reusable.yml
    with:
      nodeVersion: ${{ matrix.node }}
      afterBuild: node run-tests.js --type unit
      stepName: 'test-unit-windows-${{ matrix.node }}'
      runs_on_labels: '[""windows"",""self-hosted"",""x64""]'
      buildNativeTarget: 'x86_64-pc-windows-msvc'

    secrets: inherit

  test-new-tests-dev:
    name: Test new tests for flakes (dev)
    needs: ['optimize-ci', 'changes', 'build-native', 'build-next']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' }}

    strategy:
      fail-fast: false
      matrix:
        group: [1/5, 2/5, 3/5, 4/5, 5/5]

    uses: ./.github/workflows/build_reusable.yml
    with:
      afterBuild: |
        node scripts/test-new-tests.mjs \
          --flake-detection \
          --mode dev \
          --group ${{ matrix.group }}
      stepName: 'test-new-tests-dev-${{matrix.group}}'
      timeout_minutes: 60 # Increase the default timeout as tests are intentionally run multiple times to detect flakes

    secrets: inherit

  test-new-tests-start:
    name: Test new tests for flakes (prod)
    needs: ['optimize-ci', 'changes', 'build-native', 'build-next']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' }}

    strategy:
      fail-fast: false
      matrix:
        group: [1/5, 2/5, 3/5, 4/5, 5/5]

    uses: ./.github/workflows/build_reusable.yml
    with:
      afterBuild: |
        node scripts/test-new-tests.mjs \
          --flake-detection \
          --mode start \
          --group ${{ matrix.group }}
      stepName: 'test-new-tests-start-${{matrix.group}}'
      timeout_minutes: 60 # Increase the default timeout as tests are intentionally run multiple times to detect flakes

    secrets: inherit

  test-new-tests-deploy:
    name: Test new tests when deployed
    needs:
      ['optimize-ci', 'test-prod', 'test-new-tests-dev', 'test-new-tests-start']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' }}

    strategy:
      fail-fast: false
      matrix:
        group: [1/5, 2/5, 3/5, 4/5, 5/5]

    uses: ./.github/workflows/build_reusable.yml
    with:
      afterBuild: |
        export NEXT_E2E_TEST_TIMEOUT=240000
        export GH_PR_NUMBER=${{ github.event.pull_request && github.event.pull_request.number || '' }}
        node scripts/test-new-tests.mjs \
          --mode deploy \
          --group ${{ matrix.group }}
      stepName: 'test-new-tests-deploy-${{matrix.group}}'

    secrets: inherit

  test-dev:
    name: test dev
    needs: ['optimize-ci', 'changes', 'build-native', 'build-next']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' }}

    strategy:
      fail-fast: false
      matrix:
        exclude:
          # Excluding React 18 tests unless on `canary` branch until budget is approved.
          - react: ${{ github.event_name == 'pull_request' && !contains(github.event.pull_request.labels.*.name, 'run-react-18-tests') && '18.3.1' }}
        group: [1/6, 2/6, 3/6, 4/6, 5/6, 6/6]
        # Empty value uses default
        react: ['', '18.3.1']
    uses: ./.github/workflows/build_reusable.yml
    with:
      afterBuild: |
        export NEXT_TEST_MODE=dev
        export NEXT_TEST_REACT_VERSION=""${{ matrix.react }}""

        node run-tests.js \
          --timings \
          -g ${{ matrix.group }} \
          --type development
      stepName: 'test-dev-react-${{ matrix.react }}-${{ matrix.group }}'
    secrets: inherit

  test-dev-windows:
    name: test dev windows
    needs:
      [
        'optimize-ci',
        'changes',
        'build-native-windows',
        'build-native',
        'build-next',
      ]
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' }}

    uses: ./.github/workflows/build_reusable.yml
    with:
      afterBuild: |
        export NEXT_TEST_MODE=dev

        node run-tests.js \
          test/e2e/app-dir/app/index.test.ts \
          test/e2e/app-dir/app-edge/app-edge.test.ts
      stepName: 'test-dev-windows'
      runs_on_labels: '[""windows"",""self-hosted"",""x64""]'
      buildNativeTarget: 'x86_64-pc-windows-msvc'
    secrets: inherit

  test-integration-windows:
    name: test integration windows
    needs:
      [
        'optimize-ci',
        'changes',
        'build-native-windows',
        'build-native',
        'build-next',
      ]
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' }}

    uses: ./.github/workflows/build_reusable.yml
    with:
      nodeVersion: 18.18.2
      afterBuild: |
        node run-tests.js \
          --concurrency 4 \
          test/production/pages-dir/production/test/index.test.ts \
          test/integration/css-client-nav/test/index.test.js \
          test/integration/rewrites-has-condition/test/index.test.js \
          test/integration/create-next-app/index.test.ts \
          test/integration/create-next-app/package-manager/pnpm.test.ts
      stepName: 'test-integration-windows'
      runs_on_labels: '[""windows"",""self-hosted"",""x64""]'
      buildNativeTarget: 'x86_64-pc-windows-msvc'
    secrets: inherit

  test-prod-windows:
    name: test prod windows
    needs:
      [
        'optimize-ci',
        'changes',
        'build-native-windows',
        'build-native',
        'build-next',
      ]
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' }}

    uses: ./.github/workflows/build_reusable.yml
    with:
      afterBuild: |
        export NEXT_TEST_MODE=start

        node run-tests.js \
          test/e2e/app-dir/app/index.test.ts \
          test/e2e/app-dir/app-edge/app-edge.test.ts \
          test/e2e/app-dir/metadata-edge/index.test.ts
      stepName: 'test-prod-windows'
      runs_on_labels: '[""windows"",""self-hosted"",""x64""]'
      buildNativeTarget: 'x86_64-pc-windows-msvc'
    secrets: inherit

  test-prod:
    name: test prod
    needs: ['optimize-ci', 'changes', 'build-native', 'build-next']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' }}

    strategy:
      fail-fast: false
      matrix:
        exclude:
          # Excluding React 18 tests unless on `canary` branch until budget is approved.
          - react: ${{ github.event_name == 'pull_request' && !contains(github.event.pull_request.labels.*.name, 'run-react-18-tests') && '18.3.1' }}
        group: [1/7, 2/7, 3/7, 4/7, 5/7, 6/7, 7/7]
        # Empty value uses default
        react: ['', '18.3.1']
    uses: ./.github/workflows/build_reusable.yml
    with:
      afterBuild: |
        export NEXT_TEST_MODE=start
        export NEXT_TEST_REACT_VERSION=""${{ matrix.react }}""

        node run-tests.js --timings -g ${{ matrix.group }} --type production
      stepName: 'test-prod-react-${{ matrix.react }}-${{ matrix.group }}'
    secrets: inherit

  test-integration:
    name: test integration
    needs: ['optimize-ci', 'changes', 'build-native', 'build-next']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' }}

    strategy:
      fail-fast: false
      matrix:
        group:
          - 1/13
          - 2/13
          - 3/13
          - 4/13
          - 5/13
          - 6/13
          - 7/13
          - 8/13
          - 9/13
          - 10/13
          - 11/13
          - 12/13
          - 13/13
        # Empty value uses default
        # TODO: Run with React 18.
        # Integration tests use the installed React version in next/package.json.include:
        # We can't easily switch like we do for e2e tests.
        # Skipping this dimensions until we can figure out a way to test multiple React versions.
        react: ['']
    uses: ./.github/workflows/build_reusable.yml
    with:
      nodeVersion: 18.18.2
      afterBuild: |
        export NEXT_TEST_REACT_VERSION=""${{ matrix.react }}""

        node run-tests.js \
          --timings \
          -g ${{ matrix.group }} \
          --type integration
      stepName: 'test-integration-${{ matrix.group }}-react-${{ matrix.react }}'
    secrets: inherit

  test-firefox-safari:
    name: test firefox and safari
    needs: ['optimize-ci', 'changes', 'build-native', 'build-next']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' }}

    uses: ./.github/workflows/build_reusable.yml
    with:
      afterBuild: |
        pnpm playwright install

        # these all run without concurrency because they're heavier
        export TEST_CONCURRENCY=1

        BROWSER_NAME=firefox node run-tests.js \
          test/production/pages-dir/production/test/index.test.ts

        NEXT_TEST_MODE=start BROWSER_NAME=safari node run-tests.js \
          test/production/pages-dir/production/test/index.test.ts \
          test/e2e/basepath/basepath.test.ts \
          test/e2e/basepath/error-pages.test.ts

        BROWSER_NAME=safari DEVICE_NAME='iPhone XR' node run-tests.js \
          test/production/prerender-prefetch/index.test.ts
      stepName: 'test-firefox-safari'
    secrets: inherit

  # TODO: remove these jobs once PPR is the default
  # Manifest generated via: https://gist.github.com/wyattjoh/2ceaebd82a5bcff4819600fd60126431
  test-ppr-integration:
    name: test ppr integration
    needs: ['optimize-ci', 'changes', 'build-native', 'build-next']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' }}

    uses: ./.github/workflows/build_reusable.yml
    with:
      nodeVersion: 18.18.2
      afterBuild: |
        export __NEXT_EXPERIMENTAL_PPR=true
        export NEXT_EXTERNAL_TESTS_FILTERS=""test/ppr-tests-manifest.json""

        node run-tests.js \
          --timings \
          --type integration
      stepName: 'test-ppr-integration'
    secrets: inherit

  test-ppr-dev:
    name: test ppr dev
    needs: ['optimize-ci', 'changes', 'build-native', 'build-next']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' }}

    strategy:
      fail-fast: false
      matrix:
        group: [1/6, 2/6, 3/6, 4/6, 5/6, 6/6]
    uses: ./.github/workflows/build_reusable.yml
    with:
      afterBuild: |
        export __NEXT_EXPERIMENTAL_PPR=true
        export NEXT_EXTERNAL_TESTS_FILTERS=""test/ppr-tests-manifest.json""
        export NEXT_TEST_MODE=dev

        node run-tests.js \
          --timings \
          -g ${{ matrix.group }} \
          --type development
      stepName: 'test-ppr-dev-${{ matrix.group }}'
    secrets: inherit

  test-ppr-prod:
    name: test ppr prod
    needs: ['optimize-ci', 'changes', 'build-native', 'build-next']
    if: ${{ needs.optimize-ci.outputs.skip == 'false' && needs.changes.outputs.docs-only == 'false' }}

    strategy:
      fail-fast: false
      matrix:
        group: [1/7, 2/7, 3/7, 4/7, 5/7, 6/7, 7/7]
    uses: ./.github/workflows/build_reusable.yml
    with:
      afterBuild: |
        export __NEXT_EXPERIMENTAL_PPR=true
        export NEXT_EXTERNAL_TESTS_FILTERS=""test/ppr-tests-manifest.json""
        export NEXT_TEST_MODE=start

        node run-tests.js \
          --timings \
          -g ${{ matrix.group }} \
          --type production
      stepName: 'test-ppr-prod-${{ matrix.group }}'
    secrets: inherit

  tests-pass:
    needs:
      [
        'build-native',
        'build-next',
        'lint',
        'validate-docs-links',
        'check-types-precompiled',
        'test-unit',
        'test-dev',
        'test-prod',
        'test-integration',
        'test-firefox-safari',
        'test-ppr-dev',
        'test-ppr-prod',
        'test-ppr-integration',
        'test-cargo-unit',
        'rust-check',
        'rustdoc-check',
        'test-next-swc-wasm',
        'test-turbopack-dev',
        'test-turbopack-integration',
        'test-new-tests-dev',
        'test-new-tests-start',
        'test-new-tests-deploy',
        'test-turbopack-production',
        'test-turbopack-production-integration',
        'test-unit-windows',
        'test-dev-windows',
        'test-integration-windows',
        'test-prod-windows',
      ]

    if: always()
    runs-on: ubuntu-latest
    name: thank you, next
    steps:
      - run: exit 1
        if: ${{ always() && (contains(needs.*.result, 'failure') || contains(needs.*.result, 'cancelled')) }}
",916,41,2,"push, pull_request",42
vercel/next.js,build_reusable.yml,"name: Build Reusable

on:
  workflow_call:
    inputs:
      afterBuild:
        required: false
        description: 'additional steps to run'
        type: string
      # Toggles the mold linker. The default linker is much slower and can run into OOMs.
      # However, custom linkers won't work for wasm.
      mold:
        required: false
        description: 'whether to use the mold linker'
        type: string
      skipInstallBuild:
        required: false
        description: 'whether to skip pnpm install && pnpm build'
        type: string
      skipNativeBuild:
        required: false
        description: 'whether to skip building native modules'
        type: string
      skipNativeInstall:
        required: false
        description: 'whether to skip native postinstall script'
        type: string
        default: 'yes'
      uploadAnalyzerArtifacts:
        required: false
        description: 'whether to upload analyzer artifacts'
        type: string
      nodeVersion:
        required: false
        description: 'version of Node.js to use'
        type: string
      needsRust:
        required: false
        description: 'if rust is needed'
        type: string
      needsNextest:
        required: false
        description: 'if nextest rust dep is needed'
        type: string
      rustBuildProfile:
        required: false
        description: 'The profile to use for the build, default is `release-with-assertions`, also supports `` for debug and `release` for normal release'
        type: string
        default: 'release-with-assertions'
      uploadSwcArtifact:
        required: false
        description: 'if swc artifact needs uploading'
        type: string
      rustCacheKey:
        required: false
        description: 'rustCacheKey to cache shared target assets'
        type: string
      stepName:
        required: true
        description: 'name of the step, to be used for the upload artifact unique key '
        type: string
      timeout_minutes:
        description: 'Timeout in minutes'
        required: false
        type: number
        default: 30
      runs_on_labels:
        description: 'List of runner labels'
        required: false
        type: string
        default: '[""self-hosted"", ""linux"", ""x64"", ""metal""]'
      buildNativeTarget:
        description: 'Target for build-native step'
        required: false
        type: string
        default: 'x86_64-unknown-linux-gnu'

env:
  NAPI_CLI_VERSION: 2.18.4
  TURBO_VERSION: 2.3.3
  NODE_LTS_VERSION: 20.9.0
  # run-tests.js reads `TEST_CONCURRENCY` if no explicit `--concurrency` or `-c`
  # argument is provided
  TEST_CONCURRENCY: 8
  # disable backtrace for test snapshots
  RUST_BACKTRACE: 0

  TURBO_TEAM: 'vercel'
  TURBO_CACHE: 'remote:rw'
  TURBO_API: ${{ secrets.HOSTED_TURBO_API }}
  TURBO_TOKEN: ${{ secrets.HOSTED_TURBO_TOKEN }}
  NEXT_TELEMETRY_DISABLED: 1
  # allow not skipping install-native postinstall script if we don't have a binary available already
  NEXT_SKIP_NATIVE_POSTINSTALL: ${{ inputs.skipNativeInstall == 'yes' && '1' || '' }}
  DATADOG_API_KEY: ${{ secrets.DATA_DOG_API_KEY }}
  NEXT_JUNIT_TEST_REPORT: 'true'
  DD_ENV: 'ci'
  TEST_TIMINGS_TOKEN: ${{ secrets.TEST_TIMINGS_TOKEN }}
  NEXT_TEST_JOB: 1
  VERCEL_TEST_TOKEN: ${{ secrets.VERCEL_TEST_TOKEN }}
  VERCEL_TEST_TEAM: vtest314-next-e2e-tests
  NEXT_TEST_PREFER_OFFLINE: 1
  NEXT_CI_RUNNER: ${{ inputs.runs_on_labels }}

jobs:
  build:
    timeout-minutes: ${{ inputs.timeout_minutes }}
    runs-on: ${{ fromJson(inputs.runs_on_labels) }}

    defaults:
      run:
        shell: bash -leo pipefail {0}

    outputs:
      input_step_key: ${{ steps.var.outputs.input_step_key }}

    steps:
      - name: Check if fnm is installed
        id: check-fnm
        run: |
          if [ -x ""$(command -v fnm)"" ]; then
            echo ""fnm found.""
            echo ""found=true"" >> $GITHUB_OUTPUT
          else
            echo ""fnm not found.""
            echo ""found=false"" >> $GITHUB_OUTPUT
          fi

      - name: Install fnm
        if: steps.check-fnm.outputs.found != 'true'
        run: |
          curl -fsSL https://fnm.vercel.app/install | bash
          export PATH=""/home/runner/.local/share/fnm:$PATH""
          echo ""/home/runner/.local/share/fnm"" >> $GITHUB_PATH
          fnm env --json | jq -r 'to_entries|map(""\(.key)=\(.value|tostring)"")|.[]' | xargs -I {} echo ""{}"" >> $GITHUB_ENV

      - name: Normalize input step names into path key
        uses: actions/github-script@v7
        id: var
        with:
          script: |
            core.setOutput('input_step_key', '${{ inputs.stepName }}'.toLowerCase().replaceAll(/[/.]/g, '-').trim('-'));

      - run: fnm use --install-if-missing ${{ inputs.nodeVersion || env.NODE_LTS_VERSION }}
      - run: fnm default ${{ inputs.nodeVersion || env.NODE_LTS_VERSION }}
      - run: node -v
      - name: Prepare corepack
        if: ${{ contains(fromJson(inputs.runs_on_labels), 'ubuntu-latest') }}
        run: |
          npm i -g corepack@0.31
      - run: corepack enable
      - run: pwd

      - run: rm -rf .git

      - uses: actions/checkout@v4
        with:
          fetch-depth: 25

      # local action -> needs to run after checkout
      - name: Install Rust
        uses: ./.github/actions/setup-rust
        if: ${{ inputs.skipNativeBuild != 'yes' || inputs.needsNextest == 'yes' || inputs.needsRust == 'yes' }}

      - name: 'Install mold linker'
        if: ${{ inputs.mold == 'yes' }}
        run: |
          sudo apt update
          sudo apt install -y mold
          echo RUSTFLAGS=${RUSTFLAGS}\ -C\ link-arg=-fuse-ld=mold >> $GITHUB_ENV

      - name: Install nextest
        if: ${{ inputs.needsNextest == 'yes' }}
        uses: taiki-e/install-action@nextest

      - run: rustc --version
        if: ${{ inputs.skipNativeBuild != 'yes' || inputs.needsNextest == 'yes' || inputs.needsRust == 'yes' }}

      - run: corepack prepare --activate yarn@1.22.19 && npm i -g ""@napi-rs/cli@${NAPI_CLI_VERSION}""

      - name: Cache on ${{ github.ref_name }}
        uses: ijjk/rust-cache@turbo-cache-v1.0.8
        if: ${{ inputs.rustCacheKey }}
        with:
          cache-provider: 'turbo'
          save-if: ${{ github.ref_name == 'canary' }}
          shared-key: ${{ inputs.rustCacheKey }}-${{ inputs.buildNativeTarget }}-build-${{ inputs.rustBuildProfile }}-${{ hashFiles('.cargo/config.toml') }}

      # clean up any previous artifacts to avoid hitting disk space limits
      - run: git clean -xdf && rm -rf /tmp/next-repo-*; rm -rf /tmp/next-install-* /tmp/yarn-* /tmp/ncc-cache target

      # Configure a git user so that Create Next App can initialize git repos during integration tests.
      - name: Set CI git user
        run: |
          git config --global user.name ""vercel-ci-bot""
          git config --global user.email ""infra+ci@vercel.com""

      - run: cargo clean
        if: ${{ inputs.skipNativeBuild != 'yes' || inputs.needsNextest == 'yes' || inputs.needsRust == 'yes' }}

      # normalize versions before build-native for better cache hits
      - run: node scripts/normalize-version-bump.js
        name: normalize versions

      - run: pnpm dlx turbo@${TURBO_VERSION} run build-native-${{ inputs.rustBuildProfile }} -v --env-mode loose --remote-cache-timeout 90 --summarize -- --target ${{ inputs.buildNativeTarget }}
        if: ${{ inputs.skipNativeBuild != 'yes' }}

      - name: Upload next-swc artifact
        if: ${{ inputs.uploadSwcArtifact == 'yes' }}
        uses: actions/upload-artifact@v4
        with:
          name: next-swc-binary
          path: packages/next-swc/native/next-swc.linux-x64-gnu.node

      # undo normalize version changes for install/build
      - run: git checkout .
        if: ${{ inputs.skipInstallBuild != 'yes' }}

      - run: pnpm install
        if: ${{ inputs.skipInstallBuild != 'yes' }}

      - name: Install node-file-trace test dependencies
        if: ${{ inputs.needsNextest == 'yes' }}
        working-directory: turbopack/crates/turbopack/tests/node-file-trace
        run: pnpm install -r --side-effects-cache false

      - run: ANALYZE=1 pnpm build
        if: ${{ inputs.skipInstallBuild != 'yes' }}

      # Some packages e.g. `devlow-bench` depend on `pnpm build` to generate
      # their `dist` directory. The first run of `pnpm install` will generate
      # warnings because these don't exist yet.
      #
      # We need to run `pnpm install` a _second_ time to fix this. Fortunately,
      # this second run is very fast and cheap.
      - name: Re-run pnpm install to link built packages into node_modules/.bin
        run: pnpm install
        if: ${{ inputs.skipInstallBuild != 'yes' }}

      - run: pnpm playwright install-deps
        if: ${{ inputs.skipInstallBuild != 'yes' }}

      - run: pnpm playwright install chromium
        if: ${{ inputs.skipInstallBuild != 'yes' }}

      - run: pnpm dlx turbo@${TURBO_VERSION} run get-test-timings -- --build ${{ github.sha }}

      - run: ${{ inputs.afterBuild }}
        # defaults.run.shell sets a stronger options (`-leo pipefail`)
        # Set this back to github action's weaker defaults:
        # https://docs.github.com/en/actions/writing-workflows/workflow-syntax-for-github-actions#jobsjob_idstepsshell
        #
        # We must use a login shell: fnm installation may modify the `.profile`
        shell: bash -le {0}
        timeout-minutes: ${{ inputs.timeout_minutes }}

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: turbo-run-summary-${{ steps.var.outputs.input_step_key }}
          path: .turbo/runs
          if-no-files-found: ignore

      - name: Upload bundle analyzer artifacts
        uses: actions/upload-artifact@v4
        if: ${{ inputs.uploadAnalyzerArtifacts == 'yes' }}
        with:
          name: webpack bundle analysis stats-${{ steps.var.outputs.input_step_key }}
          path: packages/next/dist/compiled/next-server/report.*.html

      - name: Upload test report to datadog
        if: ${{ inputs.afterBuild && always() && !github.event.pull_request.head.repo.fork }}
        run: |
          # Add a `test.type` tag to distinguish between turbopack and next.js runs
          # Add a `nextjs.test_session.name` tag to help identify the job
          if [ -d ./test/test-junit-report ]; then
            pnpm dlx @datadog/datadog-ci@2.45.1 junit upload \
              --service nextjs \
              --tags test.type:nextjs \
              --tags test_session.name:""${{ inputs.stepName }}"" \
              ./test/test-junit-report
          fi
          if [ -d ./test/turbopack-test-junit-report ]; then
            pnpm dlx @datadog/datadog-ci@2.45.1 junit upload \
              --service nextjs \
              --tags test.type:turbopack \
              --tags test_session.name:""${{ inputs.stepName }}"" \
              ./test/turbopack-test-junit-report
          fi

      - name: Upload Playwright Snapshots
        uses: actions/upload-artifact@v4
        if: ${{ inputs.afterBuild && always() }}
        with:
          name: test-playwright-snapshots-${{ steps.var.outputs.input_step_key }}
          path: |
            test/traces
          if-no-files-found: ignore
",298,1,1,workflow_call,9
vercel/next.js,cancel.yml,"name: Cancel
on:
  pull_request_target:
    types:
      - edited
      - synchronize

jobs:
  cancel:
    name: 'Cancel Previous Runs'
    runs-on: ubuntu-latest
    timeout-minutes: 2
    steps:
      - uses: styfle/cancel-workflow-action@0.12.1
        with:
          workflow_id: 444921, 444987, 57419851
          access_token: ${{ github.token }}
",17,1,1,pull_request_target,1
vercel/next.js,code_freeze.yml,"on:
  workflow_dispatch:
    inputs:
      type:
        description: Enable/disable code freeze
        required: true
        type: choice
        options:
          - enable
          - disable

    secrets:
      CODE_FREEZE_TOKEN:
        required: true

name: Code Freeze

env:
  NAPI_CLI_VERSION: 2.18.4
  TURBO_VERSION: 2.3.3
  NODE_LTS_VERSION: 20

jobs:
  start:
    runs-on: ubuntu-latest

    environment: release-${{ github.event.inputs.releaseType }}
    steps:
      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: 18
          check-latest: true
      - name: Setup corepack
        run: |
          npm i -g corepack@0.31
          corepack enable

      - run: git clone https://github.com/vercel/next.js.git --depth=1 .

      # https://github.com/actions/virtual-environments/issues/1187
      - name: tune linux network
        run: sudo ethtool -K eth0 tx off rx off

      - run: node ./scripts/code-freeze.js --type ${{ github.event.inputs.type }}
        env:
          CODE_FREEZE_TOKEN: ${{ secrets.CODE_FREEZE_TOKEN }}
",47,1,1,workflow_dispatch,1
vercel/next.js,force_merge_canary_release_pr.yml,"name: Force Merge Canary Release PR

on: pull_request

permissions:
  # To bypass and merge PR
  pull-requests: write

jobs:
  force-merge-canary-release-pr:
    runs-on: ubuntu-latest
    # Validate the login, PR title, and the label to ensure the PR is
    # from the release PR and prevent spoofing.
    if: |
      github.event.pull_request.user.login == 'vercel-release-bot' &&
      github.event.pull_request.title == 'Version Packages (canary)' &&
      contains(github.event.pull_request.labels.*.name, 'created-by: CI')
    steps:
      - name: Bypass required status checks and merge PR
        run: gh pr merge --admin --squash ""$PR_URL""
        env:
          PR_URL: ${{github.event.pull_request.html_url}}
          GH_TOKEN: ${{secrets.GITHUB_TOKEN}}
",23,1,1,pull_request,0
vercel/next.js,graphite_ci_optimizer.yml,"# Avoid running the full CI on mid-stack PRs: https://graphite.dev/docs/stacking-and-ci
#
# We still run some high-signal low-cost jobs (e.g. lint, unit tests) on these mid-stack PRs, just
# not anything slow (e.g. integration tests).
#
# Because we don't use Graphite's CI batching, full CI will still run on every PR individually
# before it merges. The goal is just to avoid wasting CI capacity when frequently rebasing large
# stacks.
#
# This can by bypassed by labeling a PR with 'CI Bypass Graphite Optimization', and manually
# re-running CI.

name: Graphite CI Optimizer
on:
  workflow_call:
    outputs:
      skip:
        description: ""'true' if expensive CI checks should be skipped, 'false' otherwise.""
        value: ${{ jobs.optimize-ci.outputs.skip }}
    secrets:
      GRAPHITE_TOKEN:
        description: 'The Graphite CI optimization secret'
        # secrets are not available in forks, check-skip will just fail-open with a warning
        required: false
env:
  # FYI, if you add this label, you must *push* to the repository again to trigger a new event. Just
  # re-running in the GitHub actions UI won't work, as it will re-use the old event with the old
  # labels.
  HAS_BYPASS_LABEL: |-
    ${{
      github.event_name == 'pull_request' &&
      contains(github.event.pull_request.labels.*.name, 'CI Bypass Graphite Optimization')
    }}
jobs:
  optimize-ci:
    name: Graphite CI Optimizer
    runs-on: ubuntu-latest
    outputs:
      skip: ${{ env.HAS_BYPASS_LABEL == 'false' && steps.check-skip.outputs.skip == 'true' }}
    steps:
      - name: Optimize CI
        id: check-skip
        uses: withgraphite/graphite-ci-action@main
        with:
          graphite_token: ${{ secrets.GRAPHITE_TOKEN }}
      - name: Debug Output
        run: |
          echo 'github.event_name: ${{ github.event_name }}'
          echo ""secrets.GRAPHITE_TOKEN != '': ${{ secrets.GRAPHITE_TOKEN != '' }}""
          echo 'env.HAS_BYPASS_LABEL: ${{ env.HAS_BYPASS_LABEL }}'
          echo 'steps.check-skip.outputs.skip: ${{ steps.check-skip.outputs.skip }}'
",51,1,1,workflow_call,1
vercel/next.js,integration_tests_reusable.yml,"name: Integration Tests Reusable

on:
  workflow_call:
    inputs:
      name:
        description: A unique identifer used for uploaded assets
        type: string
      test_type:
        description: '""development"" or ""production""'
        required: true
        type: string
      run_before_test:
        description: >
          Bash code to run before executing the test (e.g. setting environment
          variables). Runs in the same step as the test.
        type: string
        default: ''
      e2e_groups:
        description: >
          Size of the matrix used for running e2e tests (controls parallelism)
        type: number
        default: 6
      integration_groups:
        description: >
          Size of the matrix used for running legacy integration tests (controls
          parallelism)
        type: number
        default: 6
      e2e_timeout_minutes:
        type: number
        default: 30
      integration_timeout_minutes:
        type: number
        default: 30
      num_retries:
        type: number
        default: 2

jobs:
  # First, build Next.js to execute across tests.
  build-next:
    name: build-next
    uses: ./.github/workflows/build_reusable.yml
    with:
      skipNativeBuild: yes
      stepName: build-next
    secrets: inherit

  build-native:
    name: build-native
    uses: ./.github/workflows/build_reusable.yml
    with:
      skipInstallBuild: yes
      stepName: build-native
    secrets: inherit

  generate-matrices:
    runs-on: [self-hosted, linux, x64, metal]
    steps:
      - id: out
        run: |
          printf 'e2e=[%s]\n' \
            ""$(seq -s, 1 ${{ inputs.e2e_groups }})"" | \
            tee -a ""$GITHUB_OUTPUT""
          printf 'integration=[%s]\n' \
            ""$(seq -s, 1 ${{ inputs.integration_groups }})"" | \
            tee -a ""$GITHUB_OUTPUT""
    outputs:
      e2e: ${{ steps.out.outputs.e2e }}
      integration: ${{ steps.out.outputs.integration }}

  # Actual test scheduling. These jobs mimic the normal test jobs.
  # Refer build_and_test.yml for more details.
  #
  # We run tests in two parts. Legacy integration tests are run separately:
  # https://github.com/vercel/next.js/blob/canary/contributing/core/testing.md#test-types-in-nextjs
  test-e2e:
    # Name must match `integrationTestJobs` in
    # `./.github/actions/next-integration-stat`
    name: >-
      Next.js integration test (E2E and ${{ inputs.test_type }})
      (${{ matrix.group }}/${{ inputs.e2e_groups }})
    needs: [build-next, build-native, generate-matrices]
    strategy:
      fail-fast: false
      matrix:
        group: ${{ fromJSON(needs.generate-matrices.outputs.e2e) }}
    uses: ./.github/workflows/build_reusable.yml
    with:
      afterBuild: |
        # e2e and ${{ inputs.test_type }} tests with `node run-tests.js`

        export NEXT_TEST_CONTINUE_ON_ERROR=TRUE
        export NEXT_TEST_MODE=${{
          inputs.test_type == 'development' && 'dev' || 'start'
        }}

        ${{ inputs.run_before_test }}

        node run-tests.js \
          --group ${{ matrix.group }}/${{ inputs.e2e_groups }} \
          --retries ${{ inputs.num_retries }} \
          --type ${{ inputs.test_type }}
      stepName: test-${{ inputs.name }}-${{ matrix.group }}
      timeout_minutes: ${{ inputs.e2e_timeout_minutes }}
    secrets: inherit

  test-integration:
    # Name must match `integrationTestJobs` in
    # `./.github/actions/next-integration-stat`
    name: >-
      Next.js integration test (Integration)
      (${{ matrix.group }}/${{ inputs.e2e_groups }})
    needs: [build-next, build-native, generate-matrices]
    strategy:
      fail-fast: false
      matrix:
        group: ${{ fromJSON(needs.generate-matrices.outputs.integration) }}
    uses: ./.github/workflows/build_reusable.yml
    with:
      nodeVersion: 18.18.2
      afterBuild: |
        # legacy integration tests with `node run-tests.js`

        export NEXT_TEST_CONTINUE_ON_ERROR=TRUE

        # HACK: Despite the name, these environment variables are just used to
        # gate tests, so they're applicable to both turbopack and rspack tests
        export ${{
          inputs.test_type == 'development' &&
            'TURBOPACK_DEV=1' ||
            'TURBOPACK_BUILD=1'
        }}

        ${{ inputs.run_before_test }}

        node run-tests.js \
          --group ${{ matrix.group }}/${{ inputs.integration_groups }} \
          --retries ${{ inputs.num_retries }} \
          --type integration
      stepName: test-${{ inputs.name }}-integration-${{ matrix.group }}
      timeout_minutes: ${{ inputs.integration_timeout_minutes }}
    secrets: inherit

  # Collect integration test results from execute_tests,
  # Store it as github artifact for next step to consume.
  collect_nextjs_development_integration_stat:
    needs: [test-e2e, test-integration]
    name: Next.js integration test development status report
    runs-on: [self-hosted, linux, x64, metal]
    if: always()
    permissions:
      pull-requests: write
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Collect integration test stat
        uses: ./.github/actions/next-integration-stat
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Store artifacts
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ inputs.name }}
          path: |
            nextjs-test-results.json
            failed-test-path-list.json
            passed-test-path-list.json
",171,6,1,workflow_call,7
vercel/next.js,issue_bankrupt.yml,"name: 'Bankrupt issues'

on:
  workflow_dispatch:
    inputs:
      created:
        description: 'created date range'
        required: true
        type: string

jobs:
  bankrupt:
    if: github.repository_owner == 'vercel'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4

      - name: 'Bankrupt issues & send notification to Slack'
        run: node ./.github/actions/next-repo-actions/dist/bankrupt/index.js
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SLACK_TOKEN: ${{ secrets.SLACK_TOKEN }}
          CREATED: ${{ github.event.inputs.created }}
",24,1,1,workflow_dispatch,2
vercel/next.js,issue_lock.yml,"name: 'Lock Threads'

on:
  schedule:
    # This runs twice a day: https://crontab.guru/#0_0,12_*_*_*
    - cron: '0 0,12 * * *'
  workflow_dispatch:

permissions:
  issues: write
  pull-requests: write

concurrency:
  group: lock

jobs:
  action:
    runs-on: ubuntu-latest
    if: github.repository_owner == 'vercel'
    steps:
      - uses: dessant/lock-threads@v5
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          add-issue-labels: 'locked'
          add-pr-labels: 'locked'
          issue-inactive-days: 14
          issue-comment: 'This closed issue has been automatically locked because it had no new activity for 2 weeks. If you are running into a similar issue, please create a new issue with the steps to reproduce. Thank you.'
          pr-inactive-days: 14
          log-output: true
",29,1,2,"schedule, workflow_dispatch",1
vercel/next.js,issue_stale.yml,"name: 'Stale issue handler'
on:
  workflow_dispatch:
  schedule:
    # This runs every day 20 minutes before midnight: https://crontab.guru/#40_23_*_*_*
    - cron: '40 23 * * *'

jobs:
  stale:
    runs-on: ubuntu-latest
    if: github.repository_owner == 'vercel'
    steps:
      - uses: actions/stale@v9
        id: issue-stale
        name: 'Mark stale issues, close stale issues'
        with:
          repo-token: ${{ secrets.STALE_TOKEN }}
          ascending: true
          days-before-issue-close: 7
          days-before-issue-stale: 545 # issues with no activity in over ~1.5 years
          days-before-pr-close: -1
          days-before-pr-stale: -1
          remove-issue-stale-when-updated: true
          stale-issue-label: 'stale'
          labels-to-add-when-unstale: 'not stale'
          stale-issue-message: 'This issue has been automatically marked as stale due to two years of inactivity. It will be closed in 7 days unless there’s further input. If you believe this issue is still relevant, please leave a comment or provide updated details. Thank you.'
          close-issue-message: 'This issue has been automatically closed due to two years of inactivity. If you’re still experiencing a similar problem or have additional details to share, please open a new issue following our current issue template. Your updated report helps us investigate and address concerns more efficiently. Thank you for your understanding!'
          operations-per-run: 300 # 1 operation per 100 issues, the rest is to label/comment/close
      - uses: actions/stale@v9
        id: stale-no-repro
        name: 'Close stale issues with no reproduction'
        with:
          repo-token: ${{ secrets.STALE_TOKEN }}
          any-of-issue-labels: 'please add a complete reproduction'
          close-issue-message: 'This issue has been automatically closed due to 2 days of inactivity and the absence of a complete reproduction. If you believe this was done in error, please leave a comment. If you are experiencing a similar issue, consider opening a new issue with a complete reproduction. Thank you.'
          days-before-issue-close: 2
          days-before-issue-stale: 1
          days-before-pr-close: -1
          days-before-pr-stale: -1
          remove-issue-stale-when-updated: true
          stale-issue-label: 'stale'
          labels-to-add-when-unstale: 'not stale'
          operations-per-run: 300 # 1 operation per 100 issues, the rest is to label/comment/close
      - uses: actions/stale@v9
        id: stale-simple-repro
        name: 'Close issues with no simple repro'
        with:
          repo-token: ${{ secrets.STALE_TOKEN }}
          any-of-issue-labels: 'please simplify reproduction'
          close-issue-message: 'This issue has been automatically closed due to 14 days of inactivity and the absence of a simple reproduction for investigation. If you believe this was done in error, please leave a comment. If you are experiencing a similar issue, consider opening a new issue with a simple reproduction. Thank you.'
          days-before-issue-close: 14
          days-before-issue-stale: 1
          days-before-pr-close: -1
          days-before-pr-stale: -1
          remove-issue-stale-when-updated: true
          stale-issue-label: 'stale'
          labels-to-add-when-unstale: 'not stale'
          operations-per-run: 300 # 1 operation per 100 issues, the rest is to label/comment/close
      - uses: actions/stale@v9
        id: stale-no-canary
        name: 'Close issues not verified on canary'
        with:
          repo-token: ${{ secrets.STALE_TOKEN }}
          any-of-issue-labels: 'please verify canary'
          close-issue-message: 'This issue has been automatically closed due to 14 days of inactivity and the absence of testing against next@canary. If you believe this was done in error, please leave a comment. If you are experiencing a similar issue, consider opening a new issue with a reproduction. Thank you.'
          days-before-issue-close: 14
          days-before-issue-stale: 1
          days-before-pr-close: -1
          days-before-pr-stale: -1
          remove-issue-stale-when-updated: true
          stale-issue-label: 'stale'
          labels-to-add-when-unstale: 'not stale'
          operations-per-run: 300 # 1 operation per 100 issues, the rest is to label/comment/close
",73,1,2,"workflow_dispatch, schedule",4
vercel/next.js,issue_version.yml,"name: 'Fetch Issues by Version'

on:
  workflow_dispatch:
    inputs:
      version:
        description: 'Next.js Version'
        required: true
        type: string

jobs:
  fetch_issues:
    if: github.repository_owner == 'vercel'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4

      - name: 'Fetch issues & send notification to Slack'
        run: node ./.github/actions/next-repo-actions/dist/issues-by-version/index.js
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SLACK_TOKEN: ${{ secrets.SLACK_TOKEN }}
          VERSION: ${{ github.event.inputs.version }}
",24,1,1,workflow_dispatch,2
vercel/next.js,issue_wrong_template.yml,"name: 'Close issues using the wrong issue template'

on:
  issues:
    types: [labeled]

jobs:
  close:
    if: github.event.label.name == 'please use the correct issue template'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
      - name: Setup corepack
        run: |
          npm i -g corepack@0.31
          corepack enable
      - name: 'Close issues using the wrong issue template'
        run: node ./.github/actions/next-repo-actions/dist/wrong-issue-template/index.js
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
",21,1,1,issues,2
vercel/next.js,notify_release.yml,"# A workflow runs when a release is published, dispatches a new event to the vercel/turbo
# to notify its release. Turbopack, and other integration workflow will subscribe to this event.
name: Notify new Next.js release
on:
  release:
    types: [published]
jobs:
  notify:
    runs-on: ubuntu-latest
    if: ${{ github.event_name == 'release' }}
    steps:
      - uses: actions/github-script@v7
        id: notify-new-release
        with:
          result-encoding: string
          retries: 3
          retry-exempt-status-codes: 400,401
          # Default github token cannot dispatch events to the remote repo, it should be
          # a PAT with access to contenst:read&write + metadata:read.
          github-token: ${{ secrets.TURBOPACK_TEST_TOKEN }}
          # Note `event_type` and `client_payload` are contract between vercel/turbo,
          # if these need to be changed both side should be updated accordingly.
          script: |
            github.request('POST /repos/{owner}/{repo}/dispatches', {
              owner: 'vercel',
              repo: 'turbo',
              event_type: 'nextjs-release-published',
              client_payload: {
                version: context.ref
              }
            })
  front-sync:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - id: nextPackageInfo
        name: Get `next` package info
        run: |
          cd packages/next 
          {
            echo 'value<<EOF'
            cat package.json
            echo EOF
          } >> ""$GITHUB_OUTPUT""
      - id: version
        name: Extract `next` version
        run: echo 'value=${{ fromJson(steps.nextPackageInfo.outputs.value).version }}' >> ""$GITHUB_OUTPUT""
      - name: Check token
        run: gh auth status
        env:
          GITHUB_TOKEN: ${{ secrets.FRONT_TEST_TOKEN }}
      - uses: actions/github-script@v7
        name: Trigger vercel/front sync
        id: trigger-front-sync
        with:
          retries: 3
          retry-exempt-status-codes: 400,401,404
          # Default github token cannot dispatch events to the remote repo, it should be
          # a PAT with Actions write access (https://docs.github.com/en/rest/actions/workflows?apiVersion=2022-11-28#create-a-workflow-dispatch-event)
          github-token: ${{ secrets.FRONT_TEST_TOKEN }}
          # Note `workflow_id` and `inputs` are contract between vercel/front,
          # if these need to be changed both side should be updated accordingly.
          script: |
            await github.request(
              ""POST /repos/{owner}/{repo}/actions/workflows/{workflow_id}/dispatches"",
              {
                owner: ""vercel"",
                repo: ""front"",
                workflow_id: ""cron-update-next.yml"",
                ref: ""main"",
                inputs: {
                  version: ""${{ steps.version.outputs.value }}"",
                },
              }
            );
            // Ideally we'd include a URL to this specific sync.
            // However, creating a workflow_dispatch event does not produce an ID: https://github.com/orgs/community/discussions/9752
            console.info(
              ""Sync started in https://github.com/vercel/front/actions/workflows/cron-update-next.yml?query=event%3Aworkflow_dispatch""
            );
            console.info(
              ""This may not start a new sync if one is already in progress. Check the logs of the cron-update-next Workflow run.""
            );
",83,2,1,release,3
vercel/next.js,popular.yml,"name: Notify about the top 15 issues/PRs/feature requests (most reacted) in the last 90 days

on:
  schedule:
    - cron: '0 10 * * 1' # Every Monday at 10AM UTC (6AM EST)
  workflow_dispatch:

jobs:
  run:
    if: github.repository_owner == 'vercel'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
      - name: Setup corepack
        run: |
          npm i -g corepack@0.31
          corepack enable
      - name: 'Issues: Send notification to Slack'
        run: node ./.github/actions/next-repo-actions/dist/issues/index.mjs
        continue-on-error: true
      - name: 'PRs: Send notification to Slack'
        run: node ./.github/actions/next-repo-actions/dist/prs/index.js
        continue-on-error: true
      - name: 'Feature requests: Send notification to Slack'
        run: node ./.github/actions/next-repo-actions/dist/feature-requests/index.mjs
        continue-on-error: true
    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      SLACK_TOKEN: ${{ secrets.SLACK_TOKEN }}
",30,1,2,"schedule, workflow_dispatch",2
vercel/next.js,pull_request_stats.yml,"on:
  pull_request:
    types: [opened, synchronize]

name: Generate Pull Request Stats

env:
  NAPI_CLI_VERSION: 2.18.4
  TURBO_VERSION: 2.3.3
  NODE_LTS_VERSION: 20
  TEST_CONCURRENCY: 6

  TURBO_TEAM: 'vercel'
  TURBO_CACHE: 'remote:rw'
  NEXT_TELEMETRY_DISABLED: 1
  # we build a dev binary for use in CI so skip downloading
  # canary next-swc binaries in the monorepo
  NEXT_SKIP_NATIVE_POSTINSTALL: 1
  TEST_TIMINGS_TOKEN: ${{ secrets.TEST_TIMINGS_TOKEN }}
  NEXT_TEST_JOB: 1
  NEXT_DISABLE_SWC_WASM: 1

jobs:
  build:
    uses: ./.github/workflows/build_reusable.yml
    secrets: inherit
    with:
      stepName: 'generate-pull-request-stats'
      uploadSwcArtifact: 'yes'
      uploadAnalyzerArtifacts: 'yes'

  stats:
    name: PR Stats
    needs: build
    timeout-minutes: 25
    runs-on:
      - 'self-hosted'
      - 'linux'
      - 'x64'
      - 'metal'
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 25

      - name: Check non-docs only change
        run: echo ""DOCS_CHANGE<<EOF"" >> $GITHUB_OUTPUT; echo ""$(node scripts/run-for-change.js --not --type docs --exec echo 'nope')"" >> $GITHUB_OUTPUT; echo 'EOF' >> $GITHUB_OUTPUT
        id: docs-change

      - uses: actions/download-artifact@v4
        if: ${{ steps.docs-change.outputs.DOCS_CHANGE == 'nope' }}
        with:
          name: next-swc-binary
          path: packages/next-swc/native

      - run: cp -r packages/next-swc/native .github/actions/next-stats-action/native
        if: ${{ steps.docs-change.outputs.DOCS_CHANGE == 'nope' }}

      - uses: ./.github/actions/next-stats-action
        if: ${{ steps.docs-change.outputs.DOCS_CHANGE == 'nope' }}
",60,2,1,pull_request,4
vercel/next.js,retry_deploy_test.yml,"name: retry-deploy-tests

on:
  workflow_run:
    workflows: ['test-e2e-deploy-release']
    types:
      - completed

env:
  SLACK_WEBHOOK_URL: ${{ secrets.BROKEN_DEPLOY_SLACK_WEBHOOK_URL }}

permissions:
  actions: write

jobs:
  retry-on-failure:
    name: retry failed jobs
    # Retry the test-e2e-deploy-release workflow up to 2 times
    if: >-
      ${{ 
        github.event.workflow_run.conclusion == 'failure' &&
        github.repository == 'vercel/next.js' &&
        github.event.workflow_run.run_attempt < 2
      }}
    runs-on: ubuntu-latest
    steps:
      - name: send retry request to GitHub API
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          gh api \
            --method POST \
            -H ""Accept: application/vnd.github+json"" \
            -H ""X-GitHub-Api-Version: 2022-11-28"" \
            /repos/${{ github.repository }}/actions/runs/${{ github.event.workflow_run.id }}/rerun-failed-jobs

  report-failure:
    name: report failure to slack
    # Report the failure to Slack if the test-e2e-deploy-release workflow has failed 2 times
    if: >-
      ${{ 
        github.event.workflow_run.conclusion == 'failure' &&
        github.event.workflow_run.run_attempt >= 2 &&
        !github.event.workflow_run.head_repository.fork
      }}
    runs-on: ubuntu-latest
    steps:
      - name: send webhook
        uses: slackapi/slack-github-action@v1.25.0
        with:
          payload: |
            {
              ""commit_title"": ${{ toJSON(github.event.workflow_run.display_title) }},
              ""commit_url"": ""github.com/${{ github.repository }}/commit/${{ github.event.workflow_run.head_sha }}"",
              ""workflow_run_url"": ""github.com/${{ github.repository }}/actions/runs/${{ github.event.workflow_run.id }}/attempts/${{ github.event.workflow_run.run_attempt }}""
            }
        env:
          SLACK_WEBHOOK_URL: ${{ env.SLACK_WEBHOOK_URL }}
",58,2,1,workflow_run,1
vercel/next.js,retry_test.yml,"name: retry-tests

on:
  workflow_run:
    workflows: ['build-and-test', 'build-and-deploy']
    branches: [canary]
    types:
      - completed

env:
  SLACK_WEBHOOK_URL: ${{ secrets.BROKEN_CANARY_SLACK_WEBHOOK_URL }}

permissions:
  actions: write

jobs:
  retry-on-failure:
    name: retry failed jobs
    # Retry the build-and-test workflow up to 3 times
    if: >-
      ${{ 
        github.event.workflow_run.conclusion == 'failure' &&
        github.repository == 'vercel/next.js' &&
        github.event.workflow_run.run_attempt < 3
      }}
    runs-on: ubuntu-latest
    steps:
      - name: send retry request to GitHub API
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          gh api \
            --method POST \
            -H ""Accept: application/vnd.github+json"" \
            -H ""X-GitHub-Api-Version: 2022-11-28"" \
            /repos/${{ github.repository }}/actions/runs/${{ github.event.workflow_run.id }}/rerun-failed-jobs

  report-failure:
    name: report failure to slack
    # Report the failure to Slack if the build-and-test workflow has failed 3 times
    # build-and-deploy is not retried so we always report it
    if: >-
      ${{ 
        github.event.workflow_run.conclusion == 'failure' &&
        github.event.workflow_run.run_attempt >= 3 &&
        !github.event.workflow_run.head_repository.fork
      }}
    runs-on: ubuntu-latest
    steps:
      - name: send webhook
        uses: slackapi/slack-github-action@v1.25.0
        with:
          # These urls are intentionally missing the protocol,
          # allowing them to be transformed into actual links in the Slack workflow
          # (through slightly hacky means).
          payload: |
            {
              ""commit_title"": ${{ toJSON(github.event.workflow_run.display_title) }},
              ""commit_url"": ""github.com/${{ github.repository }}/commit/${{ github.event.workflow_run.head_sha }}"",
              ""workflow_run_url"": ""github.com/${{ github.repository }}/actions/runs/${{ github.event.workflow_run.id }}/attempts/${{ github.event.workflow_run.run_attempt }}""
            }
        env:
          SLACK_WEBHOOK_URL: ${{ env.SLACK_WEBHOOK_URL }}
",63,2,1,workflow_run,1
vercel/next.js,rspack-nextjs-build-integration-tests.yml,"name: Rspack Next.js production integration tests

on:
  schedule:
    # Run an hour earlier than the turbopack tests, so we don't overwhelm the CI
    - cron: '0 5 * * *'
  workflow_dispatch: {}

jobs:
  test-dev:
    name: Rspack integration tests
    uses: ./.github/workflows/integration_tests_reusable.yml
    with:
      name: rspack-production
      test_type: production
      run_before_test: |
        export NEXT_RSPACK=1 NEXT_TEST_USE_RSPACK=1 \
      # Failing tests take longer (due to timeouts and retries). Since we have
      # many failing tests, we need smaller groups and longer timeouts, in case
      # a group gets stuck with a cluster of failing tests.
      e2e_groups: 12
      integration_groups: 12
      e2e_timeout_minutes: 90
      integration_timeout_minutes: 90
    secrets: inherit
",25,1,2,"schedule, workflow_dispatch",1
vercel/next.js,rspack-nextjs-dev-integration-tests.yml,"name: Rspack Next.js development integration tests

on:
  schedule:
    # Run an hour earlier than the turbopack tests, so we don't overwhelm the CI
    - cron: '0 5 * * *'
  workflow_dispatch: {}

jobs:
  test-dev:
    name: Rspack integration tests
    uses: ./.github/workflows/integration_tests_reusable.yml
    with:
      name: rspack-development
      test_type: development
      run_before_test: |
        export NEXT_RSPACK=1 NEXT_TEST_USE_RSPACK=1
      # Failing tests take longer (due to timeouts and retries). Since we have
      # many failing tests, we need smaller groups and longer timeouts, in case
      # a group gets stuck with a cluster of failing tests.
      e2e_groups: 16
      integration_groups: 16
      e2e_timeout_minutes: 90
      integration_timeout_minutes: 90
    secrets: inherit
",25,1,2,"schedule, workflow_dispatch",1
vercel/next.js,rspack-update-tests-manifest.yml,"# A recurring workflow which updates the passing/failing/skipped integration tests for Turbopack.
name: Update Rspack test manifest

on:
  schedule:
    # Every day at 7AM https://crontab.guru/#0_7_*_*_*
    - cron: '0 7 * * *'
  workflow_dispatch:

jobs:
  update_dev_manifest:
    name: Update and upload Rspack development test manifest
    if: github.repository_owner == 'vercel'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          # Commits made with the default `GITHUB_TOKEN` won't trigger workflows.
          # See: https://docs.github.com/en/actions/security-guides/automatic-token-authentication#using-the-github_token-in-a-workflow
          token: ${{ secrets.RELEASE_BOT_GITHUB_TOKEN }}

      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_LTS_VERSION }}
          check-latest: true

      - name: Setup corepack
        run: |
          npm i -g corepack@0.31
          corepack enable

      - name: Install dependencies
        shell: bash
        run: pnpm i

      - name: Create Pull Request
        shell: bash
        run: node scripts/automated-update-workflow.js
        env:
          GITHUB_TOKEN: ${{ secrets.GH_TOKEN_PULL_REQUESTS }}
          BRANCH_NAME: rspack-manifest
          # We need to use `--override` for rspack (but not for turbopack).
          # We don't currently have any CI running on every PR, so it's quite
          # possible for us to regress on tests. We need to skip the
          # only-promote-to-passing merge logic.
          SCRIPT: test/update-bundler-manifest.js --bundler rspack --test-suite dev --override
          PR_TITLE: Update Rspack development test manifest
          PR_BODY: This auto-generated PR updates the development integration test manifest used when testing Rspack.
  update_build_manifest:
    name: Update and upload Rspack production test manifest
    if: github.repository_owner == 'vercel'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          # Commits made with the default `GITHUB_TOKEN` won't trigger workflows.
          # See: https://docs.github.com/en/actions/security-guides/automatic-token-authentication#using-the-github_token-in-a-workflow
          token: ${{ secrets.RELEASE_BOT_GITHUB_TOKEN }}

      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_LTS_VERSION }}
          check-latest: true

      - name: Setup corepack
        run: |
          npm i -g corepack@0.31
          corepack enable

      - name: Install dependencies
        shell: bash
        run: pnpm i

      - name: Create Pull Request
        shell: bash
        run: node scripts/automated-update-workflow.js
        env:
          GITHUB_TOKEN: ${{ secrets.GH_TOKEN_PULL_REQUESTS }}
          BRANCH_NAME: rspack-manifest
          SCRIPT: test/update-bundler-manifest.js --bundler rspack --test-suite build --override
          PR_TITLE: Update Rspack production test manifest
          PR_BODY: This auto-generated PR updates the production integration test manifest used when testing Rspack.
",86,2,2,"schedule, workflow_dispatch",4
vercel/next.js,setup-nextjs-build.yml,"# Reusable workflow to setup next.js integration test environment.
name: Setup Next.js

on:
  workflow_call:
    inputs:
      # Allow to specify Next.js version to run integration test against.
      # If not specified, will use latest release version including canary.
      version:
        type: string
      nodeVersion:
        required: false
        description: 'version of Node.js to use'
        type: string

jobs:
  build_nextjs:
    name: Build Next.js for the turbopack integration test
    runs-on:
      - 'self-hosted'
      - 'linux'
      - 'x64'
      - 'metal'
    outputs:
      output1: ${{ steps.build-next-swc-turbopack-patch.outputs.success }}
    steps:
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ inputs.nodeVersion || env.NODE_LTS_VERSION }}
          check-latest: true
      - name: Get number of CPU cores
        uses: SimenB/github-actions-cpu-cores@v2
        id: cpu-cores

      - name: 'Setup Rust toolchain'
        uses: dtolnay/rust-toolchain@stable

      - name: Display runner information
        run: echo runner cpu count ${{ steps.cpu-cores.outputs.count }}

      - name: Find Next.js latest release version
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          # Grab the latest release version from next.js repo, including prelease. `/releases/latest` will only return latest stable release.
          echo NEXJS_LATEST_VERSION=$(gh release --repo vercel/next.js --limit 1 list | sed -n 1p | awk '{print $1}') >> $GITHUB_ENV

      - name: Set Next.js release version
        run: |
          echo ""NEXTJS_VERSION=${{ inputs.version != '' && inputs.version || env.NEXJS_LATEST_VERSION }}"" >> $GITHUB_ENV

      - name: Print Next.js release version to checkout
        run: echo ""Checking out Next.js ${{ env.NEXTJS_VERSION }}""

      - name: Checkout Next.js
        uses: actions/checkout@v4
        with:
          repository: vercel/next.js
          ref: ${{ env.NEXTJS_VERSION }}

      - name: Checkout failed test lists
        uses: actions/checkout@v4
        with:
          repository: vercel/turbo
          ref: nextjs-integration-test-data
          path: integration-test-data

      - name: Download binary
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - uses: actions/cache/restore@v3
        id: restore-build
        with:
          path: |
            ./*
          key: ${{ inputs.version }}-${{ github.sha }}

      - name: Install dependencies
        run: |
          wget https://github.com/sharkdp/hyperfine/releases/download/v1.16.1/hyperfine_1.16.1_amd64.deb
          sudo dpkg -i hyperfine_1.16.1_amd64.deb
          npm i -g corepack@0.31
          corepack enable
          pnpm install --loglevel error

      - name: Build next-swc
        run: |
          hyperfine --min-runs 1 --show-output 'pnpm run --filter=@next/swc build-native --features plugin --release'
          echo ""Successfully built next-swc with published turbopack""

      - name: Build next.js
        run: |
          pnpm run build
          strip packages/next-swc/native/next-swc.*.node
          ls -al packages/next-swc/native
          # Reduce the size of the cache bit
          cd packages/next-swc && cargo clean && cd ../../
          echo NEXT_SWC_FILESIZE: $(stat -c %s packages/next-swc/native/next-swc.linux-x64-gnu.node)
          node -e ""console.log('Host', require('os').arch(), require('os').platform())""

      # If input version is published release, detect version by running next.js build.
      - name: Detects Next.js build version
        run: |
          # This is being used in github action to collect test results. Do not change it, or should update ./.github/actions/next-integration-test to match.
          docker run --rm -v $(pwd):/work mcr.microsoft.com/playwright:v1.28.1-jammy /bin/bash -c 'curl https://install-node.vercel.app/v16 | FORCE=1 bash && cd /work && echo RUNNING NEXTJS VERSION: $(packages/next/dist/bin/next --version) && ls -al packages/next-swc/native && node -e ""console.log(\""Container\"", require(\""os\"").arch(), require(\""os\"").platform())""'

      - name: Temporary test skip
        run: |
          rm -rf test/integration/jsconfig-paths/test/index.test.js

      # Once build completes, creates a cache of the build output
      # so subsequent job to actually execute tests can reuse it.
      # Note that we do not use upload / download artifacts for this -
      # it is too heavyweight for the purpose since we do not need to persist
      # the cache across multiple runs.
      - name: Store next.js build cache with next-swc
        uses: actions/cache/save@v3
        id: cache-build
        with:
          path: |
            ./*
          key: ${{ inputs.version }}-${{ github.sha }}-${{ github.run_id }}-${{ github.run_attempt}}-${{ github.run_number }}
",125,1,1,workflow_call,8
vercel/next.js,test-turbopack-rust-bench-test.yml,"name: Turbopack Rust testing benchmarks
on:
  workflow_call:
    inputs:
      runner:
        type: string
        default: '[""self-hosted"", ""linux"", ""x64"", ""metal""]'
      os:
        type: string
        default: 'linux'
      all:
        type: boolean
        default: false

env:
  TURBOPACK_BENCH_COUNTS: '100'
  TURBOPACK_BENCH_PROGRESS: '1'

  NODE_LTS_VERSION: 20

jobs:
  test:
    name: Test
    runs-on: ${{ fromJSON(inputs.runner) }}
    steps:
      - name: Set git to use LF
        run: |
          git config --global core.autocrlf false
          git config --global core.eol lf
        if: inputs.os == 'windows'

      - name: Checkout
        uses: actions/checkout@v3

      - name: Setup Rust
        uses: ./.github/actions/setup-rust

      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_LTS_VERSION }}
          check-latest: true
      - run: corepack enable

      # We need to install the dependencies for the benchmark apps
      - run: pnpm install
        working-directory: turbopack/benchmark-apps

      - name: Build benchmarks for tests
        timeout-minutes: 120
        run: |
          cargo test --benches --workspace --release --no-fail-fast --exclude turbopack-bench --exclude next-swc-napi --no-run

      - name: Run cargo test on benchmarks
        timeout-minutes: 120
        run: |
          cargo test --benches --workspace --release --no-fail-fast --exclude turbopack-bench --exclude next-swc-napi

      - name: Build benchmarks for tests for other bundlers
        if: inputs.all
        timeout-minutes: 120
        run: |
          cargo test --benches --release -p turbopack-bench --no-run

      - name: Run cargo test on benchmarks for other bundlers
        if: inputs.all
        timeout-minutes: 120
        run: |
          cargo test --benches --release -p turbopack-bench
",69,1,1,workflow_call,3
vercel/next.js,test_e2e_deploy_release.yml,"name: test-e2e-deploy-release

on:
  # run on every release/prerelease
  release:
    types: [published]
  # allow triggering manually as well
  workflow_dispatch:
    inputs:
      nextVersion:
        description: canary or custom tarball URL
        default: canary
        type: string

env:
  VERCEL_TEST_TEAM: vtest314-next-e2e-tests
  VERCEL_TEST_TOKEN: ${{ secrets.VERCEL_TEST_TOKEN }}
  DATADOG_API_KEY: ${{ secrets.DATA_DOG_API_KEY }}
  TURBO_TEAM: 'vercel'
  TURBO_CACHE: 'remote:rw'
  TURBO_API: ${{ secrets.HOSTED_TURBO_API }}
  TURBO_TOKEN: ${{ secrets.HOSTED_TURBO_TOKEN }}
  DD_ENV: 'ci'

jobs:
  setup:
    runs-on: ubuntu-latest
    if: github.repository_owner == 'vercel'
    steps:
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_LTS_VERSION }}
          check-latest: true

      - name: Setup pnpm
        run: |
          npm i -g corepack@0.31
          corepack enable

      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 25

      - name: Setup test project
        run: |
          pnpm install
          pnpm run build
          node scripts/run-e2e-test-project-reset.mjs

  test-deploy:
    name: test deploy
    needs: setup
    uses: ./.github/workflows/build_reusable.yml
    secrets: inherit
    strategy:
      fail-fast: false
      matrix:
        group: [1/6, 2/6, 3/6, 4/6, 5/6, 6/6]
    with:
      afterBuild: npm i -g vercel@latest && NEXT_E2E_TEST_TIMEOUT=240000 NEXT_TEST_MODE=deploy NEXT_EXTERNAL_TESTS_FILTERS=""test/deploy-tests-manifest.json"" NEXT_TEST_VERSION=""${{ github.event.inputs.nextVersion || 'canary' }}"" node run-tests.js --timings -g ${{ matrix.group }} -c 2 --type e2e
      skipNativeBuild: 'yes'
      skipNativeInstall: 'no'
      stepName: 'test-deploy-${{ matrix.group }}'
      timeout_minutes: 180
      runs_on_labels: '[""ubuntu-latest""]'

  report-test-results-to-datadog:
    needs: test-deploy
    if: ${{ always() }}

    runs-on: ubuntu-latest
    name: report test results to datadog
    steps:
      - name: Download test report artifacts
        id: download-test-reports
        uses: actions/download-artifact@v4
        with:
          pattern: test-reports-*
          path: test
          merge-multiple: true

      - name: Upload test report to datadog
        run: |
          if [ -d ./test/test-junit-report ]; then
            DD_ENV=ci npx @datadog/datadog-ci@2.23.1 junit upload --tags test.type:deploy --service nextjs ./test/test-junit-report
          fi
",88,3,2,"release, workflow_dispatch",4
vercel/next.js,test_examples.yml,"# This file duplicates bunch of things from build_test_deploy

on:
  workflow_dispatch:
    inputs:
      is_dispatched:
        description: 'Leave this option enabled'
        required: true
        default: true
        type: boolean
  schedule:
    - cron: '0 */4 * * *'

name: Test examples

jobs:
  testExamples:
    # Don't execute using cron on forks
    if: (github.repository == 'vercel/next.js') || (inputs.is_dispatched == true)
    name: Test Examples
    runs-on: ubuntu-latest
    timeout-minutes: 120
    env:
      NEXT_TELEMETRY_DISABLED: 1
    strategy:
      fail-fast: false
      matrix:
        node: [18, 20]
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 25
      # https://github.com/actions/virtual-environments/issues/1187
      - name: tune linux network
        run: sudo ethtool -K eth0 tx off rx off

      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: 18
          check-latest: true
      - name: Setup corepack
        run: |
          npm i -g corepack@0.31
          corepack enable

      - run: pnpm install
      - run: pnpm build

      - run: docker run --rm -v $(pwd):/work mcr.microsoft.com/playwright:v1.35.1-focal /bin/bash -c ""cd /work && curl -s https://install-node.vercel.app/v${{ matrix.node }} | FORCE=1 bash && node -v && corepack enable > /dev/null && NEXT_TEST_JOB=1 NEXT_TEST_MODE=start xvfb-run node run-tests.js --type examples >> /proc/1/fd/1""
        name: Run test/examples
",51,1,2,"workflow_dispatch, schedule",2
vercel/next.js,triage.yml,"name: Triage issues

on:
  issues:
    types: [opened, labeled]
  issue_comment:
    types: [created]

env:
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

permissions:
  issues: write

jobs:
  triage:
    name: Nissuer
    runs-on: ubuntu-latest
    if: >-
      ${{
        (github.event_name != 'issue_comment' ||
        (github.event_name == 'issue_comment' && !contains(github.event.issue.labels.*.name, 'stale'))) &&
        github.event.issue.type.name != 'Documentation'
      }}
    steps:
      - uses: balazsorban44/nissuer@1.10.0
        with:
          label-area-prefix: ''
          label-area-match: 'name'
          label-area-section: 'Which area\(s\) are affected\? \(Select all that apply\)(.*)### Additional context'
          label-comments: |
            {
              ""good first issue"": "".github/comments/good-first-issue.md"",
              ""please add a complete reproduction"": "".github/comments/invalid-reproduction.md"",
              ""please simplify reproduction"": "".github/comments/simplify-reproduction.md"",
              ""please verify canary"": "".github/comments/verify-canary.md"",
              ""resolved"": "".github/comments/resolved.md""
            }
          reproduction-comment: '.github/comments/invalid-link.md'
          reproduction-hosts: 'github.com,bitbucket.org,gitlab.com,codesandbox.io,stackblitz.com'
          reproduction-blocklist: 'github.com/vercel/next.js.*,github.com/\\w*/?$,github.com$'
          reproduction-link-section: '### Link to the code that reproduces this issue(.*)### To Reproduce'
          reproduction-invalid-label: 'invalid link'
          reproduction-issue-labels: 'bug,'
          comment-unhelpful-weight: 0.5
          webhook-url: ${{ secrets.NISSUER_WEBHOOK_URL }}
          webhook-secret: ${{ secrets.NISSUER_WEBHOOK_SECRET }}
",47,1,2,"issues, issue_comment",1
vercel/next.js,triage_with_ai.yml,"name: Triage Issues with AI

on:
  issues:
    types: [opened]

jobs:
  triage:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
      - name: Setup corepack
        run: |
          npm i -g corepack@0.31
          corepack enable
      - name: 'Send report to Slack (AI-powered)'
        run: node ./.github/actions/next-repo-actions/dist/triage-issues-with-ai/index.js
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      SLACK_TOKEN: ${{ secrets.SLACK_TOKEN }}
      VERCEL_PROTECTION_BYPASS: ${{ secrets.VERCEL_PROTECTION_BYPASS }}
",22,1,1,issues,2
vercel/next.js,trigger_release.yml,"on:
  schedule:
    # run every day at 23:15
    - cron: '15 23 * * *'

  workflow_dispatch:
    inputs:
      releaseType:
        description: stable, canary, or release candidate?
        required: true
        type: choice
        options:
          - canary
          - stable
          - release-candidate

      semverType:
        description: semver type?
        type: choice
        options:
          - patch
          - minor
          - major

      force:
        description: create a new release even if there are no new commits
        default: false
        type: boolean

    secrets:
      RELEASE_BOT_GITHUB_TOKEN:
        required: true

name: Trigger Release

env:
  NAPI_CLI_VERSION: 2.18.4
  TURBO_VERSION: 2.3.3
  NODE_LTS_VERSION: 20

jobs:
  start:
    if: github.repository_owner == 'vercel'
    runs-on: ubuntu-latest
    env:
      NEXT_TELEMETRY_DISABLED: 1
      # we build a dev binary for use in CI so skip downloading
      # canary next-swc binaries in the monorepo
      NEXT_SKIP_NATIVE_POSTINSTALL: 1

    environment: release-${{ github.event.inputs.releaseType || 'canary' }}
    steps:
      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: 18
          check-latest: true

      - name: Clone Next.js repository
        run: git clone https://github.com/vercel/next.js.git --depth=25 --single-branch --branch ${GITHUB_REF_NAME:-canary} .

      - name: Check token
        run: gh auth status
        env:
          GITHUB_TOKEN: ${{ secrets.RELEASE_BOT_GITHUB_TOKEN }}

      - name: Get commit of the latest tag
        run: echo ""LATEST_TAG_COMMIT=$(git rev-list -n 1 $(git describe --tags --abbrev=0))"" >> $GITHUB_ENV

      - name: Get latest commit
        run: echo ""LATEST_COMMIT=$(git rev-parse HEAD)"" >> $GITHUB_ENV

      - name: Check if new commits since last tag
        if: ${{ github.event.inputs.releaseType != 'stable' && github.event.inputs.force != true }}
        run: |
          if [ ""$LATEST_TAG_COMMIT"" = ""$LATEST_COMMIT"" ]; then
            echo ""No new commits. Exiting...""
            exit 1
          fi

      # https://github.com/actions/virtual-environments/issues/1187
      - name: tune linux network
        run: sudo ethtool -K eth0 tx off rx off

      - name: Setup corepack
        run: |
          npm i -g corepack@0.31
          corepack enable
          pnpm --version

      - id: get-store-path
        run: echo STORE_PATH=$(pnpm store path) >> $GITHUB_OUTPUT

      - uses: actions/cache@v4
        timeout-minutes: 5
        id: cache-pnpm-store
        with:
          path: ${{ steps.get-store-path.outputs.STORE_PATH }}
          key: pnpm-store-${{ hashFiles('pnpm-lock.yaml') }}
          restore-keys: |
            pnpm-store-
            pnpm-store-${{ hashFiles('pnpm-lock.yaml') }}

      - run: pnpm install

      - run: pnpm run build

      - run: node ./scripts/start-release.js --release-type ${{ github.event.inputs.releaseType || 'canary' }} --semver-type ${{ github.event.inputs.semverType }}
        env:
          RELEASE_BOT_GITHUB_TOKEN: ${{ secrets.RELEASE_BOT_GITHUB_TOKEN }}
",110,1,2,"schedule, workflow_dispatch",2
vercel/next.js,trigger_release_new.yml,"name: Trigger Release (New)

on:
  # Run every day at 23:15 UTC
  # TODO: Disabled cron for now, but uncomment
  # once replaced the old release workflow.
  # schedule:
  #   - cron: '15 23 * * *'
  # Run manually
  workflow_dispatch:
    inputs:
      releaseType:
        description: Release Type
        required: true
        type: choice
        # Cron job will run canary release
        default: canary
        options:
          - canary
          - stable
          - release-candidate

      force:
        description: Forced Release
        default: false
        type: boolean

concurrency: ${{ github.workflow }}-${{ github.ref }}

env:
  NAPI_CLI_VERSION: 2.18.4
  TURBO_VERSION: 2.3.3
  NODE_LTS_VERSION: 20

permissions:
  # To create PR
  pull-requests: write

jobs:
  start:
    if: github.repository_owner == 'vercel'
    runs-on: ubuntu-latest
    env:
      NEXT_TELEMETRY_DISABLED: 1
      # we build a dev binary for use in CI so skip downloading
      # canary next-swc binaries in the monorepo
      NEXT_SKIP_NATIVE_POSTINSTALL: 1

    environment: release-${{ github.event.inputs.releaseType }}
    steps:
      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_LTS_VERSION }}
          check-latest: true

      # Since actions/checkout won't include the latest tag information,
      # use the old clone workflow while still preserving branch specific
      # checkout behavior to support backports.
      # x-ref: https://github.com/vercel/next.js/pull/63167
      - name: Clone Next.js repository
        run: git clone https://github.com/vercel/next.js.git --depth=25 --single-branch --branch ${GITHUB_REF_NAME:-canary} .

      - name: Check token
        run: gh auth status
        env:
          GITHUB_TOKEN: ${{ secrets.RELEASE_BOT_GITHUB_TOKEN }}

      - name: Get commit of the latest tag
        run: echo ""LATEST_TAG_COMMIT=$(git rev-list -n 1 $(git describe --tags --abbrev=0))"" >> $GITHUB_ENV

      - name: Get latest commit
        run: echo ""LATEST_COMMIT=$(git rev-parse HEAD)"" >> $GITHUB_ENV

      - name: Check if new commits since last tag
        if: ${{ github.event.inputs.releaseType != 'stable' && github.event.inputs.force != true }}
        run: |
          if [ ""$LATEST_TAG_COMMIT"" = ""$LATEST_COMMIT"" ]; then
            echo ""No new commits. Exiting...""
            exit 1
          fi

      # https://github.com/actions/virtual-environments/issues/1187
      - name: tune linux network
        run: sudo ethtool -K eth0 tx off rx off

      - name: Setup corepack
        run: |
          npm i -g corepack@0.31
          corepack enable
          pnpm --version

      - id: get-store-path
        run: echo STORE_PATH=$(pnpm store path) >> $GITHUB_OUTPUT

      - uses: actions/cache@v4
        timeout-minutes: 5
        id: cache-pnpm-store
        with:
          path: ${{ steps.get-store-path.outputs.STORE_PATH }}
          key: pnpm-store-${{ hashFiles('pnpm-lock.yaml') }}
          restore-keys: |
            pnpm-store-
            pnpm-store-${{ hashFiles('pnpm-lock.yaml') }}

      - run: pnpm install
      - run: pnpm run build

      - name: Create Release Pull Request
        id: changesets
        uses: changesets/action@v1
        with:
          version: pnpm ci:version
        env:
          GITHUB_TOKEN: ${{ secrets.RELEASE_BOT_GITHUB_TOKEN }}
          RELEASE_TYPE: ${{ github.event.inputs.releaseType }}

      # Add label to verify the PR is created from this workflow.
      - name: Add label to PR
        if: steps.changesets.outputs.pullRequestNumber
        run: 'gh pr edit ${{ steps.changesets.outputs.pullRequestNumber }} --add-label ""created-by: CI""'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
",123,1,1,workflow_dispatch,3
vercel/next.js,turbopack-benchmark.yml,"name: Turbopack Benchmark

on:
  workflow_dispatch:
  push:
    branches:
      - canary
  pull_request:
    types: ['opened', 'reopened', 'synchronize', 'labeled']
    paths:
      - '**/crates/**'
      - '**/Cargo.toml'
      - '**/Cargo.lock'

concurrency:
  group: ${{ github.workflow }}-${{ github.sha }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}

env:
  CI: 1
  CARGO_INCREMENTAL: 0
  # For faster CI
  RUST_LOG: 'off'
  TURBO_TEAM: 'vercel'
  TURBO_CACHE: 'remote:rw'
  TURBO_TOKEN: ${{ secrets.HOSTED_TURBO_TOKEN }}

jobs:
  benchmark-tiny:
    name: Benchmark Rust Crates (tiny)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Rust toolchain
        uses: ./.github/actions/setup-rust

      - name: Install cargo-codspeed
        uses: taiki-e/install-action@v2
        with:
          tool: cargo-codspeed@2.10.1

      - name: Build app build benchmarks
        run: cargo codspeed build -p next-api

      - name: Run the benchmarks
        uses: CodSpeedHQ/action@v3
        with:
          run: cargo codspeed run
          token: ${{ secrets.CODSPEED_TOKEN }}

  benchmark-small-apps:
    name: Benchmark Rust Crates (small apps)
    runs-on: ['self-hosted', 'linux', 'x64', 'metal']
    steps:
      - uses: actions/checkout@v4

      - name: Setup Rust toolchain
        uses: ./.github/actions/setup-rust

      - name: Install cargo-codspeed
        uses: taiki-e/install-action@v2
        with:
          tool: cargo-codspeed@2.10.1

      - name: Cache on ${{ github.ref_name }}
        uses: ijjk/rust-cache@turbo-cache-v1.0.8
        with:
          save-if: 'true'
          cache-provider: 'turbo'
          shared-key: build-turbopack-benchmark-small-apps-${{ hashFiles('.cargo/config.toml') }}

      - name: Install pnpm dependencies
        working-directory: turbopack/benchmark-apps
        run: |
          npm i -g corepack@0.31
          corepack enable
          pnpm install --loglevel error

      - name: Build app build benchmarks
        run: cargo codspeed build -p turbopack-cli small_apps

      - name: Run the benchmarks
        uses: CodSpeedHQ/action@v3
        with:
          run: cargo codspeed run
          token: ${{ secrets.CODSPEED_TOKEN }}

  benchmark-large:
    name: Benchmark Rust Crates (large)
    if: ${{ github.event.label.name == 'benchmark' || github.event_name == 'workflow_dispatch' }}
    runs-on: ['self-hosted', 'linux', 'x64', 'metal']
    steps:
      - uses: actions/checkout@v4

      - name: Setup Rust toolchain
        uses: ./.github/actions/setup-rust

      - name: Install cargo-codspeed
        uses: taiki-e/install-action@v2
        with:
          tool: cargo-codspeed@2.10.1

      - name: Build the benchmark target(s)
        run: cargo codspeed build -p turbopack -p turbopack-bench

      - name: Run the benchmarks
        uses: CodSpeedHQ/action@v3
        with:
          run: cargo codspeed run
          token: ${{ secrets.CODSPEED_TOKEN }}
",111,3,3,"workflow_dispatch, push, pull_request",13
vercel/next.js,turbopack-nextjs-build-integration-tests.yml,"name: Turbopack Next.js production integration tests

on:
  schedule:
    - cron: '0 6 * * *'
  workflow_dispatch: {}

jobs:
  test-dev:
    name: Next.js integration tests
    uses: ./.github/workflows/integration_tests_reusable.yml
    with:
      name: turbopack-production
      test_type: production
      run_before_test: |
        export IS_TURBOPACK_TEST=1 TURBOPACK_BUILD=1
      # Failing tests take longer (due to timeouts and retries). Since we have
      # many failing tests, we need smaller groups and longer timeouts, in case
      # a group gets stuck with a cluster of failing tests.
      e2e_groups: 12
      integration_groups: 12
      e2e_timeout_minutes: 60
      integration_timeout_minutes: 60
    secrets: inherit
",24,1,2,"schedule, workflow_dispatch",1
vercel/next.js,turbopack-nextjs-dev-integration-tests.yml,"name: Turbopack Next.js development integration tests

on:
  schedule:
    - cron: '0 6 * * *'
  workflow_dispatch: {}

jobs:
  test-dev:
    name: Next.js integration tests
    uses: ./.github/workflows/integration_tests_reusable.yml
    with:
      name: turbopack-development
      test_type: development
      run_before_test: |
        export IS_TURBOPACK_TEST=1 TURBOPACK_DEV=1
    secrets: inherit
",17,1,2,"schedule, workflow_dispatch",1
vercel/next.js,turbopack-update-tests-manifest.yml,"# A recurring workflow which updates the passing/failing/skipped integration tests for Turbopack.
name: Update Turbopack test manifest

on:
  schedule:
    # Every day at 7AM UTC https://crontab.guru/#0_7_*_*_*
    - cron: '0 7 * * *'
  workflow_dispatch:

jobs:
  update_dev_manifest:
    name: Update and upload Turbopack development test manifest
    if: github.repository_owner == 'vercel'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          # Commits made with the default `GITHUB_TOKEN` won't trigger workflows.
          # See: https://docs.github.com/en/actions/security-guides/automatic-token-authentication#using-the-github_token-in-a-workflow
          token: ${{ secrets.RELEASE_BOT_GITHUB_TOKEN }}

      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_LTS_VERSION }}
          check-latest: true

      - name: Setup corepack
        run: |
          npm i -g corepack@0.31
          corepack enable

      - name: Install dependencies
        shell: bash
        run: pnpm i

      - name: Create Pull Request
        shell: bash
        run: node scripts/automated-update-workflow.js
        env:
          GITHUB_TOKEN: ${{ secrets.GH_TOKEN_PULL_REQUESTS }}
          BRANCH_NAME: turbopack-manifest
          SCRIPT: test/update-bundler-manifest.js --bundler turbopack --test-suite dev
          PR_TITLE: Update Turbopack development test manifest
          PR_BODY: This auto-generated PR updates the development integration test manifest used when testing Turbopack.
  update_build_manifest:
    name: Update and upload Turbopack production test manifest
    if: github.repository_owner == 'vercel'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          # Commits made with the default `GITHUB_TOKEN` won't trigger workflows.
          # See: https://docs.github.com/en/actions/security-guides/automatic-token-authentication#using-the-github_token-in-a-workflow
          token: ${{ secrets.RELEASE_BOT_GITHUB_TOKEN }}

      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_LTS_VERSION }}
          check-latest: true

      - name: Setup corepack
        run: |
          npm i -g corepack@0.31
          corepack enable

      - name: Install dependencies
        shell: bash
        run: pnpm i

      - name: Create Pull Request
        shell: bash
        run: node scripts/automated-update-workflow.js
        env:
          GITHUB_TOKEN: ${{ secrets.GH_TOKEN_PULL_REQUESTS }}
          BRANCH_NAME: turbopack-manifest
          SCRIPT: test/update-bundler-manifest.js --bundler turbopack --test-suite build
          PR_TITLE: Update Turbopack production test manifest
          PR_BODY: This auto-generated PR updates the production integration test manifest used when testing Turbopack.
",82,2,2,"schedule, workflow_dispatch",4
vercel/next.js,update_fonts_data.yml,"name: Update Font Data

on:
  # Run every every day at midnight https://crontab.guru/#0_0_*_*_*/1
  schedule:
    - cron: '0 0 * * */1'
  # Allow manual runs
  workflow_dispatch:

env:
  NODE_LTS_VERSION: 20

jobs:
  create-pull-request:
    runs-on: ubuntu-latest
    if: github.repository_owner == 'vercel'
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          # Commits made with the default `GITHUB_TOKEN` won't trigger workflows.
          # See: https://docs.github.com/en/actions/security-guides/automatic-token-authentication#using-the-github_token-in-a-workflow
          token: ${{ secrets.RELEASE_BOT_GITHUB_TOKEN }}

      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_LTS_VERSION }}
          check-latest: true

      - name: Setup corepack
        run: |
          npm i -g corepack@0.31
          corepack enable

      - name: Install dependencies
        shell: bash
        run: pnpm i

      - name: Create Pull Request
        shell: bash
        run: node scripts/automated-update-workflow.js
        env:
          GITHUB_TOKEN: ${{ secrets.GH_TOKEN_PULL_REQUESTS }}
          BRANCH_NAME: fonts-data
          SCRIPT: scripts/update-google-fonts.js
          PR_TITLE: Update font data
          PR_BODY: This auto-generated PR updates font data with latest available
",48,1,2,"schedule, workflow_dispatch",2
vercel/next.js,update_react.yml,"name: Update React

on:
  schedule:
    # At 40 minutes past 16:00 on Mon, Tue, Wed, Thu, and Fri
    # i.e. 30min past React nightlies: https://github.com/facebook/react/blob/941e1b4a0a81ca3d5f2ac6ef35682e2f8e96dae1/.github/workflows/runtime_prereleases_nightly.yml#L6
    # TODO: automatically trigger on React release
    - cron: 40 16 * * 1,2,3,4,5
  # Allow manual runs
  workflow_dispatch:
    inputs:
      version:
        description: 'The version to update to. Uses latest Canary if omitted.'
        required: false

env:
  NODE_LTS_VERSION: 20
  PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: 1

jobs:
  create-pull-request:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          # Commits made with the default `GITHUB_TOKEN` won't trigger workflows.
          # See: https://docs.github.com/en/actions/security-guides/automatic-token-authentication#using-the-github_token-in-a-workflow
          token: ${{ secrets.RELEASE_BOT_GITHUB_TOKEN }}

      - name: Set Git author
        run: |
          git config user.name ""vercel-release-bot""
          git config user.email ""infra+release@vercel.com""

      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_LTS_VERSION }}
          check-latest: true

      - name: Setup corepack
        run: |
          npm i -g corepack@0.31
          corepack enable

      - name: Install dependencies
        shell: bash
        # Just need scripts/ but those dependencies are listed in the workspace root.
        run: pnpm install --filter .

      - name: Create Pull Request
        shell: bash
        run: pnpm sync-react --actor ""${{ github.actor }}"" --commit --create-pull --version ""${{ inputs.version }}""
        env:
          GITHUB_TOKEN: ${{ secrets.GH_TOKEN_PULL_REQUESTS }}
",56,1,2,"schedule, workflow_dispatch",2
vercel/next.js,upload-tests-manifest.yml,"# Workflow to upload next.js integration test results to KV for https://areweturboyet.com/
# This workflow assumes the `next-integration-test` workflow has been executed
# and test reports have been uploaded to the `test-results` artifact.
name: Upload bundler test manifests to areweturboyet.com

on:
  schedule:
    - cron: '0 8 * * *'
  workflow_dispatch: {}
  push:
    branches:
      - canary
    paths:
      - 'test/*-manifest.json'

jobs:
  upload_test_results:
    name: Upload test results
    runs-on: ubuntu-latest
    steps:
      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_LTS_VERSION }}
          check-latest: true

      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup corepack
        run: |
          npm i -g corepack@0.31
          corepack enable

      - name: Install dependencies
        shell: bash
        run: pnpm i

      - name: 'Upload results to ""Are We Turbo Yet"" KV'
        env:
          TURBOYET_KV_REST_API_URL: ${{ secrets.TURBOYET_KV_REST_API_URL }}
          TURBOYET_KV_REST_API_TOKEN: ${{ secrets.TURBOYET_KV_REST_API_TOKEN }}
          TURBOYET_TOKEN: ${{ secrets.TURBOYET_TOKEN }}
        uses: ./.github/actions/upload-turboyet-data
",44,1,3,"schedule, workflow_dispatch, push",3
f/awesome-chatgpt-prompts,ai_bot.yml,"name: AI Bot

on:
  issues:
    types: [opened]
  issue_comment:
    types: [created]
  pull_request_review_comment:
    types: [created]
  pull_request:
    types: [opened, edited, synchronize]

jobs:
  respond-to-commands:
    runs-on: ubuntu-latest
    if: |
      (github.actor == 'f') &&
      ((github.event_name == 'issues' && contains(github.event.issue.body, '/ai')) ||
      (github.event_name == 'issue_comment' && contains(github.event.comment.body, '/ai')) ||
      (github.event_name == 'pull_request_review_comment' && contains(github.event.comment.body, '/ai')) ||
      (github.event_name == 'pull_request' && contains(github.event.pull_request.body, '/ai')))
    permissions:
      contents: write
      pull-requests: write
      issues: write

    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: ""18""

      - name: Install dependencies
        run: npm install openai@^4.0.0 @octokit/rest@^19.0.0

      - name: Process command
        id: process
        env:
          GH_TOKEN: ${{ secrets.PAT_TOKEN }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          node << 'EOF'
          const OpenAI = require('openai');
          const { Octokit } = require('@octokit/rest');

          async function main() {
            const openai = new OpenAI({
              apiKey: process.env.OPENAI_API_KEY
            });

            const octokit = new Octokit({
              auth: process.env.GH_TOKEN
            });

            const eventName = process.env.GITHUB_EVENT_NAME;
            const eventPath = process.env.GITHUB_EVENT_PATH;
            const event = require(eventPath);

            // Double check user authorization
            const actor = event.sender?.login || event.pull_request?.user?.login || event.issue?.user?.login;
            if (actor !== 'f') {
              console.log('Unauthorized user attempted to use the bot:', actor);
              return;
            }

            // Get command and context
            let command = '';
            let issueNumber = null;
            let isPullRequest = false;

            if (eventName === 'issues') {
              command = event.issue.body;
              issueNumber = event.issue.number;
            } else if (eventName === 'issue_comment') {
              command = event.comment.body;
              issueNumber = event.issue.number;
              isPullRequest = !!event.issue.pull_request;
            } else if (eventName === 'pull_request_review_comment') {
              command = event.comment.body;
              issueNumber = event.pull_request.number;
              isPullRequest = true;
            } else if (eventName === 'pull_request') {
              command = event.pull_request.body;
              issueNumber = event.pull_request.number;
              isPullRequest = true;
            }

            if (!command.startsWith('/ai')) {
              return;
            }

            // Extract the actual command after /ai
            const aiCommand = command.substring(3).trim();

            // Handle resolve conflicts command
            if (aiCommand === 'resolve' || aiCommand === 'fix conflicts') {
              if (!isPullRequest) {
                console.log('Command rejected: Not a pull request');
                await octokit.issues.createComment({
                  owner: event.repository.owner.login,
                  repo: event.repository.name,
                  issue_number: issueNumber,
                  body: '❌ The resolve command can only be used on pull requests.'
                });
                return;
              }

              try {
                console.log('Starting resolve command execution...');

                // Get PR details
                console.log('Fetching PR details...');
                const { data: pr } = await octokit.pulls.get({
                  owner: event.repository.owner.login,
                  repo: event.repository.name,
                  pull_number: issueNumber
                });
                console.log(`Original PR found: #${issueNumber} from ${pr.user.login}`);

                // Get the PR diff to extract the new prompt
                console.log('Fetching PR file changes...');
                const { data: files } = await octokit.pulls.listFiles({
                  owner: event.repository.owner.login,
                  repo: event.repository.name,
                  pull_number: issueNumber
                });
                console.log(`Found ${files.length} changed files`);

                // Extract prompt from changes
                console.log('Analyzing changes to extract prompt information...');
                const prompts = new Map(); // Use Map to deduplicate prompts by title

                // Helper function to normalize prompt titles
                const normalizeTitle = (title) => {
                  title = title.trim();
                  // Remove ""Act as"" or ""Act as a"" or ""Act as an"" from start if present
                  title = title.replace(/^Act as (?:a |an )?/i, '');

                  // Capitalize each word except common articles and prepositions
                  const lowercaseWords = ['a', 'an', 'the', 'and', 'but', 'or', 'for', 'nor', 'on', 'at', 'to', 'for', 'with', 'in'];
                  const capitalized = title.toLowerCase().split(' ').map((word, index) => {
                    // Always capitalize first and last word
                    if (index === 0 || !lowercaseWords.includes(word)) {
                      return word.charAt(0).toUpperCase() + word.slice(1);
                    }
                    return word;
                  }).join(' ');

                  // Add ""Act as"" prefix
                  return `Act as ${capitalized}`;
                };

                // First try to find prompts in README
                let foundInReadme = false;
                for (const file of files) {
                  console.log(`Processing file: ${file.filename}`);
                  if (file.filename === 'README.md') {
                    const patch = file.patch || '';
                    const addedLines = patch.split('\n')
                      .filter(line => line.startsWith('+'))
                      .map(line => line.substring(1))
                      .join('\n');

                    console.log('Attempting to extract prompts from README changes...');
                    const promptMatches = [...addedLines.matchAll(/## (?:Act as (?:a |an )?)?([^\n]+)\n(?:Contributed by:[^\n]*\n)?(?:> )?([^#]+?)(?=\n##|\n\n##|$)/ig)];

                    for (const match of promptMatches) {
                      const actName = normalizeTitle(match[1]);
                      const promptText = match[2].trim()
                        .replace(/^(?:Contributed by:?[^\n]*\n\s*)+/i, '')
                        .trim();
                      const contributorLine = addedLines.match(/Contributed by: \[@([^\]]+)\]\(https:\/\/github\.com\/([^\)]+)\)/);
                      const contributorInfo = contributorLine
                        ? `Contributed by: [@${contributorLine[1]}](https://github.com/${contributorLine[2]})`
                        : `Contributed by: [@${pr.user.login}](https://github.com/${pr.user.login})`;

                      prompts.set(actName.toLowerCase(), { actName, promptText, contributorInfo });
                      console.log(`Found prompt in README: ""${actName}""`);
                      foundInReadme = true;
                    }
                  }
                }

                // Only look in CSV if we didn't find anything in README
                if (!foundInReadme) {
                  console.log('No prompts found in README, checking CSV...');
                  for (const file of files) {
                    if (file.filename === 'prompts.csv') {
                      const patch = file.patch || '';
                      const addedLines = patch.split('\n')
                        .filter(line => line.startsWith('+'))
                        .map(line => line.substring(1))
                        .filter(line => line.trim()); // Remove empty lines

                      console.log('Attempting to extract prompts from CSV changes...');
                      for (const line of addedLines) {
                        // Parse CSV line considering escaped quotes
                        const matches = [...line.matchAll(/""([^""]*(?:""""[^""]*)*)""/g)];
                        if (matches.length >= 2) {
                          const actName = normalizeTitle(matches[0][1].replace(/""""/g, '""').trim());
                          const promptText = matches[1][1].replace(/""""/g, '""').trim()
                            .replace(/^(?:Contributed by:?[^\n]*\n\s*)+/i, '')
                            .trim();

                          const contributorInfo = `Contributed by: [@${pr.user.login}](https://github.com/${pr.user.login})`;
                          prompts.set(actName.toLowerCase(), { actName, promptText, contributorInfo });
                          console.log(`Found prompt in CSV: ""${actName}""`);
                        }
                      }
                    }
                  }
                }

                if (prompts.size === 0) {
                  console.log('Failed to extract prompt information');
                  await octokit.issues.createComment({
                    owner: event.repository.owner.login,
                    repo: event.repository.name,
                    issue_number: issueNumber,
                    body: '❌ Could not extract prompt information from changes'
                  });
                  return;
                }

                // Get content from main branch
                console.log('Fetching current content from main branch...');
                const { data: readmeFile } = await octokit.repos.getContent({
                  owner: event.repository.owner.login,
                  repo: event.repository.name,
                  path: 'README.md',
                  ref: 'main'
                });

                const { data: csvFile } = await octokit.repos.getContent({
                  owner: event.repository.owner.login,
                  repo: event.repository.name,
                  path: 'prompts.csv',
                  ref: 'main'
                });

                // Prepare new content
                console.log('Preparing content updates...');
                let readmeContent = Buffer.from(readmeFile.content, 'base64').toString('utf-8');
                let csvContent = Buffer.from(csvFile.content, 'base64').toString('utf-8');
                if (!csvContent.endsWith('\n')) csvContent += '\n';

                // Convert Map to array for processing
                const promptsArray = Array.from(prompts.values());

                // Process each prompt
                for (const { actName, promptText, contributorInfo } of promptsArray) {
                  // Remove markdown quote character and trim whitespace
                  const cleanPrompt = promptText.replace(/^>\s*/gm, '').trim();

                  // For README: Add quote to each line
                  const readmePrompt = cleanPrompt.split('\n')
                    .map(line => `> ${line.trim()}`)
                    .join('\n');
                  const newSection = `## ${actName}\n${contributorInfo}\n\n${readmePrompt}\n\n`;

                  // For CSV: Convert to single paragraph
                  const csvPrompt = cleanPrompt.replace(/\n+/g, ' ').trim();

                  // Insert the new section before Contributors in README
                  const contributorsIndex = readmeContent.indexOf('## Contributors');
                  if (contributorsIndex === -1) {
                    console.log('Contributors section not found, appending to end');
                    readmeContent += newSection;
                  } else {
                    console.log('Inserting before Contributors section');
                    readmeContent = readmeContent.slice(0, contributorsIndex) + newSection + readmeContent.slice(contributorsIndex);
                  }

                  // Add to CSV content
                  csvContent += `""${actName.replace(/^Act as an?/i, '').replace(/""/g, '""""')}"",""${csvPrompt.replace(/""/g, '""""')}""\n`;
                }

                // Create new branch
                const branchName = `prompt/${promptsArray.map(p => p.actName.toLowerCase().replace(/[^a-z0-9]+/g, '-')).join('-')}`;
                console.log(`Creating new branch: ${branchName}`);

                // Check if branch exists and delete it
                try {
                  console.log('Checking if branch already exists...');
                  const { data: existingRef } = await octokit.git.getRef({
                    owner: event.repository.owner.login,
                    repo: event.repository.name,
                    ref: `heads/${branchName}`
                  });

                  if (existingRef) {
                    // Check for existing PRs from this branch
                    console.log('Checking for existing PRs from this branch...');
                    const { data: existingPRs } = await octokit.pulls.list({
                      owner: event.repository.owner.login,
                      repo: event.repository.name,
                      head: `${event.repository.owner.login}:${branchName}`,
                      state: 'open'
                    });

                    // Close any existing PRs
                    for (const pr of existingPRs) {
                      console.log(`Closing existing PR #${pr.number}...`);
                      await octokit.pulls.update({
                        owner: event.repository.owner.login,
                        repo: event.repository.name,
                        pull_number: pr.number,
                        state: 'closed'
                      });
                    }

                    console.log('Branch exists, deleting it...');
                    await octokit.git.deleteRef({
                      owner: event.repository.owner.login,
                      repo: event.repository.name,
                      ref: `heads/${branchName}`
                    });
                    console.log('Existing branch deleted');
                  }
                } catch (error) {
                  // 404 means branch doesn't exist, which is fine
                  if (error.status !== 404) {
                    throw error;
                  }
                  console.log('Branch does not exist, proceeding with creation');
                }

                // Get main branch ref
                const { data: mainRef } = await octokit.git.getRef({
                  owner: event.repository.owner.login,
                  repo: event.repository.name,
                  ref: 'heads/main'
                });

                // Create new branch
                await octokit.git.createRef({
                  owner: event.repository.owner.login,
                  repo: event.repository.name,
                  ref: `refs/heads/${branchName}`,
                  sha: mainRef.object.sha
                });

                // Get current files from the new branch
                console.log('Getting current file SHAs...');
                const { data: currentReadme } = await octokit.repos.getContent({
                  owner: event.repository.owner.login,
                  repo: event.repository.name,
                  path: 'README.md',
                  ref: branchName
                });

                const { data: currentCsv } = await octokit.repos.getContent({
                  owner: event.repository.owner.login,
                  repo: event.repository.name,
                  path: 'prompts.csv',
                  ref: branchName
                });

                // Update files with correct author
                console.log('Updating README.md...');
                await octokit.repos.createOrUpdateFileContents({
                  owner: event.repository.owner.login,
                  repo: event.repository.name,
                  path: 'README.md',
                  message: promptsArray.length === 1
                    ? `feat: Add ""${promptsArray[0].actName}"" to README`
                    : `feat: Add multiple prompts to README`,
                  content: Buffer.from(readmeContent).toString('base64'),
                  branch: branchName,
                  sha: currentReadme.sha,
                  committer: {
                    name: pr.user.login,
                    email: `${pr.user.login}@users.noreply.github.com`
                  },
                  author: {
                    name: pr.user.login,
                    email: `${pr.user.login}@users.noreply.github.com`
                  }
                });

                console.log('Updating prompts.csv...');
                await octokit.repos.createOrUpdateFileContents({
                  owner: event.repository.owner.login,
                  repo: event.repository.name,
                  path: 'prompts.csv',
                  message: promptsArray.length === 1
                    ? `feat: Add ""${promptsArray[0].actName}"" to prompts.csv`
                    : `feat: Add multiple prompts to prompts.csv`,
                  content: Buffer.from(csvContent).toString('base64'),
                  branch: branchName,
                  sha: currentCsv.sha,
                  committer: {
                    name: pr.user.login,
                    email: `${pr.user.login}@users.noreply.github.com`
                  },
                  author: {
                    name: pr.user.login,
                    email: `${pr.user.login}@users.noreply.github.com`
                  }
                });

                // Create new PR
                const prTitle = promptsArray.length === 1
                  ? `feat: Add ""${promptsArray[0].actName}""`
                  : `feat: Add multiple prompts (${promptsArray.map(p => `""${p.actName}""`).join(', ')})`;

                const prBody = promptsArray.length === 1
                  ? `This PR supersedes #${issueNumber} with proper formatting. Original PR by @${pr.user.login}. Added ""${promptsArray[0].actName}"" to README.md and prompts.csv, preserving original attribution.`
                  : `This PR supersedes #${issueNumber} with proper formatting. Original PR by @${pr.user.login}.\n\nAdded the following prompts:\n${promptsArray.map(p => `- ""${p.actName}""`).join('\n')}\n\nAll prompts have been added to README.md and prompts.csv, preserving original attribution.`;

                const { data: newPr } = await octokit.pulls.create({
                  owner: event.repository.owner.login,
                  repo: event.repository.name,
                  title: prTitle,
                  head: branchName,
                  base: 'main',
                  body: prBody
                });

                // Comment on original PR
                await octokit.issues.createComment({
                  owner: event.repository.owner.login,
                  repo: event.repository.name,
                  issue_number: issueNumber,
                  body: `I've created a new PR #${newPr.number} with your contribution properly formatted. This PR will be closed in favor of the new one.`
                });

                // Close original PR
                await octokit.pulls.update({
                  owner: event.repository.owner.login,
                  repo: event.repository.name,
                  pull_number: issueNumber,
                  state: 'closed'
                });

                console.log(`Created new PR #${newPr.number} and closed original PR #${issueNumber}`);

              } catch (error) {
                console.error('Error details:', error);
                await octokit.issues.createComment({
                  owner: event.repository.owner.login,
                  repo: event.repository.name,
                  issue_number: issueNumber,
                  body: `❌ Error while trying to create new PR:\n\`\`\`\n${error.message}\n\`\`\``
                });
              }
              return;
            }

            // Handle rename command specifically
            if (aiCommand.startsWith('rename') || aiCommand === 'suggest title') {
              if (!isPullRequest) {
                await octokit.issues.createComment({
                  owner: event.repository.owner.login,
                  repo: event.repository.name,
                  issue_number: issueNumber,
                  body: '❌ The rename command can only be used on pull requests.'
                });
                return;
              }

              // Get PR details for context
              const { data: pr } = await octokit.pulls.get({
                owner: event.repository.owner.login,
                repo: event.repository.name,
                pull_number: issueNumber
              });

              // Get the list of files changed in the PR
              const { data: files } = await octokit.pulls.listFiles({
                owner: event.repository.owner.login,
                repo: event.repository.name,
                pull_number: issueNumber
              });

              // Process file changes
              const fileChanges = await Promise.all(files.map(async file => {
                if (file.status === 'removed') {
                  return `Deleted: ${file.filename}`;
                }

                // Get file content for added or modified files
                if (file.status === 'added' || file.status === 'modified') {
                  const patch = file.patch || '';
                  return `${file.status === 'added' ? 'Added' : 'Modified'}: ${file.filename}\nChanges:\n${patch}`;
                }

                return `${file.status}: ${file.filename}`;
              }));

              const completion = await openai.chat.completions.create({
                model: ""gpt-3.5-turbo"",
                messages: [
                  {
                    role: ""system"",
                    content: ""You are a helpful assistant that generates clear and concise pull request titles. Follow these rules:\n1. Use conventional commit style (feat:, fix:, docs:, etc.)\n2. Focus on WHAT changed, not HOW or WHERE\n3. Keep it short and meaningful\n4. Don't mention file names or technical implementation details\n5. Return ONLY the new title, nothing else\n\nGood examples:\n- feat: Add \""Act as a Career Coach\""\n- fix: Correct typo in Linux Terminal prompt\n- docs: Update installation instructions\n- refactor: Improve error handling""
                  },
                  {
                    role: ""user"",
                    content: `Based on these file changes, generate a concise PR title:\n\n${fileChanges.join('\n\n')}`
                  }
                ],
                temperature: 0.5,
                max_tokens: 60
              });

              const newTitle = completion.choices[0].message.content.trim();

              // Update PR title
              await octokit.pulls.update({
                owner: event.repository.owner.login,
                repo: event.repository.name,
                pull_number: issueNumber,
                title: newTitle
              });

              // Add comment about the rename
              await octokit.issues.createComment({
                owner: event.repository.owner.login,
                repo: event.repository.name,
                issue_number: issueNumber,
                body: `✨ Updated PR title to: ""${newTitle}""\n\nBased on the following changes:\n\`\`\`diff\n${fileChanges.join('\n')}\n\`\`\``
              });
              return;
            }

            // Handle other commands
            const completion = await openai.chat.completions.create({
              model: ""gpt-3.5-turbo"",
              messages: [
                {
                  role: ""system"",
                  content: ""You are a helpful AI assistant that helps with GitHub repositories. You can suggest code changes, fix issues, and improve code quality.""
                },
                {
                  role: ""user"",
                  content: aiCommand
                }
              ],
              temperature: 0.7,
              max_tokens: 2000
            });

            const response = completion.choices[0].message.content;

            // If response contains code changes, create a new branch and PR
            if (response.includes('```')) {
              const branchName = `ai-bot/fix-${issueNumber}`;

              // Create new branch
              const defaultBranch = event.repository.default_branch;
              const ref = await octokit.git.getRef({
                owner: event.repository.owner.login,
                repo: event.repository.name,
                ref: `heads/${defaultBranch}`
              });

              await octokit.git.createRef({
                owner: event.repository.owner.login,
                repo: event.repository.name,
                ref: `refs/heads/${branchName}`,
                sha: ref.data.object.sha
              });

              // Extract code changes and file paths from response
              const codeBlocks = response.match(/```[\s\S]*?```/g);
              for (const block of codeBlocks) {
                const [_, filePath, ...codeLines] = block.split('\n');
                const content = Buffer.from(codeLines.join('\n')).toString('base64');

                await octokit.repos.createOrUpdateFileContents({
                  owner: event.repository.owner.login,
                  repo: event.repository.name,
                  path: filePath,
                  message: `AI Bot: Apply suggested changes for #${issueNumber}`,
                  content,
                  branch: branchName
                });
              }

              // Create PR
              await octokit.pulls.create({
                owner: event.repository.owner.login,
                repo: event.repository.name,
                title: `AI Bot: Fix for #${issueNumber}`,
                body: `This PR was automatically generated in response to #${issueNumber}\n\nChanges proposed:\n${response}`,
                head: branchName,
                base: defaultBranch
              });
            }

            // Add comment with response
            await octokit.issues.createComment({
              owner: event.repository.owner.login,
              repo: event.repository.name,
              issue_number: issueNumber,
              body: `AI Bot Response:\n\n${response}`
            });
          }

          main().catch(error => {
            console.error('Error:', error);
            process.exit(1);
          });
          EOF

      - name: Handle errors
        if: failure()
        uses: actions/github-script@v6
        with:
          script: |
            const issueNumber = context.issue.number || context.payload.pull_request.number;
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: issueNumber,
              body: '❌ Sorry, there was an error processing your command. Please try again or contact the repository maintainers.'
            });
",623,1,4,"issues, issue_comment, pull_request_review_comment, pull_request",3
f/awesome-chatgpt-prompts,auto_commands.yml,"name: Auto AI Commands

on:
  pull_request:
    types: [opened, reopened, synchronize]
  pull_request_target:
    types: [opened, reopened, synchronize]

jobs:
  check-and-comment:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
      issues: write

    steps:
      - name: Check PR status and comment
        uses: actions/github-script@v6
        with:
          script: |
            const pr = context.payload.pull_request;

            // Check if PR has conflicts
            if (pr.mergeable === false) {
              console.log('PR has conflicts, commenting /ai resolve');
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: pr.number,
                body: '/ai resolve'
              });
            }

            // Check if PR title starts with ""updated""
            if (pr.title.toLowerCase().startsWith('updated')) {
              console.log('PR title starts with ""updated"", commenting /ai suggest title');
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: pr.number,
                body: '/ai suggest title'
              });
            }
",43,1,2,"pull_request, pull_request_target",1
f/awesome-chatgpt-prompts,csv_linter.yml,"name: CSV Linter and Trailing Whitespaces

on:
  push:
  pull_request:

jobs:
  lint_and_check_trailing_whitespaces:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: ""3.8""

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install csvkit

      - name: Validate CSV structure
        run: |
          echo ""Checking CSV structure...""
          if ! csvclean -n prompts.csv 2>&1 | tee /tmp/csv_errors.log; then
            echo ""::error::CSV validation failed""
            cat /tmp/csv_errors.log
            exit 1
          fi

      - name: Check CSV format
        run: |
          echo ""Checking CSV format...""
          if ! python -c '
          import csv
          with open(""prompts.csv"", ""r"", encoding=""utf-8"") as f:
              reader = csv.reader(f)
              headers = next(reader)
              if headers != [""act"", ""prompt"", ""for_devs""]:
                  print(""Error: CSV headers must be exactly [act, prompt, for_devs]"")
                  exit(1)
              for row_num, row in enumerate(reader, 3):
                  if len(row) != 3:
                      print(f""Error: Row {row_num} has {len(row)} columns, expected 2"")
                      exit(1)
                  if not row[0] or not row[1] or not row[2]:
                      print(f""Error: Row {row_num} has empty values"")
                      exit(1)
          '; then
            echo ""::error::CSV format check failed""
            exit 1
          fi

      - name: Check Trailing Whitespaces
        run: |
          echo ""Checking for trailing whitespaces...""
          if grep -q ""[[:space:]]$"" prompts.csv; then
            echo ""::error::Found trailing whitespaces in prompts.csv""
            grep -n ""[[:space:]]$"" prompts.csv | while read -r line; do
              echo ""Line with trailing whitespace: $line""
            done
            exit 0
          fi
          echo ""No trailing whitespaces found""

      - name: Check for UTF-8 BOM and line endings
        run: |
          echo ""Checking for UTF-8 BOM and line endings...""
          if file prompts.csv | grep -q ""with BOM""; then
            echo ""::error::File contains UTF-8 BOM marker""
            exit 1
          fi
          if file prompts.csv | grep -q ""CRLF""; then
            echo ""::error::File contains Windows-style (CRLF) line endings""
            exit 1
          fi
",77,1,2,"push, pull_request",2
f/awesome-chatgpt-prompts,publish.yml,"name: Publish to GitHub Pages

on:
  push:
    branches:
      - main

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Ruby
        uses: ruby/setup-ruby@v1
        with:
          ruby-version: ""3.2""
          bundler-cache: true # This will cache dependencies

      - name: Update Bundler
        run: |
          gem update --system
          bundle update --bundler

      - name: Install dependencies
        run: |
          rm -f Gemfile.lock # Remove existing lockfile
          bundle install

      - name: Build site
        run: bundle exec jekyll build

      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./_site
",39,1,1,push,3
yangshun/tech-interview-handbook,lint.yml,"# Copied from https://github.com/facebook/docusaurus/blob/main/.github/workflows/lint.yml
name: Lint

on:
  pull_request:
    branches:
      - main

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

permissions:
  contents: read

env:
  DATABASE_URL: 'postgresql://postgres:password@localhost:5432/postgres'
  GITHUB_CLIENT_ID: '1234'
  GITHUB_CLIENT_SECRET: 'abcd'
  NEXTAUTH_SECRET: 'efgh'
  NEXTAUTH_URL: 'http://localhost:3000'
  NODE_ENV: test
  SUPABASE_ANON_KEY: 'ijkl'
  SUPABASE_URL: 'https://abcd.supabase.co'

jobs:
  lint:
    name: Lint
    timeout-minutes: 30
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      - name: Set up Node
        uses: actions/setup-node@v3
        with:
          node-version: '16'
          cache: yarn
      - name: Installation
        run: yarn
      - name: Check immutable yarn.lock
        run: git diff --exit-code
      - name: Lint
        run: yarn lint
",44,1,1,pull_request,2
yangshun/tech-interview-handbook,tsc.yml,"# Copied from https://github.com/facebook/docusaurus/blob/main/.github/workflows/lint.yml
name: Typecheck

on:
  pull_request:
    branches:
      - main

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

permissions:
  contents: read

jobs:
  tsc:
    name: Typecheck
    timeout-minutes: 30
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      - name: Set up Node
        uses: actions/setup-node@v3
        with:
          node-version: '16'
          cache: yarn
      - name: Installation
        run: yarn
      - name: Check immutable yarn.lock
        run: git diff --exit-code
      # Build the shared types in dependent packages.
      - name: Build dependencies
        run: yarn turbo run build --filter=ui
      - name: Typecheck
        run: yarn tsc
",37,1,1,pull_request,2
Chalarangelo/30-seconds-of-code,deploy-production.yml,"name: Deploy production
on:
  schedule:
    - cron: ""20 18 * * *""
  workflow_dispatch:
jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v1
    - name: Run the Netlify build hook
      env:
        DEPLOY_URL: ${{ secrets.NETLIFY_BUILD_HOOK_URL }}
        DEPLOY_TRIGGER: ${{ github.event_name }}
      run: |
        chmod +x ./bin/deploy
        ./bin/deploy production
",17,1,2,"schedule, workflow_dispatch",1
Chalarangelo/30-seconds-of-code,label.yml,"# This workflow will triage pull requests and apply a label based on the
# paths that are modified in the pull request.
#
# To use this workflow, you will need to set up a .github/labeler.yml
# file with configuration.  For more information, see:
# https://github.com/actions/labeler

name: Labeler
on: [pull_request_target]

jobs:
  label:

    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write

    steps:
    - uses: actions/labeler@v4
      with:
        repo-token: ""${{ secrets.GITHUB_TOKEN }}""
",22,1,1,pull_request_target,1
Chalarangelo/30-seconds-of-code,stale.yml,"# This workflow warns and then closes issues and PRs that have had no activity for a specified amount of time.
#
# You can adjust the behavior by modifying this file.
# For more information, see:
# https://github.com/actions/stale
name: Mark stale issues and pull requests

on:
  schedule:
  - cron: '44 0 * * *'

jobs:
  stale:

    runs-on: ubuntu-latest
    permissions:
      issues: write
      pull-requests: write

    steps:
    - uses: actions/stale@v5
      with:
        repo-token: ${{ secrets.GITHUB_TOKEN }}
        stale-issue-message: 'This issue has been inactive for a while, marking as stale.'
        stale-pr-message: 'This pull request has been inactive for a while, marking as stale.'
        stale-issue-label: 'stale'
        stale-pr-label: 'stale'
        days-before-stale: 14
        days-before-close: 7
        exempt-assignees: 'Chalarangelo'
        close-issue-message: 'This issue has been stale for a while, closing due to inactivity.'
        close-pr-message: 'This pull request has been inactive for a while, closing due to inactivity.'
",32,1,1,schedule,1
Chalarangelo/30-seconds-of-code,test.yml,"name: Run tests
permissions:
  contents: read
on:
  push:
    branches: [ master ]
  pull_request:
    branches: [ master ]
jobs:
  js_tests:
    name: 'Node.js ${{ matrix.node-version }} tests'
    runs-on: ubuntu-latest
    strategy:
      matrix:
        node-version: [22.14.0]
    steps:
    - uses: actions/checkout@v4
    - name: Use Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v1
      with:
        node-version: ${{ matrix.node-version }}
    - run: npm ci
    - run: npm test
",23,1,2,"push, pull_request",2
Genymobile/scrcpy,release.yml,"name: Build

on:
  workflow_dispatch:
    inputs:
      name:
        description: 'Version name (default is ref name)'

env:
  # $VERSION is used by release scripts
  VERSION: ${{ github.event.inputs.name || github.ref_name }}

jobs:
  test-scrcpy-server:
    runs-on: ubuntu-latest
    env:
      GRADLE: gradle  # use native gradle instead of ./gradlew in scripts
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup JDK
        uses: actions/setup-java@v4
        with:
          distribution: 'zulu'
          java-version: '17'

      - name: Test scrcpy-server
        run: release/test_server.sh

  build-scrcpy-server:
    runs-on: ubuntu-latest
    env:
      GRADLE: gradle  # use native gradle instead of ./gradlew in scripts
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup JDK
        uses: actions/setup-java@v4
        with:
          distribution: 'zulu'
          java-version: '17'

      - name: Build
        run: release/build_server.sh

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: scrcpy-server
          path: release/work/build-server/server/scrcpy-server

  test-build-scrcpy-server-without-gradle:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup JDK
        uses: actions/setup-java@v4
        with:
          distribution: 'zulu'
          java-version: '17'

      - name: Build without gradle
        run: server/build_without_gradle.sh

  test-client:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt update
          sudo apt install -y meson ninja-build nasm ffmpeg libsdl2-2.0-0 \
             libsdl2-dev libavcodec-dev libavdevice-dev libavformat-dev \
             libavutil-dev libswresample-dev libusb-1.0-0 libusb-1.0-0-dev \
             libv4l-dev

      - name: Test
        run: release/test_client.sh

  build-linux-x86_64:
    runs-on: ubuntu-22.04
    steps:
      - name: Check architecture
        run: |
            arch=$(uname -m)
            if [[ ""$arch"" != x86_64 ]]
            then
                echo ""Unexpected architecture: $arch"" >&2
                exit 1
            fi

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt update
          sudo apt install -y meson ninja-build nasm ffmpeg libsdl2-2.0-0 \
             libsdl2-dev libavcodec-dev libavdevice-dev libavformat-dev \
             libavutil-dev libswresample-dev libusb-1.0-0 libusb-1.0-0-dev \
             libv4l-dev

      - name: Build
        run: release/build_linux.sh x86_64

      # upload-artifact does not preserve permissions
      - name: Tar
        run: |
            cd release/work/build-linux-x86_64
            mkdir dist-tar
            cd dist-tar
            tar -C .. -cvf dist.tar.gz dist/

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: build-linux-x86_64-intermediate
          path: release/work/build-linux-x86_64/dist-tar/

  build-win32:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt update
          sudo apt install -y meson ninja-build nasm ffmpeg libsdl2-2.0-0 \
             libsdl2-dev libavcodec-dev libavdevice-dev libavformat-dev \
             libavutil-dev libswresample-dev libusb-1.0-0 libusb-1.0-0-dev \
             mingw-w64 mingw-w64-tools libz-mingw-w64-dev

      - name: Build
        run: release/build_windows.sh 32

      # upload-artifact does not preserve permissions
      - name: Tar
        run: |
            cd release/work/build-win32
            mkdir dist-tar
            cd dist-tar
            tar -C .. -cvf dist.tar.gz dist/

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: build-win32-intermediate
          path: release/work/build-win32/dist-tar/

  build-win64:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt update
          sudo apt install -y meson ninja-build nasm ffmpeg libsdl2-2.0-0 \
             libsdl2-dev libavcodec-dev libavdevice-dev libavformat-dev \
             libavutil-dev libswresample-dev libusb-1.0-0 libusb-1.0-0-dev \
             mingw-w64 mingw-w64-tools libz-mingw-w64-dev

      - name: Build
        run: release/build_windows.sh 64

      # upload-artifact does not preserve permissions
      - name: Tar
        run: |
            cd release/work/build-win64
            mkdir dist-tar
            cd dist-tar
            tar -C .. -cvf dist.tar.gz dist/

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: build-win64-intermediate
          path: release/work/build-win64/dist-tar/

  build-macos-aarch64:
    runs-on: macos-latest
    steps:
      - name: Check architecture
        run: |
            arch=$(uname -m)
            if [[ ""$arch"" != arm64 ]]
            then
                echo ""Unexpected architecture: $arch"" >&2
                exit 1
            fi

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install dependencies
        run: |
            brew install meson ninja nasm libiconv zlib automake autoconf \
                libtool

      - name: Build
        env:
          # the default Xcode (and macOS SDK) version can be found at
          # <https://github.com/actions/runner-images/blob/main/images/macos/macos-15-Readme.md#xcode>
          #
          # then the minimal supported deployment target of that macOS SDK can be found at
          # <https://developer.apple.com/support/xcode/#minimum-requirements>
          MACOSX_DEPLOYMENT_TARGET: 10.13
        run: release/build_macos.sh aarch64

      # upload-artifact does not preserve permissions
      - name: Tar
        run: |
            cd release/work/build-macos-aarch64
            mkdir dist-tar
            cd dist-tar
            tar -C .. -cvf dist.tar.gz dist/

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: build-macos-aarch64-intermediate
          path: release/work/build-macos-aarch64/dist-tar/

  build-macos-x86_64:
    runs-on: macos-13
    steps:
      - name: Check architecture
        run: |
            arch=$(uname -m)
            if [[ ""$arch"" != x86_64 ]]
            then
                echo ""Unexpected architecture: $arch"" >&2
                exit 1
            fi

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install dependencies
        run: brew install meson ninja nasm libiconv zlib automake
             # autoconf and libtool are already installed on macos-13

      - name: Build
        env:
          # the default Xcode (and macOS SDK) version can be found at
          # <https://github.com/actions/runner-images/blob/main/images/macos/macos-13-Readme.md#xcode>
          #
          # then the minimal supported deployment target of that macOS SDK can be found at
          # <https://developer.apple.com/support/xcode/#minimum-requirements>
          MACOSX_DEPLOYMENT_TARGET: 10.13
        run: release/build_macos.sh x86_64

      # upload-artifact does not preserve permissions
      - name: Tar
        run: |
            cd release/work/build-macos-x86_64
            mkdir dist-tar
            cd dist-tar
            tar -C .. -cvf dist.tar.gz dist/

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: build-macos-x86_64-intermediate
          path: release/work/build-macos-x86_64/dist-tar/

  package-linux-x86_64:
    needs:
      - build-scrcpy-server
      - build-linux-x86_64
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download scrcpy-server
        uses: actions/download-artifact@v4
        with:
          name: scrcpy-server
          path: release/work/build-server/server/

      - name: Download build-linux-x86_64
        uses: actions/download-artifact@v4
        with:
          name: build-linux-x86_64-intermediate
          path: release/work/build-linux-x86_64/dist-tar/

      # upload-artifact does not preserve permissions
      - name: Detar
        run: |
            cd release/work/build-linux-x86_64
            tar xf dist-tar/dist.tar.gz

      - name: Package
        run: release/package_client.sh linux-x86_64 tar.gz

      - name: Upload release
        uses: actions/upload-artifact@v4
        with:
          name: release-linux-x86_64
          path: release/output/

  package-win32:
    needs:
      - build-scrcpy-server
      - build-win32
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download scrcpy-server
        uses: actions/download-artifact@v4
        with:
          name: scrcpy-server
          path: release/work/build-server/server/

      - name: Download build-win32
        uses: actions/download-artifact@v4
        with:
          name: build-win32-intermediate
          path: release/work/build-win32/dist-tar/

      # upload-artifact does not preserve permissions
      - name: Detar
        run: |
            cd release/work/build-win32
            tar xf dist-tar/dist.tar.gz

      - name: Package
        run: release/package_client.sh win32 zip

      - name: Upload release
        uses: actions/upload-artifact@v4
        with:
          name: release-win32
          path: release/output/

  package-win64:
    needs:
      - build-scrcpy-server
      - build-win64
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download scrcpy-server
        uses: actions/download-artifact@v4
        with:
          name: scrcpy-server
          path: release/work/build-server/server/

      - name: Download build-win64
        uses: actions/download-artifact@v4
        with:
          name: build-win64-intermediate
          path: release/work/build-win64/dist-tar/

      # upload-artifact does not preserve permissions
      - name: Detar
        run: |
            cd release/work/build-win64
            tar xf dist-tar/dist.tar.gz

      - name: Package
        run: release/package_client.sh win64 zip

      - name: Upload release
        uses: actions/upload-artifact@v4
        with:
          name: release-win64
          path: release/output

  package-macos-aarch64:
    needs:
      - build-scrcpy-server
      - build-macos-aarch64
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download scrcpy-server
        uses: actions/download-artifact@v4
        with:
          name: scrcpy-server
          path: release/work/build-server/server/

      - name: Download build-macos-aarch64
        uses: actions/download-artifact@v4
        with:
          name: build-macos-aarch64-intermediate
          path: release/work/build-macos-aarch64/dist-tar/

      # upload-artifact does not preserve permissions
      - name: Detar
        run: |
            cd release/work/build-macos-aarch64
            tar xf dist-tar/dist.tar.gz

      - name: Package
        run: release/package_client.sh macos-aarch64 tar.gz

      - name: Upload release
        uses: actions/upload-artifact@v4
        with:
          name: release-macos-aarch64
          path: release/output/

  package-macos-x86_64:
    needs:
      - build-scrcpy-server
      - build-macos-x86_64
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download scrcpy-server
        uses: actions/download-artifact@v4
        with:
          name: scrcpy-server
          path: release/work/build-server/server/

      - name: Download build-macos
        uses: actions/download-artifact@v4
        with:
          name: build-macos-x86_64-intermediate
          path: release/work/build-macos-x86_64/dist-tar/

      # upload-artifact does not preserve permissions
      - name: Detar
        run: |
            cd release/work/build-macos-x86_64
            tar xf dist-tar/dist.tar.gz

      - name: Package
        run: release/package_client.sh macos-x86_64 tar.gz

      - name: Upload release
        uses: actions/upload-artifact@v4
        with:
          name: release-macos-x86_64
          path: release/output/

  release:
    needs:
      - build-scrcpy-server
      - package-linux-x86_64
      - package-win32
      - package-win64
      - package-macos-aarch64
      - package-macos-x86_64
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download scrcpy-server
        uses: actions/download-artifact@v4
        with:
          name: scrcpy-server
          path: release/work/build-server/server/

      - name: Download release-linux-x86_64
        uses: actions/download-artifact@v4
        with:
          name: release-linux-x86_64
          path: release/output/

      - name: Download release-win32
        uses: actions/download-artifact@v4
        with:
          name: release-win32
          path: release/output/

      - name: Download release-win64
        uses: actions/download-artifact@v4
        with:
          name: release-win64
          path: release/output/

      - name: Download release-macos-aarch64
        uses: actions/download-artifact@v4
        with:
          name: release-macos-aarch64
          path: release/output/

      - name: Download release-macos-x86_64
        uses: actions/download-artifact@v4
        with:
          name: release-macos-x86_64
          path: release/output/

      - name: Package server
        run: release/package_server.sh

      - name: Generate checksums
        run: release/generate_checksums.sh

      - name: Upload release artifact
        uses: actions/upload-artifact@v4
        with:
          name: scrcpy-release-${{ env.VERSION }}
          path: release/output
",514,15,1,workflow_dispatch,46
facebook/react-native,autorebase.yml,"name: Automatic Rebase
# This workflow is used to automatically rebase a PR when a comment is made
# containing the text ""/rebase"". It uses the cirrus-actions/rebase action.
# See https://github.com/cirrus-actions/rebase
on:
  issue_comment:
    types: [created]
permissions:
  contents: read
jobs:
  rebase:
    name: Rebase
    permissions:
      contents: write # for cirrus-actions/rebase to push code to rebase
      pull-requests: read # for cirrus-actions/rebase to get info about PR
    runs-on: ubuntu-latest
    if: github.event.issue.pull_request != '' && contains(github.event.comment.body, '/rebase')
    steps:
      - name: Checkout the latest code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0 # otherwise, you will fail to push refs to dest repo
      - name: Automatic Rebase
        uses: cirrus-actions/rebase@1.8
        env:
          GITHUB_TOKEN: ${{ secrets.REACT_NATIVE_BOT_GITHUB_TOKEN }}
",27,1,1,issue_comment,2
facebook/react-native,bump-podfile-lock.yml,"name: Bump Podfile.lock

on:
  workflow_call: # this directive allow us to call this workflow from other workflows

jobs:
  bump-podfile-lock:
    runs-on: macos-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          fetch-tags: true
      - name: Install dependencies
        uses: ./.github/actions/yarn-install
      - name: Configure git
        run: |
          git config --local user.email ""bot@reactnative.dev""
          git config --local user.name ""React Native Bot""
      - name: Extract branch name
        run: |
          TAG=""${{ github.ref_name }}"";
          BRANCH_NAME=$(echo ""$TAG"" | sed -E 's/v([0-9]+\.[0-9]+)\.[0-9]+(-rc\.[0-9]+)?/\1-stable/')
          echo ""Branch Name is $BRANCH_NAME""
          echo ""BRANCH_NAME=$BRANCH_NAME"" >> $GITHUB_ENV
      - name: Checkout release branch
        run: |
          git checkout ""$BRANCH_NAME""
          git fetch
          git pull origin ""$BRANCH_NAME""
      - name: Bump podfile.lock
        run: |
          cd packages/rn-tester
          bundle install
          bundle exec pod update hermes-engine --no-repo-update
      - name: Commit changes
        run: |
          git add packages/rn-tester/Podfile.lock
          git commit -m ""[LOCAL] Bump Podfile.lock""
          git push origin ""$BRANCH_NAME""
",41,1,1,workflow_call,2
facebook/react-native,cache-reaper.yml,"name: Keep Github Actions Cache < 10GB

on:
  workflow_dispatch:
  schedule:
    # Run every 2hrs during weekdays
    - cron: ""0 0/2 * * 1-5""

jobs:
  cache-cleaner:
    if: github.repository == 'facebook/react-native'
    runs-on: ubuntu-latest
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - uses: actions/checkout@v4
      - name: Setup Node.js
        uses: ./.github/actions/setup-node
      - name: Trim the cache
        run: node scripts/clean-gha-cache.js
",20,1,2,"workflow_dispatch, schedule",2
facebook/react-native,check-for-reproducer.yml,"name: Check for reproducer
# This workflow is triggered when issue is created or edited.
on:
  issues:
    types: [opened, edited]

jobs:
  check-for-reproducer:
    runs-on: ubuntu-latest
    if: |
      github.repository == 'facebook/react-native' && github.event.issue.pull_request == null && github.event.issue.state == 'open' && !contains(github.event.issue.labels.*.name, ':open_umbrella: Umbrella')
    steps:
      - uses: actions/checkout@v4
      - uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.REACT_NATIVE_BOT_GITHUB_TOKEN }}
          script: |
            const checkForReproducer = require('./.github/workflow-scripts/checkForReproducer.js')
            await checkForReproducer(github, context)
",19,1,1,issues,2
facebook/react-native,check-nightly.yml,"# This jobs runs every day 2 hours after the nightly job and its purpose is to report
# a failure in case the nightly failed to be published. We are going to hook this to an internal automation.
name: Check Nightlies

on:
  workflow_dispatch:
  # nightly build @ 4:15 AM UTC
  schedule:
    - cron: '15 4 * * *'

jobs:
  check-nightly:
    runs-on: ubuntu-latest
    if: github.repository == 'facebook/react-native'
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Check nightly
        run: |
          TODAY=$(date ""+%Y%m%d"")
          echo ""Checking nightly for $TODAY""
          NIGHTLY=""$(npm view react-native | grep $TODAY)""
          if [[ -z $NIGHTLY ]]; then
            echo 'Nightly job failed.'
            exit 1
          else
            echo 'Nightly Worked, All Good!'
          fi

  test-libraries:
    uses: ./.github/workflows/test-libraries-on-nightlies.yml
    needs: check-nightly
    secrets:
      discord_webhook_url: ${{ secrets.NIGHTLY_DISCORD_WEBHOOK }}
",34,2,2,"workflow_dispatch, schedule",2
facebook/react-native,close-pr.yml,"name: Label closed PR as merged and leave a comment
on:
  push

permissions:
  contents: read
  pull-requests: write

jobs:
  comment-and-label:
    runs-on: ubuntu-latest
    if: github.repository == 'facebook/react-native'
    steps:
      - uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.REACT_NATIVE_BOT_GITHUB_TOKEN }}
          script: |
            if(!context.payload.commits || !context.payload.commits.length) return;
            const sha = context.payload.commits[0].id;

            const {commit, author} = (await github.rest.repos.getCommit({
              ref: sha,
              owner: context.repo.owner,
              repo: context.repo.repo,
            })).data;

            // Looking at the commit message, checks which PR number, if any, was closed by this commit
            const getClosedPrIfExists = (commit) => {
              if(!commit || !commit.message) return;
              const prClosingRegex = /Closes https:\/\/github.com\/facebook\/react-native\/pull\/([0-9]+)|Pull Request resolved: https:\/\/github.com\/facebook\/react-native\/pull\/([0-9]+)/;
              const prClosingMatch = commit.message.match(prClosingRegex);
              if(!prClosingMatch || (!prClosingMatch[1] && ! prClosingMatch[2])) return;
              return prClosingMatch[1] ?? prClosingMatch[2];
            };

            const closedPrNumber = getClosedPrIfExists(commit);
            if(!closedPrNumber) return;

            const pr = (await github.rest.pulls.get({
              pull_number: closedPrNumber,
              owner: context.repo.owner,
              repo: context.repo.repo,
            })).data;

            const authorName = author?.login ? `@${author.login}` : commit.author.name;

            github.rest.issues.createComment({
              issue_number: closedPrNumber,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `This pull request was successfully merged by ${authorName} in **${sha}**\n\n<sup>[When will my fix make it into a release?](https://github.com/reactwg/react-native-releases/blob/main/docs/faq.md#when-will-my-fix-make-it-into-a-release) | [How to file a pick request?](https://github.com/reactwg/react-native-releases/blob/main/docs/faq.md#how-to-open-a-pick-request)</sup>`
            });

            // If the PR has already been processed (labeled as Merged), skip it
            const mergedLabel = ""Merged"";
            if(pr.labels && pr.labels.some(label => label.name === mergedLabel)) return;

            github.rest.issues.addLabels({
              issue_number: closedPrNumber,
              owner: context.repo.owner,
              repo: context.repo.repo,
              labels: [mergedLabel]
            });
",63,1,1,push,1
facebook/react-native,create-draft-release.yml,"name: Create Draft Release

on:
  workflow_call:

jobs:
  create-draft-release:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          fetch-tags: true
      - name: Install dependencies
        uses: ./.github/actions/yarn-install
      - name: Configure Git
        shell: bash
        run: |
          git config --local user.email ""bot@reactnative.dev""
          git config --local user.name ""React Native Bot""
      - name: Create draft release
        uses: actions/github-script@v6
        with:
          script: |
            const {createDraftRelease} = require('./.github/workflow-scripts/createDraftRelease.js');
            const version = '${{ github.ref_name }}';
            const {isLatest} = require('./.github/workflow-scripts/publishTemplate.js');
            await createDraftRelease(version, isLatest(), '${{secrets.REACT_NATIVE_BOT_GITHUB_TOKEN}}');
",29,1,1,workflow_call,3
facebook/react-native,create-release.yml,"name: Create release

on:
  workflow_dispatch:
    inputs:
      version:
        description: ""The version of React Native we want to release. For example 0.75.0-rc.0""
        required: true
        type: string
      is-latest-on-npm:
        description: ""Whether we want to tag this release as latest on NPM""
        required: true
        type: boolean
        default: false
      dry-run:
        description: ""Whether the job should be executed in dry-run mode or not""
        type: boolean
        default: true

jobs:
  create_release:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.REACT_NATIVE_BOT_GITHUB_TOKEN }}
          fetch-depth: 0
          fetch-tags: 'true'
      - name: Check if on stable branch
        id: check_stable_branch
        run: |
          BRANCH=""$(git branch --show-current)""
          PATTERN='^0\.[0-9]+-stable$'
          if [[ $BRANCH =~ $PATTERN ]]; then
            echo ""On a stable branch""
            echo ""ON_STABLE_BRANCH=true"" >> $GITHUB_OUTPUT
          fi
      - name: Print output
        run: echo ""ON_STABLE_BRANCH ${{steps.check_stable_branch.outputs.ON_STABLE_BRANCH}}""
      - name: Check if tag already exists
        id: check_if_tag_exists
        run: |
          TAG=""v${{ inputs.version }}""
          TAG_EXISTS=$(git tag -l ""$TAG"")
          if [[ -n ""$TAG_EXISTS"" ]]; then
            echo ""Version tag already exists!""
            echo ""TAG_EXISTS=true"" >> $GITHUB_OUTPUT
          fi
      - name: Execute Prepare Release
        if: ${{ steps.check_stable_branch.outputs.ON_STABLE_BRANCH && !steps.check_if_tag_exists.outputs.TAG_EXISTS }}
        uses: ./.github/actions/create-release
        with:
          version: ${{ inputs.version }}
          is-latest-on-npm: ${{ inputs.is-latest-on-npm }}
          dry-run: ${{ inputs.dry-run }}
",56,1,1,workflow_dispatch,2
facebook/react-native,danger-pr.yml,"name: Run Danger on PR

on:
  pull_request_target:
    types: [opened, edited, reopened, synchronize]

permissions:
  actions: write
  checks: write
  contents: write
  issues: write
  pull-requests: write
  statuses: write

jobs:
  danger:
    runs-on: ubuntu-latest
    if: github.repository == 'facebook/react-native'
    steps:
      - uses: actions/checkout@v4
      - name: Setup Node.js
        uses: ./.github/actions/setup-node
      - name: Run yarn install
        uses: ./.github/actions/yarn-install
      - name: Danger
        run: yarn danger ci --use-github-checks --failOnErrors
        working-directory: private/react-native-bots
        env:
          DANGER_GITHUB_API_TOKEN: ${{ secrets.REACT_NATIVE_BOT_GITHUB_TOKEN }}
",29,1,1,pull_request_target,4
facebook/react-native,generate-changelog.yml,"name: Generate Changelog

on:
  workflow_call:

jobs:
  generate-changelog:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          fetch-tags: true
      - name: Install dependencies
        uses: ./.github/actions/yarn-install
      - name: Configure Git
        shell: bash
        run: |
          git config --local user.email ""bot@reactnative.dev""
          git config --local user.name ""React Native Bot""
      - name: Generate Changelog
        uses: actions/github-script@v6
        with:
          script: |
            const {generateChangelog} = require('./.github/workflow-scripts/generateChangelog');
            const version = '${{ github.ref_name }}';
            await generateChangelog(version, '${{secrets.REACT_NATIVE_BOT_GITHUB_TOKEN}}');
",28,1,1,workflow_call,3
facebook/react-native,monitor-new-issues.yml,"name: Monitor React Native New Issues

on:
  schedule:
    - cron: ""0 0,6,12,18 * * *""
  workflow_dispatch:

# Reminder for when we have to update the schedule (before Jan 2026):
# the secrets.ONCALL_SCHEDULE secret must be on a single line and must have all the `""` escaped as `\""`.
# Only a meta engineer can update it through the OSS internal portal.

jobs:
  monitor-issues:
    runs-on: ubuntu-latest
    if: github.repository == 'facebook/react-native'
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Set up Node.js
        uses: ./.github/actions/setup-node
      - name: Install dependencies
        uses:  ./.github/actions/yarn-install
      - name: Extract next oncall
        run: |
          ONCALLS=$(node ./.github/workflow-scripts/extractIssueOncalls.js ""${{ secrets.ONCALL_SCHEDULE }}"")
          ONCALL1=$(echo $ONCALLS | cut -d ' ' -f 1)
          ONCALL2=$(echo $ONCALLS | cut -d ' ' -f 2)
          echo ""oncall1=$ONCALL1"" >> $GITHUB_ENV
          echo ""oncall2=$ONCALL2"" >> $GITHUB_ENV
      - name: Print oncalls
        run: |
          echo ""oncall1: ${{ env.oncall1 }}""
          echo ""oncall2: ${{ env.oncall2 }}""
      - name: Monitor New Issues
        uses: react-native-community/repo-monitor@v1.0.1
        with:
          task: ""monitor-issues""
          git_secret: ${{ secrets.GITHUB_TOKEN }}
          notifier: ""discord""
          fetch_data_interval: 6
          repo_owner: ""facebook""
          repo_name: ""react-native""
          discord_webhook_url: ""${{ secrets.DISCORD_WEBHOOK_URL }}""
          discord_id_type: ""user""
          discord_ids: ""${{ env.oncall1 }},${{ env.oncall2 }}""
",45,1,2,"schedule, workflow_dispatch",4
facebook/react-native,needs-attention.yml,"name: Issue Needs Attention
# This workflow is triggered on issue comments.
on:
  issue_comment:
    types: [created]

permissions:
  contents: read

jobs:
  applyNeedsAttentionLabel:
    permissions:
      contents: read # for actions/checkout to fetch code
      issues: write # for react-native-community/needs-attention to label issues
    name: Apply Needs Attention Label
    runs-on: ubuntu-latest
    if: github.repository == 'facebook/react-native'
    steps:
      - uses: actions/checkout@v4
      - name: Apply Needs Attention Label
        uses: react-native-community/needs-attention@v2.0.0
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          response-required-label: ""Needs: Author Feedback""
          needs-attention-label: ""Needs: Attention""
        id: needs-attention
      - name: Result
        run: echo '${{ steps.needs-attention.outputs.result }}'
",28,1,1,issue_comment,2
facebook/react-native,nightly.yml,"name: Nightly

on:
  workflow_dispatch:
  # nightly build @ 2:15 AM UTC
  schedule:
    - cron: ""15 2 * * *""

jobs:
  set_release_type:
    runs-on: ubuntu-latest
    if: github.repository == 'facebook/react-native'
    outputs:
      RELEASE_TYPE: ${{ steps.set_release_type.outputs.RELEASE_TYPE }}
    env:
      EVENT_NAME: ${{ github.event_name }}
      REF: ${{ github.ref }}
    steps:
      - id: set_release_type
        run: |
          echo ""Setting release type to nightly""
          echo ""RELEASE_TYPE=nightly"" >> $GITHUB_OUTPUT

  prepare_hermes_workspace:
    runs-on: ubuntu-latest
    env:
      HERMES_WS_DIR: /tmp/hermes
      HERMES_VERSION_FILE: packages/react-native/sdks/.hermesversion
    outputs:
      react-native-version: ${{ steps.prepare-hermes-workspace.outputs.react-native-version }}
      hermes-version: ${{ steps.prepare-hermes-workspace.outputs.hermes-version }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Prepare Hermes Workspace
        id: prepare-hermes-workspace
        uses: ./.github/actions/prepare-hermes-workspace
        with:
          hermes-ws-dir: ${{ env.HERMES_WS_DIR }}
          hermes-version-file: ${{ env.HERMES_VERSION_FILE }}

  build_hermesc_apple:
    runs-on: macos-14
    needs: prepare_hermes_workspace
    env:
      HERMES_WS_DIR: /tmp/hermes
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Build HermesC Apple
        uses: ./.github/actions/build-hermesc-apple
        with:
          hermes-version: ${{ needs.prepare_hermes_workspace.output.hermes-version }}
          react-native-version: ${{ needs.prepare_hermes_workspace.output.react-native-version }}

  build_apple_slices_hermes:
    runs-on: macos-14
    needs: [build_hermesc_apple, prepare_hermes_workspace]
    env:
      HERMES_WS_DIR: /tmp/hermes
      HERMES_TARBALL_ARTIFACTS_DIR: /tmp/hermes/hermes-runtime-darwin
      HERMES_OSXBIN_ARTIFACTS_DIR: /tmp/hermes/osx-bin
      IOS_DEPLOYMENT_TARGET: ""15.1""
      XROS_DEPLOYMENT_TARGET: ""1.0""
      MAC_DEPLOYMENT_TARGET: ""10.15""
    strategy:
      fail-fast: false
      matrix:
        flavor: [Debug, Release]
        slice: [macosx, iphoneos, iphonesimulator, appletvos, appletvsimulator, catalyst, xros, xrsimulator]
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Build Slice
        uses: ./.github/actions/build-apple-slices-hermes
        with:
          flavor: ${{ matrix.flavor }}
          slice: ${{ matrix.slice}}
          hermes-version: ${{ needs.prepare_hermes_workspace.outputs.hermes-version }}
          react-native-version: ${{ needs.prepare_hermes_workspace.outputs.react-native-version }}

  build_hermes_macos:
    runs-on: macos-14
    needs: [build_apple_slices_hermes, prepare_hermes_workspace]
    env:
      HERMES_WS_DIR: /tmp/hermes
      HERMES_TARBALL_ARTIFACTS_DIR: /tmp/hermes/hermes-runtime-darwin
    continue-on-error: true
    strategy:
      fail-fast: false
      matrix:
        flavor: [Debug, Release]
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Build Hermes MacOS
        uses: ./.github/actions/build-hermes-macos
        with:
          hermes-version: ${{ needs.prepare_hermes_workspace.outputs.hermes-version }}
          react-native-version: ${{ needs.prepare_hermes_workspace.outputs.react-native-version }}
          flavor: ${{ matrix.flavor }}

  prebuild_apple_dependencies:
    uses: ./.github/workflows/prebuild-ios-dependencies.yml
    secrets: inherit

  prebuild_react_native_core:
    uses: ./.github/workflows/prebuild-ios-core.yml
    secrets: inherit
    needs: [prebuild_apple_dependencies, build_hermes_macos]

  build_hermesc_linux:
    runs-on: ubuntu-latest
    needs: prepare_hermes_workspace
    env:
      HERMES_WS_DIR: /tmp/hermes
      HERMES_TARBALL_ARTIFACTS_DIR: /tmp/hermes/hermes-runtime-darwin
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Build HermesC Linux
        uses: ./.github/actions/build-hermesc-linux
        with:
          hermes-version: ${{ needs.prepare_hermes_workspace.outputs.hermes-version }}
          react-native-version: ${{ needs.prepare_hermes_workspace.outputs.react-native-version }}

  build_hermesc_windows:
    runs-on: windows-2025
    needs: prepare_hermes_workspace
    env:
      HERMES_WS_DIR: 'D:\tmp\hermes'
      HERMES_TARBALL_ARTIFACTS_DIR: 'D:\tmp\hermes\hermes-runtime-darwin'
      HERMES_OSXBIN_ARTIFACTS_DIR: 'D:\tmp\hermes\osx-bin'
      ICU_URL: ""https://github.com/unicode-org/icu/releases/download/release-64-2/icu4c-64_2-Win64-MSVC2017.zip""
      MSBUILD_DIR: 'C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\MSBuild\Current\Bin'
      CMAKE_DIR: 'C:\Program Files\CMake\bin'
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Build HermesC Windows
        uses: ./.github/actions/build-hermesc-windows
        with:
          hermes-version: ${{ needs.prepare_hermes_workspace.outputs.hermes-version }}
          react-native-version: ${{ needs.prepare_hermes_workspace.outputs.react-native-version }}

  build_android:
    runs-on: 8-core-ubuntu
    needs: [set_release_type]
    container:
      image: reactnativecommunity/react-native-android:latest
      env:
        TERM: ""dumb""
        GRADLE_OPTS: ""-Dorg.gradle.daemon=false""
        ORG_GRADLE_PROJECT_SIGNING_PWD: ${{ secrets.ORG_GRADLE_PROJECT_SIGNING_PWD }}
        ORG_GRADLE_PROJECT_SIGNING_KEY: ${{ secrets.ORG_GRADLE_PROJECT_SIGNING_KEY }}
        ORG_GRADLE_PROJECT_SONATYPE_USERNAME: ${{ secrets.ORG_GRADLE_PROJECT_SONATYPE_USERNAME }}
        ORG_GRADLE_PROJECT_SONATYPE_PASSWORD: ${{ secrets.ORG_GRADLE_PROJECT_SONATYPE_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Build Android
        uses: ./.github/actions/build-android
        with:
          release-type: ${{ needs.set_release_type.outputs.RELEASE_TYPE }}
          gradle-cache-encryption-key: ${{ secrets.GRADLE_CACHE_ENCRYPTION_KEY }}

  build_npm_package:
    runs-on: 8-core-ubuntu
    needs:
      [
        set_release_type,
        prepare_hermes_workspace,
        build_hermes_macos,
        build_hermesc_linux,
        build_hermesc_windows,
        build_android,
        prebuild_apple_dependencies,
        prebuild_react_native_core,
      ]
    container:
      image: reactnativecommunity/react-native-android:latest
      env:
        TERM: ""dumb""
        GRADLE_OPTS: ""-Dorg.gradle.daemon=false""
        # By default we only build ARM64 to save time/resources. For release/nightlies, we override this value to build all archs.
        ORG_GRADLE_PROJECT_reactNativeArchitectures: ""arm64-v8a""
    env:
      HERMES_WS_DIR: /tmp/hermes
      GHA_NPM_TOKEN: ${{ secrets.GHA_NPM_TOKEN }}
      ORG_GRADLE_PROJECT_SIGNING_PWD: ${{ secrets.ORG_GRADLE_PROJECT_SIGNING_PWD }}
      ORG_GRADLE_PROJECT_SIGNING_KEY: ${{ secrets.ORG_GRADLE_PROJECT_SIGNING_KEY }}
      ORG_GRADLE_PROJECT_SONATYPE_USERNAME: ${{ secrets.ORG_GRADLE_PROJECT_SONATYPE_USERNAME }}
      ORG_GRADLE_PROJECT_SONATYPE_PASSWORD: ${{ secrets.ORG_GRADLE_PROJECT_SONATYPE_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Build and Publish NPM Package
        uses: ./.github/actions/build-npm-package
        with:
          hermes-ws-dir: ${{ env.HERMES_WS_DIR }}
          release-type: ${{ needs.set_release_type.outputs.RELEASE_TYPE }}
          gha-npm-token: ${{ env.GHA_NPM_TOKEN }}
          gradle-cache-encryption-key: ${{ secrets.GRADLE_CACHE_ENCRYPTION_KEY }}
",203,11,2,"workflow_dispatch, schedule",18
facebook/react-native,on-issue-labeled.yml,"name: On Issue Labeled
# This workflow is triggered when a label is added to an issue.
on:
  issues:
    types: [labeled]

permissions:
  contents: write
  issues: write

jobs:
  # Runs automatic checks on issues labeled with ""Needs: Triage"",
  # then invokes actOnLabel to react to any added labels
  triage-issue:
    runs-on: ubuntu-latest
    if: ""${{ github.repository == 'facebook/react-native' && contains(github.event.label.name, 'Needs: Triage :mag:') }}""
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Verify RN version
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.REACT_NATIVE_BOT_GITHUB_TOKEN }}
          script: |
            const verifyVersion = require('./.github/workflow-scripts/verifyVersion.js')
            const labelWithContext = await verifyVersion(github, context);

            if(labelWithContext && labelWithContext.label) {
              await github.rest.issues.addLabels({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                labels: [labelWithContext.label]
              })

              const actOnLabel = require('./.github/workflow-scripts/actOnLabel.js')
              await actOnLabel(github, context, labelWithContext)
            }

      - name: Add descriptive label
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.REACT_NATIVE_BOT_GITHUB_TOKEN }}
          script: |
            const addDescriptiveLabel = require('./.github/workflow-scripts/addDescriptiveLabels.js')
            await addDescriptiveLabel(github, context);

  # Reacts to the label that triggered this workflow (added manually or via other workflows)
  act-on-label:
    runs-on: ubuntu-latest
    if: github.repository == 'facebook/react-native'
    steps:
      - uses: actions/checkout@v4
      - uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.REACT_NATIVE_BOT_GITHUB_TOKEN }}
          script: |
            const actOnLabel = require('./.github/workflow-scripts/actOnLabel.js')
            await actOnLabel(github, context, {label: context.payload.label.name})
",60,2,1,issues,5
facebook/react-native,prebuild-ios-core.yml,"name: Prebuild iOS Dependencies

on:
  workflow_call: # this directive allow us to call this workflow from other workflows


jobs:
  build-rn-slice:
    runs-on: macos-14
    strategy:
      fail-fast: false
      matrix:
        flavor: ['Debug', 'Release']
        slice: [
          'ios',
          'ios-simulator',
          'mac-catalyst',
        ]
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Restore cache if present
        id: restore-ios-slice
        uses: actions/cache/restore@v4
        with:
          key: v3-ios-core-${{ matrix.slice }}-${{ matrix.flavor }}-${{ hashFiles('packages/react-native/Package.swift') }}-${{ hashFiles('packages/react-native/scripts/ios-prebuild/setup.js') }}
          path: packages/react-native/
      - name: Setup node.js
        if: steps.restore-ios-slice.outputs.cache-hit != 'true'
        uses: ./.github/actions/setup-node
      - name: Setup xcode
        if: steps.restore-ios-slice.outputs.cache-hit != 'true'
        uses: ./.github/actions/setup-xcode
        with:
          xcode-version: '16.2.0'
      - name: Yarn Install
        if: steps.restore-ios-slice.outputs.cache-hit != 'true'
        uses: ./.github/actions/yarn-install
      - name: Download Hermes
        if: steps.restore-ios-slice.outputs.cache-hit != 'true'
        uses: actions/download-artifact@v4
        with:
          name: hermes-darwin-bin-${{ matrix.flavor }}
          path: /tmp/hermes/hermes-runtime-darwin
      - name: Extract Hermes
        if: steps.restore-ios-slice.outputs.cache-hit != 'true'
        shell: bash
        run: |
          HERMES_TARBALL_ARTIFACTS_DIR=/tmp/hermes/hermes-runtime-darwin
          if [ ! -d $HERMES_TARBALL_ARTIFACTS_DIR ]; then
            echo ""Hermes tarball artifacts dir not present ($HERMES_TARBALL_ARTIFACTS_DIR).""
            exit 0
          fi

          TARBALL_FILENAME=$(node ./packages/react-native/scripts/hermes/get-tarball-name.js --buildType ""${{ matrix.flavor }}"")
          TARBALL_PATH=$HERMES_TARBALL_ARTIFACTS_DIR/$TARBALL_FILENAME

          echo ""Looking for $TARBALL_FILENAME in $HERMES_TARBALL_ARTIFACTS_DIR""
          echo ""$TARBALL_PATH""

          if [ ! -f $TARBALL_PATH ]; then
            echo ""Hermes tarball not present ($TARBALL_PATH). Build Hermes from source.""
            exit 0
          fi

          echo ""Found Hermes tarball at $TARBALL_PATH""
          echo ""HERMES_ENGINE_TARBALL_PATH=$TARBALL_PATH"" >> $GITHUB_ENV
      - name: Download ReactNativeDependencies
        uses: actions/download-artifact@v4
        with:
          name: ReactNativeDependencies${{ matrix.flavor }}.xcframework.tar.gz
          path: /tmp/third-party/
      - name: Extract ReactNativeDependencies
        if: steps.restore-ios-slice.outputs.cache-hit != 'true'
        shell: bash
        run: |
          # Extract ReactNativeDependencies
          tar -xzf /tmp/third-party/ReactNativeDependencies${{ matrix.flavor }}.xcframework.tar.gz -C /tmp/third-party/

          # Create destination folder
          mkdir -p packages/react-native/third-party/

          # Move the XCFramework in the destination directory
          mv /tmp/third-party/packages/react-native/third-party/ReactNativeDependencies.xcframework packages/react-native/third-party/ReactNativeDependencies.xcframework

          VERSION=$(jq -r '.version' package.json)
          echo ""$VERSION-${{matrix.flavor}}"" > ""packages/react-native/third-party/version.txt""
          cat ""packages/react-native/third-party/version.txt""
          # Check destination directory
          ls -lR packages/react-native/third-party/
      - name: Setup the workspace
        if: steps.restore-ios-slice.outputs.cache-hit != 'true'
        shell: bash
        run: |
          cd packages/react-native
          node scripts/ios-prebuild.js -s -f ""${{ matrix.flavor }}""
      - name: Build React Native
        if: steps.restore-ios-slice.outputs.cache-hit != 'true'
        shell: bash
        run: |
          # This is going to be replaced by a CLI script
          cd packages/react-native
          node scripts/ios-prebuild -b -f ""${{ matrix.flavor }}"" -p ""${{ matrix.slice }}""
      - name: Upload headers
        uses: actions/upload-artifact@v4
        with:
          name: prebuild-ios-core-headers-${{ matrix.flavor }}-${{ matrix.slice }}
          path:
            packages/react-native/.build/headers
      - name: Upload artifacts
        uses: actions/upload-artifact@v4.3.4
        with:
          name: prebuild-ios-core-slice-${{ matrix.flavor }}-${{ matrix.slice }}
          path: |
            packages/react-native/.build/output/spm/${{ matrix.flavor }}/Build/Products
      - name: Save Cache
        uses: actions/cache/save@v4
        if: ${{ github.ref == 'refs/heads/main' }} # To avoid that the cache explode
        with:
          key: v3-ios-core-${{ matrix.slice }}-${{ matrix.flavor }}-${{ hashFiles('packages/react-native/Package.swift') }}-${{ hashFiles('packages/react-native/scripts/ios-prebuild/setup.js') }}
          path: |
            packages/react-native/.build/output/spm/${{ matrix.flavor }}/Build/Products
            packages/react-native/.build/headers

  compose-xcframework:
    runs-on: macos-14
    needs: [build-rn-slice]
    strategy:
      fail-fast: false
      matrix:
        flavor: ['Debug', 'Release']
    env:
      REACT_ORG_CODE_SIGNING_P12_CERT: ${{ secrets.REACT_ORG_CODE_SIGNING_P12_CERT }}
      REACT_ORG_CODE_SIGNING_P12_CERT_PWD: ${{ secrets.REACT_ORG_CODE_SIGNING_P12_CERT_PWD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Restore cache if present
        id: restore-ios-xcframework
        uses: actions/cache/restore@v4
        with:
          path: packages/react-native/.build/output/xcframeworks
          key: v2-ios-core-xcframework-${{ matrix.flavor }}-${{ hashFiles('packages/react-native/Package.swift') }}-${{ hashFiles('packages/react-native/scripts/ios-prebuild/setup.js') }}
      - name: Setup node.js
        if: steps.restore-ios-xcframework.outputs.cache-hit != 'true'
        uses: ./.github/actions/setup-node
      - name: Setup xcode
        if: steps.restore-ios-xcframework.outputs.cache-hit != 'true'
        uses: ./.github/actions/setup-xcode
        with:
          xcode-version: '16.2.0'
      - name: Yarn Install
        if: steps.restore-ios-xcframework.outputs.cache-hit != 'true'
        uses: ./.github/actions/yarn-install
      - name: Download slice artifacts
        if: steps.restore-ios-xcframework.outputs.cache-hit != 'true'
        uses: actions/download-artifact@v4
        with:
          pattern: prebuild-ios-core-slice-${{ matrix.flavor }}-*
          path: packages/react-native/.build/output/spm/${{ matrix.flavor }}/Build/Products
          merge-multiple: true
      - name: Download headers
        if: steps.restore-ios-xcframework.outputs.cache-hit != 'true'
        uses: actions/download-artifact@v4
        with:
          pattern: prebuild-ios-core-headers-${{ matrix.flavor }}-*
          path: packages/react-native/.build/headers
          merge-multiple: true
      - name: Setup Keychain
        if: ${{ steps.restore-ios-xcframework.outputs.cache-hit != 'true' && env.REACT_ORG_CODE_SIGNING_P12_CERT != '' }}
        uses: apple-actions/import-codesign-certs@v3 # https://github.com/marketplace/actions/import-code-signing-certificates
        with:
          p12-file-base64: ${{ secrets.REACT_ORG_CODE_SIGNING_P12_CERT }}
          p12-password: ${{ secrets.REACT_ORG_CODE_SIGNING_P12_CERT_PWD }}
      - name: Create XCFramework
        if: ${{ steps.restore-ios-xcframework.outputs.cache-hit != 'true' && env.REACT_ORG_CODE_SIGNING_P12_CERT == '' }}
        run: |
          cd packages/react-native
          node scripts/ios-prebuild -c -f ""${{ matrix.flavor }}""
      - name: Create and Sign XCFramework
        if: ${{ steps.restore-ios-xcframework.outputs.cache-hit != 'true' && env.REACT_ORG_CODE_SIGNING_P12_CERT != '' }}
        run: |
          cd packages/react-native
          node scripts/ios-prebuild -c -f ""${{ matrix.flavor }}"" -i ""React Org""
      - name: Compress and Rename XCFramework
        if: steps.restore-ios-xcframework.outputs.cache-hit != 'true'
        run: |
          cd packages/react-native/.build/output/xcframeworks/${{matrix.flavor}}
          tar -cz -f ../ReactCore${{matrix.flavor}}.xcframework.tar.gz React.xcframework
      - name: Compress and Rename dSYM
        if: steps.restore-ios-xcframework.outputs.cache-hit != 'true'
        run: |
          cd packages/react-native/.build/output/xcframeworks/${{matrix.flavor}}/Symbols
          tar -cz -f ../../ReactCore${{ matrix.flavor }}.framework.dSYM.tar.gz .
      - name: Upload XCFramework Artifact
        uses: actions/upload-artifact@v4
        with:
          name: ReactCore${{ matrix.flavor }}.xcframework.tar.gz
          path: packages/react-native/.build/output/xcframeworks/ReactCore${{matrix.flavor}}.xcframework.tar.gz
      - name: Upload dSYM Artifact
        uses: actions/upload-artifact@v4
        with:
          name: ReactCore${{ matrix.flavor }}.framework.dSYM.tar.gz
          path: packages/react-native/.build/output/xcframeworks/ReactCore${{matrix.flavor}}.framework.dSYM.tar.gz
      - name: Save cache if present
        if: ${{ github.ref == 'refs/heads/main' }} # To avoid that the cache explode
        uses: actions/cache/save@v4
        with:
          path: |
            packages/react-native/.build/output/xcframeworks/ReactCore${{matrix.flavor}}.xcframework.tar.gz
            packages/react-native/.build/output/xcframeworks/ReactCore${{matrix.flavor}}.framework.dSYM.tar.gz
          key: v2-ios-core-xcframework-${{ matrix.flavor }}-${{ hashFiles('packages/react-native/Package.swift') }}-${{ hashFiles('packages/react-native/scripts/ios-prebuild/setup.js') }}
",212,2,1,workflow_call,21
facebook/react-native,prebuild-ios-dependencies.yml,"name: Prebuild iOS Dependencies

on:
  workflow_call: # this directive allow us to call this workflow from other workflows


jobs:
  prepare_workspace:
    name: Prepare workspace
    runs-on: macos-14
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Setup node.js
        uses: ./.github/actions/setup-node
      - name: Restore cache if present
        id: restore-ios-prebuilds
        uses: actions/cache/restore@v4
        with:
          path: packages/react-native/third-party/
          key: v2-ios-dependencies-${{ hashfiles('scripts/releases/ios-prebuild/configuration.js') }}
          enableCrossOsArchive: true
      - name: Yarn Install
        if: steps.restore-ios-prebuilds.outputs.cache-hit != 'true'
        uses: ./.github/actions/yarn-install
      - name: Prepare Dependencies
        if: steps.restore-ios-prebuilds.outputs.cache-hit != 'true'
        run: |
          node scripts/releases/prepare-ios-prebuilds.js -s
      - name: Generate Package.swift
        if: steps.restore-ios-prebuilds.outputs.cache-hit != 'true'
        run: |
          node scripts/releases/prepare-ios-prebuilds.js -w
      - name: Upload Artifacts
        uses: actions/upload-artifact@v4.3.4
        with:
          name: ios-prebuilds-workspace
          path: packages/react-native/third-party/
      - name: Save Cache
        uses: actions/cache/save@v4
        if: ${{ github.ref == 'refs/heads/main' }} # To avoid that the cache explode
        with:
          key: v2-ios-dependencies-${{ hashfiles('scripts/releases/ios-prebuild/configuration.js') }}
          enableCrossOsArchive: true
          path: packages/react-native/third-party/

  build-apple-slices:
    name: Build Apple Slice
    runs-on: macos-14
    needs: [prepare_workspace]
    strategy:
      fail-fast: false
      matrix:
        flavor: ['Debug', 'Release']
        slice: ['ios',
                'ios-simulator',
                'macos',
                'mac-catalyst',
                'tvos',
                'tvos-simulator',
                'xros',
                'xros-simulator']
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Setup node.js
        uses: ./.github/actions/setup-node
      - name: Setup xcode
        uses: ./.github/actions/setup-xcode
        with:
          xcode-version: '16.1'
      - name: Restore slice folder
        id: restore-slice-folder
        uses: actions/cache/restore@v4
        with:
          path: packages/react-native/third-party/.build/Build/Products
          key: v2-ios-dependencies-slice-folder-${{ matrix.slice }}-${{ matrix.flavor }}-${{ hashfiles('scripts/releases/ios-prebuild/configuration.js') }}
      - name: Yarn Install
        if: steps.restore-slice-folder.outputs.cache-hit != 'true'
        uses: ./.github/actions/yarn-install
      - name: Restore workspace
        if: steps.restore-slice-folder.outputs.cache-hit != 'true'
        uses: actions/download-artifact@v4
        with:
          name: ios-prebuilds-workspace
          path: packages/react-native/third-party/
      - name: Print third-party folder structure
        run: ls -lR packages/react-native/third-party
      - name: Install VisionOS
        if: ${{ steps.restore-slice-folder.outputs.cache-hit != 'true' && (matrix.slice == 'xros' || matrix.slice == 'xros-simulator') }}
        run: |
          # https://github.com/actions/runner-images/issues/10559
          sudo xcodebuild -runFirstLaunch
          sudo xcrun simctl list
          sudo xcodebuild -downloadPlatform visionOS
          sudo xcodebuild -runFirstLaunch
      - name: Build slice ${{ matrix.slice }} for ${{ matrix.flavor }}
        if: steps.restore-slice-folder.outputs.cache-hit != 'true'
        run:  node scripts/releases/prepare-ios-prebuilds.js -b -p ${{ matrix.slice }} -r ${{ matrix.flavor }}
      - name: Upload Artifacts
        uses: actions/upload-artifact@v4.3.4
        with:
          name: prebuild-slice-${{ matrix.flavor }}-${{ matrix.slice }}
          path: |
            packages/react-native/third-party/.build/Build/Products
      - name: Save Cache
        uses: actions/cache/save@v4
        if: ${{ github.ref == 'refs/heads/main' }} # To avoid that the cache explode
        with:
          key: v2-ios-dependencies-slice-folder-${{ matrix.slice }}-${{ matrix.flavor }}-${{ hashfiles('scripts/releases/ios-prebuild/configuration.js') }}
          enableCrossOsArchive: true
          path: |
            packages/react-native/third-party/.build/Build/Products

  create-xcframework:
    name: Prepare XCFramework
    runs-on: macos-14
    needs: [build-apple-slices]
    strategy:
      fail-fast: false
      matrix:
        flavor: [Debug, Release]
    env:
      REACT_ORG_CODE_SIGNING_P12_CERT: ${{ secrets.REACT_ORG_CODE_SIGNING_P12_CERT }}
      REACT_ORG_CODE_SIGNING_P12_CERT_PWD: ${{ secrets.REACT_ORG_CODE_SIGNING_P12_CERT_PWD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Setup node.js
        uses: ./.github/actions/setup-node
      - name: Setup xcode
        uses: ./.github/actions/setup-xcode
        with:
          xcode-version: '16.1'
      - name: Restore XCFramework
        id: restore-xcframework
        uses: actions/cache/restore@v4
        with:
          path: |
            packages/react-native/third-party/
          key: v2-ios-dependencies-xcframework-${{ matrix.flavor }}-${{ hashfiles('scripts/releases/ios-prebuild/configuration.js') }}
      # If cache hit, we already have our binary. We don't need to do anything.
      - name: Yarn Install
        if: steps.restore-xcframework.outputs.cache-hit != 'true'
        uses: ./.github/actions/yarn-install
      - name: Restore workspace
        if: steps.restore-xcframework.outputs.cache-hit != 'true'
        uses: actions/download-artifact@v4
        with:
          name: ios-prebuilds-workspace
          path: packages/react-native/third-party/
      - name: Download slices
        if: steps.restore-xcframework.outputs.cache-hit != 'true'
        uses: actions/download-artifact@v4
        with:
          pattern: prebuild-slice-${{ matrix.flavor }}-*
          path: packages/react-native/third-party/.build/Build/Products
          merge-multiple: true
      - name: Setup Keychain
        if: ${{ steps.restore-xcframework.outputs.cache-hit != 'true' && env.REACT_ORG_CODE_SIGNING_P12_CERT != '' }}
        uses: apple-actions/import-codesign-certs@v3 # https://github.com/marketplace/actions/import-code-signing-certificates
        with:
          p12-file-base64: ${{ secrets.REACT_ORG_CODE_SIGNING_P12_CERT }}
          p12-password: ${{ secrets.REACT_ORG_CODE_SIGNING_P12_CERT_PWD }}
      - name: Create XCFramework
        if: ${{ steps.restore-xcframework.outputs.cache-hit != 'true' && env.REACT_ORG_CODE_SIGNING_P12_CERT == '' }}
        run: node scripts/releases/prepare-ios-prebuilds.js -c
      - name: Create and Sign XCFramework
        if: ${{ steps.restore-xcframework.outputs.cache-hit != 'true' && env.REACT_ORG_CODE_SIGNING_P12_CERT != '' }}
        run: node scripts/releases/prepare-ios-prebuilds.js -c -i ""React Org""
      - name: Compress and Rename XCFramework
        if: steps.restore-xcframework.outputs.cache-hit != 'true'
        run: |
          tar -cz -f packages/react-native/third-party/ReactNativeDependencies${{ matrix.flavor }}.xcframework.tar.gz \
            packages/react-native/third-party/ReactNativeDependencies.xcframework
      - name: Show Symbol folder content
        if: steps.restore-xcframework.outputs.cache-hit != 'true'
        run: ls -lR packages/react-native/third-party/Symbols
      - name: Compress and Rename dSYM
        if: steps.restore-xcframework.outputs.cache-hit != 'true'
        run: |
          tar -cz -f packages/react-native/third-party/Symbols/ReactNativeDependencies${{ matrix.flavor }}.framework.dSYM.tar.gz \
            packages/react-native/third-party/Symbols/ReactNativeDependencies.framework.dSYM
      - name: Upload XCFramework Artifact
        uses: actions/upload-artifact@v4
        with:
          name: ReactNativeDependencies${{ matrix.flavor }}.xcframework.tar.gz
          path: packages/react-native/third-party/ReactNativeDependencies${{ matrix.flavor }}.xcframework.tar.gz
      - name: Upload dSYM Artifact
        uses: actions/upload-artifact@v4
        with:
          name: ReactNativeDependencies${{ matrix.flavor }}.framework.dSYM.tar.gz
          path: |
            packages/react-native/third-party/Symbols/ReactNativeDependencies${{ matrix.flavor }}.framework.dSYM.tar.gz
      - name: Save XCFramework in Cache
        if: ${{ github.ref == 'refs/heads/main' }} # To avoid that the cache explode
        uses: actions/cache/save@v4
        with:
          path: |
            packages/react-native/third-party/ReactNativeDependencies${{ matrix.flavor }}.xcframework.tar.gz
            packages/react-native/third-party/ReactNativeDependencies${{ matrix.flavor }}.framework.dSYM.tar.gz
          key: v2-ios-dependencies-xcframework-${{ matrix.flavor }}-${{ hashfiles('scripts/releases/ios-prebuild/configuration.js') }}
",202,3,1,workflow_call,25
facebook/react-native,publish-bumped-packages.yml,"name: Publish Bumped Packages

on:
  push:
    branches:
      - ""main""
      - ""*-stable""

jobs:
  publish_bumped_packages:
    runs-on: ubuntu-latest
    env:
      GHA_NPM_TOKEN: ${{ secrets.GHA_NPM_TOKEN }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Setup node.js
        uses: ./.github/actions/setup-node
      - name: Run Yarn Install
        uses: ./.github/actions/yarn-install
      - name: Build packages
        run: yarn build
      - name: Build types
        run: yarn build-types
      - name: Set NPM auth token
        run: echo ""//registry.npmjs.org/:_authToken=$GHA_NPM_TOKEN"" > ~/.npmrc
      - name: Find and publish all bumped packages
        run: node ./scripts/releases-ci/publish-updated-packages.js
",28,1,1,push,3
facebook/react-native,publish-release.yml,"name: Publish Release
on:
  push:
    tags:
      - ""v0.*.*"" # This should match v0.X.Y
      - ""v0.*.*-rc.*"" # This should match v0.X.Y-RC.0
jobs:
  set_release_type:
    runs-on: ubuntu-latest
    if: github.repository == 'facebook/react-native'
    outputs:
      RELEASE_TYPE: ${{ steps.set_release_type.outputs.RELEASE_TYPE }}
    env:
      EVENT_NAME: ${{ github.event_name }}
      REF: ${{ github.ref }}
    steps:
      - id: set_release_type
        run: |
          echo ""Setting release type to release""
          echo ""RELEASE_TYPE=release"" >> $GITHUB_OUTPUT

  prepare_hermes_workspace:
    runs-on: ubuntu-latest
    env:
      HERMES_WS_DIR: /tmp/hermes
      HERMES_VERSION_FILE: packages/react-native/sdks/.hermesversion
    outputs:
      react-native-version: ${{ steps.prepare-hermes-workspace.outputs.react-native-version }}
      hermes-version: ${{ steps.prepare-hermes-workspace.outputs.hermes-version }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Prepare Hermes Workspace
        id: prepare-hermes-workspace
        uses: ./.github/actions/prepare-hermes-workspace
        with:
          hermes-ws-dir: ${{ env.HERMES_WS_DIR }}
          hermes-version-file: ${{ env.HERMES_VERSION_FILE }}

  build_hermesc_apple:
    runs-on: macos-14
    needs: prepare_hermes_workspace
    env:
      HERMES_WS_DIR: /tmp/hermes
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Build HermesC Apple
        uses: ./.github/actions/build-hermesc-apple
        with:
          hermes-version: ${{ needs.prepare_hermes_workspace.output.hermes-version }}
          react-native-version: ${{ needs.prepare_hermes_workspace.output.react-native-version }}
  build_apple_slices_hermes:
    runs-on: macos-14
    needs: [build_hermesc_apple, prepare_hermes_workspace]
    env:
      HERMES_WS_DIR: /tmp/hermes
      HERMES_TARBALL_ARTIFACTS_DIR: /tmp/hermes/hermes-runtime-darwin
      HERMES_OSXBIN_ARTIFACTS_DIR: /tmp/hermes/osx-bin
      IOS_DEPLOYMENT_TARGET: ""15.1""
      XROS_DEPLOYMENT_TARGET: ""1.0""
      MAC_DEPLOYMENT_TARGET: ""10.15""
    strategy:
      fail-fast: false
      matrix:
        flavor: [Debug, Release]
        slice: [macosx, iphoneos, iphonesimulator, appletvos, appletvsimulator, catalyst, xros, xrsimulator]
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Build Slice
        uses: ./.github/actions/build-apple-slices-hermes
        with:
          flavor: ${{ matrix.flavor }}
          slice: ${{ matrix.slice}}
          hermes-version: ${{ needs.prepare_hermes_workspace.outputs.hermes-version }}
          react-native-version: ${{ needs.prepare_hermes_workspace.outputs.react-native-version }}

  build_hermes_macos:
    runs-on: macos-14
    needs: [build_apple_slices_hermes, prepare_hermes_workspace]
    env:
      HERMES_WS_DIR: /tmp/hermes
      HERMES_TARBALL_ARTIFACTS_DIR: /tmp/hermes/hermes-runtime-darwin
    continue-on-error: true
    strategy:
      fail-fast: false
      matrix:
        flavor: [Debug, Release]
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Build Hermes MacOS
        uses: ./.github/actions/build-hermes-macos
        with:
          hermes-version: ${{ needs.prepare_hermes_workspace.outputs.hermes-version }}
          react-native-version: ${{ needs.prepare_hermes_workspace.outputs.react-native-version }}
          flavor: ${{ matrix.flavor }}

  prebuild_apple_dependencies:
    uses: ./.github/workflows/prebuild-ios-dependencies.yml
    secrets: inherit

  prebuild_react_native_core:
    uses: ./.github/workflows/prebuild-ios-core.yml
    secrets: inherit
    needs: [prebuild_apple_dependencies, build_hermes_macos]

  build_hermesc_linux:
    runs-on: ubuntu-latest
    needs: prepare_hermes_workspace
    env:
      HERMES_WS_DIR: /tmp/hermes
      HERMES_TARBALL_ARTIFACTS_DIR: /tmp/hermes/hermes-runtime-darwin
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Build HermesC Linux
        uses: ./.github/actions/build-hermesc-linux
        with:
          hermes-version: ${{ needs.prepare_hermes_workspace.outputs.hermes-version }}
          react-native-version: ${{ needs.prepare_hermes_workspace.outputs.react-native-version }}

  build_hermesc_windows:
    runs-on: windows-2025
    needs: prepare_hermes_workspace
    env:
      HERMES_WS_DIR: 'D:\tmp\hermes'
      HERMES_TARBALL_ARTIFACTS_DIR: 'D:\tmp\hermes\hermes-runtime-darwin'
      HERMES_OSXBIN_ARTIFACTS_DIR: 'D:\tmp\hermes\osx-bin'
      ICU_URL: ""https://github.com/unicode-org/icu/releases/download/release-64-2/icu4c-64_2-Win64-MSVC2017.zip""
      MSBUILD_DIR: 'C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\MSBuild\Current\Bin'
      CMAKE_DIR: 'C:\Program Files\CMake\bin'
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Build HermesC Windows
        uses: ./.github/actions/build-hermesc-windows
        with:
          hermes-version: ${{ needs.prepare_hermes_workspace.outputs.hermes-version }}
          react-native-version: ${{ needs.prepare_hermes_workspace.outputs.react-native-version }}

  build_npm_package:
    runs-on: 8-core-ubuntu
    needs:
      [
        set_release_type,
        prepare_hermes_workspace,
        build_hermes_macos,
        build_hermesc_linux,
        build_hermesc_windows,
        prebuild_apple_dependencies,
        prebuild_react_native_core,
      ]
    container:
      image: reactnativecommunity/react-native-android:latest
      env:
        TERM: ""dumb""
        GRADLE_OPTS: ""-Dorg.gradle.daemon=false""
        # By default we only build ARM64 to save time/resources. For release/nightlies, we override this value to build all archs.
        ORG_GRADLE_PROJECT_reactNativeArchitectures: ""arm64-v8a""
    env:
      HERMES_WS_DIR: /tmp/hermes
      GHA_NPM_TOKEN: ${{ secrets.GHA_NPM_TOKEN }}
      ORG_GRADLE_PROJECT_SIGNING_PWD: ${{ secrets.ORG_GRADLE_PROJECT_SIGNING_PWD }}
      ORG_GRADLE_PROJECT_SIGNING_KEY: ${{ secrets.ORG_GRADLE_PROJECT_SIGNING_KEY }}
      ORG_GRADLE_PROJECT_SONATYPE_USERNAME: ${{ secrets.ORG_GRADLE_PROJECT_SONATYPE_USERNAME }}
      ORG_GRADLE_PROJECT_SONATYPE_PASSWORD: ${{ secrets.ORG_GRADLE_PROJECT_SONATYPE_PASSWORD }}
      REACT_NATIVE_BOT_GITHUB_TOKEN: ${{ secrets.REACT_NATIVE_BOT_GITHUB_TOKEN }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          fetch-tags: true
      - name: Build and Publish NPM Package
        uses: ./.github/actions/build-npm-package
        with:
          hermes-ws-dir: ${{ env.HERMES_WS_DIR }}
          release-type: ${{ needs.set_release_type.outputs.RELEASE_TYPE }}
          gha-npm-token: ${{ env.GHA_NPM_TOKEN }}
          gradle-cache-encryption-key: ${{ secrets.GRADLE_CACHE_ENCRYPTION_KEY }}
      - name: Publish @react-native-community/template
        id: publish-template-to-npm
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.REACT_NATIVE_BOT_GITHUB_TOKEN }}
          script: |
            const {publishTemplate} = require('./.github/workflow-scripts/publishTemplate.js')
            const version = ""${{ github.ref_name }}""
            const isDryRun = false
            await publishTemplate(github, version, isDryRun);
      - name: Wait for template to be published
        timeout-minutes: 3
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.REACT_NATIVE_BOT_GITHUB_TOKEN }}
          script: |
            const {verifyPublishedTemplate, isLatest} = require('./.github/workflow-scripts/publishTemplate.js')
            const version = ""${{ github.ref_name }}""
            await verifyPublishedTemplate(version, isLatest());
      - name: Update rn-diff-purge to generate upgrade-support diff
        run: |
          curl -X POST https://api.github.com/repos/react-native-community/rn-diff-purge/dispatches \
            -H ""Accept: application/vnd.github.v3+json"" \
            -H ""Authorization: Bearer $REACT_NATIVE_BOT_GITHUB_TOKEN"" \
            -d ""{\""event_type\"": \""publish\"", \""client_payload\"": { \""version\"": \""${{ github.ref_name }}\"" }}""
      - name: Verify Release is on NPM
        timeout-minutes: 3
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.REACT_NATIVE_BOT_GITHUB_TOKEN }}
          script: |
            const {verifyReleaseOnNpm} = require('./.github/workflow-scripts/verifyReleaseOnNpm.js');
            const {isLatest} = require('./.github/workflow-scripts/publishTemplate.js');
            const version = ""${{ github.ref_name }}"";
            await verifyReleaseOnNpm(version, isLatest());
      - name: Verify that artifacts are on Maven
        uses: actions/github-script@v6
        with:
          script: |
            const {verifyArtifactsAreOnMaven} = require('./.github/workflow-scripts/verifyArtifactsAreOnMaven.js');
            const version = ""${{ github.ref_name }}"";
            await verifyArtifactsAreOnMaven(version);

  generate_changelog:
    needs: build_npm_package
    uses: ./.github/workflows/generate-changelog.yml
    secrets: inherit

  bump_podfile_lock:
    needs: build_npm_package
    uses: ./.github/workflows/bump-podfile-lock.yml
    secrets: inherit

  create_draft_release:
    needs: generate_changelog
    uses: ./.github/workflows/create-draft-release.yml
    secrets: inherit
",239,13,1,push,23
facebook/react-native,retry-workflow.yml,"name: Retry workflow
# Based on https://stackoverflow.com/a/78314483

on:
    workflow_dispatch:
        inputs:
            run_id:
                required: true
jobs:
    rerun:
        runs-on: ubuntu-latest
        steps:
            - name: rerun ${{ inputs.run_id }}
              env:
                  GH_REPO: ${{ github.repository }}
                  GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
              run: |
                  gh run watch ${{ inputs.run_id }} > /dev/null 2>&1
                  gh run rerun ${{ inputs.run_id }} --failed
",19,1,1,workflow_dispatch,0
facebook/react-native,stale-bot.yml,"name: Stale bot
on:
  schedule:
    - cron: ""*/10 5 * * *""
jobs:
  stale:
    runs-on: ubuntu-latest
    if: github.repository == 'facebook/react-native'
    permissions:
      issues: write
      pull-requests: write
    steps:
      - uses: actions/stale@v9
        with:
          repo-token: ${{ secrets.REACT_NATIVE_BOT_GITHUB_TOKEN }}
          days-before-stale: 180
          stale-issue-message: 'This issue is stale because it has been open for 180 days with no activity. It will be closed in 7 days unless you comment on it or remove the ""Stale"" label.'
          stale-pr-message: 'This PR is stale because it has been open for 180 days with no activity. It will be closed in 7 days unless you comment on it or remove the ""Stale"" label.'
          close-issue-message: 'This issue was closed because it has been stalled for 7 days with no activity.'
          close-pr-message: 'This PR was closed because it has been stalled for 7 days with no activity.'
          exempt-issue-labels: 'Help Wanted :octocat:, Good first issue, Never gets stale, Issue: Author Provided Repro'
          exempt-pr-labels: 'Help Wanted :octocat:, Never gets stale'
  stale-asc:
    runs-on: ubuntu-latest
    if: github.repository == 'facebook/react-native'
    permissions:
      issues: write
      pull-requests: write
    steps:
      - uses: actions/stale@v9
        with:
          ascending: true
          repo-token: ${{ secrets.REACT_NATIVE_BOT_GITHUB_TOKEN }}
          days-before-stale: 180
          stale-issue-message: 'This issue is stale because it has been open 180 days with no activity. Remove stale label or comment or this will be closed in 7 days.'
          stale-pr-message: 'This PR is stale because it has been open 180 days with no activity. Remove stale label or comment or this will be closed in 7 days.'
          close-issue-message: 'This issue was closed because it has been stalled for 7 days with no activity.'
          close-pr-message: 'This PR was closed because it has been stalled for 7 days with no activity.'
          exempt-issue-labels: 'Help Wanted :octocat:, Good first issue, Never gets stale, Issue: Author Provided Repro'
          exempt-pr-labels: 'Help Wanted :octocat:, Never gets stale'
  stale-needs-author-feedback:
    runs-on: ubuntu-latest
    if: github.repository == 'facebook/react-native'
    permissions:
      issues: write
      pull-requests: write
    steps:
      - uses: actions/stale@v9
        with:
          repo-token: ${{ secrets.REACT_NATIVE_BOT_GITHUB_TOKEN }}
          any-of-labels: 'Needs: Author Feedback'
          days-before-stale: 24
          stale-issue-message: ""This issue is waiting for author's feedback since 24 days. Please provide the requested feedback or this will be closed in 7 days.""
          stale-pr-message: ""This PR is waiting for author's feedback since 24 days. Please provide the requested feedback or this will be closed in 7 days""
          close-issue-message: ""This issue was closed because the author hasn't provided the requested feedback after 7 days.""
          close-pr-message: ""This PR was closed because the author hasn't provided the requested feedback after 7 days.""
          exempt-issue-labels: ""Help Wanted :octocat:, Good first issue, Never gets stale, Issue: Author Provided Repro""
          exempt-pr-labels: ""Help Wanted :octocat:, Never gets stale""
  stale-needs-author-feedback-asc:
    runs-on: ubuntu-latest
    if: github.repository == 'facebook/react-native'
    permissions:
      issues: write
      pull-requests: write
    steps:
      - uses: actions/stale@v9
        with:
          ascending: true
          repo-token: ${{ secrets.REACT_NATIVE_BOT_GITHUB_TOKEN }}
          any-of-labels: 'Needs: Author Feedback'
          days-before-stale: 24
          stale-issue-message: ""This issue is waiting for author's feedback since 24 days. Please provide the requested feedback or this will be closed in 7 days.""
          stale-pr-message: ""This PR is waiting for author's feedback since 24 days. Please provide the requested feedback or this will be closed in 7 days""
          close-issue-message: ""This issue was closed because the author hasn't provided the requested feedback after 7 days.""
          close-pr-message: ""This PR was closed because the author hasn't provided the requested feedback after 7 days.""
          exempt-issue-labels: ""Help Wanted :octocat:, Good first issue, Never gets stale, Issue: Author Provided Repro""
          exempt-pr-labels: ""Help Wanted :octocat:, Never gets stale""
",77,4,1,schedule,4
facebook/react-native,test-all.yml,"name: Test All

on:
  workflow_dispatch:
    inputs:
      run-e2e-tests:
        description: Whether to run E2E tests or not
        type: boolean
        default: false
  pull_request:
  push:
    branches:
      - main
      - ""*-stable""

jobs:
  set_release_type:
    runs-on: ubuntu-latest
    outputs:
      RELEASE_TYPE: ${{ steps.set_release_type.outputs.RELEASE_TYPE }}
    env:
      EVENT_NAME: ${{ github.event_name }}
      REF: ${{ github.ref }}
    steps:
      - id: set_release_type
        run: |
          if [[ $EVENT_NAME == ""schedule"" ]]; then
            echo ""Setting release type to nightly""
            echo ""RELEASE_TYPE=nightly"" >> $GITHUB_OUTPUT
          elif [[ $EVENT_NAME == ""push"" && $REF == refs/tags/v* ]]; then
            echo ""Setting release type to release""
            echo ""RELEASE_TYPE=release"" >> $GITHUB_OUTPUT
          else
            echo ""Setting release type to dry-run""
            echo ""RELEASE_TYPE=dry-run"" >> $GITHUB_OUTPUT
          fi

          echo ""Should I run E2E tests? ${{ inputs.run-e2e-tests }}""

  prepare_hermes_workspace:
    runs-on: ubuntu-latest
    env:
      HERMES_WS_DIR: /tmp/hermes
      HERMES_VERSION_FILE: packages/react-native/sdks/.hermesversion
    outputs:
      react-native-version: ${{ steps.prepare-hermes-workspace.outputs.react-native-version }}
      hermes-version: ${{ steps.prepare-hermes-workspace.outputs.hermes-version }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Prepare Hermes Workspace
        id: prepare-hermes-workspace
        uses: ./.github/actions/prepare-hermes-workspace
        with:
          hermes-ws-dir: ${{ env.HERMES_WS_DIR }}
          hermes-version-file: ${{ env.HERMES_VERSION_FILE }}

  build_hermesc_apple:
    runs-on: macos-14
    needs: prepare_hermes_workspace
    env:
      HERMES_WS_DIR: /tmp/hermes
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Build HermesC Apple
        uses: ./.github/actions/build-hermesc-apple
        with:
          hermes-version: ${{ needs.prepare_hermes_workspace.outputs.hermes-version }}
          react-native-version: ${{ needs.prepare_hermes_workspace.outputs.react-native-version }}

  build_apple_slices_hermes:
    runs-on: macos-14
    needs: [build_hermesc_apple, prepare_hermes_workspace]
    env:
      HERMES_WS_DIR: /tmp/hermes
      HERMES_TARBALL_ARTIFACTS_DIR: /tmp/hermes/hermes-runtime-darwin
      HERMES_OSXBIN_ARTIFACTS_DIR: /tmp/hermes/osx-bin
      IOS_DEPLOYMENT_TARGET: ""15.1""
      XROS_DEPLOYMENT_TARGET: ""1.0""
      MAC_DEPLOYMENT_TARGET: ""10.15""
    strategy:
      fail-fast: false
      matrix:
        flavor: [Debug, Release]
        slice: [macosx, iphoneos, iphonesimulator, appletvos, appletvsimulator, catalyst, xros, xrsimulator]
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Build Slice
        uses: ./.github/actions/build-apple-slices-hermes
        with:
          flavor: ${{ matrix.flavor }}
          slice: ${{ matrix.slice}}
          hermes-version: ${{ needs.prepare_hermes_workspace.outputs.hermes-version }}
          react-native-version: ${{ needs.prepare_hermes_workspace.outputs.react-native-version }}

  build_hermes_macos:
    runs-on: macos-14
    needs: [build_apple_slices_hermes, prepare_hermes_workspace]
    env:
      HERMES_WS_DIR: /tmp/hermes
      HERMES_TARBALL_ARTIFACTS_DIR: /tmp/hermes/hermes-runtime-darwin
    continue-on-error: true
    strategy:
      fail-fast: false
      matrix:
        flavor: [Debug, Release]
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Build Hermes MacOS
        uses: ./.github/actions/build-hermes-macos
        with:
          hermes-version: ${{ needs.prepare_hermes_workspace.outputs.hermes-version }}
          react-native-version: ${{ needs.prepare_hermes_workspace.outputs.react-native-version }}
          flavor: ${{ matrix.flavor }}

  prebuild_apple_dependencies:
    uses: ./.github/workflows/prebuild-ios-dependencies.yml
    secrets: inherit

  prebuild_react_native_core:
    uses: ./.github/workflows/prebuild-ios-core.yml
    secrets: inherit
    needs: [prebuild_apple_dependencies, build_hermes_macos]

  test_ios_rntester_ruby_3_2_0:
    runs-on: macos-14
    needs:
      [build_apple_slices_hermes, prepare_hermes_workspace, build_hermes_macos, prebuild_apple_dependencies]
    env:
      HERMES_WS_DIR: /tmp/hermes
      HERMES_TARBALL_ARTIFACTS_DIR: /tmp/hermes/hermes-runtime-darwin
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Run it
        uses: ./.github/actions/test-ios-rntester
        with:
          ruby-version: ""3.2.0""
          hermes-version: ${{ needs.prepare_hermes_workspace.outputs.hermes-version }}
          react-native-version: ${{ needs.prepare_hermes_workspace.outputs.react-native-version }}

  test_ios_rntester_dynamic_frameworks:
    runs-on: macos-14
    needs:
      [build_apple_slices_hermes, prepare_hermes_workspace, build_hermes_macos, prebuild_apple_dependencies]
    env:
      HERMES_WS_DIR: /tmp/hermes
      HERMES_TARBALL_ARTIFACTS_DIR: /tmp/hermes/hermes-runtime-darwin
    continue-on-error: true
    strategy:
      fail-fast: false
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Run it
        uses: ./.github/actions/test-ios-rntester
        with:
          use-frameworks: DynamicFrameworks
          hermes-version: ${{ needs.prepare_hermes_workspace.outputs.hermes-version }}
          react-native-version: ${{ needs.prepare_hermes_workspace.outputs.react-native-version }}

  test_ios_rntester:
    runs-on: macos-14-large
    needs:
      [build_apple_slices_hermes, prepare_hermes_workspace, build_hermes_macos, prebuild_apple_dependencies]
    env:
      HERMES_WS_DIR: /tmp/hermes
      HERMES_TARBALL_ARTIFACTS_DIR: /tmp/hermes/hermes-runtime-darwin
    continue-on-error: true
    strategy:
      fail-fast: false
      matrix:
        flavor: [Debug, Release]
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Run it
        uses: ./.github/actions/test-ios-rntester
        with:
          run-unit-tests: ""false""
          use-frameworks: StaticLibraries
          hermes-version: ${{ needs.prepare_hermes_workspace.outputs.hermes-version }}
          react-native-version: ${{ needs.prepare_hermes_workspace.outputs.react-native-version }}
          flavor: ${{ matrix.flavor }}

  test_e2e_ios_rntester:
    if: ${{ github.ref == 'refs/heads/main' || contains(github.ref,  'stable') || inputs.run-e2e-tests }}
    runs-on: macos-14-large
    needs:
      [test_ios_rntester]
    env:
      HERMES_WS_DIR: /tmp/hermes
      HERMES_TARBALL_ARTIFACTS_DIR: /tmp/hermes/hermes-runtime-darwin
    strategy:
      fail-fast: false
      matrix:
        flavor: [Debug, Release]
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Setup Node.js
        uses: ./.github/actions/setup-node
      - name: Download App
        uses: actions/download-artifact@v4
        with:
          name: RNTesterApp-NewArch-${{ matrix.flavor }}
          path: /tmp/RNTesterBuild/RNTester.app
      - name: Check downloaded folder content
        run: ls -lR /tmp/RNTesterBuild
      - name: Setup xcode
        uses: ./.github/actions/setup-xcode
      - name: Run E2E Tests
        uses: ./.github/actions/maestro-ios
        with:
          app-path: ""/tmp/RNTesterBuild/RNTester.app""
          app-id: com.meta.RNTester.localDevelopment
          maestro-flow: ./packages/rn-tester/.maestro/
          flavor: ${{ matrix.flavor }}

  test_e2e_ios_templateapp:
    if: ${{ github.ref == 'refs/heads/main' || contains(github.ref,  'stable') || inputs.run-e2e-tests }}
    runs-on: macos-14-large
    needs: [build_npm_package, prebuild_apple_dependencies]
    env:
      HERMES_WS_DIR: /tmp/hermes
      HERMES_TARBALL_ARTIFACTS_DIR: /tmp/hermes/hermes-runtime-darwin
    strategy:
      fail-fast: false
      matrix:
        flavor: [Debug, Release]
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Setup xcode
        uses: ./.github/actions/setup-xcode
      - name: Setup node.js
        uses: ./.github/actions/setup-node
      - name: Run yarn
        uses: ./.github/actions/yarn-install
      - name: Setup ruby
        uses: ruby/setup-ruby@v1
        with:
          ruby-version: 2.6.10
      - name: Download Hermes
        uses: actions/download-artifact@v4
        with:
          name: hermes-darwin-bin-${{matrix.flavor}}
          path: /tmp/react-native-tmp
      - name: Download React Native Package
        uses: actions/download-artifact@v4
        with:
          name: react-native-package
          path: /tmp/react-native-tmp
      - name: Print /tmp folder
        run: ls -lR /tmp/react-native-tmp
      - name: Download ReactNativeDependencies
        uses: actions/download-artifact@v4
        with:
          name: ReactNativeDependencies${{ matrix.flavor }}.xcframework.tar.gz
          path: /tmp/third-party
      - name: Print third-party folder
        shell: bash
        run: ls -lR /tmp/third-party
      - name: Prepare artifacts
        run: |
          REACT_NATIVE_PKG=$(find /tmp/react-native-tmp -type f -name ""*.tgz"")
          echo ""React Native tgs is $REACT_NATIVE_PKG""

          HERMES_PATH=$(find /tmp/react-native-tmp -type f -name ""*.tar.gz"")
          echo ""Hermes path is $HERMES_PATH""

          # For stable branches, we want to use the stable branch of the template
          # In all the other cases, we want to use ""main""
          BRANCH=${{ github.ref_name }}
          if ! [[ $BRANCH == *-stable* ]]; then
            BRANCH=main
          fi

          node ./scripts/e2e/init-project-e2e.js --projectName RNTestProject --currentBranch $BRANCH --directory /tmp/RNTestProject --pathToLocalReactNative $REACT_NATIVE_PKG

          cd /tmp/RNTestProject/ios
          bundle install
          NEW_ARCH_ENABLED=1

          export RCT_USE_LOCAL_RN_DEP=/tmp/third-party/ReactNativeDependencies${{ matrix.flavor }}.xcframework.tar.gz
          HERMES_ENGINE_TARBALL_PATH=$HERMES_PATH RCT_NEW_ARCH_ENABLED=$NEW_ARCH_ENABLED bundle exec pod install

          xcodebuild \
            -scheme ""RNTestProject"" \
            -workspace RNTestProject.xcworkspace \
            -configuration ""${{ matrix.flavor }}"" \
            -sdk ""iphonesimulator"" \
            -destination ""generic/platform=iOS Simulator"" \
            -derivedDataPath ""/tmp/RNTestProject""
      - name: Run E2E Tests
        uses: ./.github/actions/maestro-ios
        with:
          app-path: ""/tmp/RNTestProject/Build/Products/${{ matrix.flavor }}-iphonesimulator/RNTestProject.app""
          app-id: org.reactjs.native.example.RNTestProject
          maestro-flow: ./scripts/e2e/.maestro/
          flavor: ${{ matrix.flavor }}
          working-directory: /tmp/RNTestProject

  test_e2e_android_templateapp:
    if: ${{ github.ref == 'refs/heads/main' || contains(github.ref,  'stable') || inputs.run-e2e-tests }}
    runs-on: 4-core-ubuntu
    needs: build_npm_package
    strategy:
      fail-fast: false
      matrix:
        flavor: [debug, release]
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Setup node.js
        uses: ./.github/actions/setup-node
      - name: Run yarn
        uses: ./.github/actions/yarn-install
      - name: Set up JDK 17
        uses: actions/setup-java@v2
        with:
          java-version: '17'
          distribution: 'zulu'
      - name: Download Maven Local
        uses: actions/download-artifact@v4
        with:
          name: maven-local
          path: /tmp/react-native-tmp/maven-local
      - name: Download React Native Package
        uses: actions/download-artifact@v4
        with:
          name: react-native-package
          path: /tmp/react-native-tmp
      - name: Print /tmp folder
        run: ls -lR /tmp/react-native-tmp
      - name: Prepare artifacts
        id: prepare-artifacts
        run: |
          REACT_NATIVE_PKG=$(find /tmp/react-native-tmp -type f -name ""*.tgz"")
          echo ""React Native tgs is $REACT_NATIVE_PKG""

          MAVEN_LOCAL=/tmp/react-native-tmp/maven-local
          echo ""Maven local path is $MAVEN_LOCAL""

          # For stable branches, we want to use the stable branch of the template
          # In all the other cases, we want to use ""main""
          BRANCH=${{ github.ref_name }}
          if ! [[ $BRANCH == *-stable* ]]; then
            BRANCH=main
          fi
          node ./scripts/e2e/init-project-e2e.js --projectName RNTestProject --currentBranch $BRANCH  --directory /tmp/RNTestProject --pathToLocalReactNative $REACT_NATIVE_PKG

          echo ""Feed maven local to gradle.properties""
          cd /tmp/RNTestProject
          echo ""react.internal.mavenLocalRepo=$MAVEN_LOCAL"" >> android/gradle.properties

          # Build
          cd android
          CAPITALIZED_FLAVOR=$(echo ""${{ matrix.flavor }}"" | awk '{print toupper(substr($0, 1, 1)) substr($0, 2)}')
          ./gradlew assemble$CAPITALIZED_FLAVOR --no-daemon -PreactNativeArchitectures=x86

      - name: Run E2E Tests
        uses: ./.github/actions/maestro-android
        timeout-minutes: 60
        with:
          app-path: /tmp/RNTestProject/android/app/build/outputs/apk/${{ matrix.flavor }}/app-${{ matrix.flavor }}.apk
          app-id: com.rntestproject
          maestro-flow: ./scripts/e2e/.maestro/
          install-java: 'false'
          flavor: ${{ matrix.flavor }}
          working-directory: /tmp/RNTestProject

  build_hermesc_linux:
    runs-on: ubuntu-latest
    needs: prepare_hermes_workspace
    env:
      HERMES_WS_DIR: /tmp/hermes
      HERMES_TARBALL_ARTIFACTS_DIR: /tmp/hermes/hermes-runtime-darwin
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Build HermesC Linux
        uses: ./.github/actions/build-hermesc-linux
        with:
          hermes-version: ${{ needs.prepare_hermes_workspace.outputs.hermes-version }}
          react-native-version: ${{ needs.prepare_hermes_workspace.outputs.react-native-version }}

  build_hermesc_windows:
    runs-on: windows-2025
    needs: prepare_hermes_workspace
    env:
      HERMES_WS_DIR: 'D:\tmp\hermes'
      HERMES_TARBALL_ARTIFACTS_DIR: 'D:\tmp\hermes\hermes-runtime-darwin'
      HERMES_OSXBIN_ARTIFACTS_DIR: 'D:\tmp\hermes\osx-bin'
      ICU_URL: ""https://github.com/unicode-org/icu/releases/download/release-64-2/icu4c-64_2-Win64-MSVC2017.zip""
      MSBUILD_DIR: 'C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\MSBuild\Current\Bin'
      CMAKE_DIR: 'C:\Program Files\CMake\bin'
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Build HermesC Windows
        uses: ./.github/actions/build-hermesc-windows
        with:
          hermes-version: ${{ needs.prepare_hermes_workspace.outputs.hermes-version }}
          react-native-version: ${{ needs.prepare_hermes_workspace.outputs.react-native-version }}

  build_android:
    runs-on: 8-core-ubuntu
    needs: [set_release_type]
    container:
      image: reactnativecommunity/react-native-android:latest
      env:
        TERM: ""dumb""
        GRADLE_OPTS: ""-Dorg.gradle.daemon=false""
        ORG_GRADLE_PROJECT_SIGNING_PWD: ${{ secrets.ORG_GRADLE_PROJECT_SIGNING_PWD }}
        ORG_GRADLE_PROJECT_SIGNING_KEY: ${{ secrets.ORG_GRADLE_PROJECT_SIGNING_KEY }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Build Android
        uses: ./.github/actions/build-android
        with:
          release-type: ${{ needs.set_release_type.outputs.RELEASE_TYPE }}
          run-e2e-tests: ${{ github.ref == 'refs/heads/main' || contains(github.ref,  'stable') || inputs.run-e2e-tests }}
          gradle-cache-encryption-key: ${{ secrets.GRADLE_CACHE_ENCRYPTION_KEY }}

  test_e2e_android_rntester:
    if: ${{ github.ref == 'refs/heads/main' || contains(github.ref,  'stable') || inputs.run-e2e-tests }}
    runs-on: 4-core-ubuntu
    needs: [build_android]
    strategy:
      fail-fast: false
      matrix:
        flavor: [debug, release]
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Setup node.js
        uses: ./.github/actions/setup-node
      - name: Install node dependencies
        uses: ./.github/actions/yarn-install
      - name: Download APK
        uses: actions/download-artifact@v4
        with:
          name: rntester-${{ matrix.flavor }}
          path: ./packages/rn-tester/android/app/build/outputs/apk/${{ matrix.flavor }}/
      - name: Print folder structure
        run: ls -lR ./packages/rn-tester/android/app/build/outputs/apk/${{ matrix.flavor }}/
      - name: Run E2E Tests
        uses: ./.github/actions/maestro-android
        timeout-minutes: 60
        with:
          app-path: ./packages/rn-tester/android/app/build/outputs/apk/${{ matrix.flavor }}/app-x86-${{ matrix.flavor }}.apk
          app-id: com.facebook.react.uiapp
          maestro-flow: ./packages/rn-tester/.maestro
          flavor: ${{ matrix.flavor }}

  build_npm_package:
    runs-on: 8-core-ubuntu
    needs:
      [
        set_release_type,
        prepare_hermes_workspace,
        build_hermes_macos,
        build_hermesc_linux,
        build_hermesc_windows,
        build_android,
        prebuild_apple_dependencies,
        prebuild_react_native_core,
      ]
    container:
      image: reactnativecommunity/react-native-android:latest
      env:
        TERM: ""dumb""
        GRADLE_OPTS: ""-Dorg.gradle.daemon=false""
    env:
      HERMES_WS_DIR: /tmp/hermes
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Build NPM Package
        uses: ./.github/actions/build-npm-package
        with:
          hermes-ws-dir: ${{ env.HERMES_WS_DIR }}
          release-type: ${{ needs.set_release_type.outputs.RELEASE_TYPE }}
          gradle-cache-encryption-key: ${{ secrets.GRADLE_CACHE_ENCRYPTION_KEY }}

  test_android_helloworld:
    runs-on: 4-core-ubuntu
    needs: build_npm_package
    container:
      image: reactnativecommunity/react-native-android:latest
    env:
      # Set the encoding to resolve a known character encoding issue with decompressing tar.gz files in conatiners
      # via Gradle: https://github.com/gradle/gradle/issues/23391#issuecomment-1878979127
      LC_ALL: C.UTF8
      YARN_ENABLE_IMMUTABLE_INSTALLS: false
      TERM: ""dumb""
      GRADLE_OPTS: ""-Dorg.gradle.daemon=false""
      TARGET_ARCHITECTURE: ""arm64-v8a""
    continue-on-error: true
    strategy:
      fail-fast: false
      matrix:
        flavor: [Debug, Release]
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Setup git safe folders
        run: git config --global --add safe.directory '*'
      - name: Download npm package artifact
        uses: actions/download-artifact@v4.1.3
        with:
          name: react-native-package
          path: build
      - name: Download maven-local artifact
        uses: actions/download-artifact@v4.1.3
        with:
          name: maven-local
          path: /tmp/maven-local
      - name: Setup gradle
        uses: ./.github/actions/setup-gradle
        with:
          cache-encryption-key: ${{ secrets.GRADLE_CACHE_ENCRYPTION_KEY }}
      - name: Run yarn install
        uses: ./.github/actions/yarn-install
      - name: Prepare the Helloworld application
        shell: bash
        run: node ./scripts/e2e/init-project-e2e.js --useHelloWorld --pathToLocalReactNative ""$GITHUB_WORKSPACE/build/$(cat build/react-native-package-version)""
      - name: Build the Helloworld application for ${{ matrix.flavor }} with Architecture set to New Architecture.
        shell: bash
        run: |
          cd private/helloworld/android
          args=()
          if [[ ${{ matrix.flavor }} == ""Release"" ]]; then
            args+=(--prod)
          fi
          yarn build android ""${args[@]}"" -P reactNativeArchitectures=""$TARGET_ARCHITECTURE"" -P react.internal.mavenLocalRepo=""/tmp/maven-local""
      - name: Upload artifact
        uses: actions/upload-artifact@v4.3.4
        with:
          name: helloworld-apk-${{ matrix.flavor }}-NewArch-hermes
          path: ./private/helloworld/android/app/build/outputs/apk/
          compression-level: 0

  test_ios_helloworld_with_ruby_3_2_0:
    runs-on: macos-14
    needs: [prepare_hermes_workspace, build_hermes_macos, prebuild_apple_dependencies] # prepare_hermes_workspace must be there because we need its reference to retrieve a couple of outputs
    env:
      PROJECT_NAME: iOSTemplateProject
      HERMES_WS_DIR: /tmp/hermes
      YARN_ENABLE_IMMUTABLE_INSTALLS: false
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - uses: ./.github/actions/test-ios-helloworld
        with:
          ruby-version: 3.2.0
          flavor: Debug
          hermes-version: ${{ needs.prepare_hermes_workspace.outputs.hermes-version }}
          react-native-version: ${{ needs.prepare_hermes_workspace.outputs.react-native-version }}

  test_ios_helloworld:
    runs-on: macos-14
    needs: [prepare_hermes_workspace, build_hermes_macos, prebuild_apple_dependencies] # prepare_hermes_workspace must be there because we need its reference to retrieve a couple of outputs
    strategy:
      matrix:
        flavor: [Debug, Release]
        use_frameworks: [StaticLibraries, DynamicFrameworks]
        exclude:
          # This config is tested with Ruby 3.2.0. Let's not double test it.
          - flavor: Debug
            use_frameworks: StaticLibraries
    env:
      PROJECT_NAME: iOSTemplateProject
      HERMES_WS_DIR: /tmp/hermes
      YARN_ENABLE_IMMUTABLE_INSTALLS: false
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - uses: ./.github/actions/test-ios-helloworld
        with:
          flavor: ${{ matrix.flavor }}
          use-frameworks: ${{ matrix.use_frameworks }}
          hermes-version: ${{ needs.prepare_hermes_workspace.outputs.hermes-version }}
          react-native-version: ${{ needs.prepare_hermes_workspace.outputs.react-native-version }}

  test_js:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        node-version: [""24"", ""22""]
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Test JS
        uses: ./.github/actions/test-js
        with:
          node-version: ${{ matrix.node-version }}

  lint:
    runs-on: ubuntu-latest
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Run all the linters
        uses: ./.github/actions/lint
        with:
          github-token: ${{ env.GH_TOKEN }}

  # This job should help with the E2E flakyness.
  # In case E2E tests fails, it launches a new retry-workflow workflow, passing the current run_id as input.
  # The retry-workflow reruns only the failed jobs of the current test-all workflow using
  # ```
  # gh run rerun ${{ inputs.run_id }} --failed
  # ```
  # From https://stackoverflow.com/a/78314483 it seems like that adding the extra workflow
  # rather then calling directly this command should improve stability of this solution.
  # This is exactly the same as rerunning failed tests from the GH UI, but automated.
  rerun-failed-jobs:
    runs-on: ubuntu-latest
    needs: [test_e2e_ios_rntester, test_e2e_android_rntester, test_e2e_ios_templateapp, test_e2e_android_templateapp]
    if: always()
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Rerun failed jobs in the current workflow
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          SHOULD_RETRY=${{fromJSON(github.run_attempt) < 3}}
          if [[ $SHOULD_RETRY == ""false"" ]]; then
            exit 0
          fi

          RNTESTER_ANDROID_FAILED=${{ needs.test_e2e_android_rntester.result == 'failure' }}
          TEMPLATE_ANDROID_FAILED=${{ needs.test_e2e_android_templateapp.result == 'failure' }}
          RNTESTER_IOS_FAILED=${{ needs.test_e2e_ios_rntester.result == 'failure' }}
          TEMPLATE_IOS_FAILED=${{ needs.test_e2e_ios_templateapp.result == 'failure' }}

          echo ""RNTESTER_ANDROID_FAILED: $RNTESTER_ANDROID_FAILED""
          echo ""TEMPLATE_ANDROID_FAILED: $TEMPLATE_ANDROID_FAILED""
          echo ""RNTESTER_IOS_FAILED: $RNTESTER_IOS_FAILED""
          echo ""TEMPLATE_IOS_FAILED: $TEMPLATE_IOS_FAILED""

          if [[ $RNTESTER_ANDROID_FAILED == ""true"" || $TEMPLATE_ANDROID_FAILED == ""true"" || $RNTESTER_IOS_FAILED == ""true"" || $TEMPLATE_IOS_FAILED == ""true"" ]]; then
            echo ""Rerunning failed jobs in the current workflow""
            gh workflow run retry-workflow.yml -F run_id=${{ github.run_id }}
          fi
",655,24,3,"workflow_dispatch, pull_request, push",65
facebook/react-native,test-libraries-on-nightlies.yml,"name: Test Libraries on Nightlies

on:
  workflow_call:
    secrets:
      discord_webhook_url:
        required: true


# We use the matrix.library entry to specify the dependency we want to use
# The key is used directly as the <pkg> in the `yarn add <pkg>` command.
jobs:
  runner-setup:
    runs-on: ubuntu-latest
    outputs:
      runners: '{""ios"":""macos-14-large"", ""android"": ""ubuntu-latest""}'
    steps:
      - run: echo no-op

  test-library-on-nightly:
    name: ""[${{ matrix.platform }}] ${{ matrix.library }}""
    needs: runner-setup
    runs-on: ${{ fromJSON(needs.runner-setup.outputs.runners)[matrix.platform] }}
    continue-on-error: true
    strategy:
      matrix:
        library: [
          ""react-native-async-storage"",
          ""react-native-blob-util"",
          ""@react-native-clipboard/clipboard"",
          ""@react-native-community/datetimepicker"",
          ""react-native-gesture-handler"",
          ""react-native-image-picker"",
          ""react-native-linear-gradient"",
          ""@react-native-masked-view/masked-view"",
          # ""react-native-maps"", React Native Maps with the New Arch support has a complex cocoapods setup for iOS. It needs a dedicated workflow.
          ""@react-native-community/netinfo"",
          ""react-native-reanimated@nightly react-native-worklets@nightly"", #reanimated requires worklet to be explicitly installed as a separate package
          ""react-native-svg"",
          ""react-native-video"",
          ""react-native-webview"",
          ""react-native-mmkv"",
          ""react-native-screens"",
          ""react-native-pager-view"",
          ""@react-native-community/slider"",
          # additional OSS libs used internally
          ""scandit-react-native-datacapture-barcode scandit-react-native-datacapture-core"",
          ""react-native-contacts"",
          ""react-native-device-info"",
          ""react-native-email-link"",
          ""@dr.pogodin/react-native-fs"",
          ""react-native-permissions"",
          ""react-native-vector-icons"",
          ""react-native-masked-view"",
          ""@react-native-community/image-editor"",
        ]
        platform: [ios, android]
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Set up Node.js
        uses: ./.github/actions/setup-node
      - name: Test ${{ matrix.library }}
        id: run-test
        uses: ./.github/actions/test-library-on-nightly
        with:
          library-npm-package: ${{ matrix.library }}
          platform: ${{ matrix.platform}}
      - name: Save outcome
        id: save-outcome
        if: always()
        run: |
          LIB_FOLDER=$(echo ""${{matrix.library}}""  | tr ' ' '_' | tr '/' '_')
          echo ""${{matrix.library}}: ${{steps.run-test.outcome}}"" > ""/tmp/$LIB_FOLDER-${{ matrix.platform }}-outcome""
          echo ""lib_folder=$LIB_FOLDER"" >> $GITHUB_OUTPUT
      - name: Upload Artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.save-outcome.outputs.lib_folder }}-${{ matrix.platform }}-outcome
          path: /tmp/${{ steps.save-outcome.outputs.lib_folder }}-${{ matrix.platform }}-outcome


  collect-results:
    runs-on: ubuntu-latest
    needs: [test-library-on-nightly]
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Restore outcomes
        uses: actions/download-artifact@v4
        with:
          pattern: '*-outcome'
          path: /tmp
      - name: Collect failures
        uses: actions/github-script@v6
        with:
          script: |
            const {collectResults} = require('./.github/workflow-scripts/collectNightlyOutcomes.js');
            await collectResults('${{secrets.discord_webhook_url}}');
",100,3,1,workflow_call,7
microsoft/PowerToys,dependency-review.yml,"# Dependency Review Action
#
# This Action will scan dependency manifest files that change as part of a Pull Request,
# surfacing known-vulnerable versions of the packages declared or updated in the PR.
# Once installed, if the workflow run is marked as required,
# PRs introducing known-vulnerable packages will be blocked from merging.
#
# As recommended by Microsoft's security guidelines (https://docs.opensource.microsoft.com/security/tsg/actions/#requirements-for-security-hardening-your-own-github-actions), 
# 3rd-party actions should be pinned to a specific commit hash to prevent supply chain attacks. 
# This update aligns with best practices; 1st/2nd-party actions is not required hash pinning.
#
# Source repository: https://github.com/actions/dependency-review-action
name: 'Dependency Review'
on: [pull_request]

permissions:
  contents: read

jobs:
  dependency-review:
    runs-on: ubuntu-latest
    steps:
      - name: 'Checkout Repository'
        uses: actions/checkout@v4
      - name: 'Dependency Review'
        uses: actions/dependency-review-action@v4",26,1,1,pull_request,2
microsoft/PowerToys,msstore-submissions.yml,"name: Store submission on release

on:
  workflow_dispatch:
  release:
    types: [published]

permissions:
  id-token: write

jobs:

  microsoft_store:
    name: Publish Microsoft Store
    environment: store
    runs-on: ubuntu-latest
    steps:
      - name: BODGY - Set up Gnome Keyring for future Cert Auth
        run: |-
          sudo apt-get install -y gnome-keyring
          export $(dbus-launch --sh-syntax)
          export $(echo 'anypass_just_to_unlock' | gnome-keyring-daemon --unlock)
          export $(echo 'anypass_just_to_unlock' | gnome-keyring-daemon --start --components=gpg,pkcs11,secrets,ssh)

      - name: Log in to Azure
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          enable-AzPSSession: true

      - name: Get latest URL from public releases
        id: releaseVars
        run: |
          release=$(curl https://api.github.com/repos/Microsoft/PowerToys/releases | jq '[.[]|select(.name | contains(""Release""))][0]')
          assets=$(jq -n ""$release"" | jq '.assets')
          powerToysSetup=$(jq -n ""$assets"" | jq '[.[]|select(.name | contains(""PowerToysUserSetup""))]')
          echo powerToysInstallerX64Url=$(jq -n ""$powerToysSetup"" | jq -r '[.[]|select(.name | contains(""x64""))][0].browser_download_url') >> $GITHUB_OUTPUT
          echo powerToysInstallerArm64Url=$(jq -n ""$powerToysSetup"" | jq -r '[.[]|select(.name | contains(""arm64""))][0].browser_download_url') >> $GITHUB_OUTPUT

      - name: Setup .NET 9.0
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '9.0.x'

      - uses: microsoft/setup-msstore-cli@v1

      - name: Fetch Store Credential
        uses: azure/cli@v2
        with:
          azcliversion: latest
          inlineScript: |-
            az keyvault secret download --vault-name ${{ secrets.AZURE_KEYVAULT_NAME }} -n ${{ secrets.AZURE_AUTH_CERT_NAME }} -f cert.pfx.b64
            base64 -d < cert.pfx.b64 > cert.pfx

      - name: Configure Store Credentials
        run: |-
          msstore reconfigure -cfp cert.pfx -c ${{ secrets.AZURE_CLIENT_ID }} -t ${{ secrets.AZURE_TENANT_ID }} -s ${{ secrets.SELLER_ID }}

      - name: Update draft submission
        run: |-
          msstore submission update ${{ secrets.PRODUCT_ID }} '{
            ""packages"":[
                {
                  ""packageUrl"":""${{ steps.releaseVars.outputs.powerToysInstallerX64Url }}"",
                  ""languages"":[""zh-hans"", ""zh-hant"", ""en"", ""cs"", ""nl"", ""fr"", ""pt"", ""pt-br"", ""de"", ""hu"", ""it"", ""ja"", ""ko"", ""pl"", ""ru"", ""es"", ""tr""],
                  ""architectures"":[""X64""],
                  ""installerParameters"":""/quiet /norestart"",
                  ""isSilentInstall"":true
                },
                {
                  ""packageUrl"":""${{ steps.releaseVars.outputs.powerToysInstallerArm64Url }}"",
                  ""languages"":[""zh-hans"", ""zh-hant"", ""en"", ""cs"", ""nl"", ""fr"", ""pt"", ""pt-br"", ""de"", ""hu"", ""it"", ""ja"", ""ko"", ""pl"", ""ru"", ""es"", ""tr""],
                  ""architectures"":[""Arm64""],
                  ""installerParameters"":""/quiet /norestart"",
                  ""isSilentInstall"":true
                }
            ]
          }'

      - name: Publish Submission
        run: |-
          msstore submission publish ${{ secrets.PRODUCT_ID }}

      - name: Clean up auth certificate
        if: always()
        run: |-
          rm -f cert.pfx cert.pfx.b64
",89,1,2,"workflow_dispatch, release",4
microsoft/PowerToys,package-submissions.yml,"name: WinGet submission on release

on:
  workflow_dispatch:
  release:
    types: [published]

jobs:
  winget:
    name: Publish winget package

    # winget-create is only supported on Windows
    runs-on: windows-latest

    # winget-create will read the following environment variable to access the GitHub token needed for submitting a PR
    # See https://aka.ms/winget-create-token
    env:
      WINGET_CREATE_GITHUB_TOKEN: ${{ secrets.PT_WINGET }}

    # Only submit stable releases
    if: ${{ !github.event.release.prerelease }}
    steps:
      - name: Submit Microsoft.PowerToys package to Windows Package Manager Community Repository
        run: |
          # Get installer info from GitHub release event
          $assets = '${{ toJSON(github.event.release.assets) }}' | ConvertFrom-Json
          $x64UserInstallerUrl = $assets | Where-Object -Property name -match 'PowerToysUserSetup.*x64' | Select -ExpandProperty browser_download_url
          $x64MachineInstallerUrl = $assets | Where-Object -Property name -match 'PowerToysSetup.*x64' | Select -ExpandProperty browser_download_url
          $arm64UserInstallerUrl = $assets | Where-Object -Property name -match 'PowerToysUserSetup.*arm64' | Select -ExpandProperty browser_download_url
          $arm64MachineInstallerUrl = $assets | Where-Object -Property name -match 'PowerToysSetup.*arm64' | Select -ExpandProperty browser_download_url
          $packageVersion = (${{ toJSON(github.event.release.tag_name) }}).Trim('v')

          # Update package using wingetcreate
          curl.exe -JLO https://aka.ms/wingetcreate/latest
          .\wingetcreate.exe update Microsoft.PowerToys `
            --version $packageVersion `
            --urls ""$x64UserInstallerUrl|user"" ""$x64MachineInstallerUrl|machine"" ""$arm64UserInstallerUrl|user"" ""$arm64MachineInstallerUrl|machine"" `
            --submit
",38,1,2,"workflow_dispatch, release",0
microsoft/PowerToys,spelling2.yml,"# spelling.yml is blocked per https://github.com/check-spelling/check-spelling/security/advisories/GHSA-g86g-chm8-7r2p
name: Spell checking

# Comment management is handled through a secondary job, for details see:
# https://github.com/check-spelling/check-spelling/wiki/Feature%3A-Restricted-Permissions
#
# `jobs.comment-push` runs when a push is made to a repository and the `jobs.spelling` job needs to make a comment
#   (in odd cases, it might actually run just to collapse a comment, but that's fairly rare)
#   it needs `contents: write` in order to add a comment.
#
# `jobs.comment-pr` runs when a pull_request is made to a repository and the `jobs.spelling` job needs to make a comment
#   or collapse a comment (in the case where it had previously made a comment and now no longer needs to show a comment)
#   it needs `pull-requests: write` in order to manipulate those comments.

# Updating pull request branches is managed via comment handling.
# For details, see: https://github.com/check-spelling/check-spelling/wiki/Feature:-Update-expect-list
#
# These elements work together to make it happen:
#
# `on.issue_comment`
#   This event listens to comments by users asking to update the metadata.
#
# `jobs.update`
#   This job runs in response to an issue_comment and will push a new commit
#   to update the spelling metadata.
#
# `with.experimental_apply_changes_via_bot`
#   Tells the action to support and generate messages that enable it
#   to make a commit to update the spelling metadata.
#
# `with.ssh_key`
#   In order to trigger workflows when the commit is made, you can provide a
#   secret (typically, a write-enabled github deploy key).
#
#   For background, see: https://github.com/check-spelling/check-spelling/wiki/Feature:-Update-with-deploy-key

# SARIF reporting
#
# Access to SARIF reports is generally restricted (by GitHub) to members of the repository.
#
# Requires enabling `security-events: write`
# and configuring the action with `use_sarif: 1`
#
#   For information on the feature, see: https://github.com/check-spelling/check-spelling/wiki/Feature:-SARIF-output

# Minimal workflow structure:
#
# on:
#   push:
#     ...
#   pull_request_target:
#     ...
# jobs:
#   # you only want the spelling job, all others should be omitted
#   spelling:
#     # remove `security-events: write` and `use_sarif: 1`
#     # remove `experimental_apply_changes_via_bot: 1`
#     ... otherwise adjust the `with:` as you wish

on:
  push:
    branches:
      - ""**""
    tags-ignore:
      - ""**""
  pull_request_target:
    branches:
      - ""**""
    types:
      - ""opened""
      - ""reopened""
      - ""synchronize""
  issue_comment:
    types:
      - ""created""

jobs:
  spelling:
    name: Check Spelling
    permissions:
      contents: read
      pull-requests: read
      actions: read
      security-events: write
    outputs:
      followup: ${{ steps.spelling.outputs.followup }}
    runs-on: ubuntu-latest
    if: ${{ contains(github.event_name, 'pull_request') || github.event_name == 'push' }}
    concurrency:
      group: spelling-${{ github.event.pull_request.number || github.ref }}
      # note: If you use only_check_changed_files, you do not want cancel-in-progress
      cancel-in-progress: true
    steps:
      - name: check-spelling
        id: spelling
        uses: check-spelling/check-spelling@c635c2f3f714eec2fcf27b643a1919b9a811ef2e # v0.0.25
        with:
          config: .github/actions/spell-check
          suppress_push_for_open_pull_request: ${{ github.actor != 'dependabot[bot]' && 1 }}
          checkout: true
          check_file_names: 1
          spell_check_this: microsoft/PowerToys@main
          post_comment: 0
          use_magic_file: 1
          report-timing: 1
          warnings: bad-regex,binary-file,deprecated-feature,ignored-expect-variant,large-file,limited-references,no-newline-at-eof,noisy-file,non-alpha-in-dictionary,token-is-substring,unexpected-line-ending,whitespace-in-dictionary,minified-file,unsupported-configuration,no-files-to-check,unclosed-block-ignore-begin,unclosed-block-ignore-end
          experimental_apply_changes_via_bot: 1
          use_sarif: ${{ (!github.event.pull_request || (github.event.pull_request.head.repo.full_name == github.repository)) && 1 }}
          check_extra_dictionaries: """"
          dictionary_source_prefixes: >
            {
            ""cspell"": ""https://raw.githubusercontent.com/check-spelling/cspell-dicts/v20241114/dictionaries/""
            }
          extra_dictionaries: |
            cspell:software-terms/softwareTerms.txt
            cspell:cpp/stdlib-cpp.txt
            cspell:filetypes/filetypes.txt
            cspell:cpp/stdlib-c.txt
            cspell:lorem-ipsum/dictionary.txt
            cspell:python/python/python-lib.txt
            cspell:php/php.txt
            cspell:fullstack/fullstack.txt
            cspell:dotnet/dotnet.txt
            cspell:swift/swift.txt
            cspell:node/node.txt
            cspell:dart/dart.txt
            cspell:django/django.txt
            cspell:python/python/python.txt
            cspell:powershell/powershell.txt
            cspell:npm/npm.txt
            cspell:golang/go.txt
            cspell:cpp/compiler-msvc.txt
            cspell:csharp/csharp.txt
            cspell:html/html.txt
            cspell:java/java.txt
            cspell:aws/aws.txt
            cspell:typescript/typescript.txt
            cspell:cpp/lang-keywords.txt
            cspell:python/common/extra.txt
            cspell:scala/scala.txt
            cspell:shell/shell-all-words.txt
            cspell:css/css.txt
            cspell:r/r.txt
            cspell:java/java-terms.txt
            cspell:cpp/stdlib-cerrno.txt
            cspell:k8s/k8s.txt

  comment-push:
    name: Report (Push)
    # If your workflow isn't running on push, you can remove this job
    runs-on: ubuntu-latest
    needs: spelling
    permissions:
      actions: read
      contents: write
    if: (success() || failure()) && needs.spelling.outputs.followup && github.event_name == 'push'
    steps:
      - name: comment
        uses: check-spelling/check-spelling@c635c2f3f714eec2fcf27b643a1919b9a811ef2e # v0.0.25
        with:
          config: .github/actions/spell-check
          checkout: true
          spell_check_this: microsoft/PowerToys@main
          task: ${{ needs.spelling.outputs.followup }}

  comment-pr:
    name: Report (PR)
    # If you workflow isn't running on pull_request*, you can remove this job
    runs-on: ubuntu-latest
    needs: spelling
    permissions:
      actions: read
      contents: read
      pull-requests: write
    if: (success() || failure()) && needs.spelling.outputs.followup && contains(github.event_name, 'pull_request')
    steps:
      - name: comment
        uses: check-spelling/check-spelling@c635c2f3f714eec2fcf27b643a1919b9a811ef2e # v0.0.25
        with:
          config: .github/actions/spell-check
          checkout: true
          spell_check_this: check-spelling/spell-check-this@prerelease
          task: ${{ needs.spelling.outputs.followup }}
          experimental_apply_changes_via_bot: ${{ github.repository_owner != 'microsoft' && 1 }}

  update:
    name: Update PR
    permissions:
      contents: write
      pull-requests: write
      actions: read
    runs-on: ubuntu-latest
    if: ${{
      github.repository_owner != 'microsoft' &&
      github.event_name == 'issue_comment' &&
      github.event.issue.pull_request &&
      contains(github.event.comment.body, '@check-spelling-bot apply') &&
      contains(github.event.comment.body, 'https://')
      }}
    concurrency:
      group: spelling-update-${{ github.event.issue.number }}
      cancel-in-progress: false
    steps:
      - name: apply spelling updates
        uses: check-spelling/check-spelling@c635c2f3f714eec2fcf27b643a1919b9a811ef2e # v0.0.25
        with:
          experimental_apply_changes_via_bot: ${{ github.repository_owner != 'microsoft' && 1 }}
          checkout: true
          ssh_key: ""${{ secrets.CHECK_SPELLING }}""
",209,4,3,"push, pull_request_target, issue_comment",4
electron/electron,archaeologist-dig.yml,"name: Archaeologist

on:
  pull_request:

jobs:
   archaeologist-dig:
    name: Archaeologist Dig
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Electron
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 #v4.0.2
        with:
          fetch-depth: 0
      - name: Setup Node.js/npm
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020
        with:
          node-version: 20.19.x
      - name: Setting Up Dig Site
        run: |
          echo ""remote: ${{ github.event.pull_request.head.repo.clone_url }}""
          echo ""sha ${{ github.event.pull_request.head.sha }}""
          echo ""base ref ${{ github.event.pull_request.base.ref }}""
          git clone https://github.com/electron/electron.git electron          
          cd electron
          mkdir -p artifacts
          git remote add fork ${{ github.event.pull_request.head.repo.clone_url }} && git fetch fork
          git checkout  ${{ github.event.pull_request.head.sha }}
          git merge-base origin/${{ github.event.pull_request.base.ref }} HEAD > .dig-old
          echo  ${{ github.event.pull_request.head.sha }} > .dig-new
          cp .dig-old artifacts

      - name: Generating Types for SHA in .dig-new
        uses: ./.github/actions/generate-types
        with:
          sha-file: .dig-new
          filename: electron.new.d.ts
      - name: Generating Types for SHA in .dig-old
        uses: ./.github/actions/generate-types
        with:
          sha-file: .dig-old
          filename: electron.old.d.ts
      - name: Upload artifacts
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 #v4.6.2
        with:
          name: artifacts
          path: electron/artifacts
          include-hidden-files: true
      - name: Set job output
        run: |
          git diff --no-index electron.old.d.ts electron.new.d.ts > patchfile || true
          if [ -s patchfile ]; then
            echo ""Changes Detected""
            echo ""## Changes Detected"" > $GITHUB_STEP_SUMMARY
            echo ""Looks like the \`electron.d.ts\` file changed."" >> $GITHUB_STEP_SUMMARY
            echo """" >> $GITHUB_STEP_SUMMARY
            echo ""\`\`\`\`\`\`diff"" >> $GITHUB_STEP_SUMMARY
            cat patchfile >> $GITHUB_STEP_SUMMARY
            echo ""\`\`\`\`\`\`"" >> $GITHUB_STEP_SUMMARY    
          else
            echo ""No Changes Detected""
            echo ""## No Changes"" > $GITHUB_STEP_SUMMARY
            echo ""We couldn't see any changes in the \`electron.d.ts\` artifact"" >> $GITHUB_STEP_SUMMARY
          fi
        working-directory: ./electron/artifacts
",65,1,1,pull_request,5
electron/electron,audit-branch-ci.yml,"name: Audit CI on Branches

on:
  workflow_dispatch:
  schedule:
    # Run every 2 hours
    - cron: '0 */2 * * *'

permissions: {}

jobs:
  audit_branch_ci:
    name: Audit CI on Branches
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - run: npm install @actions/cache @electron/fiddle-core
      - uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea # v7.0.1
        id: audit-errors
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const cache = require('@actions/cache');
            const { ElectronVersions } = require('@electron/fiddle-core');

            const runsWithErrors = [];

            // Only want the most recent workflow run that wasn't skipped or cancelled
            const isValidWorkflowRun = (run) => !['skipped', 'cancelled'].includes(run.conclusion);

            const versions = await ElectronVersions.create(undefined, { ignoreCache: true });
            const branches = versions.supportedMajors.map((branch) => `${branch}-x-y`);

            for (const branch of [""main"", ...branches]) {
              const latestCheckRuns = new Map();
              const allCheckRuns = await github.paginate(github.rest.checks.listForRef, {
                owner: ""electron"",
                repo: ""electron"",
                ref: branch,
                status: 'completed',
              });

              // Sort the check runs by completed_at so that multiple check runs on the
              // same ref (like a scheduled workflow) only looks at the most recent one
              for (const checkRun of allCheckRuns.filter(
                (run) => !['skipped', 'cancelled'].includes(run.conclusion),
              ).sort((a, b) => new Date(b.completed_at) - new Date(a.completed_at))) {
                if (!latestCheckRuns.has(checkRun.name)) {
                  latestCheckRuns.set(checkRun.name, checkRun);
                }
              }

              // Check for runs which had error annotations
              for (const checkRun of Array.from(latestCheckRuns.values())) {
                if (checkRun.name === ""Audit CI on Branches"") {
                  continue; // Skip the audit workflow itself
                }

                const annotations = (await github.rest.checks.listAnnotations({
                  owner: ""electron"",
                  repo: ""electron"",
                  check_run_id: checkRun.id,
                })).data ?? [];

                if (
                  annotations.find(
                    ({ annotation_level, message }) =>
                      annotation_level === ""failure"" &&
                      !message.startsWith(""Process completed with exit code"") &&
                      !message.startsWith(""Response status code does not indicate success"") &&
                      !/Unable to make request/.test(message) &&
                      !/The requested URL returned error/.test(message),
                  )
                ) {
                  checkRun.hasErrorAnnotations = true;
                } else {
                  continue;
                }

                // Check if this is a known failure from a previous audit run
                const cacheKey = `check-run-error-annotations-${checkRun.id}`;
                const cacheHit =
                  (await cache.restoreCache(['/dev/null'], cacheKey, undefined, {
                    lookupOnly: true,
                  })) !== undefined;

                if (cacheHit) {
                  checkRun.isStale = true;
                }

                checkRun.branch = branch;
                runsWithErrors.push(checkRun);
  
                // Create a cache entry (only the name matters) to keep track of
                // failures we've seen from previous runs to mark them as stale
                if (!cacheHit) {
                  await cache.saveCache(['/dev/null'], cacheKey);
                }
              }
            }

            if (runsWithErrors.length > 0) {
              core.setOutput('errorsFound', true);
              core.summary.addHeading('⚠️ Runs with Errors');
              core.summary.addTable([
                [
                  { data: 'Branch', header: true },
                  { data: 'Workflow Run', header: true },
                  { data: 'Status', header: true },
                ],
                ...runsWithErrors
                  .sort(
                    (a, b) =>
                      a.branch.localeCompare(b.branch) ||
                      a.name.localeCompare(b.name),
                  )
                  .map((run) => [
                    run.branch,
                    `<a href=""${run.html_url}"">${run.name}</a>`,
                    run.isStale
                      ? '📅 Stale'
                      : run.hasErrorAnnotations
                      ? '⚠️ Errors'
                      : '✅ Succeeded',
                  ]),
              ]);

              // Set this as failed so it's easy to scan runs to find failures
              if (runsWithErrors.find((run) => !run.isStale)) {
                process.exitCode = 1;
              }
            } else {
              core.summary.addRaw('🎉 No runs with errors');
            }

            await core.summary.write();
      - name: Send Slack message if errors
        if: ${{ always() && steps.audit-errors.outputs.errorsFound && github.ref == 'refs/heads/main' }}
        uses: slackapi/slack-github-action@b0fa283ad8fea605de13dc3f449259339835fc52 # v2.1.0
        with:
          payload: |
            link: ""https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}""
          webhook: ${{ secrets.CI_ERRORS_SLACK_WEBHOOK_URL }}
          webhook-type: webhook-trigger
",145,1,2,"workflow_dispatch, schedule",2
electron/electron,branch-created.yml,"name: Branch Created

on:
  workflow_dispatch:
    inputs:
      branch-name:
        description: Branch name (e.g. `29-x-y`)
        required: true
        type: string
  create:

permissions: {}

jobs:
  release-branch-created:
    name: Release Branch Created
    if: ${{ github.event_name == 'workflow_dispatch' || (github.event.ref_type == 'branch' && endsWith(github.event.ref, '-x-y') && !startsWith(github.event.ref, 'roller')) }}
    permissions:
      contents: read
      pull-requests: write
      repository-projects: write  # Required for labels
    runs-on: ubuntu-latest
    steps:
      - name: Determine Major Version
        id: check-major-version
        env:
          BRANCH_NAME: ${{ github.event.inputs.branch-name || github.event.ref }}
        run: |
          if [[ ""$BRANCH_NAME"" =~ ^([0-9]+)-x-y$ ]]; then
            echo ""MAJOR=${BASH_REMATCH[1]}"" >> ""$GITHUB_OUTPUT""
          else
            echo ""Not a release branch: $BRANCH_NAME""
          fi
      - name: New Release Branch Tasks
        if: ${{ steps.check-major-version.outputs.MAJOR }}
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GH_REPO: electron/electron
          MAJOR: ${{ steps.check-major-version.outputs.MAJOR }}
          NUM_SUPPORTED_VERSIONS: 3
        run: |
          PREVIOUS_MAJOR=$((MAJOR - 1))
          UNSUPPORTED_MAJOR=$((MAJOR - NUM_SUPPORTED_VERSIONS - 1))

          # Create new labels
          gh label create $MAJOR-x-y --color 8d9ee8 || true
          gh label create target/$MAJOR-x-y --color ad244f --description ""PR should also be added to the \""${MAJOR}-x-y\"" branch."" || true
          gh label create merged/$MAJOR-x-y --color 61a3c6 --description ""PR was merged to the \""${MAJOR}-x-y\"" branch."" || true
          gh label create in-flight/$MAJOR-x-y --color db69a6 || true
          gh label create needs-manual-bp/$MAJOR-x-y --color 8b5dba || true

          # Change color of old labels
          gh label edit $UNSUPPORTED_MAJOR-x-y --color ededed || true
          gh label edit target/$UNSUPPORTED_MAJOR-x-y --color ededed || true
          gh label edit merged/$UNSUPPORTED_MAJOR-x-y --color ededed || true
          gh label edit in-flight/$UNSUPPORTED_MAJOR-x-y --color ededed || true
          gh label edit needs-manual-bp/$UNSUPPORTED_MAJOR-x-y --color ededed || true

          # Add the new target label to any PRs which:
          #   * target the previous major
          #   * are in-flight for the previous major
          #   * need manual backport for the previous major
          for PREVIOUS_MAJOR_LABEL in target/$PREVIOUS_MAJOR-x-y in-flight/$PREVIOUS_MAJOR-x-y needs-manual-bp/$PREVIOUS_MAJOR-x-y; do
            PULL_REQUESTS=$(gh pr list --label $PREVIOUS_MAJOR_LABEL --jq .[].number --json number --limit 500)
            if [[ $PULL_REQUESTS ]]; then
              echo $PULL_REQUESTS | xargs -n 1 gh pr edit --add-label target/$MAJOR-x-y || true
            fi
          done
      - name: Generate GitHub App token
        if: ${{ steps.check-major-version.outputs.MAJOR }}
        uses: electron/github-app-auth-action@384fd19694fe7b6dcc9a684746c6976ad78228ae # v1.1.1
        id: generate-token
        with:
          creds: ${{ secrets.RELEASE_BOARD_GH_APP_CREDS }}
          org: electron
      - name: Generate Release Project Board Metadata
        if: ${{ steps.check-major-version.outputs.MAJOR }}
        uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea # v7.0.1
        id: generate-project-metadata
        with:
          script: |
            const major = ${{ steps.check-major-version.outputs.MAJOR }}
            const nextMajor = major + 1
            const prevMajor = major - 1

            core.setOutput(""major"", major)
            core.setOutput(""next-major"", nextMajor)
            core.setOutput(""prev-major"", prevMajor)
            core.setOutput(""prev-prev-major"", prevMajor - 1)
            core.setOutput(""template-view"", JSON.stringify({
                major,
                ""next-major"": nextMajor,
                ""prev-major"": prevMajor,
            }))
      - name: Create Release Project Board
        if: ${{ steps.check-major-version.outputs.MAJOR }}
        uses: dsanders11/project-actions/copy-project@2134fe7cc71c58b7ae259c82a8e63c6058255678 # v1.7.0
        id: create-release-board
        with:
          drafts: true
          project-number: 64
          # TODO - Set to public once GitHub fixes their GraphQL bug
          # public: true
          # TODO - Enable once GitHub doesn't require overly broad, read
          #        and write permission for repo ""Contents"" to link
          # link-to-repository: electron/electron
          template-view: ${{ steps.generate-project-metadata.outputs.template-view }}
          title: ${{ steps.generate-project-metadata.outputs.major }}-x-y
          token: ${{ steps.generate-token.outputs.token }}
      - name: Dump Release Project Board Contents
        if: ${{ steps.check-major-version.outputs.MAJOR }}
        run: gh project item-list ${{ steps.create-release-board.outputs.number }} --owner electron --format json | jq
        env:
          GITHUB_TOKEN: ${{ steps.generate-token.outputs.token }}
      - name: Find Previous Release Project Board
        if: ${{ steps.check-major-version.outputs.MAJOR }}
        uses: dsanders11/project-actions/find-project@2134fe7cc71c58b7ae259c82a8e63c6058255678 # v1.7.0
        id: find-prev-release-board
        with:
          fail-if-project-not-found: false
          title: ${{ steps.generate-project-metadata.outputs.prev-prev-major }}-x-y
          token: ${{ steps.generate-token.outputs.token }}
      - name: Close Previous Release Project Board
        if: ${{ steps.find-prev-release-board.outputs.number }}
        uses: dsanders11/project-actions/close-project@2134fe7cc71c58b7ae259c82a8e63c6058255678 # v1.7.0
        with:
          project-number: ${{ steps.find-prev-release-board.outputs.number }}
          token: ${{ steps.generate-token.outputs.token }}
",128,1,2,"workflow_dispatch, create",5
electron/electron,build-git-cache.yml,"name: Build Git Cache
# This workflow updates git cache on the cross-instance cache volumes
# It runs daily at midnight.

on:  
  schedule:
    - cron: ""0 0 * * *""

jobs:
  build-git-cache-linux:
    runs-on: electron-arc-linux-amd64-32core
    container:
      image: ghcr.io/electron/build:bc2f48b2415a670de18d13605b1cf0eb5fdbaae1
      options: --user root
      volumes:
        - /mnt/cross-instance-cache:/mnt/cross-instance-cache
    env:
      CHROMIUM_GIT_COOKIE: ${{ secrets.CHROMIUM_GIT_COOKIE }}      
      GCLIENT_EXTRA_ARGS: '--custom-var=checkout_arm=True --custom-var=checkout_arm64=True'
    steps:
    - name: Checkout Electron
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      with:
        path: src/electron
        fetch-depth: 0
    - name: Build Git Cache
      uses: ./src/electron/.github/actions/build-git-cache
      with:
        target-platform: linux

  build-git-cache-windows:
    runs-on: electron-arc-linux-amd64-32core
    container:
      image: ghcr.io/electron/build:bc2f48b2415a670de18d13605b1cf0eb5fdbaae1
      options: --user root --device /dev/fuse --cap-add SYS_ADMIN
      volumes:
        - /mnt/win-cache:/mnt/win-cache
    env:
      CHROMIUM_GIT_COOKIE: ${{ secrets.CHROMIUM_GIT_COOKIE }}
      GCLIENT_EXTRA_ARGS: '--custom-var=checkout_win=True'
      TARGET_OS: 'win'
    steps:
    - name: Checkout Electron
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      with:
        path: src/electron
        fetch-depth: 0
    - name: Build Git Cache
      uses: ./src/electron/.github/actions/build-git-cache
      with:
        target-platform: win

  build-git-cache-macos:
    runs-on: electron-arc-linux-amd64-32core
    # This job updates the same git cache as linux, so it needs to run after the linux one.
    needs: build-git-cache-linux
    container:
      image: ghcr.io/electron/build:bc2f48b2415a670de18d13605b1cf0eb5fdbaae1
      options: --user root
      volumes:
        - /mnt/cross-instance-cache:/mnt/cross-instance-cache
    env:
      CHROMIUM_GIT_COOKIE: ${{ secrets.CHROMIUM_GIT_COOKIE }}
      GCLIENT_EXTRA_ARGS: '--custom-var=checkout_mac=True --custom-var=host_os=mac'
    steps:
    - name: Checkout Electron
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      with:
        path: src/electron
        fetch-depth: 0
    - name: Build Git Cache
      uses: ./src/electron/.github/actions/build-git-cache
      with:
        target-platform: macos",74,3,1,schedule,6
electron/electron,build.yml,"name: Build

on:
  workflow_dispatch:
    inputs:
      build-image-sha:
        type: string
        description: 'SHA for electron/build image'
        default: '424eedbf277ad9749ffa9219068aa72ed4a5e373'
        required: true
      skip-macos:
        type: boolean
        description: 'Skip macOS builds'
        default: false
        required: false
      skip-linux:
        type: boolean
        description: 'Skip Linux builds'
        default: false
        required: false
      skip-windows:
        type: boolean
        description: 'Skip Windows builds'
        default: false
        required: false
      skip-lint:
        type: boolean
        description: 'Skip lint check'
        default: false
        required: false
  push:
    branches:
      - main
      - '[1-9][0-9]-x-y'
  pull_request:

defaults:
  run:
    shell: bash

jobs:
  setup:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: read
    outputs:
        docs: ${{ steps.filter.outputs.docs }}
        src: ${{ steps.filter.outputs.src }}
        build-image-sha: ${{ steps.set-output.outputs.build-image-sha }}
        docs-only: ${{ steps.set-output.outputs.docs-only }}
    steps:
    - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 #v4.0.2
      with:
        ref: ${{ github.event.pull_request.head.sha }}
    - uses: dorny/paths-filter@de90cc6fb38fc0963ad72b210f1f284cd68cea36 # v3.0.2
      id: filter
      with:
        filters: |
          docs:
            - 'docs/**'
          src:
            - '!docs/**'
    - name: Set Outputs for Build Image SHA & Docs Only
      id: set-output
      run: |
        if [ -z ""${{ inputs.build-image-sha }}"" ]; then
          echo ""build-image-sha=424eedbf277ad9749ffa9219068aa72ed4a5e373"" >> ""$GITHUB_OUTPUT""
        else
          echo ""build-image-sha=${{ inputs.build-image-sha }}"" >> ""$GITHUB_OUTPUT""
        fi
        echo ""docs-only=${{ steps.filter.outputs.docs == 'true' && steps.filter.outputs.src == 'false' }}"" >> ""$GITHUB_OUTPUT""

  # Lint Jobs
  lint:
    needs: setup
    if: ${{ !inputs.skip-lint }}
    uses: ./.github/workflows/pipeline-electron-lint.yml
    with:
      container: '{""image"":""ghcr.io/electron/build:${{ needs.setup.outputs.build-image-sha }}"",""options"":""--user root""}'
    secrets: inherit

  # Docs Only Jobs
  docs-only:
    needs: setup
    if: ${{ needs.setup.outputs.docs-only == 'true' }}
    uses: ./.github/workflows/pipeline-electron-docs-only.yml
    with:
      container: '{""image"":""ghcr.io/electron/build:${{ needs.setup.outputs.build-image-sha }}"",""options"":""--user root""}'
    secrets: inherit

  # Checkout Jobs
  checkout-macos:
    needs: setup
    if: ${{ needs.setup.outputs.src == 'true' && !inputs.skip-macos}}
    runs-on: electron-arc-linux-amd64-32core
    container:
      image: ghcr.io/electron/build:${{ needs.setup.outputs.build-image-sha }}
      options: --user root
      volumes:
        - /mnt/cross-instance-cache:/mnt/cross-instance-cache
        - /var/run/sas:/var/run/sas
    env:
      CHROMIUM_GIT_COOKIE: ${{ secrets.CHROMIUM_GIT_COOKIE }}
      GCLIENT_EXTRA_ARGS: '--custom-var=checkout_mac=True --custom-var=host_os=mac'
    outputs:
      build-image-sha: ${{ needs.setup.outputs.build-image-sha }}
    steps:
    - name: Checkout Electron
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      with:
        path: src/electron
        fetch-depth: 0
        ref: ${{ github.event.pull_request.head.sha }}
    - name: Checkout & Sync & Save
      uses: ./src/electron/.github/actions/checkout
      with:
        generate-sas-token: 'true'
        target-platform: macos

  checkout-linux:
    needs: setup
    if: ${{ needs.setup.outputs.src == 'true' && !inputs.skip-linux}}
    runs-on: electron-arc-linux-amd64-32core
    container:
      image: ghcr.io/electron/build:${{ needs.setup.outputs.build-image-sha }}
      options: --user root
      volumes:
        - /mnt/cross-instance-cache:/mnt/cross-instance-cache
        - /var/run/sas:/var/run/sas
    env:
      CHROMIUM_GIT_COOKIE: ${{ secrets.CHROMIUM_GIT_COOKIE }}
      DD_API_KEY: ${{ secrets.DD_API_KEY }}
      GCLIENT_EXTRA_ARGS: '--custom-var=checkout_arm=True --custom-var=checkout_arm64=True'
      PATCH_UP_APP_CREDS: ${{ secrets.PATCH_UP_APP_CREDS }}
    outputs:
      build-image-sha: ${{ needs.setup.outputs.build-image-sha}}
    steps:
    - name: Checkout Electron
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      with:
        path: src/electron
        fetch-depth: 0
        ref: ${{ github.event.pull_request.head.sha }}
    - name: Checkout & Sync & Save
      uses: ./src/electron/.github/actions/checkout
      with:
        target-platform: linux

  checkout-windows:
    needs: setup
    if: ${{ needs.setup.outputs.src == 'true' && !inputs.skip-windows }}
    runs-on: electron-arc-linux-amd64-32core
    container:
      image: ghcr.io/electron/build:${{ needs.setup.outputs.build-image-sha }}
      options: --user root --device /dev/fuse --cap-add SYS_ADMIN
      volumes:
        - /mnt/win-cache:/mnt/win-cache
        - /var/run/sas:/var/run/sas
    env:
      CHROMIUM_GIT_COOKIE: ${{ secrets.CHROMIUM_GIT_COOKIE }}
      CHROMIUM_GIT_COOKIE_WINDOWS_STRING: ${{ secrets.CHROMIUM_GIT_COOKIE_WINDOWS_STRING }}
      GCLIENT_EXTRA_ARGS: '--custom-var=checkout_win=True'
      TARGET_OS: 'win'
      ELECTRON_DEPOT_TOOLS_WIN_TOOLCHAIN: '1'
    outputs:
      build-image-sha: ${{ needs.setup.outputs.build-image-sha}}
    steps:
    - name: Checkout Electron
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      with:
        path: src/electron
        fetch-depth: 0
        ref: ${{ github.event.pull_request.head.sha }}
    - name: Checkout & Sync & Save
      uses: ./src/electron/.github/actions/checkout
      with:
        generate-sas-token: 'true'
        target-platform: win

  # GN Check Jobs
  macos-gn-check:
    uses: ./.github/workflows/pipeline-segment-electron-gn-check.yml
    needs: checkout-macos
    with:
      target-platform: macos
      target-archs: x64 arm64
      check-runs-on: macos-15
      gn-build-type: testing
    secrets: inherit

  linux-gn-check:
    uses: ./.github/workflows/pipeline-segment-electron-gn-check.yml
    needs: checkout-linux
    with:
      target-platform: linux
      target-archs: x64 arm arm64
      check-runs-on: electron-arc-linux-amd64-8core
      check-container: '{""image"":""ghcr.io/electron/build:${{ needs.checkout-linux.outputs.build-image-sha }}"",""options"":""--user root"",""volumes"":[""/mnt/cross-instance-cache:/mnt/cross-instance-cache""]}'
      gn-build-type: testing
    secrets: inherit

  windows-gn-check:
    uses: ./.github/workflows/pipeline-segment-electron-gn-check.yml
    needs: checkout-windows
    with:
      target-platform: win
      target-archs: x64 x86 arm64
      check-runs-on: electron-arc-linux-amd64-8core
      check-container: '{""image"":""ghcr.io/electron/build:${{ needs.checkout-windows.outputs.build-image-sha }}"",""options"":""--user root --device /dev/fuse --cap-add SYS_ADMIN"",""volumes"":[""/mnt/win-cache:/mnt/win-cache""]}'
      gn-build-type: testing
    secrets: inherit

  # Build Jobs - These cascade into testing jobs
  macos-x64:
    permissions:
      contents: read
      issues: read
      pull-requests: read
    uses: ./.github/workflows/pipeline-electron-build-and-test.yml
    needs: checkout-macos
    with:
      build-runs-on: macos-15-xlarge
      test-runs-on: macos-13
      target-platform: macos
      target-arch: x64
      is-release: false
      gn-build-type: testing
      generate-symbols: false
      upload-to-storage: '0'
    secrets: inherit
  
  macos-arm64:
    permissions:
      contents: read
      issues: read
      pull-requests: read
    uses: ./.github/workflows/pipeline-electron-build-and-test.yml
    needs: checkout-macos
    with:
      build-runs-on: macos-15-xlarge
      test-runs-on: macos-14
      target-platform: macos
      target-arch: arm64
      is-release: false
      gn-build-type: testing
      generate-symbols: false
      upload-to-storage: '0'
    secrets: inherit

  linux-x64:
    permissions:
      contents: read
      issues: read
      pull-requests: read
    uses: ./.github/workflows/pipeline-electron-build-and-test-and-nan.yml
    needs: checkout-linux
    with:
      build-runs-on: electron-arc-linux-amd64-32core
      test-runs-on: electron-arc-linux-amd64-4core
      build-container: '{""image"":""ghcr.io/electron/build:${{ needs.checkout-linux.outputs.build-image-sha }}"",""options"":""--user root"",""volumes"":[""/mnt/cross-instance-cache:/mnt/cross-instance-cache""]}'
      test-container: '{""image"":""ghcr.io/electron/build:${{ needs.checkout-linux.outputs.build-image-sha }}"",""options"":""--user root --privileged --init""}'
      target-platform: linux
      target-arch: x64
      is-release: false
      gn-build-type: testing
      generate-symbols: false
      upload-to-storage: '0'
    secrets: inherit

  linux-x64-asan:
    permissions:
      contents: read
      issues: read
      pull-requests: read
    uses: ./.github/workflows/pipeline-electron-build-and-test.yml
    needs: checkout-linux
    with:
      build-runs-on: electron-arc-linux-amd64-32core
      test-runs-on: electron-arc-linux-amd64-4core
      build-container: '{""image"":""ghcr.io/electron/build:${{ needs.checkout-linux.outputs.build-image-sha }}"",""options"":""--user root"",""volumes"":[""/mnt/cross-instance-cache:/mnt/cross-instance-cache""]}'
      test-container: '{""image"":""ghcr.io/electron/build:${{ needs.checkout-linux.outputs.build-image-sha }}"",""options"":""--user root --privileged --init""}'
      target-platform: linux
      target-arch: x64
      is-release: false
      gn-build-type: testing
      generate-symbols: false
      upload-to-storage: '0'
      is-asan: true
    secrets: inherit
  
  linux-arm:
    permissions:
      contents: read
      issues: read
      pull-requests: read
    uses: ./.github/workflows/pipeline-electron-build-and-test.yml
    needs: checkout-linux
    with:
      build-runs-on: electron-arc-linux-amd64-32core
      test-runs-on: electron-arc-linux-arm64-4core
      build-container: '{""image"":""ghcr.io/electron/build:${{ needs.checkout-linux.outputs.build-image-sha }}"",""options"":""--user root"",""volumes"":[""/mnt/cross-instance-cache:/mnt/cross-instance-cache""]}'
      test-container: '{""image"":""ghcr.io/electron/test:arm32v7-${{ needs.checkout-linux.outputs.build-image-sha }}"",""options"":""--user root --privileged --init"",""volumes"":[""/home/runner/externals:/mnt/runner-externals""]}'
      target-platform: linux
      target-arch: arm
      is-release: false
      gn-build-type: testing
      generate-symbols: false
      upload-to-storage: '0'
    secrets: inherit
  
  linux-arm64:
    permissions:
      contents: read
      issues: read
      pull-requests: read
    uses: ./.github/workflows/pipeline-electron-build-and-test.yml
    needs: checkout-linux
    with:
      build-runs-on: electron-arc-linux-amd64-32core
      test-runs-on: electron-arc-linux-arm64-4core
      build-container: '{""image"":""ghcr.io/electron/build:${{ needs.checkout-linux.outputs.build-image-sha }}"",""options"":""--user root"",""volumes"":[""/mnt/cross-instance-cache:/mnt/cross-instance-cache""]}'
      test-container: '{""image"":""ghcr.io/electron/test:arm64v8-${{ needs.checkout-linux.outputs.build-image-sha }}"",""options"":""--user root --privileged --init""}'
      target-platform: linux
      target-arch: arm64
      is-release: false
      gn-build-type: testing
      generate-symbols: false
      upload-to-storage: '0'
    secrets: inherit

  windows-x64:
    permissions:
      contents: read
      issues: read
      pull-requests: read
    uses: ./.github/workflows/pipeline-electron-build-and-test.yml
    needs: checkout-windows
    if: ${{ needs.setup.outputs.src == 'true' && !inputs.skip-windows }}
    with:
      build-runs-on: electron-arc-windows-amd64-16core
      test-runs-on: windows-latest
      target-platform: win
      target-arch: x64
      is-release: false
      gn-build-type: testing
      generate-symbols: false
      upload-to-storage: '0'
    secrets: inherit

  windows-x86:
    permissions:
      contents: read
      issues: read
      pull-requests: read
    uses: ./.github/workflows/pipeline-electron-build-and-test.yml
    needs: checkout-windows
    if: ${{ needs.setup.outputs.src == 'true' && !inputs.skip-windows }}
    with:
      build-runs-on: electron-arc-windows-amd64-16core
      test-runs-on: windows-latest
      target-platform: win
      target-arch: x86
      is-release: false
      gn-build-type: testing
      generate-symbols: false
      upload-to-storage: '0'
    secrets: inherit

  windows-arm64:
    permissions:
      contents: read
      issues: read
      pull-requests: read
    uses: ./.github/workflows/pipeline-electron-build-and-test.yml
    needs: checkout-windows
    if: ${{ needs.setup.outputs.src == 'true' && !inputs.skip-windows }}
    with:
      build-runs-on: electron-arc-windows-amd64-16core
      test-runs-on: electron-hosted-windows-arm64-4core
      target-platform: win
      target-arch: arm64
      is-release: false
      gn-build-type: testing
      generate-symbols: false
      upload-to-storage: '0'
    secrets: inherit

  gha-done:
    name: GitHub Actions Completed
    runs-on: ubuntu-latest
    needs: [docs-only, macos-x64, macos-arm64, linux-x64, linux-x64-asan, linux-arm, linux-arm64, windows-x64, windows-x86, windows-arm64]
    if: always() && !contains(needs.*.result, 'failure')
    steps: 
    - name: GitHub Actions Jobs Done
      run: |
        echo ""All GitHub Actions Jobs are done""
",396,19,3,"workflow_dispatch, push, pull_request",22
electron/electron,clean-src-cache.yml,"name: Clean Source Cache

description: |
  This workflow cleans up the source cache on the cross-instance cache volume
  to free up space. It runs daily at midnight and clears files older than 15 days.

on:
  schedule:
    - cron: ""0 0 * * *""

jobs:
  clean-src-cache:
    runs-on: electron-arc-linux-amd64-32core
    container:
      image: ghcr.io/electron/build:bc2f48b2415a670de18d13605b1cf0eb5fdbaae1
      options: --user root
      volumes:
        - /mnt/cross-instance-cache:/mnt/cross-instance-cache
        - /mnt/win-cache:/mnt/win-cache
    steps:
    - name: Cleanup Source Cache
      shell: bash
      run: |
        df -h /mnt/cross-instance-cache
        find /mnt/cross-instance-cache -type f -mtime +15 -delete
        df -h /mnt/cross-instance-cache
        df -h /mnt/win-cache
        find /mnt/win-cache -type f -mtime +15 -delete
        df -h /mnt/win-cache
",29,1,1,schedule,0
electron/electron,issue-commented.yml,"name: Issue Commented

on:
  issue_comment:
    types:
      - created

permissions: {}

jobs:
  issue-commented:
    name: Remove blocked/{need-info,need-repro} on comment
    if: ${{ (contains(github.event.issue.labels.*.name, 'blocked/need-repro') || contains(github.event.issue.labels.*.name, 'blocked/need-info ❌')) && !contains(fromJSON('[""MEMBER"", ""OWNER"", ""COLLABORATOR""]'), github.event.comment.author_association) && github.event.comment.user.type != 'Bot' }}
    runs-on: ubuntu-latest
    steps:
      - name: Generate GitHub App token
        uses: electron/github-app-auth-action@384fd19694fe7b6dcc9a684746c6976ad78228ae # v1.1.1
        id: generate-token
        with:
          creds: ${{ secrets.ISSUE_TRIAGE_GH_APP_CREDS }}
      - name: Remove label
        env:
          GITHUB_TOKEN: ${{ steps.generate-token.outputs.token }}
          ISSUE_URL: ${{ github.event.issue.html_url }}
        run: |
          gh issue edit $ISSUE_URL --remove-label 'blocked/need-repro','blocked/need-info ❌'
",26,1,1,issue_comment,1
electron/electron,issue-labeled.yml,"name: Issue Labeled

on:
  issues:
    types: [labeled]

permissions:  # added using https://github.com/step-security/secure-workflows
  contents: read

jobs:
  issue-labeled-with-status:
    name: status/{confirmed,reviewed} label added
    if: github.event.label.name == 'status/confirmed' || github.event.label.name == 'status/reviewed'
    runs-on: ubuntu-latest
    steps:
      - name: Generate GitHub App token
        uses: electron/github-app-auth-action@384fd19694fe7b6dcc9a684746c6976ad78228ae # v1.1.1
        id: generate-token
        with:
          creds: ${{ secrets.ISSUE_TRIAGE_GH_APP_CREDS }}
          org: electron
      - name: Set status
        uses: dsanders11/project-actions/edit-item@2134fe7cc71c58b7ae259c82a8e63c6058255678 # v1.7.0
        with:
          token: ${{ steps.generate-token.outputs.token }}
          project-number: 90
          field: Status
          field-value: ✅ Triaged
          fail-if-item-not-found: false
  issue-labeled-blocked:
    name: blocked/* label added
    if: startsWith(github.event.label.name, 'blocked/')
    runs-on: ubuntu-latest
    steps:
      - name: Generate GitHub App token
        uses: electron/github-app-auth-action@384fd19694fe7b6dcc9a684746c6976ad78228ae # v1.1.1
        id: generate-token
        with:
          creds: ${{ secrets.ISSUE_TRIAGE_GH_APP_CREDS }}
          org: electron
      - name: Set status
        uses: dsanders11/project-actions/edit-item@2134fe7cc71c58b7ae259c82a8e63c6058255678 # v1.7.0
        with:
          token: ${{ steps.generate-token.outputs.token }}
          project-number: 90
          field: Status
          field-value: 🛑 Blocked
          fail-if-item-not-found: false
  issue-labeled-blocked-need-repro:
    name: blocked/need-repro label added
    if: github.event.label.name == 'blocked/need-repro'
    permissions:
      issues: write  # for actions-cool/issues-helper to update issues
    runs-on: ubuntu-latest
    steps:
      - name: Check if comment needed
        id: check-for-comment
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GH_REPO: electron/electron
        run: |
          set -eo pipefail
          COMMENT_COUNT=$(gh issue view ${{ github.event.issue.number }} --comments --json comments | jq '[ .comments[] | select(.author.login == ""electron-issue-triage"" or .authorAssociation == ""OWNER"" or .authorAssociation == ""MEMBER"") | select(.body | startswith(""<!-- blocked/need-repro -->"")) ] | length')
          if [[ $COMMENT_COUNT -eq 0 ]]; then
            echo ""SHOULD_COMMENT=1"" >> ""$GITHUB_OUTPUT""
          fi
      - name: Generate GitHub App token
        if: ${{ steps.check-for-comment.outputs.SHOULD_COMMENT }}
        uses: electron/github-app-auth-action@384fd19694fe7b6dcc9a684746c6976ad78228ae # v1.1.1
        id: generate-token
        with:
          creds: ${{ secrets.ISSUE_TRIAGE_GH_APP_CREDS }}
      - name: Create comment
        if: ${{ steps.check-for-comment.outputs.SHOULD_COMMENT }}
        uses: actions-cool/issues-helper@a610082f8ac0cf03e357eb8dd0d5e2ba075e017e # v3.6.0
        with:
          actions: 'create-comment'
          token: ${{ steps.generate-token.outputs.token }}
          body: |
            <!-- blocked/need-repro -->

            Hello @${{ github.event.issue.user.login }}. Thanks for reporting this and helping to make Electron better!

            Would it be possible for you to make a standalone testcase with only the code necessary to reproduce the issue? For example, [Electron Fiddle](https://www.electronjs.org/fiddle) is a great tool for making small test cases and makes it easy to publish your test case to a [gist](https://gist.github.com) that Electron maintainers can use.

            Stand-alone test cases make fixing issues go more smoothly: it ensure everyone's looking at the same issue, it removes all unnecessary variables from the equation, and it can also provide the basis for automated regression tests.

            Now adding the https://github.com/electron/electron/labels/blocked%2Fneed-repro label for this reason. After you make a test case, please link to it in a followup comment. This issue will be closed in 10 days if the above is not addressed.
",88,3,1,issues,6
electron/electron,issue-opened.yml,"name: Issue Opened

on:
  issues:
    types:
      - opened

permissions: {}

jobs:
  add-to-issue-triage:
    if: ${{ contains(github.event.issue.labels.*.name, 'bug :beetle:') }}
    runs-on: ubuntu-latest
    steps:
      - name: Generate GitHub App token
        uses: electron/github-app-auth-action@384fd19694fe7b6dcc9a684746c6976ad78228ae # v1.1.1
        id: generate-token
        with:
          creds: ${{ secrets.ISSUE_TRIAGE_GH_APP_CREDS }}
          org: electron
      - name: Add to Issue Triage
        uses: dsanders11/project-actions/add-item@2134fe7cc71c58b7ae259c82a8e63c6058255678 # v1.7.0
        with:
          field: Reporter
          field-value: ${{ github.event.issue.user.login }}
          project-number: 90
          token: ${{ steps.generate-token.outputs.token }}
  set-labels:
    if: ${{ contains(github.event.issue.labels.*.name, 'bug :beetle:') }}
    runs-on: ubuntu-latest
    steps:
      - name: Generate GitHub App token
        uses: electron/github-app-auth-action@384fd19694fe7b6dcc9a684746c6976ad78228ae # v1.1.1
        id: generate-token
        with:
          creds: ${{ secrets.ISSUE_TRIAGE_GH_APP_CREDS }}
          org: electron
      - run: npm install @electron/fiddle-core@1.3.3 mdast-util-from-markdown@2.0.0 unist-util-select@5.1.0 semver@7.6.0
      - name: Add labels
        uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea # v7.0.1
        id: add-labels
        env:
          ISSUE_BODY: ${{ github.event.issue.body }}
        with:
          github-token: ${{ steps.generate-token.outputs.token }}
          script: |
            const { fromMarkdown } = await import('${{ github.workspace }}/node_modules/mdast-util-from-markdown/index.js');
            const { select } = await import('${{ github.workspace }}/node_modules/unist-util-select/index.js');
            const semver = await import('${{ github.workspace }}/node_modules/semver/index.js');

            const [ owner, repo ] = '${{ github.repository }}'.split('/');
            const issue_number = ${{ github.event.issue.number }};

            const tree = fromMarkdown(process.env.ISSUE_BODY);

            const labels = [];

            const electronVersion = select('heading:has(> text[value=""Electron Version""]) + paragraph > text', tree)?.value.trim();
            if (electronVersion !== undefined) {
              // It's possible for multiple versions to be listed -
              // for now check for comma or space separated version.
              const versions = electronVersion.split(/, | /);
              let hasSupportedVersion = false;

              for (const version of versions) {
                const major = semver.coerce(version, { loose: true })?.major;
                if (major) {
                  const versionLabel = `${major}-x-y`;
                  let labelExists = false;

                  try {
                    await github.rest.issues.getLabel({
                      owner,
                      repo,
                      name: versionLabel,
                    });
                    labelExists = true;
                  } catch {}

                  const { ElectronVersions } = await import('${{ github.workspace }}/node_modules/@electron/fiddle-core/dist/index.js');
                  const electronVersions = await ElectronVersions.create(undefined, { ignoreCache: true });
                  const validVersions = [...electronVersions.supportedMajors, ...electronVersions.prereleaseMajors];

                  if (validVersions.includes(major)) {
                    hasSupportedVersion = true;
                    if (labelExists) {
                      labels.push(versionLabel);
                    }
                  }
                }
              }

              if (!hasSupportedVersion) {
                core.setOutput('unsupportedMajor', true);
                labels.push('blocked/need-info ❌');
              }
            }

            const operatingSystems = select('heading:has(> text[value=""What operating system(s) are you using?""]) + paragraph > text', tree)?.value.trim().split(', ');
            const platformLabels = new Set();
            for (const operatingSystem of (operatingSystems ?? [])) {
              switch (operatingSystem) {
                case 'Windows':
                  platformLabels.add('platform/windows');
                  break;
                case 'macOS':
                  platformLabels.add('platform/macOS');
                  break;
                case 'Ubuntu':
                case 'Other Linux':
                  platformLabels.add('platform/linux');
                  break;
              }
            }

            if (platformLabels.size === 3) {
              labels.push('platform/all');
            } else {
              labels.push(...platformLabels);
            }

            const gistUrl = select('heading:has(> text[value=""Testcase Gist URL""]) + paragraph > text', tree)?.value.trim();
            if (gistUrl !== undefined && gistUrl.startsWith('https://gist.github.com/')) {
              labels.push('has-repro-gist');
            }

            if (labels.length) {
              await github.rest.issues.addLabels({
                owner,
                repo,
                issue_number,
                labels,
              });
            }
      - name: Create unsupported major comment
        if: ${{ steps.add-labels.outputs.unsupportedMajor }}
        uses: actions-cool/issues-helper@a610082f8ac0cf03e357eb8dd0d5e2ba075e017e # v3.6.0
        with:
          actions: 'create-comment'
          token: ${{ steps.generate-token.outputs.token }}
          body: |
            <!-- end-of-life -->

            Hello @${{ github.event.issue.user.login }}. Thanks for reporting this and helping to make Electron better!
            
            The version of Electron reported in this issue has reached end-of-life and is [no longer supported](https://www.electronjs.org/docs/latest/tutorial/electron-timelines#timeline). If you're still experiencing this issue on a [supported version](https://www.electronjs.org/releases/stable) of Electron, please update this issue to reflect that version of Electron.

            Now adding the https://github.com/electron/electron/labels/blocked%2Fneed-info%20%E2%9D%8C label for this reason. This issue will be closed in 10 days if the above is not addressed.
",148,2,1,issues,5
electron/electron,issue-transferred.yml,"name: Issue Transferred

on:
  issues:
    types: [transferred]

permissions: {}

jobs:
  issue-transferred:
    name: Issue Transferred
    runs-on: ubuntu-latest
    if: ${{ !github.event.changes.new_repository.private }}
    steps:
      - name: Generate GitHub App token
        uses: electron/github-app-auth-action@384fd19694fe7b6dcc9a684746c6976ad78228ae # v1.1.1
        id: generate-token
        with:
          creds: ${{ secrets.ISSUE_TRIAGE_GH_APP_CREDS }}
          org: electron
      - name: Remove from issue triage
        uses: dsanders11/project-actions/delete-item@2134fe7cc71c58b7ae259c82a8e63c6058255678 # v1.7.0
        with:
          token: ${{ steps.generate-token.outputs.token }}
          project-number: 90
          item: ${{ github.event.changes.new_issue.html_url }}
          fail-if-item-not-found: false
",27,1,1,issues,2
electron/electron,issue-unlabeled.yml,"name: Issue Unlabeled

on:
  issues:
    types: [unlabeled]

permissions:
  contents: read

jobs:
  issue-unlabeled-blocked:
    name: All blocked/* labels removed
    if: startsWith(github.event.label.name, 'blocked/') && github.event.issue.state == 'open'
    runs-on: ubuntu-latest
    steps:
      - name: Check for any blocked labels
        id: check-for-blocked-labels
        run: |
          set -eo pipefail
          BLOCKED_LABEL_COUNT=$(echo '${{ toJSON(github.event.issue.labels.*.name) }}' | jq '[ .[] | select(startswith(""blocked/"")) ] | length')
          if [[ $BLOCKED_LABEL_COUNT -eq 0 ]]; then
            echo ""NOT_BLOCKED=1"" >> ""$GITHUB_OUTPUT""
          fi
      - name: Generate GitHub App token
        if: ${{ steps.check-for-blocked-labels.outputs.NOT_BLOCKED }}
        uses: electron/github-app-auth-action@384fd19694fe7b6dcc9a684746c6976ad78228ae # v1.1.1
        id: generate-token
        with:
          creds: ${{ secrets.ISSUE_TRIAGE_GH_APP_CREDS }}
          org: electron
      - name: Set status
        if: ${{ steps.check-for-blocked-labels.outputs.NOT_BLOCKED }}
        uses: dsanders11/project-actions/edit-item@2134fe7cc71c58b7ae259c82a8e63c6058255678 # v1.7.0
        with:
          token: ${{ steps.generate-token.outputs.token }}
          project-number: 90
          field: Status
          field-value: 📥 Was Blocked
          fail-if-item-not-found: false
",39,1,1,issues,2
electron/electron,linux-publish.yml,"name: Publish Linux

on:
  workflow_dispatch:
    inputs:
      build-image-sha:
        type: string
        description: 'SHA for electron/build image'
        default: '424eedbf277ad9749ffa9219068aa72ed4a5e373'
      upload-to-storage:
        description: 'Uploads to Azure storage'
        required: false
        default: '1'
        type: string
      run-linux-publish:
        description: 'Run the publish jobs vs just the build jobs'
        type: boolean
        default: false

jobs:
  checkout-linux:
    runs-on: electron-arc-linux-amd64-32core
    container:
      image: ghcr.io/electron/build:${{ inputs.build-image-sha }}
      options: --user root
      volumes:
        - /mnt/cross-instance-cache:/mnt/cross-instance-cache
        - /var/run/sas:/var/run/sas
    env:
      CHROMIUM_GIT_COOKIE: ${{ secrets.CHROMIUM_GIT_COOKIE }}
      GCLIENT_EXTRA_ARGS: '--custom-var=checkout_arm=True --custom-var=checkout_arm64=True'
    steps:
    - name: Checkout Electron
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      with:
        path: src/electron
        fetch-depth: 0
    - name: Checkout & Sync & Save
      uses: ./src/electron/.github/actions/checkout

  publish-x64:
    uses: ./.github/workflows/pipeline-segment-electron-build.yml
    needs: checkout-linux
    with:
      environment: production-release
      build-runs-on: electron-arc-linux-amd64-32core
      build-container: '{""image"":""ghcr.io/electron/build:${{ inputs.build-image-sha }}"",""options"":""--user root"",""volumes"":[""/mnt/cross-instance-cache:/mnt/cross-instance-cache""]}'
      target-platform: linux
      target-arch: x64
      is-release: true
      gn-build-type: release
      generate-symbols: true
      strip-binaries: true
      upload-to-storage: ${{ inputs.upload-to-storage }}
    secrets: inherit

  publish-arm:
    uses: ./.github/workflows/pipeline-segment-electron-build.yml
    needs: checkout-linux
    with:
      environment: production-release
      build-runs-on: electron-arc-linux-amd64-32core
      build-container: '{""image"":""ghcr.io/electron/build:${{ inputs.build-image-sha }}"",""options"":""--user root"",""volumes"":[""/mnt/cross-instance-cache:/mnt/cross-instance-cache""]}'
      target-platform: linux
      target-arch: arm
      is-release: true
      gn-build-type: release
      generate-symbols: true
      strip-binaries: true
      upload-to-storage: ${{ inputs.upload-to-storage }}
    secrets: inherit

  publish-arm64:
    uses: ./.github/workflows/pipeline-segment-electron-build.yml
    needs: checkout-linux
    with:
      environment: production-release
      build-runs-on: electron-arc-linux-amd64-32core
      build-container: '{""image"":""ghcr.io/electron/build:${{ inputs.build-image-sha }}"",""options"":""--user root"",""volumes"":[""/mnt/cross-instance-cache:/mnt/cross-instance-cache""]}'
      target-platform: linux
      target-arch: arm64
      is-release: true
      gn-build-type: release
      generate-symbols: true
      strip-binaries: true
      upload-to-storage: ${{ inputs.upload-to-storage }}
    secrets: inherit
",87,4,1,workflow_dispatch,5
electron/electron,macos-publish.yml,"name: Publish MacOS

on:
  workflow_dispatch:
    inputs:
      build-image-sha:
        type: string
        description: 'SHA for electron/build image'
        default: '424eedbf277ad9749ffa9219068aa72ed4a5e373'
        required: true
      upload-to-storage:
        description: 'Uploads to Azure storage'
        required: false
        default: '1'
        type: string
      run-macos-publish:
        description: 'Run the publish jobs vs just the build jobs'
        type: boolean
        default: false

jobs:
  checkout-macos:
    runs-on: electron-arc-linux-amd64-32core
    container:
      image: ghcr.io/electron/build:${{ inputs.build-image-sha }}
      options: --user root
      volumes:
        - /mnt/cross-instance-cache:/mnt/cross-instance-cache
        - /var/run/sas:/var/run/sas
    env:
      CHROMIUM_GIT_COOKIE: ${{ secrets.CHROMIUM_GIT_COOKIE }}
      GCLIENT_EXTRA_ARGS: '--custom-var=checkout_mac=True --custom-var=host_os=mac'
    steps:
    - name: Checkout Electron
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      with:
        path: src/electron
        fetch-depth: 0
    - name: Checkout & Sync & Save
      uses: ./src/electron/.github/actions/checkout
      with:
        generate-sas-token: 'true'
        target-platform: macos

  publish-x64-darwin:
    uses: ./.github/workflows/pipeline-segment-electron-build.yml
    needs: checkout-macos
    with:
      environment: production-release
      build-runs-on: macos-15-xlarge
      target-platform: macos
      target-arch: x64
      target-variant: darwin
      is-release: true
      gn-build-type: release
      generate-symbols: true
      upload-to-storage: ${{ inputs.upload-to-storage }}
    secrets: inherit

  publish-x64-mas:
    uses: ./.github/workflows/pipeline-segment-electron-build.yml
    needs: checkout-macos
    with:
      environment: production-release
      build-runs-on: macos-15-xlarge
      target-platform: macos
      target-arch: x64
      target-variant: mas
      is-release: true
      gn-build-type: release
      generate-symbols: true
      upload-to-storage: ${{ inputs.upload-to-storage }}
    secrets: inherit

  publish-arm64-darwin:
    uses: ./.github/workflows/pipeline-segment-electron-build.yml
    needs: checkout-macos
    with:
      environment: production-release
      build-runs-on: macos-15-xlarge
      target-platform: macos
      target-arch: arm64
      target-variant: darwin
      is-release: true
      gn-build-type: release
      generate-symbols: true
      upload-to-storage: ${{ inputs.upload-to-storage }}
    secrets: inherit

  publish-arm64-mas:
    uses: ./.github/workflows/pipeline-segment-electron-build.yml
    needs: checkout-macos
    with:
      environment: production-release
      build-runs-on: macos-15-xlarge
      target-platform: macos
      target-arch: arm64
      target-variant: mas
      is-release: true
      gn-build-type: release
      generate-symbols: true
      upload-to-storage: ${{ inputs.upload-to-storage }}
    secrets: inherit
",103,5,1,workflow_dispatch,6
electron/electron,non-maintainer-dependency-change.yml,"name: Check for Non-Maintainer Dependency Change

on:
  pull_request_target:
    paths:
      - 'yarn.lock'
      - 'spec/yarn.lock'

permissions: {}

jobs:
  check-for-non-maintainer-dependency-change:
    name: Check for non-maintainer dependency change
    if: ${{ !contains(fromJSON('[""MEMBER"", ""OWNER""]'), github.event.pull_request.author_association) && github.event.pull_request.user.type != 'Bot' && !github.event.pull_request.draft }}
    permissions:
      contents: read
      pull-requests: write
    runs-on: ubuntu-latest
    steps:
      - name: Check for existing review
        id: check-for-review
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          PR_URL: ${{ github.event.pull_request.html_url }}
        run: |
          set -eo pipefail
          REVIEW_COUNT=$(gh pr view $PR_URL --json reviews | jq '[ .reviews[] | select(.author.login == ""github-actions"") | select(.body | startswith(""<!-- no-dependency-change -->"")) ] | length')
          if [[ $REVIEW_COUNT -eq 0 ]]; then
            echo ""SHOULD_REVIEW=1"" >> ""$GITHUB_OUTPUT""
          fi
      - name: Request changes
        if: ${{ steps.check-for-review.outputs.SHOULD_REVIEW }}
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          PR_URL: ${{ github.event.pull_request.html_url }}
        run: |
          printf ""<!-- no-dependency-change -->\n\nHello @${{ github.event.pull_request.user.login }}! It looks like this pull request touches one of our dependency files, and per [our contribution policy](https://github.com/electron/electron/blob/main/CONTRIBUTING.md#dependencies-upgrades-policy) we do not accept these types of changes in PRs."" | gh pr review $PR_URL -r --body-file=-
",37,1,1,pull_request_target,0
electron/electron,pipeline-electron-build-and-test-and-nan.yml,"name: Electron Build & Test (+ Node + NaN) Pipeline

on:
  workflow_call:
    inputs:
      target-platform:
        type: string
        description: 'Platform to run on, can be macos, win or linux.'
        required: true
      target-arch:
        type: string
        description: 'Arch to build for, can be x64, arm64 or arm'
        required: true
      build-runs-on:
        type: string
        description: 'What host to run the build'
        required: true
      test-runs-on:
        type: string
        description: 'What host to run the tests on'
        required: true
      build-container:
        type: string
        description: 'JSON container information for aks runs-on'
        required: false
        default: '{""image"":null}'
      test-container:
        type: string
        description: 'JSON container information for testing'
        required: false
        default: '{""image"":null}'
      is-release:
        description: 'Whether this build job is a release job'
        required: true
        type: boolean
        default: false
      gn-build-type:
        description: 'The gn build type - testing or release'
        required: true
        type: string
        default: testing
      generate-symbols: 
        description: 'Whether or not to generate symbols'
        required: true
        type: boolean
        default: false
      upload-to-storage: 
        description: 'Whether or not to upload build artifacts to external storage'
        required: true
        type: string
        default: '0'
      is-asan: 
        description: 'Building the Address Sanitizer (ASan) Linux build'
        required: false
        type: boolean
        default: false

concurrency:
  group: electron-build-and-test-and-nan-${{ inputs.target-platform }}-${{ inputs.target-arch }}-${{ github.ref_protected == true && github.run_id || github.ref }}
  cancel-in-progress: ${{ github.ref_protected != true }}

jobs:
  build:
    uses: ./.github/workflows/pipeline-segment-electron-build.yml
    with:
      build-runs-on: ${{ inputs.build-runs-on }}
      build-container: ${{ inputs.build-container }}
      target-platform: ${{ inputs.target-platform }}
      target-arch: ${{ inputs.target-arch }}
      is-release: ${{ inputs.is-release }}
      gn-build-type: ${{ inputs.gn-build-type }}
      generate-symbols: ${{ inputs.generate-symbols }}
      upload-to-storage: ${{ inputs.upload-to-storage }}
    secrets: inherit
  test:
    uses: ./.github/workflows/pipeline-segment-electron-test.yml
    needs: build
    with:
      target-arch: ${{ inputs.target-arch }}
      target-platform: ${{ inputs.target-platform }}
      test-runs-on: ${{ inputs.test-runs-on }}
      test-container: ${{ inputs.test-container }}
    secrets: inherit
  nn-test:
    uses: ./.github/workflows/pipeline-segment-node-nan-test.yml
    needs: build
    with:
      target-arch: ${{ inputs.target-arch }}
      target-platform: ${{ inputs.target-platform }}
      test-runs-on: ${{ inputs.test-runs-on }}
      test-container: ${{ inputs.test-container }}
      gn-build-type: ${{ inputs.gn-build-type }}
    secrets: inherit
",93,3,1,workflow_call,3
electron/electron,pipeline-electron-build-and-test.yml,"name: Electron Build & Test Pipeline

on:
  workflow_call:
    inputs:
      target-platform:
        type: string
        description: 'Platform to run on, can be macos, win or linux'
        required: true
      target-arch:
        type: string
        description: 'Arch to build for, can be x64, arm64 or arm'
        required: true
      build-runs-on:
        type: string
        description: 'What host to run the build'
        required: true
      test-runs-on:
        type: string
        description: 'What host to run the tests on'
        required: true
      build-container:
        type: string
        description: 'JSON container information for aks runs-on'
        required: false
        default: '{""image"":null}'
      test-container:
        type: string
        description: 'JSON container information for testing'
        required: false
        default: '{""image"":null}'
      is-release:
        description: 'Whether this build job is a release job'
        required: true
        type: boolean
        default: false
      gn-build-type:
        description: 'The gn build type - testing or release'
        required: true
        type: string
        default: testing
      generate-symbols: 
        description: 'Whether or not to generate symbols'
        required: true
        type: boolean
        default: false
      upload-to-storage: 
        description: 'Whether or not to upload build artifacts to external storage'
        required: true
        type: string
        default: '0'
      is-asan: 
        description: 'Building the Address Sanitizer (ASan) Linux build'
        required: false
        type: boolean
        default: false

concurrency:
  group: electron-build-and-test-${{ inputs.target-platform }}-${{ inputs.target-arch }}-${{ github.ref_protected == true && github.run_id || github.ref }}
  cancel-in-progress: ${{ github.ref_protected != true }}

permissions:
  contents: read
  issues: read
  pull-requests: read  

jobs:
  build:
    uses: ./.github/workflows/pipeline-segment-electron-build.yml
    with:
      build-runs-on: ${{ inputs.build-runs-on }}
      build-container: ${{ inputs.build-container }}
      target-platform: ${{ inputs.target-platform }}
      target-arch: ${{ inputs.target-arch }}
      is-release: ${{ inputs.is-release }}
      gn-build-type: ${{ inputs.gn-build-type }}
      generate-symbols: ${{ inputs.generate-symbols }}
      upload-to-storage: ${{ inputs.upload-to-storage }}
      is-asan: ${{ inputs.is-asan}}
    secrets: inherit
  test:
    uses: ./.github/workflows/pipeline-segment-electron-test.yml
    needs: build
    with:
      target-arch: ${{ inputs.target-arch }}
      target-platform: ${{ inputs.target-platform }}
      test-runs-on: ${{ inputs.test-runs-on }}
      test-container: ${{ inputs.test-container }}
      is-asan: ${{ inputs.is-asan}}
    secrets: inherit
",90,2,1,workflow_call,2
electron/electron,pipeline-electron-docs-only.yml,"name: Electron Docs Compile

on:
  workflow_call:
    inputs:
      container:
        required: true
        description: 'Container to run the docs-only ts compile in'
        type: string

concurrency:
  group: electron-docs-only-${{ github.ref }}
  cancel-in-progress: true

jobs:
  docs-only:
    name: Docs Only Compile
    runs-on: electron-arc-linux-amd64-4core
    timeout-minutes: 20
    container: ${{ fromJSON(inputs.container) }}
    steps:
    - name: Checkout Electron
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      with:
        path: src/electron
        fetch-depth: 0
        ref: ${{ github.event.pull_request.head.sha }}
    - name: Install Dependencies
      uses: ./src/electron/.github/actions/install-dependencies
    - name: Run TS/JS compile
      shell: bash
      run: |
        cd src/electron
        node script/yarn create-typescript-definitions
        node script/yarn tsc -p tsconfig.default_app.json --noEmit
        for f in build/webpack/*.js
        do
            out=""${f:29}""
            if [ ""$out"" != ""base.js"" ]; then
            node script/yarn webpack --config $f --output-filename=$out --output-path=./.tmp --env mode=development
            fi
        done
",42,1,1,workflow_call,2
electron/electron,pipeline-electron-lint.yml,"name: Electron Lint

on:
  workflow_call:
    inputs:
      container:
        required: true
        description: 'Container to run lint in'
        type: string

concurrency:
  group: electron-lint-${{ github.ref_protected == true && github.run_id || github.ref }}
  cancel-in-progress: ${{ github.ref_protected != true }}

env:
  CHROMIUM_GIT_COOKIE: ${{ secrets.CHROMIUM_GIT_COOKIE }}  

jobs:
  lint:
    name: Lint
    runs-on: electron-arc-linux-amd64-4core
    timeout-minutes: 20
    container: ${{ fromJSON(inputs.container) }}
    steps:
    - name: Checkout Electron
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      with:
        path: src/electron
        fetch-depth: 0
        ref: ${{ github.event.pull_request.head.sha }}
    - name: Install Dependencies
      uses: ./src/electron/.github/actions/install-dependencies
    - name: Set Chromium Git Cookie
      uses: ./src/electron/.github/actions/set-chromium-cookie
    - name: Setup third_party Depot Tools
      shell: bash
      run: |
        # ""depot_tools"" has to be checkout into ""//third_party/depot_tools"" so pylint.py can a ""pylintrc"" file.
        git clone --filter=tree:0 https://chromium.googlesource.com/chromium/tools/depot_tools.git src/third_party/depot_tools
        echo ""$(pwd)/src/third_party/depot_tools"" >> $GITHUB_PATH
    - name: Download GN Binary
      shell: bash
      run: |
        chromium_revision=""$(grep -A1 chromium_version src/electron/DEPS | tr -d '\n' | cut -d\' -f4)""
        gn_version=""$(curl -sL ""https://chromium.googlesource.com/chromium/src/+/${chromium_revision}/DEPS?format=TEXT"" | base64 -d | grep gn_version | head -n1 | cut -d\' -f4)""

        cipd ensure -ensure-file - -root . <<-CIPD
        \$ServiceURL https://chrome-infra-packages.appspot.com/
        @Subdir src/buildtools/linux64
        gn/gn/linux-amd64 $gn_version
        CIPD

        buildtools_path=""$(pwd)/src/buildtools""
        echo ""CHROMIUM_BUILDTOOLS_PATH=$buildtools_path"" >> $GITHUB_ENV
    - name: Download clang-format Binary
      shell: bash
      run: |
        chromium_revision=""$(grep -A1 chromium_version src/electron/DEPS | tr -d '\n' | cut -d\' -f4)""

        mkdir -p src/buildtools
        curl -sL ""https://chromium.googlesource.com/chromium/src/+/${chromium_revision}/buildtools/DEPS?format=TEXT"" | base64 -d > src/buildtools/DEPS

        gclient sync --spec=""solutions=[{'name':'src/buildtools','url':None,'deps_file':'DEPS','custom_vars':{'process_deps':True},'managed':False}]""
    - name: Add ESLint problem matcher
      shell: bash
      run: echo ""::add-matcher::src/electron/.github/problem-matchers/eslint-stylish.json""
    - name: Run Lint
      shell: bash
      run: |
        # gn.py tries to find a gclient root folder starting from the current dir.
        # When it fails and returns ""None"" path, the whole script fails. Let's ""fix"" it.
        touch .gclient
        # Another option would be to checkout ""buildtools"" inside the Electron checkout,
        # but then we would lint its contents (at least gn format), and it doesn't pass it.

        cd src/electron
        node script/yarn install --frozen-lockfile
        node script/yarn lint
    - name: Run Script Typechecker
      shell: bash
      run: |
        cd src/electron
        node script/yarn tsc -p tsconfig.script.json
    
",84,1,1,workflow_call,3
electron/electron,pipeline-segment-electron-build.yml,"name: Pipeline Segment - Electron Build

on:
  workflow_call:
    inputs:
      environment:
        description: using the production or testing environment
        required: false
        type: string
      target-platform:
        type: string
        description: 'Platform to run on, can be macos, win or linux'
        required: true
      target-arch:
        type: string
        description: 'Arch to build for, can be x64, arm64, ia32 or arm'
        required: true
      target-variant:
        type: string
        description: 'Variant to build for, no effect on non-macOS target platforms. Can be darwin, mas or all.'
        default: all
      build-runs-on:
        type: string
        description: 'What host to run the build'
        required: true
      build-container:
        type: string
        description: 'JSON container information for aks runs-on'
        required: false
        default: '{""image"":null}'
      is-release:
        description: 'Whether this build job is a release job'
        required: true
        type: boolean
        default: false
      gn-build-type:
        description: 'The gn build type - testing or release'
        required: true
        type: string
        default: testing
      generate-symbols: 
        description: 'Whether or not to generate symbols'
        required: true
        type: boolean
        default: false
      upload-to-storage: 
        description: 'Whether or not to upload build artifacts to external storage'
        required: true
        type: string
        default: '0'
      strip-binaries: 
        description: 'Strip the binaries before release (Linux only)'
        required: false
        type: boolean
        default: false
      is-asan: 
        description: 'Building the Address Sanitizer (ASan) Linux build'
        required: false
        type: boolean
        default: false


concurrency:
  group: electron-build-${{ inputs.target-platform }}-${{ inputs.target-arch }}-${{ inputs.target-variant }}-${{ inputs.is-asan }}-${{ github.ref_protected == true && github.run_id || github.ref }}
  cancel-in-progress: ${{ github.ref_protected != true }}

env:
  CHROMIUM_GIT_COOKIE: ${{ secrets.CHROMIUM_GIT_COOKIE }}
  CHROMIUM_GIT_COOKIE_WINDOWS_STRING: ${{ secrets.CHROMIUM_GIT_COOKIE_WINDOWS_STRING }}
  ELECTRON_ARTIFACTS_BLOB_STORAGE: ${{ secrets.ELECTRON_ARTIFACTS_BLOB_STORAGE }}
  ELECTRON_RBE_JWT: ${{ secrets.ELECTRON_RBE_JWT }}
  SUDOWOODO_EXCHANGE_URL: ${{ secrets.SUDOWOODO_EXCHANGE_URL }}
  SUDOWOODO_EXCHANGE_TOKEN: ${{ secrets.SUDOWOODO_EXCHANGE_TOKEN }}
  GCLIENT_EXTRA_ARGS: ${{ inputs.target-platform == 'macos' && '--custom-var=checkout_mac=True --custom-var=host_os=mac' || inputs.target-platform == 'win' && '--custom-var=checkout_win=True' || '--custom-var=checkout_arm=True --custom-var=checkout_arm64=True' }}
  ELECTRON_OUT_DIR: Default

jobs:
  build:
    defaults:
      run:
        shell: bash
    runs-on: ${{ inputs.build-runs-on }}
    container: ${{ fromJSON(inputs.build-container) }}
    environment: ${{ inputs.environment }}
    env:
      TARGET_ARCH: ${{ inputs.target-arch }}
    steps:
    - name: Create src dir
      run: |
        mkdir src
    - name: Checkout Electron
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      with:
        path: src/electron
        fetch-depth: 0
        ref: ${{ github.event.pull_request.head.sha }}
    - name: Free up space (macOS)
      if: ${{ inputs.target-platform == 'macos' }}
      uses: ./src/electron/.github/actions/free-space-macos
    - name: Check disk space after freeing up space
      if: ${{ inputs.target-platform == 'macos' }}
      run: df -h
    - name: Setup Node.js/npm
      if: ${{ inputs.target-platform == 'macos' }}
      uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020
      with:
        node-version: 20.19.x
        cache: yarn
        cache-dependency-path: src/electron/yarn.lock
    - name: Install Dependencies
      uses: ./src/electron/.github/actions/install-dependencies
    - name: Install AZCopy
      if: ${{ inputs.target-platform == 'macos' }}
      run: brew install azcopy
    - name: Set GN_EXTRA_ARGS for Linux
      if: ${{ inputs.target-platform == 'linux' }}
      run: |
        if [ ""${{ inputs.target-arch  }}"" = ""arm"" ]; then
          if [ ""${{ inputs.is-release  }}"" = true ]; then
            GN_EXTRA_ARGS='target_cpu=""arm"" build_tflite_with_xnnpack=false symbol_level=1'
          else
            GN_EXTRA_ARGS='target_cpu=""arm"" build_tflite_with_xnnpack=false'
          fi
        elif [ ""${{ inputs.target-arch }}"" = ""arm64"" ]; then
          GN_EXTRA_ARGS='target_cpu=""arm64"" fatal_linker_warnings=false enable_linux_installer=false'
        elif [ ""${{ inputs.is-asan }}"" = true ]; then
          GN_EXTRA_ARGS='is_asan=true'
        fi
        echo ""GN_EXTRA_ARGS=$GN_EXTRA_ARGS"" >> $GITHUB_ENV
    - name: Set Chromium Git Cookie
      uses: ./src/electron/.github/actions/set-chromium-cookie
    - name: Install Build Tools
      uses: ./src/electron/.github/actions/install-build-tools      
    - name: Generate DEPS Hash
      run: |
        node src/electron/script/generate-deps-hash.js
        DEPSHASH=v1-src-cache-$(cat src/electron/.depshash)
        echo ""DEPSHASH=$DEPSHASH"" >> $GITHUB_ENV
        echo ""CACHE_PATH=$DEPSHASH.tar"" >> $GITHUB_ENV
    - name: Restore src cache via AZCopy
      if: ${{ inputs.target-platform != 'linux' }}
      uses: ./src/electron/.github/actions/restore-cache-azcopy
      with:
        target-platform: ${{ inputs.target-platform }}
    - name: Restore src cache via AKS
      if: ${{ inputs.target-platform == 'linux' }}
      uses: ./src/electron/.github/actions/restore-cache-aks
    - name: Checkout Electron
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      with:
        path: src/electron
        fetch-depth: 0
        ref: ${{ github.event.pull_request.head.sha }}
    - name: Fix Sync
      if: ${{ inputs.target-platform != 'linux' }}
      uses: ./src/electron/.github/actions/fix-sync
      with:
        target-platform: ${{ inputs.target-platform }}
      env:
        ELECTRON_DEPOT_TOOLS_DISABLE_LOG: true
    - name: Init Build Tools
      run: |
        e init -f --root=$(pwd) --out=Default ${{ inputs.gn-build-type }} --import ${{ inputs.gn-build-type }} --target-cpu ${{ inputs.target-arch }}
    - name: Run Electron Only Hooks
      run: |
        e d gclient runhooks --spec=""solutions=[{'name':'src/electron','url':None,'deps_file':'DEPS','custom_vars':{'process_deps':False},'managed':False}]""
    - name: Regenerate DEPS Hash
      run: |
        (cd src/electron && git checkout .) && node src/electron/script/generate-deps-hash.js
        echo ""DEPSHASH=$(cat src/electron/.depshash)"" >> $GITHUB_ENV
    - name: Add CHROMIUM_BUILDTOOLS_PATH to env
      run: echo ""CHROMIUM_BUILDTOOLS_PATH=$(pwd)/src/buildtools"" >> $GITHUB_ENV
    - name: Setup Number of Ninja Processes
      run: |
        echo ""NUMBER_OF_NINJA_PROCESSES=${{ inputs.target-platform != 'macos' && '300' || '200' }}"" >> $GITHUB_ENV
    - name: Free up space (macOS)
      if: ${{ inputs.target-platform == 'macos' }}
      uses: ./src/electron/.github/actions/free-space-macos
    - name: Build Electron
      if: ${{ inputs.target-platform != 'macos' || (inputs.target-variant == 'all' || inputs.target-variant == 'darwin') }}
      uses: ./src/electron/.github/actions/build-electron
      with:
        target-arch: ${{ inputs.target-arch }}
        target-platform: ${{ inputs.target-platform }}
        artifact-platform: ${{ inputs.target-platform == 'macos' && 'darwin' || inputs.target-platform }}
        is-release: '${{ inputs.is-release }}'
        generate-symbols: '${{ inputs.generate-symbols }}'
        strip-binaries: '${{ inputs.strip-binaries }}'
        upload-to-storage: '${{ inputs.upload-to-storage }}'
        is-asan: '${{ inputs.is-asan }}'
    - name: Set GN_EXTRA_ARGS for MAS Build
      if: ${{ inputs.target-platform == 'macos' && (inputs.target-variant == 'all' || inputs.target-variant == 'mas') }}
      run: |
        echo ""MAS_BUILD=true"" >> $GITHUB_ENV
        GN_EXTRA_ARGS='is_mas_build=true'
        echo ""GN_EXTRA_ARGS=$GN_EXTRA_ARGS"" >> $GITHUB_ENV
    - name: Build Electron (MAS)
      if: ${{ inputs.target-platform == 'macos' && (inputs.target-variant == 'all' || inputs.target-variant == 'mas') }}
      uses: ./src/electron/.github/actions/build-electron
      with:
        target-arch: ${{ inputs.target-arch }}
        target-platform: ${{ inputs.target-platform }}
        artifact-platform: 'mas'
        is-release: '${{ inputs.is-release }}'
        generate-symbols: '${{ inputs.generate-symbols }}'
        upload-to-storage: '${{ inputs.upload-to-storage }}'
        step-suffix: '(mas)'
",207,1,1,workflow_call,13
electron/electron,pipeline-segment-electron-gn-check.yml,"name: Pipeline Segment - Electron GN Check

on:
  workflow_call:
    inputs:
      target-platform:
        type: string
        description: 'Platform to run on, can be macos, win or linux'
        required: true
      target-archs:
        type: string
        description: 'Archs to check for, can be x64, x86, arm64 or arm space separated'
        required: true
      check-runs-on:
        type: string
        description: 'What host to run the tests on'
        required: true
      check-container:
        type: string
        description: 'JSON container information for aks runs-on'
        required: false
        default: '{""image"":null}'
      gn-build-type:
        description: 'The gn build type - testing or release'
        required: true
        type: string
        default: testing

concurrency:
  group: electron-gn-check-${{ inputs.target-platform }}-${{ github.ref }}
  cancel-in-progress: true

env:
  ELECTRON_RBE_JWT: ${{ secrets.ELECTRON_RBE_JWT }}
  GCLIENT_EXTRA_ARGS: ${{ inputs.target-platform == 'macos' && '--custom-var=checkout_mac=True --custom-var=host_os=mac' || (inputs.target-platform == 'linux' && '--custom-var=checkout_arm=True --custom-var=checkout_arm64=True' || '--custom-var=checkout_win=True') }}
  ELECTRON_OUT_DIR: Default

jobs:
  gn-check:
    defaults:
      run:
        shell: bash
    runs-on: ${{ inputs.check-runs-on }}
    container: ${{ fromJSON(inputs.check-container) }}
    steps:
    - name: Checkout Electron
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      with:
        path: src/electron
        fetch-depth: 0
        ref: ${{ github.event.pull_request.head.sha }}
    - name: Cleanup disk space on macOS
      if: ${{ inputs.target-platform == 'macos' }}
      shell: bash
      run: |   
        sudo mkdir -p $TMPDIR/del-target

        tmpify() {
          if [ -d ""$1"" ]; then
            sudo mv ""$1"" $TMPDIR/del-target/$(echo $1|shasum -a 256|head -n1|cut -d "" "" -f1)
          fi
        }   
        tmpify /Library/Developer/CoreSimulator
        tmpify ~/Library/Developer/CoreSimulator
        sudo rm -rf $TMPDIR/del-target
    - name: Check disk space after freeing up space
      if: ${{ inputs.target-platform == 'macos' }}
      run: df -h
    - name: Set Chromium Git Cookie
      uses: ./src/electron/.github/actions/set-chromium-cookie
    - name: Install Build Tools
      uses: ./src/electron/.github/actions/install-build-tools
    - name: Enable windows toolchain
      if: ${{ inputs.target-platform == 'win' }}
      run: |
        echo ""ELECTRON_DEPOT_TOOLS_WIN_TOOLCHAIN=1"" >> $GITHUB_ENV
    - name: Generate DEPS Hash
      run: |
        node src/electron/script/generate-deps-hash.js
        DEPSHASH=v1-src-cache-$(cat src/electron/.depshash)
        echo ""DEPSHASH=$DEPSHASH"" >> $GITHUB_ENV
        echo ""CACHE_PATH=$DEPSHASH.tar"" >> $GITHUB_ENV
    - name: Restore src cache via AZCopy
      if: ${{ inputs.target-platform == 'macos' }}
      uses: ./src/electron/.github/actions/restore-cache-azcopy
      with:
        target-platform: ${{ inputs.target-platform }}      
    - name: Restore src cache via AKS
      if: ${{ inputs.target-platform == 'linux' || inputs.target-platform == 'win' }}
      uses: ./src/electron/.github/actions/restore-cache-aks
      with:
        target-platform: ${{ inputs.target-platform }}
    - name: Run Electron Only Hooks
      run: |
        echo ""solutions=[{'name':'src/electron','url':None,'deps_file':'DEPS','custom_vars':{'process_deps':False},'managed':False}]"" > tmpgclient
        if [ ""${{ inputs.target-platform }}"" = ""win"" ]; then
          echo ""solutions=[{'name':'src/electron','url':None,'deps_file':'DEPS','custom_vars':{'process_deps':False,'install_sysroot':False,'checkout_win':True},'managed':False}]"" > tmpgclient
          echo ""target_os=['win']"" >> tmpgclient
        fi
        e d gclient runhooks --gclientfile=tmpgclient

        # Fix VS Toolchain
        if [ ""${{ inputs.target-platform }}"" = ""win"" ]; then
          rm -rf src/third_party/depot_tools/win_toolchain/vs_files
          e d python3 src/build/vs_toolchain.py update --force
        fi
    - name: Regenerate DEPS Hash
      run: |
        (cd src/electron && git checkout .) && node src/electron/script/generate-deps-hash.js
        echo ""DEPSHASH=$(cat src/electron/.depshash)"" >> $GITHUB_ENV
    - name: Add CHROMIUM_BUILDTOOLS_PATH to env
      run: echo ""CHROMIUM_BUILDTOOLS_PATH=$(pwd)/src/buildtools"" >> $GITHUB_ENV
    - name: Checkout Electron
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      with:
        path: src/electron
        fetch-depth: 0
        ref: ${{ github.event.pull_request.head.sha }}
    - name: Install Dependencies
      uses: ./src/electron/.github/actions/install-dependencies
    - name: Default GN gen
      run: |
        cd src/electron
        git pack-refs
    - name: Run GN Check for ${{ inputs.target-archs }}
      run: |
        for target_cpu in ${{ inputs.target-archs }}
        do
          e init -f --root=$(pwd) --out=Default ${{ inputs.gn-build-type }} --import ${{ inputs.gn-build-type }} --target-cpu $target_cpu
          cd src
          export GN_EXTRA_ARGS=""target_cpu=\""$target_cpu\""""
          if [ ""${{ inputs.target-platform }}"" = ""linux"" ]; then
            if [ ""$target_cpu"" = ""arm"" ]; then
              export GN_EXTRA_ARGS=""$GN_EXTRA_ARGS build_tflite_with_xnnpack=false""
            elif [ ""$target_cpu"" = ""arm64"" ]; then
              export GN_EXTRA_ARGS=""$GN_EXTRA_ARGS fatal_linker_warnings=false enable_linux_installer=false""
            fi
          fi
          if [ ""${{ inputs.target-platform }}"" = ""win"" ]; then
            export GN_EXTRA_ARGS=""$GN_EXTRA_ARGS use_v8_context_snapshot=true target_os=\""win\""""
          fi

          e build --only-gen

          e d gn check out/Default //electron:electron_lib
          e d gn check out/Default //electron:electron_app
          e d gn check out/Default //electron/shell/common:mojo
          e d gn check out/Default //electron/shell/common:plugin

          # Check the hunspell filenames
          node electron/script/gen-hunspell-filenames.js --check
          node electron/script/gen-libc++-filenames.js --check
          cd ..
        done
    - name: Wait for active SSH sessions
      if: always() && !cancelled()
      shell: bash
      run: |
        while [ -f /var/.ssh-lock ]
        do
          sleep 60
        done
",162,1,1,workflow_call,7
electron/electron,pipeline-segment-electron-test.yml,"name: Pipeline Segment - Electron Test

on:
  workflow_call:
    inputs:
      target-platform:
        type: string
        description: 'Platform to run on, can be macos, win or linux'
        required: true
      target-arch:
        type: string
        description: 'Arch to build for, can be x64, arm64 or arm'
        required: true
      test-runs-on:
        type: string
        description: 'What host to run the tests on'
        required: true
      test-container:
        type: string
        description: 'JSON container information for aks runs-on'
        required: false
        default: '{""image"":null}'
      is-asan: 
        description: 'Building the Address Sanitizer (ASan) Linux build'
        required: false
        type: boolean
        default: false

concurrency:
  group: electron-test-${{ inputs.target-platform }}-${{ inputs.target-arch }}-${{ inputs.is-asan }}-${{ github.ref_protected == true && github.run_id || github.ref }}
  cancel-in-progress: ${{ github.ref_protected != true }}

permissions:
  contents: read
  issues: read
  pull-requests: read

env:
  CHROMIUM_GIT_COOKIE: ${{ secrets.CHROMIUM_GIT_COOKIE }}
  CHROMIUM_GIT_COOKIE_WINDOWS_STRING: ${{ secrets.CHROMIUM_GIT_COOKIE_WINDOWS_STRING }}
  ELECTRON_OUT_DIR: Default
  ELECTRON_RBE_JWT: ${{ secrets.ELECTRON_RBE_JWT }}

jobs:
  test:
    defaults:
      run:
        shell: bash
    runs-on: ${{ inputs.test-runs-on }}
    container: ${{ fromJSON(inputs.test-container) }}
    strategy:
      fail-fast: false
      matrix:
        build-type: ${{ inputs.target-platform == 'macos' && fromJSON('[""darwin"",""mas""]') || (inputs.target-platform == 'win' && fromJSON('[""win""]') || fromJSON('[""linux""]')) }}
        shard: ${{ inputs.target-platform == 'linux' && fromJSON('[1, 2, 3]') || fromJSON('[1, 2]') }}
    env:
      BUILD_TYPE: ${{ matrix.build-type }}
      TARGET_ARCH: ${{ inputs.target-arch }}
      ARTIFACT_KEY: ${{ matrix.build-type }}_${{ inputs.target-arch }}
    steps:
    - name: Fix node20 on arm32 runners
      if: ${{ inputs.target-arch == 'arm' && inputs.target-platform == 'linux' }}
      run: |
        cp $(which node) /mnt/runner-externals/node20/bin/
    - name: Install Git on Windows arm64 runners
      if: ${{ inputs.target-arch == 'arm64' && inputs.target-platform == 'win' }}
      shell: powershell
      run: |
        Set-ExecutionPolicy Bypass -Scope Process -Force
        [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072
        iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))
        choco install -y --no-progress git.install --params ""'/GitAndUnixToolsOnPath'""
        choco install -y --no-progress git
        choco install -y --no-progress python --version 3.11.9
        choco install -y --no-progress visualstudio2022-workload-vctools --package-parameters ""--add Microsoft.VisualStudio.Component.VC.Tools.ARM64""
        echo ""C:\Program Files\Git\cmd"" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
        echo ""C:\Program Files\Git\bin"" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
        echo ""C:\Python311"" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
        cp ""C:\Python311\python.exe"" ""C:\Python311\python3.exe""
    - name: Setup Node.js/npm
      if: ${{ inputs.target-platform == 'win' }}
      uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020
      with:
        node-version: 20.19.x
    - name: Add TCC permissions on macOS
      if: ${{ inputs.target-platform == 'macos' }}
      run: |
        configure_user_tccdb () {
          local values=$1
          local dbPath=""$HOME/Library/Application Support/com.apple.TCC/TCC.db""
          local sqlQuery=""INSERT OR REPLACE INTO access VALUES($values);""
          sqlite3 ""$dbPath"" ""$sqlQuery""
        }

        configure_sys_tccdb () {
          local values=$1
          local dbPath=""/Library/Application Support/com.apple.TCC/TCC.db""
          local sqlQuery=""INSERT OR REPLACE INTO access VALUES($values);""
          sudo sqlite3 ""$dbPath"" ""$sqlQuery""
        }

        userValuesArray=(
            ""'kTCCServiceMicrophone','/usr/local/opt/runner/provisioner/provisioner',1,2,4,1,NULL,NULL,0,'UNUSED',NULL,0,1687786159""
            ""'kTCCServiceCamera','/usr/local/opt/runner/provisioner/provisioner',1,2,4,1,NULL,NULL,0,'UNUSED',NULL,0,1687786159""
            ""'kTCCServiceBluetoothAlways','/usr/local/opt/runner/provisioner/provisioner',1,2,4,1,NULL,NULL,0,'UNUSED',NULL,0,1687786159""
        )
        for values in ""${userValuesArray[@]}""; do
          # Sonoma and higher have a few extra values
          # Ref: https://github.com/actions/runner-images/blob/main/images/macos/scripts/build/configure-tccdb-macos.sh
          if [ ""$OSTYPE"" = ""darwin23"" ]; then
            configure_user_tccdb ""$values,NULL,NULL,'UNUSED',${values##*,}""
            configure_sys_tccdb ""$values,NULL,NULL,'UNUSED',${values##*,}""
          else
            configure_user_tccdb ""$values""
            configure_sys_tccdb ""$values""
          fi
        done
    - name: Turn off the unexpectedly quit dialog on macOS
      if: ${{ inputs.target-platform == 'macos' }}
      run: defaults write com.apple.CrashReporter DialogType server
    - name: Checkout Electron
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      with:
        path: src/electron
        fetch-depth: 0
        ref: ${{ github.event.pull_request.head.sha }}
    - name: Install Dependencies
      uses: ./src/electron/.github/actions/install-dependencies
    - name: Set Chromium Git Cookie
      uses: ./src/electron/.github/actions/set-chromium-cookie
    - name: Get Depot Tools
      timeout-minutes: 5
      run: |
        git config --global core.filemode false
        git config --global core.autocrlf false
        git config --global branch.autosetuprebase always
        git config --global core.fscache true
        git config --global core.preloadindex true
        git clone --filter=tree:0 https://chromium.googlesource.com/chromium/tools/depot_tools.git
        # Ensure depot_tools does not update.
        test -d depot_tools && cd depot_tools
        touch .disable_auto_update
    - name: Add Depot Tools to PATH
      run: echo ""$(pwd)/depot_tools"" >> $GITHUB_PATH
    - name: Load ASan specific environment variables
      if: ${{ inputs.is-asan == true }}
      run: |
        echo ""ARTIFACT_KEY=${{ matrix.build-type }}_${{ inputs.target-arch }}_asan"" >> $GITHUB_ENV
        echo ""DISABLE_CRASH_REPORTER_TESTS=true"" >> $GITHUB_ENV
        echo ""IS_ASAN=true"" >> $GITHUB_ENV
    - name: Download Generated Artifacts
      uses: actions/download-artifact@d3f86a106a0bac45b974a628896c90dbdf5c8093
      with:
        name: generated_artifacts_${{ env.ARTIFACT_KEY }}
        path: ./generated_artifacts_${{ matrix.build-type }}_${{ inputs.target-arch }}
    - name: Download Src Artifacts
      uses: actions/download-artifact@d3f86a106a0bac45b974a628896c90dbdf5c8093
      with:
        name: src_artifacts_${{ env.ARTIFACT_KEY }}
        path: ./src_artifacts_${{ matrix.build-type }}_${{ inputs.target-arch }}
    - name: Restore Generated Artifacts
      run: ./src/electron/script/actions/restore-artifacts.sh
    - name: Unzip Dist, Mksnapshot & Chromedriver (win)
      if: ${{ inputs.target-platform == 'win' }}
      shell: powershell
      run: |
        Set-ExecutionPolicy Bypass -Scope Process -Force
        cd src/out/Default
        Expand-Archive -Force dist.zip -DestinationPath ./
        Expand-Archive -Force chromedriver.zip -DestinationPath ./
        Expand-Archive -Force mksnapshot.zip -DestinationPath ./
    - name: Unzip Dist, Mksnapshot & Chromedriver (unix)
      if: ${{ inputs.target-platform != 'win' }}
      run: |
        cd src/out/Default
        unzip -:o dist.zip
        unzip -:o chromedriver.zip
        unzip -:o mksnapshot.zip
    - name: Import & Trust Self-Signed Codesigning Cert on MacOS
      if: ${{ inputs.target-platform == 'macos' && inputs.target-arch == 'x64' }}
      run: |
        sudo security authorizationdb write com.apple.trust-settings.admin allow
        cd src/electron
        ./script/codesign/generate-identity.sh
    - name: Install Datadog CLI
      run: |
        cd src/electron
        node script/yarn global add @datadog/datadog-ci
    - name: Run Electron Tests
      shell: bash
      env:
        MOCHA_REPORTER: mocha-multi-reporters
        MOCHA_MULTI_REPORTERS: mocha-junit-reporter, tap
        ELECTRON_DISABLE_SECURITY_WARNINGS: 1
        ELECTRON_SKIP_NATIVE_MODULE_TESTS: true
        DISPLAY: ':99.0'
        NPM_CONFIG_MSVS_VERSION: '2022'
      run: |
        cd src/electron
        export ELECTRON_TEST_RESULTS_DIR=`pwd`/junit
        # Get which tests are on this shard
        tests_files=$(node script/split-tests ${{ matrix.shard }} ${{ inputs.target-platform == 'linux' && 3 || 2 }})

        # Run tests
        if [ ""${{ inputs.target-platform }}"" != ""linux"" ]; then
          echo ""About to start tests""
          if [ ""${{ inputs.target-platform }}"" = ""win"" ]; then
            if [ ""${{ inputs.target-arch }}"" = ""x86"" ]; then
              export npm_config_arch=""ia32""
            fi
            if [ ""${{ inputs.target-arch }}"" = ""arm64"" ]; then
              export ELECTRON_FORCE_TEST_SUITE_EXIT=""true""
            fi
          fi
          node script/yarn test --runners=main --trace-uncaught --enable-logging --files $tests_files
        else
          chown :builduser .. && chmod g+w ..
          chown -R :builduser . && chmod -R g+w .
          chmod 4755 ../out/Default/chrome-sandbox
          runuser -u builduser -- git config --global --add safe.directory $(pwd)
          if [ ""${{ inputs.is-asan }}"" == ""true"" ]; then
            cd ..
            ASAN_SYMBOLIZE=""$PWD/tools/valgrind/asan/asan_symbolize.py --executable-path=$PWD/out/Default/electron""
            export ASAN_OPTIONS=""symbolize=0 handle_abort=1""
            export G_SLICE=always-malloc
            export NSS_DISABLE_ARENA_FREE_LIST=1
            export NSS_DISABLE_UNLOAD=1
            export LLVM_SYMBOLIZER_PATH=$PWD/third_party/llvm-build/Release+Asserts/bin/llvm-symbolizer
            export MOCHA_TIMEOUT=180000
            echo ""Piping output to ASAN_SYMBOLIZE ($ASAN_SYMBOLIZE)""
            cd electron
            runuser -u builduser -- xvfb-run script/actions/run-tests.sh script/yarn test --runners=main --trace-uncaught --enable-logging --files $tests_files | $ASAN_SYMBOLIZE
          else
            runuser -u builduser -- xvfb-run script/actions/run-tests.sh script/yarn test --runners=main --trace-uncaught --enable-logging --files $tests_files
          fi
        fi
    - name: Upload Test results to Datadog
      env:
        DD_ENV: ci
        DD_SERVICE: electron
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
        DD_CIVISIBILITY_LOGS_ENABLED: true
        DD_TAGS: ""os.architecture:${{ inputs.target-arch }},os.family:${{ inputs.target-platform }},os.platform:${{ inputs.target-platform }},asan:${{ inputs.is-asan }}""
      run: |
        if ! [ -z $DD_API_KEY ] && [ -f src/electron/junit/test-results-main.xml ]; then
          export DATADOG_PATH=`node src/electron/script/yarn global bin`
          $DATADOG_PATH/datadog-ci junit upload src/electron/junit/test-results-main.xml
        fi          
      if: always() && !cancelled()
    - name: Upload Test Artifacts
      if: always() && !cancelled()
      uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
      with:
        name: test_artifacts_${{ env.ARTIFACT_KEY }}_${{ matrix.shard }}
        path: src/electron/spec/artifacts
        if-no-files-found: ignore
    - name: Wait for active SSH sessions
      if: always() && !cancelled()
      shell: bash
      run: |
        while [ -f /var/.ssh-lock ]
        do
          sleep 60
        done
",264,1,1,workflow_call,7
electron/electron,pipeline-segment-node-nan-test.yml,"name: Pipeline Segment - Node/Nan Test

on:
  workflow_call:
    inputs:
      target-platform:
        type: string
        description: 'Platform to run on, can be macos, win or linux'
        required: true
      target-arch:
        type: string
        description: 'Arch to build for, can be x64, arm64 or arm'
        required: true
      test-runs-on:
        type: string
        description: 'What host to run the tests on'
        required: true
      test-container:
        type: string
        description: 'JSON container information for aks runs-on'
        required: false
        default: '{""image"":null}'
      gn-build-type:
        description: 'The gn build type - testing or release'
        required: true
        type: string
        default: testing

concurrency:
  group: electron-node-nan-test-${{ inputs.target-platform }}-${{ inputs.target-arch }}-${{ github.ref_protected == true && github.run_id || github.ref }}
  cancel-in-progress: ${{ github.ref_protected != true }}

env:
  CHROMIUM_GIT_COOKIE: ${{ secrets.CHROMIUM_GIT_COOKIE }}
  ELECTRON_OUT_DIR: Default
  ELECTRON_RBE_JWT: ${{ secrets.ELECTRON_RBE_JWT }}

jobs:
  node-tests:
    name: Run Node.js Tests
    runs-on: electron-arc-linux-amd64-8core
    timeout-minutes: 30
    env: 
      TARGET_ARCH: ${{ inputs.target-arch }}
      BUILD_TYPE: linux
    container: ${{ fromJSON(inputs.test-container) }}
    steps:
    - name: Checkout Electron
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      with:
        path: src/electron
        fetch-depth: 0
        ref: ${{ github.event.pull_request.head.sha }}
    - name: Set Chromium Git Cookie
      uses: ./src/electron/.github/actions/set-chromium-cookie
    - name: Install Build Tools
      uses: ./src/electron/.github/actions/install-build-tools
    - name: Init Build Tools
      run: |
        e init -f --root=$(pwd) --out=Default ${{ inputs.gn-build-type }} --import ${{ inputs.gn-build-type }} --target-cpu ${{ inputs.target-arch }}
    - name: Install Dependencies
      uses: ./src/electron/.github/actions/install-dependencies
    - name: Download Generated Artifacts
      uses: actions/download-artifact@d3f86a106a0bac45b974a628896c90dbdf5c8093
      with:
        name: generated_artifacts_${{ env.BUILD_TYPE }}_${{ env.TARGET_ARCH }}
        path: ./generated_artifacts_${{ env.BUILD_TYPE }}_${{ env.TARGET_ARCH }}
    - name: Download Src Artifacts
      uses: actions/download-artifact@d3f86a106a0bac45b974a628896c90dbdf5c8093
      with:
        name: src_artifacts_linux_${{ env.TARGET_ARCH }}
        path: ./src_artifacts_linux_${{ env.TARGET_ARCH }}
    - name: Restore Generated Artifacts
      run: ./src/electron/script/actions/restore-artifacts.sh
    - name: Unzip Dist
      run: |
        cd src/out/Default
        unzip -:o dist.zip
    - name: Setup Linux for Headless Testing
      run: sh -e /etc/init.d/xvfb start
    - name: Run Node.js Tests
      run: |
        cd src
        node electron/script/node-spec-runner.js --default --jUnitDir=junit
    - name: Wait for active SSH sessions
      if: always() && !cancelled()
      shell: bash
      run: |
        while [ -f /var/.ssh-lock ]
        do
          sleep 60
        done
  nan-tests:
    name: Run Nan Tests
    runs-on: electron-arc-linux-amd64-4core
    timeout-minutes: 30
    env: 
      TARGET_ARCH: ${{ inputs.target-arch }}
      BUILD_TYPE: linux
    container: ${{ fromJSON(inputs.test-container) }}
    steps:
    - name: Checkout Electron
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      with:
        path: src/electron
        fetch-depth: 0
        ref: ${{ github.event.pull_request.head.sha }}
    - name: Set Chromium Git Cookie
      uses: ./src/electron/.github/actions/set-chromium-cookie
    - name: Install Build Tools
      uses: ./src/electron/.github/actions/install-build-tools
    - name: Init Build Tools
      run: |
        e init -f --root=$(pwd) --out=Default ${{ inputs.gn-build-type }}
    - name: Install Dependencies
      uses: ./src/electron/.github/actions/install-dependencies
    - name: Download Generated Artifacts
      uses: actions/download-artifact@d3f86a106a0bac45b974a628896c90dbdf5c8093
      with:
        name: generated_artifacts_${{ env.BUILD_TYPE }}_${{ env.TARGET_ARCH }}
        path: ./generated_artifacts_${{ env.BUILD_TYPE }}_${{ env.TARGET_ARCH }}
    - name: Download Src Artifacts
      uses: actions/download-artifact@d3f86a106a0bac45b974a628896c90dbdf5c8093
      with:
        name: src_artifacts_linux_${{ env.TARGET_ARCH }}
        path: ./src_artifacts_linux_${{ env.TARGET_ARCH }}
    - name: Restore Generated Artifacts
      run: ./src/electron/script/actions/restore-artifacts.sh
    - name: Unzip Dist
      run: |
        cd src/out/Default
        unzip -:o dist.zip
    - name: Setup Linux for Headless Testing
      run: sh -e /etc/init.d/xvfb start
    - name: Run Nan Tests
      run: |
        cd src
        node electron/script/nan-spec-runner.js
    - name: Wait for active SSH sessions
      shell: bash
      if: always() && !cancelled()
      run: |
        while [ -f /var/.ssh-lock ]
        do
          sleep 60
        done
",146,2,1,workflow_call,12
electron/electron,pull-request-labeled.yml,"name: Pull Request Labeled

on:
  pull_request_target:
    types: [labeled]

permissions: {}

jobs:
  pull-request-labeled-backport-requested:
    name: backport/requested label added
    if: github.event.label.name == 'backport/requested 🗳'
    runs-on: ubuntu-latest
    steps:
      - name: Trigger Slack workflow
        uses: slackapi/slack-github-action@b0fa283ad8fea605de13dc3f449259339835fc52 # v2.1.0
        with:
          webhook: ${{ secrets.BACKPORT_REQUESTED_SLACK_WEBHOOK_URL }} 
          webhook-type: webhook-trigger
          payload: |
            {
              ""url"": ""${{ github.event.pull_request.html_url }}""
            }
  pull-request-labeled-deprecation-review-complete:
    name: deprecation-review/complete label added
    if: github.event.label.name == 'deprecation-review/complete ✅'
    runs-on: ubuntu-latest
    steps:
      - name: Generate GitHub App token
        uses: electron/github-app-auth-action@384fd19694fe7b6dcc9a684746c6976ad78228ae # v1.1.1
        id: generate-token
        with:
          creds: ${{ secrets.RELEASE_BOARD_GH_APP_CREDS }}
          org: electron
      - name: Set status
        uses: dsanders11/project-actions/edit-item@2134fe7cc71c58b7ae259c82a8e63c6058255678 # v1.7.0
        with:
          token: ${{ steps.generate-token.outputs.token }}
          project-number: 94
          field: Status
          field-value: ✅ Reviewed
",41,2,1,pull_request_target,3
electron/electron,scorecards.yml,"name: Scorecards supply-chain security
on:
  # Only the default branch is supported.
  branch_protection_rule:
  schedule:
    - cron: '44 17 * * 0'
  push:
    branches: [ ""main"" ]

# Declare default permissions as read only.
permissions: read-all

jobs:
  analysis:
    name: Scorecards analysis
    runs-on: ubuntu-latest
    permissions:
      # Needed to upload the results to code-scanning dashboard.
      security-events: write
      # Used to receive a badge.
      id-token: write

    steps:
      - name: ""Checkout code""
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          persist-credentials: false

      # This is a pre-submit / pre-release.
      - name: ""Run analysis""
        uses: ossf/scorecard-action@05b42c624433fc40578a4040d5cf5e36ddca8cde # v2.4.2
        with:
          results_file: results.sarif
          results_format: sarif

          # Publish the results for public repositories to enable scorecard badges. For more details, see
          # https://github.com/ossf/scorecard-action#publishing-results.
          # For private repositories, `publish_results` will automatically be set to `false`, regardless
          # of the value entered here.
          publish_results: true

      # Upload the results as artifacts (optional). Commenting out will disable uploads of run results in SARIF
      # format to the repository Actions tab.
      - name: ""Upload artifact""
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: SARIF file
          path: results.sarif
          retention-days: 5

      # Upload the results to GitHub's code scanning dashboard.
      - name: ""Upload to code-scanning""
        uses: github/codeql-action/upload-sarif@ce28f5bb42b7a9f2c824e633a3f6ee835bab6858 # v3.29.0
        with:
          sarif_file: results.sarif
",55,1,3,"branch_protection_rule, schedule, push",4
electron/electron,semantic.yml,"name: ""Check Semantic Commit""

on:
  pull_request:
    types:
      - opened
      - edited
      - synchronize

permissions:
  contents: read

jobs:
  main:
    permissions:
      pull-requests: read  # for amannn/action-semantic-pull-request to analyze PRs
      statuses: write  # for amannn/action-semantic-pull-request to mark status of analyzed PR
    name: Validate PR Title
    runs-on: ubuntu-latest
    steps:
      - name: semantic-pull-request
        uses: amannn/action-semantic-pull-request@0723387faaf9b38adef4775cd42cfd5155ed6017 # v5.5.3
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          validateSingleCommit: false
",26,1,1,pull_request,2
electron/electron,stable-prep-items.yml,"name: Check Stable Prep Items

on:
  schedule:
    - cron:  '0 */12 * * *'
  workflow_dispatch:

permissions: {}

jobs:
  check-stable-prep-items:
    name: Check Stable Prep Items
    runs-on: ubuntu-latest
    steps:
      - name: Generate GitHub App token
        uses: electron/github-app-auth-action@384fd19694fe7b6dcc9a684746c6976ad78228ae # v1.1.1
        id: generate-token
        with:
          creds: ${{ secrets.RELEASE_BOARD_GH_APP_CREDS }}
          org: electron
      - name: Find Newest Release Project Board
        id: find-project-number
        env:
          GITHUB_TOKEN: ${{ steps.generate-token.outputs.token }}
        run: |
          set -eo pipefail
          PROJECT_NUMBER=$(gh project list --owner electron --format json | jq -r '.projects | map(select(.title | test(""^[0-9]+-x-y$""))) | max_by(.number) | .number')
          echo ""PROJECT_NUMBER=$PROJECT_NUMBER"" >> ""$GITHUB_OUTPUT""
      - name: Update Completed Stable Prep Items
        uses: dsanders11/project-actions/completed-by@2134fe7cc71c58b7ae259c82a8e63c6058255678 # v1.7.0
        with:
          field: Prep Status
          field-value: ✅ Complete
          project-number: ${{ steps.find-project-number.outputs.PROJECT_NUMBER }}
          token: ${{ steps.generate-token.outputs.token }}
",35,1,2,"schedule, workflow_dispatch",2
electron/electron,stale.yml,"name: 'Close stale issues'
on:
  workflow_dispatch:
  schedule:
    # 1:30am every day
    - cron: '30 1 * * *'

permissions: {}

jobs:
  stale:
    runs-on: ubuntu-latest
    steps:
      - name: Generate GitHub App token
        uses: electron/github-app-auth-action@384fd19694fe7b6dcc9a684746c6976ad78228ae # v1.1.1
        id: generate-token
        with:
          creds: ${{ secrets.ISSUE_TRIAGE_GH_APP_CREDS }}
      - uses: actions/stale@5bef64f19d7facfb25b37b414482c7164d639639 # tag: v9.1.0
        with:
          repo-token: ${{ steps.generate-token.outputs.token }}
          days-before-stale: 90
          days-before-close: 30
          stale-issue-label: stale
          operations-per-run: 1750
          stale-issue-message: >
            This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, ""bump""), and we'll keep it open. If you have any new additional information—in particular, if this is still reproducible in the [latest version of Electron](https://www.electronjs.org/releases/stable) or in the [beta](https://www.electronjs.org/releases/beta)—please include it with your comment!
          close-issue-message: >
            This issue has been closed due to inactivity, and will not be monitored.  If this is a bug and you can reproduce this issue on a [supported version of Electron](https://www.electronjs.org/docs/latest/tutorial/electron-timelines#timeline) please open a new issue and include instructions for reproducing the issue.
          exempt-issue-labels: ""discussion,security \U0001F512,enhancement :sparkles:,status/confirmed,stale-exempt,upgrade-follow-up""
          only-pr-labels: not-a-real-label
  pending-repro:
    runs-on: ubuntu-latest
    if: ${{ always() }}
    needs: stale
    steps:
      - name: Generate GitHub App token
        uses: electron/github-app-auth-action@384fd19694fe7b6dcc9a684746c6976ad78228ae # v1.1.1
        id: generate-token
        with:
          creds: ${{ secrets.ISSUE_TRIAGE_GH_APP_CREDS }}
      - uses: actions/stale@5bef64f19d7facfb25b37b414482c7164d639639 # tag: v9.1.0
        with:
          repo-token: ${{ steps.generate-token.outputs.token }}
          days-before-stale: -1
          days-before-close: 10
          remove-stale-when-updated: false
          stale-issue-label: blocked/need-repro
          stale-pr-label: not-a-real-label
          operations-per-run: 1750
          close-issue-message: >
            Unfortunately, without a way to reproduce this issue, we're unable to continue investigation. This issue has been closed and will not be monitored further. If you're able to provide a minimal test case that reproduces this issue on a [supported version of Electron](https://www.electronjs.org/docs/latest/tutorial/electron-timelines#timeline) please open a new issue and include instructions for reproducing the issue.
",52,2,2,"workflow_dispatch, schedule",4
electron/electron,windows-publish.yml,"name: Publish Windows

on:
  workflow_dispatch:
    inputs:
      build-image-sha:
        type: string
        description: 'SHA for electron/build image'
        default: '424eedbf277ad9749ffa9219068aa72ed4a5e373'
        required: true
      upload-to-storage:
        description: 'Uploads to Azure storage'
        required: false
        default: '1'
        type: string
      run-windows-publish:
        description: 'Run the publish jobs vs just the build jobs'
        type: boolean
        default: false

jobs:
  checkout-windows:
    runs-on: electron-arc-linux-amd64-32core
    container:
      image: ghcr.io/electron/build:${{ inputs.build-image-sha }}
      options: --user root --device /dev/fuse --cap-add SYS_ADMIN
      volumes:
        - /mnt/win-cache:/mnt/win-cache
        - /var/run/sas:/var/run/sas
    env:
      CHROMIUM_GIT_COOKIE_WINDOWS_STRING: ${{ secrets.CHROMIUM_GIT_COOKIE_WINDOWS_STRING }}
      GCLIENT_EXTRA_ARGS: '--custom-var=checkout_win=True'
      TARGET_OS: 'win'
      ELECTRON_DEPOT_TOOLS_WIN_TOOLCHAIN: '1'
    outputs:
      build-image-sha: ${{ inputs.build-image-sha }}
    steps:
    - name: Checkout Electron
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
      with:
        path: src/electron
        fetch-depth: 0
    - name: Checkout & Sync & Save
      uses: ./src/electron/.github/actions/checkout
      with:
        generate-sas-token: 'true'
        target-platform: win

  publish-x64-win:
    uses: ./.github/workflows/pipeline-segment-electron-build.yml
    needs: checkout-windows
    with:
      environment: production-release
      build-runs-on: electron-arc-windows-amd64-16core
      target-platform: win
      target-arch: x64
      is-release: true
      gn-build-type: release
      generate-symbols: true
      upload-to-storage: ${{ inputs.upload-to-storage }}
    secrets: inherit

  publish-arm64-win:
    uses: ./.github/workflows/pipeline-segment-electron-build.yml
    needs: checkout-windows
    with:
      environment: production-release
      build-runs-on: electron-arc-windows-amd64-16core
      target-platform: win
      target-arch: arm64
      is-release: true
      gn-build-type: release
      generate-symbols: true
      upload-to-storage: ${{ inputs.upload-to-storage }}
    secrets: inherit

  publish-x86-win:
    uses: ./.github/workflows/pipeline-segment-electron-build.yml
    needs: checkout-windows
    with:
      environment: production-release
      build-runs-on: electron-arc-windows-amd64-16core
      target-platform: win
      target-arch: x86
      is-release: true
      gn-build-type: release
      generate-symbols: true
      upload-to-storage: ${{ inputs.upload-to-storage }}
    secrets: inherit
",89,4,1,workflow_dispatch,5
yt-dlp/yt-dlp,build.yml,"name: Build Artifacts
on:
  workflow_call:
    inputs:
      version:
        required: true
        type: string
      channel:
        required: false
        default: stable
        type: string
      unix:
        default: true
        type: boolean
      linux_static:
        default: true
        type: boolean
      linux_arm:
        default: true
        type: boolean
      macos:
        default: true
        type: boolean
      macos_legacy:
        default: true
        type: boolean
      windows:
        default: true
        type: boolean
      windows32:
        default: true
        type: boolean
      origin:
        required: false
        default: ''
        type: string
    secrets:
      GPG_SIGNING_KEY:
        required: false

  workflow_dispatch:
    inputs:
      version:
        description: |
          VERSION: yyyy.mm.dd[.rev] or rev
        required: true
        type: string
      channel:
        description: |
          SOURCE of this build's updates: stable/nightly/master/<repo>
        required: true
        default: stable
        type: string
      unix:
        description: yt-dlp, yt-dlp.tar.gz
        default: true
        type: boolean
      linux_static:
        description: yt-dlp_linux
        default: true
        type: boolean
      linux_arm:
        description: yt-dlp_linux_aarch64, yt-dlp_linux_armv7l
        default: true
        type: boolean
      macos:
        description: yt-dlp_macos, yt-dlp_macos.zip
        default: true
        type: boolean
      macos_legacy:
        description: yt-dlp_macos_legacy
        default: true
        type: boolean
      windows:
        description: yt-dlp.exe, yt-dlp_win.zip
        default: true
        type: boolean
      windows32:
        description: yt-dlp_x86.exe
        default: true
        type: boolean
      origin:
        description: Origin
        required: false
        default: 'current repo'
        type: choice
        options:
        - 'current repo'

permissions:
  contents: read

jobs:
  process:
    runs-on: ubuntu-latest
    outputs:
      origin: ${{ steps.process_origin.outputs.origin }}
    steps:
      - name: Process origin
        id: process_origin
        run: |
          echo ""origin=${{ inputs.origin == 'current repo' && github.repository || inputs.origin }}"" | tee ""$GITHUB_OUTPUT""

  unix:
    needs: process
    if: inputs.unix
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Needed for changelog
      - uses: actions/setup-python@v5
        with:
          python-version: ""3.10""
      - name: Install Requirements
        run: |
          sudo apt -y install zip pandoc man sed
      - name: Prepare
        run: |
          python devscripts/update-version.py -c ""${{ inputs.channel }}"" -r ""${{ needs.process.outputs.origin }}"" ""${{ inputs.version }}""
          python devscripts/update_changelog.py -vv
          python devscripts/make_lazy_extractors.py
      - name: Build Unix platform-independent binary
        run: |
          make all tar
      - name: Verify --update-to
        if: vars.UPDATE_TO_VERIFICATION
        run: |
          chmod +x ./yt-dlp
          cp ./yt-dlp ./yt-dlp_downgraded
          version=""$(./yt-dlp --version)""
          ./yt-dlp_downgraded -v --update-to yt-dlp/yt-dlp@2023.03.04
          downgraded_version=""$(./yt-dlp_downgraded --version)""
          [[ ""$version"" != ""$downgraded_version"" ]]
      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-bin-${{ github.job }}
          path: |
            yt-dlp
            yt-dlp.tar.gz
          compression-level: 0

  linux_static:
    needs: process
    if: inputs.linux_static
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Build static executable
        env:
          channel: ${{ inputs.channel }}
          origin: ${{ needs.process.outputs.origin }}
          version: ${{ inputs.version }}
        run: |
          mkdir ~/build
          cd bundle/docker
          docker compose up --build static
          sudo chown ""${USER}:docker"" ~/build/yt-dlp_linux
      - name: Verify --update-to
        if: vars.UPDATE_TO_VERIFICATION
        run: |
          chmod +x ~/build/yt-dlp_linux
          cp ~/build/yt-dlp_linux ~/build/yt-dlp_linux_downgraded
          version=""$(~/build/yt-dlp_linux --version)""
          ~/build/yt-dlp_linux_downgraded -v --update-to yt-dlp/yt-dlp@2023.03.04
          downgraded_version=""$(~/build/yt-dlp_linux_downgraded --version)""
          [[ ""$version"" != ""$downgraded_version"" ]]
      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-bin-${{ github.job }}
          path: |
            ~/build/yt-dlp_linux
          compression-level: 0

  linux_arm:
    needs: process
    if: inputs.linux_arm
    permissions:
      contents: read
      packages: write # for creating cache
    runs-on: ubuntu-latest
    strategy:
      matrix:
        architecture:
          - armv7
          - aarch64

    steps:
      - uses: actions/checkout@v4
        with:
          path: ./repo
      - name: Virtualized Install, Prepare & Build
        uses: yt-dlp/run-on-arch-action@v3
        with:
          # Ref: https://github.com/uraimo/run-on-arch-action/issues/55
          env: |
            GITHUB_WORKFLOW: build
          githubToken: ${{ github.token }} # To cache image
          arch: ${{ matrix.architecture }}
          distro: ubuntu20.04 # Standalone executable should be built on minimum supported OS
          dockerRunArgs: --volume ""${PWD}/repo:/repo""
          install: | # Installing Python 3.10 from the Deadsnakes repo raises errors
            apt update
            apt -y install zlib1g-dev libffi-dev python3.9 python3.9-dev python3.9-distutils python3-pip \
              python3-secretstorage  # Cannot build cryptography wheel in virtual armv7 environment
            python3.9 -m pip install -U pip wheel 'setuptools>=71.0.2'
            # XXX: Keep this in sync with pyproject.toml (it can't be accessed at this stage) and exclude secretstorage
            python3.9 -m pip install -U Pyinstaller mutagen pycryptodomex brotli certifi cffi \
              'requests>=2.32.2,<3' 'urllib3>=1.26.17,<3' 'websockets>=13.0'

          run: |
            cd repo
            python3.9 devscripts/install_deps.py -o --include build
            python3.9 devscripts/install_deps.py --include pyinstaller  # Cached versions may be out of date
            python3.9 devscripts/update-version.py -c ""${{ inputs.channel }}"" -r ""${{ needs.process.outputs.origin }}"" ""${{ inputs.version }}""
            python3.9 devscripts/make_lazy_extractors.py
            python3.9 -m bundle.pyinstaller

            if ${{ vars.UPDATE_TO_VERIFICATION && 'true' || 'false' }}; then
              arch=""${{ (matrix.architecture == 'armv7' && 'armv7l') || matrix.architecture }}""
              chmod +x ./dist/yt-dlp_linux_${arch}
              cp ./dist/yt-dlp_linux_${arch} ./dist/yt-dlp_linux_${arch}_downgraded
              version=""$(./dist/yt-dlp_linux_${arch} --version)""
              ./dist/yt-dlp_linux_${arch}_downgraded -v --update-to yt-dlp/yt-dlp@2023.03.04
              downgraded_version=""$(./dist/yt-dlp_linux_${arch}_downgraded --version)""
              [[ ""$version"" != ""$downgraded_version"" ]]
            fi

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-bin-linux_${{ matrix.architecture }}
          path: | # run-on-arch-action designates armv7l as armv7
            repo/dist/yt-dlp_linux_${{ (matrix.architecture == 'armv7' && 'armv7l') || matrix.architecture }}
          compression-level: 0

  macos:
    needs: process
    if: inputs.macos
    permissions:
      contents: read
      actions: write  # For cleaning up cache
    runs-on: macos-13

    steps:
      - uses: actions/checkout@v4
      # NB: Building universal2 does not work with python from actions/setup-python

      - name: Restore cached requirements
        id: restore-cache
        uses: actions/cache/restore@v4
        env:
          SEGMENT_DOWNLOAD_TIMEOUT_MINS: 1
        with:
          path: |
            ~/yt-dlp-build-venv
          key: cache-reqs-${{ github.job }}-${{ github.ref }}

      - name: Install Requirements
        run: |
          brew install coreutils
          python3 -m venv ~/yt-dlp-build-venv
          source ~/yt-dlp-build-venv/bin/activate
          python3 devscripts/install_deps.py -o --include build
          python3 devscripts/install_deps.py --print --include pyinstaller > requirements.txt
          # We need to ignore wheels otherwise we break universal2 builds
          python3 -m pip install -U --no-binary :all: -r requirements.txt
          # We need to fuse our own universal2 wheels for curl_cffi
          python3 -m pip install -U 'delocate==0.11.0'
          mkdir curl_cffi_whls curl_cffi_universal2
          python3 devscripts/install_deps.py --print -o --include curl-cffi > requirements.txt
          for platform in ""macosx_11_0_arm64"" ""macosx_11_0_x86_64""; do
            python3 -m pip download \
              --only-binary=:all: \
              --platform ""${platform}"" \
              -d curl_cffi_whls \
              -r requirements.txt
          done
          ( # Overwrite x86_64-only libs with fat/universal2 libs or else Pyinstaller will do the opposite
            # See https://github.com/yt-dlp/yt-dlp/pull/10069
            cd curl_cffi_whls
            mkdir -p curl_cffi/.dylibs
            python_libdir=$(python3 -c 'import sys; from pathlib import Path; print(Path(sys.path[1]).parent)')
            for dylib in lib{ssl,crypto}.3.dylib; do
              cp ""${python_libdir}/${dylib}"" ""curl_cffi/.dylibs/${dylib}""
              for wheel in curl_cffi*macos*x86_64.whl; do
                zip ""${wheel}"" ""curl_cffi/.dylibs/${dylib}""
              done
            done
          )
          python3 -m delocate.cmd.delocate_fuse curl_cffi_whls/curl_cffi*.whl -w curl_cffi_universal2
          python3 -m delocate.cmd.delocate_fuse curl_cffi_whls/cffi*.whl -w curl_cffi_universal2
          for wheel in curl_cffi_universal2/*cffi*.whl; do
            mv -n -- ""${wheel}"" ""${wheel/x86_64/universal2}""
          done
          python3 -m pip install --force-reinstall -U curl_cffi_universal2/*cffi*.whl

      - name: Prepare
        run: |
          python3 devscripts/update-version.py -c ""${{ inputs.channel }}"" -r ""${{ needs.process.outputs.origin }}"" ""${{ inputs.version }}""
          python3 devscripts/make_lazy_extractors.py
      - name: Build
        run: |
          source ~/yt-dlp-build-venv/bin/activate
          python3 -m bundle.pyinstaller --target-architecture universal2 --onedir
          (cd ./dist/yt-dlp_macos && zip -r ../yt-dlp_macos.zip .)
          python3 -m bundle.pyinstaller --target-architecture universal2

      - name: Verify --update-to
        if: vars.UPDATE_TO_VERIFICATION
        run: |
          chmod +x ./dist/yt-dlp_macos
          cp ./dist/yt-dlp_macos ./dist/yt-dlp_macos_downgraded
          version=""$(./dist/yt-dlp_macos --version)""
          ./dist/yt-dlp_macos_downgraded -v --update-to yt-dlp/yt-dlp@2023.03.04
          downgraded_version=""$(./dist/yt-dlp_macos_downgraded --version)""
          [[ ""$version"" != ""$downgraded_version"" ]]

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-bin-${{ github.job }}
          path: |
            dist/yt-dlp_macos
            dist/yt-dlp_macos.zip
          compression-level: 0

      - name: Cleanup cache
        if: steps.restore-cache.outputs.cache-hit == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          cache_key: cache-reqs-${{ github.job }}-${{ github.ref }}
        run: |
          gh cache delete ""${cache_key}""

      - name: Cache requirements
        uses: actions/cache/save@v4
        with:
          path: |
            ~/yt-dlp-build-venv
          key: cache-reqs-${{ github.job }}-${{ github.ref }}

  macos_legacy:
    needs: process
    if: inputs.macos_legacy
    runs-on: macos-13

    steps:
      - uses: actions/checkout@v4
      - name: Install Python
        # We need the official Python, because the GA ones only support newer macOS versions
        env:
          PYTHON_VERSION: 3.10.5
          MACOSX_DEPLOYMENT_TARGET: 10.9 # Used up by the Python build tools
        run: |
          # Hack to get the latest patch version. Uncomment if needed
          #brew install python@3.10
          #export PYTHON_VERSION=$( $(brew --prefix)/opt/python@3.10/bin/python3 --version | cut -d ' ' -f 2 )
          curl ""https://www.python.org/ftp/python/${PYTHON_VERSION}/python-${PYTHON_VERSION}-macos11.pkg"" -o ""python.pkg""
          sudo installer -pkg python.pkg -target /
          python3 --version
      - name: Install Requirements
        run: |
          brew install coreutils
          python3 devscripts/install_deps.py --user -o --include build
          python3 devscripts/install_deps.py --user --include pyinstaller

      - name: Prepare
        run: |
          python3 devscripts/update-version.py -c ""${{ inputs.channel }}"" -r ""${{ needs.process.outputs.origin }}"" ""${{ inputs.version }}""
          python3 devscripts/make_lazy_extractors.py
      - name: Build
        run: |
          python3 -m bundle.pyinstaller
          mv dist/yt-dlp_macos dist/yt-dlp_macos_legacy

      - name: Verify --update-to
        if: vars.UPDATE_TO_VERIFICATION
        run: |
          chmod +x ./dist/yt-dlp_macos_legacy
          cp ./dist/yt-dlp_macos_legacy ./dist/yt-dlp_macos_legacy_downgraded
          version=""$(./dist/yt-dlp_macos_legacy --version)""
          ./dist/yt-dlp_macos_legacy_downgraded -v --update-to yt-dlp/yt-dlp@2023.03.04
          downgraded_version=""$(./dist/yt-dlp_macos_legacy_downgraded --version)""
          [[ ""$version"" != ""$downgraded_version"" ]]

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-bin-${{ github.job }}
          path: |
            dist/yt-dlp_macos_legacy
          compression-level: 0

  windows:
    needs: process
    if: inputs.windows
    runs-on: windows-latest

    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ""3.10""
      - name: Install Requirements
        run: | # Custom pyinstaller built with https://github.com/yt-dlp/pyinstaller-builds
          python devscripts/install_deps.py -o --include build
          python devscripts/install_deps.py --include curl-cffi
          python -m pip install -U ""https://yt-dlp.github.io/Pyinstaller-Builds/x86_64/pyinstaller-6.13.0-py3-none-any.whl""

      - name: Prepare
        run: |
          python devscripts/update-version.py -c ""${{ inputs.channel }}"" -r ""${{ needs.process.outputs.origin }}"" ""${{ inputs.version }}""
          python devscripts/make_lazy_extractors.py
      - name: Build
        run: |
          python -m bundle.pyinstaller
          python -m bundle.pyinstaller --onedir
          Compress-Archive -Path ./dist/yt-dlp/* -DestinationPath ./dist/yt-dlp_win.zip

      - name: Verify --update-to
        if: vars.UPDATE_TO_VERIFICATION
        run: |
          foreach ($name in @(""yt-dlp"")) {
            Copy-Item ""./dist/${name}.exe"" ""./dist/${name}_downgraded.exe""
            $version = & ""./dist/${name}.exe"" --version
            & ""./dist/${name}_downgraded.exe"" -v --update-to yt-dlp/yt-dlp@2023.03.04
            $downgraded_version = & ""./dist/${name}_downgraded.exe"" --version
            if ($version -eq $downgraded_version) {
              exit 1
            }
          }

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-bin-${{ github.job }}
          path: |
            dist/yt-dlp.exe
            dist/yt-dlp_win.zip
          compression-level: 0

  windows32:
    needs: process
    if: inputs.windows32
    runs-on: windows-latest

    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ""3.10""
          architecture: ""x86""
      - name: Install Requirements
        run: |
          python devscripts/install_deps.py -o --include build
          python devscripts/install_deps.py
          python -m pip install -U ""https://yt-dlp.github.io/Pyinstaller-Builds/i686/pyinstaller-6.13.0-py3-none-any.whl""

      - name: Prepare
        run: |
          python devscripts/update-version.py -c ""${{ inputs.channel }}"" -r ""${{ needs.process.outputs.origin }}"" ""${{ inputs.version }}""
          python devscripts/make_lazy_extractors.py
      - name: Build
        run: |
          python -m bundle.pyinstaller

      - name: Verify --update-to
        if: vars.UPDATE_TO_VERIFICATION
        run: |
          foreach ($name in @(""yt-dlp_x86"")) {
            Copy-Item ""./dist/${name}.exe"" ""./dist/${name}_downgraded.exe""
            $version = & ""./dist/${name}.exe"" --version
            & ""./dist/${name}_downgraded.exe"" -v --update-to yt-dlp/yt-dlp@2023.03.04
            $downgraded_version = & ""./dist/${name}_downgraded.exe"" --version
            if ($version -eq $downgraded_version) {
              exit 1
            }
          }

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-bin-${{ github.job }}
          path: |
            dist/yt-dlp_x86.exe
          compression-level: 0

  meta_files:
    if: always() && !cancelled()
    needs:
      - process
      - unix
      - linux_static
      - linux_arm
      - macos
      - macos_legacy
      - windows
      - windows32
    runs-on: ubuntu-latest
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifact
          pattern: build-bin-*
          merge-multiple: true

      - name: Make SHA2-SUMS files
        run: |
          cd ./artifact/
          # make sure SHA sums are also printed to stdout
          sha256sum -- * | tee ../SHA2-256SUMS
          sha512sum -- * | tee ../SHA2-512SUMS
          # also print as permanent annotations to the summary page
          while read -r shasum; do
            echo ""::notice title=${shasum##* }::sha256: ${shasum% *}""
          done < ../SHA2-256SUMS

      - name: Make Update spec
        run: |
          cat >> _update_spec << EOF
          # This file is used for regulating self-update
          lock 2022.08.18.36 .+ Python 3\.6
          lock 2023.11.16 (?!win_x86_exe).+ Python 3\.7
          lock 2023.11.16 win_x86_exe .+ Windows-(?:Vista|2008Server)
          lock 2024.10.22 py2exe .+
          lock 2024.10.22 linux_(?:armv7l|aarch64)_exe .+-glibc2\.(?:[12]?\d|30)\b
          lock 2024.10.22 (?!\w+_exe).+ Python 3\.8
          lock 2024.10.22 win(?:_x86)?_exe Python 3\.[78].+ Windows-(?:7-|2008ServerR2)
          lockV2 yt-dlp/yt-dlp 2022.08.18.36 .+ Python 3\.6
          lockV2 yt-dlp/yt-dlp 2023.11.16 (?!win_x86_exe).+ Python 3\.7
          lockV2 yt-dlp/yt-dlp 2023.11.16 win_x86_exe .+ Windows-(?:Vista|2008Server)
          lockV2 yt-dlp/yt-dlp 2024.10.22 py2exe .+
          lockV2 yt-dlp/yt-dlp 2024.10.22 linux_(?:armv7l|aarch64)_exe .+-glibc2\.(?:[12]?\d|30)\b
          lockV2 yt-dlp/yt-dlp 2024.10.22 (?!\w+_exe).+ Python 3\.8
          lockV2 yt-dlp/yt-dlp 2024.10.22 win(?:_x86)?_exe Python 3\.[78].+ Windows-(?:7-|2008ServerR2)
          lockV2 yt-dlp/yt-dlp-nightly-builds 2023.11.15.232826 (?!win_x86_exe).+ Python 3\.7
          lockV2 yt-dlp/yt-dlp-nightly-builds 2023.11.15.232826 win_x86_exe .+ Windows-(?:Vista|2008Server)
          lockV2 yt-dlp/yt-dlp-nightly-builds 2024.10.22.051025 py2exe .+
          lockV2 yt-dlp/yt-dlp-nightly-builds 2024.10.22.051025 linux_(?:armv7l|aarch64)_exe .+-glibc2\.(?:[12]?\d|30)\b
          lockV2 yt-dlp/yt-dlp-nightly-builds 2024.10.22.051025 (?!\w+_exe).+ Python 3\.8
          lockV2 yt-dlp/yt-dlp-nightly-builds 2024.10.22.051025 win(?:_x86)?_exe Python 3\.[78].+ Windows-(?:7-|2008ServerR2)
          lockV2 yt-dlp/yt-dlp-master-builds 2023.11.15.232812 (?!win_x86_exe).+ Python 3\.7
          lockV2 yt-dlp/yt-dlp-master-builds 2023.11.15.232812 win_x86_exe .+ Windows-(?:Vista|2008Server)
          lockV2 yt-dlp/yt-dlp-master-builds 2024.10.22.045052 py2exe .+
          lockV2 yt-dlp/yt-dlp-master-builds 2024.10.22.060347 linux_(?:armv7l|aarch64)_exe .+-glibc2\.(?:[12]?\d|30)\b
          lockV2 yt-dlp/yt-dlp-master-builds 2024.10.22.060347 (?!\w+_exe).+ Python 3\.8
          lockV2 yt-dlp/yt-dlp-master-builds 2024.10.22.060347 win(?:_x86)?_exe Python 3\.[78].+ Windows-(?:7-|2008ServerR2)
          EOF

      - name: Sign checksum files
        env:
          GPG_SIGNING_KEY: ${{ secrets.GPG_SIGNING_KEY }}
        if: env.GPG_SIGNING_KEY != ''
        run: |
          gpg --batch --import <<< ""${{ secrets.GPG_SIGNING_KEY }}""
          for signfile in ./SHA*SUMS; do
            gpg --batch --detach-sign ""$signfile""
          done

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-${{ github.job }}
          path: |
            _update_spec
            SHA*SUMS*
          compression-level: 0
          overwrite: true
",572,9,2,"workflow_call, workflow_dispatch",22
yt-dlp/yt-dlp,codeql.yml,"name: ""CodeQL""

on:
  push:
    branches: [ 'master', 'gh-pages', 'release' ]
  pull_request:
    # The branches below must be a subset of the branches above
    branches: [ 'master' ]
  schedule:
    - cron: '59 11 * * 5'

jobs:
  analyze:
    name: Analyze
    runs-on: ubuntu-latest
    permissions:
      actions: read
      contents: read
      security-events: write

    strategy:
      fail-fast: false
      matrix:
        language: [ 'python' ]
        # CodeQL supports [ 'cpp', 'csharp', 'go', 'java', 'javascript', 'python', 'ruby' ]
        # Use only 'java' to analyze code written in Java, Kotlin or both
        # Use only 'javascript' to analyze code written in JavaScript, TypeScript or both
        # Learn more about CodeQL language support at https://aka.ms/codeql-docs/language-support

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    # Initializes the CodeQL tools for scanning.
    - name: Initialize CodeQL
      uses: github/codeql-action/init@v3
      with:
        languages: ${{ matrix.language }}
        # If you wish to specify custom queries, you can do so here or in a config file.
        # By default, queries listed here will override any specified in a config file.
        # Prefix the list here with ""+"" to use these queries and those in the config file.

        # For more details on CodeQL's query packs, refer to: https://docs.github.com/en/code-security/code-scanning/automatically-scanning-your-code-for-vulnerabilities-and-errors/configuring-code-scanning#using-queries-in-ql-packs
        # queries: security-extended,security-and-quality


    # Autobuild attempts to build any compiled languages (C/C++, C#, Go, Java, or Swift).
    # If this step fails, then you should remove it and run the build manually (see below)
    - name: Autobuild
      uses: github/codeql-action/autobuild@v3

    # ℹ️ Command-line programs to run using the OS shell.
    # 📚 See https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#jobsjob_idstepsrun

    #   If the Autobuild fails above, remove it and uncomment the following three lines.
    #   modify them (or add more) to build your code if your project, please refer to the EXAMPLE below for guidance.

    # - run: |
    #     echo ""Run, Build Application using script""
    #     ./location_of_script_within_repo/buildscript.sh

    - name: Perform CodeQL Analysis
      uses: github/codeql-action/analyze@v3
      with:
        category: ""/language:${{matrix.language}}""
",65,1,3,"push, pull_request, schedule",4
yt-dlp/yt-dlp,core.yml,"name: Core Tests
on:
  push:
    paths:
      - .github/**
      - devscripts/**
      - test/**
      - yt_dlp/**.py
      - '!yt_dlp/extractor/**.py'
      - yt_dlp/extractor/__init__.py
      - yt_dlp/extractor/common.py
      - yt_dlp/extractor/extractors.py
  pull_request:
    paths:
      - .github/**
      - devscripts/**
      - test/**
      - yt_dlp/**.py
      - '!yt_dlp/extractor/**.py'
      - yt_dlp/extractor/__init__.py
      - yt_dlp/extractor/common.py
      - yt_dlp/extractor/extractors.py
permissions:
  contents: read

concurrency:
  group: core-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}

jobs:
  tests:
    name: Core Tests
    if: ""!contains(github.event.head_commit.message, 'ci skip')""
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest]
        # CPython 3.9 is in quick-test
        python-version: ['3.10', '3.11', '3.12', '3.13', pypy-3.10]
        include:
        # atleast one of each CPython/PyPy tests must be in windows
        - os: windows-latest
          python-version: '3.9'
        - os: windows-latest
          python-version: '3.10'
        - os: windows-latest
          python-version: '3.12'
        - os: windows-latest
          python-version: '3.13'
        - os: windows-latest
          python-version: pypy-3.10
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install test requirements
      run: python3 ./devscripts/install_deps.py --include test --include curl-cffi
    - name: Run tests
      timeout-minutes: 15
      continue-on-error: False
      run: |
        python3 -m yt_dlp -v || true  # Print debug head
        python3 ./devscripts/run_tests.py --pytest-args '--reruns 2 --reruns-delay 3.0' core
",66,1,2,"push, pull_request",2
yt-dlp/yt-dlp,download.yml,"name: Download Tests
on: [push, pull_request]
permissions:
  contents: read

jobs:
  quick:
    name: Quick Download Tests
    if: ""contains(github.event.head_commit.message, 'ci run dl')""
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: 3.9
    - name: Install test requirements
      run: python3 ./devscripts/install_deps.py --include dev
    - name: Run tests
      continue-on-error: true
      run: python3 ./devscripts/run_tests.py download

  full:
    name: Full Download Tests
    if: ""contains(github.event.head_commit.message, 'ci run dl all')""
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: true
      matrix:
        os: [ubuntu-latest]
        python-version: ['3.10', '3.11', '3.12', '3.13', pypy-3.10]
        include:
        # atleast one of each CPython/PyPy tests must be in windows
        - os: windows-latest
          python-version: '3.9'
        - os: windows-latest
          python-version: pypy-3.10
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install test requirements
      run: python3 ./devscripts/install_deps.py --include dev
    - name: Run tests
      continue-on-error: true
      run: python3 ./devscripts/run_tests.py download
",48,2,2,"push, pull_request",4
yt-dlp/yt-dlp,issue-lockdown.yml,"name: Issue Lockdown
on:
  issues:
    types: [opened]

permissions:
  issues: write

jobs:
  lockdown:
    name: Issue Lockdown
    if: vars.ISSUE_LOCKDOWN
    runs-on: ubuntu-latest
    steps:
      - name: ""Lock new issue""
        env:
          GH_TOKEN: ${{ github.token }}
          ISSUE_NUMBER: ${{ github.event.issue.number }}
          REPOSITORY: ${{ github.repository }}
        run: |
          gh issue lock ""${ISSUE_NUMBER}"" -R ""${REPOSITORY}""
",21,1,1,issues,0
yt-dlp/yt-dlp,quick-test.yml,"name: Quick Test
on: [push, pull_request]
permissions:
  contents: read

jobs:
  tests:
    name: Core Test
    if: ""!contains(github.event.head_commit.message, 'ci skip all')""
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.9
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'
    - name: Install test requirements
      run: python3 ./devscripts/install_deps.py -o --include test
    - name: Run tests
      timeout-minutes: 15
      run: |
        python3 -m yt_dlp -v || true
        python3 ./devscripts/run_tests.py --pytest-args '--reruns 2 --reruns-delay 3.0' core
  check:
    name: Code check
    if: ""!contains(github.event.head_commit.message, 'ci skip all')""
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - uses: actions/setup-python@v5
      with:
        python-version: '3.9'
    - name: Install dev dependencies
      run: python3 ./devscripts/install_deps.py -o --include static-analysis
    - name: Make lazy extractors
      run: python3 ./devscripts/make_lazy_extractors.py
    - name: Run ruff
      run: ruff check --output-format github .
    - name: Run autopep8
      run: autopep8 --diff .
    - name: Check file mode
      run: git ls-files --format=""%(objectmode) %(path)"" yt_dlp/ | ( ! grep -v ""^100644"" )
",42,2,2,"push, pull_request",4
yt-dlp/yt-dlp,release-master.yml,"name: Release (master)
on:
  push:
    branches:
      - master
    paths:
      - ""yt_dlp/**.py""
      - ""!yt_dlp/version.py""
      - ""bundle/*.py""
      - ""pyproject.toml""
      - ""Makefile""
      - "".github/workflows/build.yml""
concurrency:
  group: release-master
permissions:
  contents: read

jobs:
  release:
    if: vars.BUILD_MASTER != ''
    uses: ./.github/workflows/release.yml
    with:
      prerelease: true
      source: master
    permissions:
      contents: write
      packages: write  # For package cache
      actions: write  # For cleaning up cache
      id-token: write  # mandatory for trusted publishing
    secrets: inherit

  publish_pypi:
    needs: [release]
    if: vars.MASTER_PYPI_PROJECT != ''
    runs-on: ubuntu-latest
    permissions:
      id-token: write  # mandatory for trusted publishing
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: dist
          name: build-pypi
      - name: Publish to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          verbose: true
",47,2,1,push,3
yt-dlp/yt-dlp,release-nightly.yml,"name: Release (nightly)
on:
  schedule:
    - cron: '23 23 * * *'
permissions:
  contents: read

jobs:
  check_nightly:
    if: vars.BUILD_NIGHTLY != ''
    runs-on: ubuntu-latest
    outputs:
      commit: ${{ steps.check_for_new_commits.outputs.commit }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Check for new commits
        id: check_for_new_commits
        run: |
          relevant_files=(
            ""yt_dlp/*.py""
            ':!yt_dlp/version.py'
            ""bundle/*.py""
            ""pyproject.toml""
            ""Makefile""
            "".github/workflows/build.yml""
          )
          echo ""commit=$(git log --format=%H -1 --since=""24 hours ago"" -- ""${relevant_files[@]}"")"" | tee ""$GITHUB_OUTPUT""

  release:
    needs: [check_nightly]
    if: ${{ needs.check_nightly.outputs.commit }}
    uses: ./.github/workflows/release.yml
    with:
      prerelease: true
      source: nightly
    permissions:
      contents: write
      packages: write  # For package cache
      actions: write  # For cleaning up cache
      id-token: write  # mandatory for trusted publishing
    secrets: inherit

  publish_pypi:
    needs: [release]
    if: vars.NIGHTLY_PYPI_PROJECT != ''
    runs-on: ubuntu-latest
    permissions:
      id-token: write  # mandatory for trusted publishing
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: dist
          name: build-pypi
      - name: Publish to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          verbose: true
",60,3,1,schedule,4
yt-dlp/yt-dlp,release.yml,"name: Release
on:
  workflow_call:
    inputs:
      source:
        required: false
        default: ''
        type: string
      target:
        required: false
        default: ''
        type: string
      version:
        required: false
        default: ''
        type: string
      prerelease:
        required: false
        default: true
        type: boolean
  workflow_dispatch:
    inputs:
      source:
        description: |
          SOURCE of this release's updates:
          channel, repo, tag, or channel/repo@tag
          (default: <current_repo>)
        required: false
        default: ''
        type: string
      target:
        description: |
          TARGET to publish this release to:
          channel, tag, or channel@tag
          (default: <source> if writable else <current_repo>[@source_tag])
        required: false
        default: ''
        type: string
      version:
        description: |
          VERSION: yyyy.mm.dd[.rev] or rev
          (default: auto-generated)
        required: false
        default: ''
        type: string
      prerelease:
        description: Pre-release
        default: false
        type: boolean

permissions:
  contents: read

jobs:
  prepare:
    permissions:
      contents: write
    runs-on: ubuntu-latest
    outputs:
      channel: ${{ steps.setup_variables.outputs.channel }}
      version: ${{ steps.setup_variables.outputs.version }}
      target_repo: ${{ steps.setup_variables.outputs.target_repo }}
      target_repo_token: ${{ steps.setup_variables.outputs.target_repo_token }}
      target_tag: ${{ steps.setup_variables.outputs.target_tag }}
      pypi_project: ${{ steps.setup_variables.outputs.pypi_project }}
      pypi_suffix: ${{ steps.setup_variables.outputs.pypi_suffix }}
      head_sha: ${{ steps.get_target.outputs.head_sha }}

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - uses: actions/setup-python@v5
        with:
          python-version: ""3.10""

      - name: Process inputs
        id: process_inputs
        run: |
          cat << EOF
          ::group::Inputs
          prerelease=${{ inputs.prerelease }}
          source=${{ inputs.source }}
          target=${{ inputs.target }}
          version=${{ inputs.version }}
          ::endgroup::
          EOF
          IFS='@' read -r source_repo source_tag <<<""${{ inputs.source }}""
          IFS='@' read -r target_repo target_tag <<<""${{ inputs.target }}""
          cat << EOF >> ""$GITHUB_OUTPUT""
          source_repo=${source_repo}
          source_tag=${source_tag}
          target_repo=${target_repo}
          target_tag=${target_tag}
          EOF

      - name: Setup variables
        id: setup_variables
        env:
          source_repo: ${{ steps.process_inputs.outputs.source_repo }}
          source_tag: ${{ steps.process_inputs.outputs.source_tag }}
          target_repo: ${{ steps.process_inputs.outputs.target_repo }}
          target_tag: ${{ steps.process_inputs.outputs.target_tag }}
        run: |
          # unholy bash monstrosity (sincere apologies)
          fallback_token () {
            if ${{ !secrets.ARCHIVE_REPO_TOKEN }}; then
              echo ""::error::Repository access secret ${target_repo_token^^} not found""
              exit 1
            fi
            target_repo_token=ARCHIVE_REPO_TOKEN
            return 0
          }

          source_is_channel=0
          [[ ""${source_repo}"" == 'stable' ]] && source_repo='yt-dlp/yt-dlp'
          if [[ -z ""${source_repo}"" ]]; then
            source_repo='${{ github.repository }}'
          elif [[ '${{ vars[format('{0}_archive_repo', env.source_repo)] }}' ]]; then
            source_is_channel=1
            source_channel='${{ vars[format('{0}_archive_repo', env.source_repo)] }}'
          elif [[ -z ""${source_tag}"" && ""${source_repo}"" != */* ]]; then
            source_tag=""${source_repo}""
            source_repo='${{ github.repository }}'
          fi
          resolved_source=""${source_repo}""
          if [[ ""${source_tag}"" ]]; then
            resolved_source=""${resolved_source}@${source_tag}""
          elif [[ ""${source_repo}"" == 'yt-dlp/yt-dlp' ]]; then
            resolved_source='stable'
          fi

          revision=""${{ (inputs.prerelease || !vars.PUSH_VERSION_COMMIT) && '$(date -u +""%H%M%S"")' || '' }}""
          version=""$(
            python devscripts/update-version.py \
            -c ""${resolved_source}"" -r ""${{ github.repository }}"" ${{ inputs.version || '$revision' }} | \
            grep -Po ""version=\K\d+\.\d+\.\d+(\.\d+)?"")""

          if [[ ""${target_repo}"" ]]; then
            if [[ -z ""${target_tag}"" ]]; then
              if [[ '${{ vars[format('{0}_archive_repo', env.target_repo)] }}' ]]; then
                target_tag=""${source_tag:-${version}}""
              else
                target_tag=""${target_repo}""
                target_repo='${{ github.repository }}'
              fi
            fi
            if [[ ""${target_repo}"" != '${{ github.repository}}' ]]; then
              target_repo='${{ vars[format('{0}_archive_repo', env.target_repo)] }}'
              target_repo_token='${{ env.target_repo }}_archive_repo_token'
              ${{ !!secrets[format('{0}_archive_repo_token', env.target_repo)] }} || fallback_token
              pypi_project='${{ vars[format('{0}_pypi_project', env.target_repo)] }}'
              pypi_suffix='${{ vars[format('{0}_pypi_suffix', env.target_repo)] }}'
            fi
          else
            target_tag=""${source_tag:-${version}}""
            if ((source_is_channel)); then
              target_repo=""${source_channel}""
              target_repo_token='${{ env.source_repo }}_archive_repo_token'
              ${{ !!secrets[format('{0}_archive_repo_token', env.source_repo)] }} || fallback_token
              pypi_project='${{ vars[format('{0}_pypi_project', env.source_repo)] }}'
              pypi_suffix='${{ vars[format('{0}_pypi_suffix', env.source_repo)] }}'
            else
              target_repo='${{ github.repository }}'
            fi
          fi

          if [[ ""${target_repo}"" == '${{ github.repository }}' ]] && ${{ !inputs.prerelease }}; then
            pypi_project='${{ vars.PYPI_PROJECT }}'
          fi

          echo ""::group::Output variables""
          cat << EOF | tee -a ""$GITHUB_OUTPUT""
          channel=${resolved_source}
          version=${version}
          target_repo=${target_repo}
          target_repo_token=${target_repo_token}
          target_tag=${target_tag}
          pypi_project=${pypi_project}
          pypi_suffix=${pypi_suffix}
          EOF
          echo ""::endgroup::""

      - name: Update documentation
        env:
          version: ${{ steps.setup_variables.outputs.version }}
          target_repo: ${{ steps.setup_variables.outputs.target_repo }}
        if: |
          !inputs.prerelease && env.target_repo == github.repository
        run: |
          python devscripts/update_changelog.py -vv
          make doc

      - name: Push to release
        id: push_release
        env:
          version: ${{ steps.setup_variables.outputs.version }}
          target_repo: ${{ steps.setup_variables.outputs.target_repo }}
        if: |
          !inputs.prerelease && env.target_repo == github.repository
        run: |
          git config --global user.name ""github-actions[bot]""
          git config --global user.email ""41898282+github-actions[bot]@users.noreply.github.com""
          git add -u
          git commit -m ""Release ${{ env.version }}"" \
            -m ""Created by: ${{ github.event.sender.login }}"" -m "":ci skip all""
          git push origin --force ${{ github.event.ref }}:release

      - name: Get target commitish
        id: get_target
        run: |
          echo ""head_sha=$(git rev-parse HEAD)"" >> ""$GITHUB_OUTPUT""

      - name: Update master
        env:
          target_repo: ${{ steps.setup_variables.outputs.target_repo }}
        if: |
          vars.PUSH_VERSION_COMMIT != '' && !inputs.prerelease && env.target_repo == github.repository
        run: git push origin ${{ github.event.ref }}

  build:
    needs: prepare
    uses: ./.github/workflows/build.yml
    with:
      version: ${{ needs.prepare.outputs.version }}
      channel: ${{ needs.prepare.outputs.channel }}
      origin: ${{ needs.prepare.outputs.target_repo }}
    permissions:
      contents: read
      packages: write  # For package cache
      actions: write  # For cleaning up cache
    secrets:
      GPG_SIGNING_KEY: ${{ secrets.GPG_SIGNING_KEY }}

  publish_pypi:
    needs: [prepare, build]
    if: ${{ needs.prepare.outputs.pypi_project }}
    runs-on: ubuntu-latest
    permissions:
      id-token: write  # mandatory for trusted publishing

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - uses: actions/setup-python@v5
        with:
          python-version: ""3.10""

      - name: Install Requirements
        run: |
          sudo apt -y install pandoc man
          python devscripts/install_deps.py -o --include build

      - name: Prepare
        env:
          version: ${{ needs.prepare.outputs.version }}
          suffix: ${{ needs.prepare.outputs.pypi_suffix }}
          channel: ${{ needs.prepare.outputs.channel }}
          target_repo: ${{ needs.prepare.outputs.target_repo }}
          pypi_project: ${{ needs.prepare.outputs.pypi_project }}
        run: |
          python devscripts/update-version.py -c ""${{ env.channel }}"" -r ""${{ env.target_repo }}"" -s ""${{ env.suffix }}"" ""${{ env.version }}""
          python devscripts/update_changelog.py -vv
          python devscripts/make_lazy_extractors.py
          sed -i -E '0,/(name = "")[^""]+("")/s//\1${{ env.pypi_project }}\2/' pyproject.toml

      - name: Build
        run: |
          rm -rf dist/*
          make pypi-files
          printf '%s\n\n' \
            'Official repository: <https://github.com/yt-dlp/yt-dlp>' \
            '**PS**: Some links in this document will not work since this is a copy of the README.md from Github' > ./README.md.new
          cat ./README.md >> ./README.md.new && mv -f ./README.md.new ./README.md
          python devscripts/set-variant.py pip -M ""You installed yt-dlp with pip or using the wheel from PyPi; Use that to update""
          make clean-cache
          python -m build --no-isolation .

      - name: Upload artifacts
        if: github.event_name != 'workflow_dispatch'
        uses: actions/upload-artifact@v4
        with:
          name: build-pypi
          path: |
            dist/*
          compression-level: 0

      - name: Publish to PyPI
        if: github.event_name == 'workflow_dispatch'
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          verbose: true

  publish:
    needs: [prepare, build]
    permissions:
      contents: write
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - uses: actions/download-artifact@v4
        with:
          path: artifact
          pattern: build-*
          merge-multiple: true
      - uses: actions/setup-python@v5
        with:
          python-version: ""3.10""

      - name: Generate release notes
        env:
          head_sha: ${{ needs.prepare.outputs.head_sha }}
          target_repo: ${{ needs.prepare.outputs.target_repo }}
          target_tag: ${{ needs.prepare.outputs.target_tag }}
        run: |
          printf '%s' \
            '[![Installation](https://img.shields.io/badge/-Which%20file%20to%20download%3F-white.svg?style=for-the-badge)]' \
              '(https://github.com/${{ github.repository }}#installation ""Installation instructions"") ' \
            '[![Discord](https://img.shields.io/discord/807245652072857610?color=blue&labelColor=555555&label=&logo=discord&style=for-the-badge)]' \
              '(https://discord.gg/H5MNcFW63r ""Discord"") ' \
            '[![Donate](https://img.shields.io/badge/_-Donate-red.svg?logo=githubsponsors&labelColor=555555&style=for-the-badge)]' \
              '(https://github.com/yt-dlp/yt-dlp/blob/master/Collaborators.md#collaborators ""Donate"") ' \
            '[![Documentation](https://img.shields.io/badge/-Docs-brightgreen.svg?style=for-the-badge&logo=GitBook&labelColor=555555)]' \
              '(https://github.com/${{ github.repository }}' \
              '${{ env.target_repo == github.repository && format('/tree/{0}', env.target_tag) || '' }}#readme ""Documentation"") ' \
            ${{ env.target_repo == 'yt-dlp/yt-dlp' && '\
              ""[![Nightly](https://img.shields.io/badge/Nightly%20builds-purple.svg?style=for-the-badge)]"" \
              ""(https://github.com/yt-dlp/yt-dlp-nightly-builds/releases/latest \""Nightly builds\"") "" \
              ""[![Master](https://img.shields.io/badge/Master%20builds-lightblue.svg?style=for-the-badge)]"" \
              ""(https://github.com/yt-dlp/yt-dlp-master-builds/releases/latest \""Master builds\"")""' || '' }} > ./RELEASE_NOTES
          printf '\n\n' >> ./RELEASE_NOTES
          cat >> ./RELEASE_NOTES << EOF
          #### A description of the various files is in the [README](https://github.com/${{ github.repository }}#release-files)
          ---
          $(python ./devscripts/make_changelog.py -vv --collapsible)
          EOF
          printf '%s\n\n' '**This is a pre-release build**' >> ./PRERELEASE_NOTES
          cat ./RELEASE_NOTES >> ./PRERELEASE_NOTES
          printf '%s\n\n' 'Generated from: https://github.com/${{ github.repository }}/commit/${{ env.head_sha }}' >> ./ARCHIVE_NOTES
          cat ./RELEASE_NOTES >> ./ARCHIVE_NOTES

      - name: Publish to archive repo
        env:
          GH_TOKEN: ${{ secrets[needs.prepare.outputs.target_repo_token] }}
          GH_REPO: ${{ needs.prepare.outputs.target_repo }}
          version: ${{ needs.prepare.outputs.version }}
          channel: ${{ needs.prepare.outputs.channel }}
        if: |
          inputs.prerelease && env.GH_TOKEN != '' && env.GH_REPO != '' && env.GH_REPO != github.repository
        run: |
          title=""${{ startswith(env.GH_REPO, 'yt-dlp/') && 'yt-dlp ' || '' }}${{ env.channel }}""
          gh release create \
            --notes-file ARCHIVE_NOTES \
            --title ""${title} ${{ env.version }}"" \
            ${{ env.version }} \
            artifact/*

      - name: Prune old release
        env:
          GH_TOKEN: ${{ github.token }}
          version: ${{ needs.prepare.outputs.version }}
          target_repo: ${{ needs.prepare.outputs.target_repo }}
          target_tag: ${{ needs.prepare.outputs.target_tag }}
        if: |
          env.target_repo == github.repository && env.target_tag != env.version
        run: |
          gh release delete --yes --cleanup-tag ""${{ env.target_tag }}"" || true
          git tag --delete ""${{ env.target_tag }}"" || true
          sleep 5  # Enough time to cover deletion race condition

      - name: Publish release
        env:
          GH_TOKEN: ${{ github.token }}
          version: ${{ needs.prepare.outputs.version }}
          target_repo: ${{ needs.prepare.outputs.target_repo }}
          target_tag: ${{ needs.prepare.outputs.target_tag }}
          head_sha: ${{ needs.prepare.outputs.head_sha }}
        if: |
          env.target_repo == github.repository
        run: |
          title=""${{ github.repository == 'yt-dlp/yt-dlp' && 'yt-dlp ' || '' }}""
          title+=""${{ env.target_tag != env.version && format('{0} ', env.target_tag) || '' }}""
          gh release create \
            --notes-file ${{ inputs.prerelease && 'PRERELEASE_NOTES' || 'RELEASE_NOTES' }} \
            --target ${{ env.head_sha }} \
            --title ""${title}${{ env.version }}"" \
            ${{ inputs.prerelease && '--prerelease' || '' }} \
            ${{ env.target_tag }} \
            artifact/*
",394,4,2,"workflow_call, workflow_dispatch",10
yt-dlp/yt-dlp,sanitize-comment.yml,"name: Sanitize comment

on:
  issue_comment:
    types: [created, edited]

permissions:
  issues: write

jobs:
  sanitize-comment:
    name: Sanitize comment
    if: vars.SANITIZE_COMMENT && !github.event.issue.pull_request
    runs-on: ubuntu-latest
    steps:
      - name: Sanitize comment
        uses: yt-dlp/sanitize-comment@v1
",17,1,1,issue_comment,1
krahets/hello-algo,c.yml,"# This starter workflow is for a CMake project running on multiple platforms. There is a different starter workflow if you just want a single platform.
# See: https://github.com/actions/starter-workflows/blob/main/ci/cmake-single-platform.yml

name: C

on:
  push:
    branches: [""main""]
    paths: [""codes/c/**/*.c"", ""codes/c/**/*.h""]
  pull_request:
    branches: [""main""]
    paths: [""codes/c/**/*.c"", ""codes/c/**/*.h""]
  workflow_dispatch:

jobs:
  build:
    runs-on: ${{ matrix.os }}

    strategy:
      # Set fail-fast to false to ensure that feedback is delivered for all matrix combinations. Consider changing this to true when your workflow is stable.
      fail-fast: true

      # Set up a matrix to run the following 3 configurations:
      # 1. <Windows, Release, latest MSVC compiler toolchain on the default runner image, default generator>
      # 2. <Linux, Release, latest GCC compiler toolchain on the default runner image, default generator>
      # 3. <Linux, Release, latest Clang compiler toolchain on the default runner image, default generator>
      #
      # To add more build types (Release, Debug, RelWithDebInfo, etc.) customize the build_type list.
      matrix:
        os: [ubuntu-latest, windows-latest]
        build_type: [Release]
        c_compiler: [gcc, clang, cl]
        include:
          - os: windows-latest
            c_compiler: cl
            cpp_compiler: cl
          - os: ubuntu-latest
            c_compiler: gcc
            cpp_compiler: g++
          - os: ubuntu-latest
            c_compiler: clang
            cpp_compiler: clang++
        exclude:
          - os: windows-latest
            c_compiler: gcc
          - os: windows-latest
            c_compiler: clang
          - os: ubuntu-latest
            c_compiler: cl

    steps:
      - uses: actions/checkout@v4

      - name: Configure CMake
        # Configure CMake in a 'build' subdirectory. `CMAKE_BUILD_TYPE` is only required if you are using a single-configuration generator such as make.
        # See https://cmake.org/cmake/help/latest/variable/CMAKE_BUILD_TYPE.html?highlight=cmake_build_type
        run: >
          cmake -B ${{ github.workspace }}/codes/c/build
          -DCMAKE_CXX_COMPILER=${{ matrix.cpp_compiler }}
          -DCMAKE_C_COMPILER=${{ matrix.c_compiler }}
          -DCMAKE_BUILD_TYPE=${{ matrix.build_type }}
          -S ${{ github.workspace }}/codes/c

      - name: Build
        # Build your program with the given configuration. Note that --config is needed because the default Windows generator is a multi-config generator (Visual Studio generator).
        run: cmake --build ${{ github.workspace }}/codes/c/build --config ${{ matrix.build_type }}

      - name: Test
        working-directory: ${{ github.workspace }}/codes/c/build
        # Execute tests defined by the CMake configuration. Note that --build-config is needed because the default Windows generator is a multi-config generator (Visual Studio generator).
        # See https://cmake.org/cmake/help/latest/manual/ctest.1.html for more detail
        run: ctest --build-config ${{ matrix.build_type }}
",72,1,3,"push, pull_request, workflow_dispatch",1
krahets/hello-algo,cpp.yml,"# This starter workflow is for a CMake project running on multiple platforms. There is a different starter workflow if you just want a single platform.
# See: https://github.com/actions/starter-workflows/blob/main/ci/cmake-single-platform.yml

name: C++

on:
  push:
    branches: [""main""]
    paths: [""codes/cpp/**/*.cpp"", ""codes/cpp/**/*.hpp""]
  pull_request:
    branches: [""main""]
    paths: [""codes/cpp/**/*.cpp"", ""codes/cpp/**/*.hpp""]
  workflow_dispatch:

jobs:
  build:
    runs-on: ${{ matrix.os }}

    strategy:
      # Set fail-fast to false to ensure that feedback is delivered for all matrix combinations. Consider changing this to true when your workflow is stable.
      fail-fast: true

      # Set up a matrix to run the following 3 configurations:
      # 1. <Windows, Release, latest MSVC compiler toolchain on the default runner image, default generator>
      # 2. <Linux, Release, latest GCC compiler toolchain on the default runner image, default generator>
      # 3. <Linux, Release, latest Clang compiler toolchain on the default runner image, default generator>
      #
      # To add more build types (Release, Debug, RelWithDebInfo, etc.) customize the build_type list.
      matrix:
        os: [ubuntu-latest, windows-latest]
        build_type: [Release]
        c_compiler: [gcc, clang, cl]
        include:
          - os: windows-latest
            c_compiler: cl
            cpp_compiler: cl
          - os: ubuntu-latest
            c_compiler: gcc
            cpp_compiler: g++
          - os: ubuntu-latest
            c_compiler: clang
            cpp_compiler: clang++
        exclude:
          - os: windows-latest
            c_compiler: gcc
          - os: windows-latest
            c_compiler: clang
          - os: ubuntu-latest
            c_compiler: cl

    steps:
      - uses: actions/checkout@v4

      - name: Configure CMake
        # Configure CMake in a 'build' subdirectory. `CMAKE_BUILD_TYPE` is only required if you are using a single-configuration generator such as make.
        # See https://cmake.org/cmake/help/latest/variable/CMAKE_BUILD_TYPE.html?highlight=cmake_build_type
        run: >
          cmake -B ${{ github.workspace }}/codes/cpp/build
          -DCMAKE_CXX_COMPILER=${{ matrix.cpp_compiler }}
          -DCMAKE_C_COMPILER=${{ matrix.c_compiler }}
          -DCMAKE_BUILD_TYPE=${{ matrix.build_type }}
          -S ${{ github.workspace }}/codes/cpp

      - name: Build
        # Build your program with the given configuration. Note that --config is needed because the default Windows generator is a multi-config generator (Visual Studio generator).
        run: cmake --build ${{ github.workspace }}/codes/cpp/build --config ${{ matrix.build_type }}

      - name: Test
        working-directory: ${{ github.workspace }}/codes/cpp/build
        # Execute tests defined by the CMake configuration. Note that --build-config is needed because the default Windows generator is a multi-config generator (Visual Studio generator).
        # See https://cmake.org/cmake/help/latest/manual/ctest.1.html for more detail
        run: ctest --build-config ${{ matrix.build_type }}
",72,1,3,"push, pull_request, workflow_dispatch",1
krahets/hello-algo,dart.yml,"# This workflow will install Dart SDK, run format, analyze and build with Dart

name: Dart

on:
  push:
    branches: [""main""]
    paths: [""codes/dart/**/*.dart""]
  pull_request:
    branches: [""main""]
    paths: [""codes/dart/**/*.dart""]
  workflow_dispatch:

permissions:
  contents: read

jobs:
  build:
    name: Dart ${{ matrix.dart-sdk }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        dart-sdk: [stable]
    steps:
      - uses: actions/checkout@v4
      - name: Set up Dart ${{ matrix.dart-sdk }}
        uses: dart-lang/setup-dart@v1
        with:
          sdk: ${{ matrix.dart-sdk}}
      - name: Run format
        run: dart format codes/dart
      - name: Run analyze
        run: dart analyze codes/dart
      - name: Run build
        run: dart codes/dart/build.dart
",36,1,3,"push, pull_request, workflow_dispatch",2
krahets/hello-algo,dotnet.yml,"# This workflow will build a .NET project
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-net

name: .NET

on:
  push:
    branches: [ ""main"" ]
    paths: [""codes/csharp/**/*.cs""]
  pull_request:
    branches: [ ""main"" ]
    paths: [""codes/csharp/**/*.cs""]
  workflow_dispatch:

jobs:
  build:
    name: .NET ${{ matrix.dotnet-version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    defaults:
      run:
        working-directory: codes/csharp/
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        dotnet-version: [""8.0.x""]

    steps:
    - uses: actions/checkout@v4

    - name: Setup .NET ${{ matrix.dotnet-version }}
      uses: actions/setup-dotnet@v3
      with:
        dotnet-version: ${{ matrix.dotnet-version }}
    - name: Restore dependencies
      run: dotnet restore hello-algo.csproj
    - name: Build
      run: dotnet build --no-restore hello-algo.csproj
    - name: Test with dotnet
      run: dotnet test hello-algo.csproj
",39,1,3,"push, pull_request, workflow_dispatch",2
krahets/hello-algo,go.yml,"name: Go

on:
  push:
    branches: [ ""main"" ]
    paths: [""codes/go/**/*.go""]
  pull_request:
    branches: [ ""main"" ]
    paths: [""codes/go/**/*.go""]
  workflow_dispatch:

jobs:
  build:
    name: Go ${{ matrix.go-version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    defaults:
      run:
        working-directory: codes/go/
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        go-version: [""1.19.x""]

    steps:
    - uses: actions/checkout@v4

    - name: Setup Go ${{ matrix.go-version }}
      uses: actions/setup-go@v3
      with:
        go-version: ${{ matrix.go-version }}
    - name: Check out code into the Go module directory
      run: go get -v -t -d ./...
    - name: Build
      run: go build -v ./...
    - name: Test with Go
      run: go test -v ./...
",36,1,3,"push, pull_request, workflow_dispatch",2
krahets/hello-algo,java.yml,"# # This workflow will install OpenJDK and build the Java project
# For more information see: https://github.com/actions/setup-java

name: Java

on:
  push:
    branches: [ ""main"" ]
    paths: [""codes/java/**/*.java""]
  pull_request:
    branches: [ ""main"" ]
    paths: [""codes/java/**/*.java""]
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-20.04
    strategy:
      matrix:
        java: [ '11', '17' ]
    name: Java ${{ matrix.Java }} sample
    steps:
      - uses: actions/checkout@v4
      - name: Setup java
        uses: actions/setup-java@v3
        with:
          distribution: 'temurin'
          java-version: ${{ matrix.java }}
      - run: javac -d codes/java/build codes/java/**/*.java
",29,1,3,"push, pull_request, workflow_dispatch",2
krahets/hello-algo,javascript.yml,"name: JavaScript

on:
  push:
    branches: ['main']
    paths: ['codes/javascript/**/*.js']
  pull_request:
    branches: ['main']
    paths: ['codes/javascript/**/*.js']
  workflow_dispatch:

jobs:
  build:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 20.x
      - uses: denoland/setup-deno@v1
        with:
          deno-version: v1.x
      - name: Run JavaScript Code
        run: deno run -A codes/javascript/test_all.js
",27,1,3,"push, pull_request, workflow_dispatch",3
krahets/hello-algo,kotlin.yml,"name: Kotlin

on:
  push:
    branches: [ ""main"" ]
    paths: [""codes/kotlin/**/*.kt""]
  pull_request:
    branches: [ ""main"" ]
    paths: [""codes/kotlin/**/*.kt""]
  workflow_dispatch:

jobs:
  build:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ ubuntu-latest, macos-latest ]

    name: Kotlin on ${{ matrix.os }}
    steps:
      - uses: actions/checkout@v4.1.2

      - name: Build JAR
        run: kotlinc codes/kotlin/**/*.kt -include-runtime -d codes/kotlin/build/test.jar
",24,1,3,"push, pull_request, workflow_dispatch",1
krahets/hello-algo,python.yml,"# This workflow will install Python dependencies, run tests and lint with Python
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python

name: Python

on:
  push:
    branches: [""main""]
    paths: [""codes/python/**/*.py""]
  pull_request:
    branches: [""main""]
    paths: [""codes/python/**/*.py""]
  workflow_dispatch:

permissions:
  contents: read

jobs:
  build:
    name: Python ${{ matrix.python-version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: [""3.10"", ""3.11""]
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install black
      - name: Lint with black
        run: |
          black codes/python
      - name: Test python code
        run: |
          python codes/python/test_all.py
",41,1,3,"push, pull_request, workflow_dispatch",2
krahets/hello-algo,ruby.yml,"# This workflow uses actions that are not certified by GitHub.
# They are provided by a third-party and are governed by separate terms of service, privacy policy, and support documentation.
# This workflow will download a prebuilt Ruby version, install dependencies and run tests with Rake。
# For more information see: https://github.com/marketplace/actions/setup-ruby-jruby-and-truffleruby

name: Ruby

on:
  push:
    branches: [ ""main"" ]
    paths: [""codes/ruby/**/*.rb""]
  pull_request:
    branches: [ ""main"" ]
    paths: [""codes/ruby/**/*.rb""]
  workflow_dispatch:

permissions:
  contents: read

jobs:
  test:

    name: Ruby ${{ matrix.ruby-version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        ruby-version: ['3.3']

    steps:
    - uses: actions/checkout@v4
    - name: Set up Ruby
      uses: ruby/setup-ruby@v1
      with:
        ruby-version: ${{ matrix.ruby-version }}
    - name: Run tests
      run: ruby codes/ruby/test_all.rb
",37,1,3,"push, pull_request, workflow_dispatch",2
krahets/hello-algo,rust.yml,"name: Rust

on:
  push:
    branches: [""main""]
    paths: [""codes/rust/**/*.rs"", ""codes/rust/Cargo.toml""]
  pull_request:
    branches: [""main""]
    paths: [""codes/rust/**/*.rs"", ""codes/rust/Cargo.toml""]

jobs:
  build:
    runs-on: ${{ matrix.os }}

    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]

    steps:
      - uses: brndnmtthws/rust-action-rustup@v1
        with:
            toolchain: nightly
      
      - uses: actions/checkout@v4
      
      - name: Build
        run: cargo build --manifest-path=codes/rust/Cargo.toml && cargo build --manifest-path=codes/rust/Cargo.toml --release
",27,1,2,"push, pull_request",2
krahets/hello-algo,swift.yml,"# This workflow will build a Swift project
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-swift

name: Swift

on:
  push:
    branches: [""main""]
    paths: [""codes/swift/**/*.swift""]
  pull_request:
    branches: [""main""]
    paths: [""codes/swift/**/*.swift""]
  workflow_dispatch:

jobs:
  build:
    name: Swift on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [""ubuntu-22.04"", ""macos-14""]
    steps:
      - uses: actions/checkout@v4
      - name: Build
        run: swift build --package-path codes/swift
",25,1,3,"push, pull_request, workflow_dispatch",1
krahets/hello-algo,typescript.yml,"name: TypeScript

on:
  push:
    branches: ['main']
    paths: ['codes/typescript/**/*.ts']
  pull_request:
    branches: ['main']
    paths: ['codes/typescript/**/*.ts']
  workflow_dispatch:

jobs:
  build:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 20.x
      - name: Install dependencies
        run: cd codes/typescript && npm install
      - name: Check TypeScript code
        run: cd codes/typescript && npm run check
",26,1,3,"push, pull_request, workflow_dispatch",2
n8n-io/n8n,benchmark-destroy-nightly.yml,"name: Destroy Benchmark Env

on:
  schedule:
    - cron: '0 5 * * *'
  workflow_dispatch:

permissions:
  id-token: write
  contents: read

concurrency:
  group: benchmark
  cancel-in-progress: false

jobs:
  build:
    runs-on: ubuntu-latest
    environment: benchmarking

    steps:
      - name: Checkout
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1

      - name: Azure login
        uses: azure/login@6c251865b4e6290e7b78be643ea2d005bc51f69a # v2.1.1
        with:
          client-id: ${{ secrets.BENCHMARK_ARM_CLIENT_ID }}
          tenant-id: ${{ secrets.BENCHMARK_ARM_TENANT_ID }}
          subscription-id: ${{ secrets.BENCHMARK_ARM_SUBSCRIPTION_ID }}

      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 22.x

      - name: Setup corepack and pnpm
        run: |
          npm i -g corepack@0.33
          corepack enable

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Destroy cloud env
        run: pnpm destroy-cloud-env
        working-directory: packages/@n8n/benchmark
",46,1,2,"schedule, workflow_dispatch",3
n8n-io/n8n,benchmark-nightly.yml,"name: Run Nightly Benchmark
run-name: Benchmark ${{ inputs.n8n_tag || 'nightly' }}

on:
  schedule:
    - cron: '30 1,2,3 * * *'
  workflow_dispatch:
    inputs:
      debug:
        description: 'Use debug logging'
        required: true
        default: 'false'
      n8n_tag:
        description: 'Name of the n8n docker tag to run the benchmark against.'
        required: true
        default: 'nightly'
      benchmark_tag:
        description: 'Name of the benchmark cli docker tag to run the benchmark with.'
        required: true
        default: 'latest'

env:
  ARM_CLIENT_ID: ${{ secrets.BENCHMARK_ARM_CLIENT_ID }}
  ARM_SUBSCRIPTION_ID: ${{ secrets.BENCHMARK_ARM_SUBSCRIPTION_ID }}
  ARM_TENANT_ID: ${{ secrets.BENCHMARK_ARM_TENANT_ID }}
  N8N_TAG: ${{ inputs.n8n_tag || 'nightly' }}
  N8N_BENCHMARK_TAG: ${{ inputs.benchmark_tag || 'latest' }}
  DEBUG: ${{ inputs.debug == 'true' && '--debug' || '' }}

permissions:
  id-token: write
  contents: read

concurrency:
  group: benchmark
  cancel-in-progress: false

jobs:
  build:
    runs-on: ubuntu-latest
    environment: benchmarking

    steps:
      - name: Checkout
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1

      - uses: hashicorp/setup-terraform@b9cd54a3c349d3f38e8881555d616ced269862dd # v3
        with:
          terraform_version: '1.8.5'

      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 22.x

      - name: Setup corepack and pnpm
        run: |
          npm i -g corepack@0.33
          corepack enable

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Azure login
        uses: azure/login@6c251865b4e6290e7b78be643ea2d005bc51f69a # v2.1.1
        with:
          client-id: ${{ env.ARM_CLIENT_ID }}
          tenant-id: ${{ env.ARM_TENANT_ID }}
          subscription-id: ${{ env.ARM_SUBSCRIPTION_ID }}

      - name: Destroy any existing environment
        run: pnpm destroy-cloud-env
        working-directory: packages/@n8n/benchmark

      - name: Provision the environment
        run: pnpm provision-cloud-env ${{ env.DEBUG }}
        working-directory: packages/@n8n/benchmark

      - name: Run the benchmark
        env:
          BENCHMARK_RESULT_WEBHOOK_URL: ${{ secrets.BENCHMARK_RESULT_WEBHOOK_URL }}
          BENCHMARK_RESULT_WEBHOOK_AUTH_HEADER: ${{ secrets.BENCHMARK_RESULT_WEBHOOK_AUTH_HEADER }}
          N8N_LICENSE_CERT: ${{ secrets.N8N_BENCHMARK_LICENSE_CERT }}
        run: |
          pnpm benchmark-in-cloud \
            --vus 5 \
            --duration 1m \
            --n8nTag ${{ env.N8N_TAG }} \
            --benchmarkTag ${{ env.N8N_BENCHMARK_TAG }} \
            ${{ env.DEBUG }}
        working-directory: packages/@n8n/benchmark

        # We need to login again because the access token expires
      - name: Azure login
        if: always()
        uses: azure/login@6c251865b4e6290e7b78be643ea2d005bc51f69a # v2.1.1
        with:
          client-id: ${{ env.ARM_CLIENT_ID }}
          tenant-id: ${{ env.ARM_TENANT_ID }}
          subscription-id: ${{ env.ARM_SUBSCRIPTION_ID }}

      - name: Destroy the environment
        if: always()
        run: pnpm destroy-cloud-env ${{ env.DEBUG }}
        working-directory: packages/@n8n/benchmark
",104,1,2,"schedule, workflow_dispatch",5
n8n-io/n8n,check-documentation-urls.yml,"name: Check Documentation URLs

on:
  release:
    types: [published]
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch:

jobs:
  check-docs-urls:
    runs-on: ubuntu-latest

    timeout-minutes: 5

    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1

      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 22.x

      - name: Setup corepack and pnpm
        run: |
          npm i -g corepack@0.33
          corepack enable

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Build relevant packages
        run: pnpm build:nodes

      - run: npm install --prefix=.github/scripts --no-package-lock

      - name: Test URLs
        run: node .github/scripts/validate-docs-links.js

      - name: Notify Slack on failure
        uses: act10ns/slack@44541246747a30eb3102d87f7a4cc5471b0ffb7d # v2.1.0
        if: failure()
        with:
          status: ${{ job.status }}
          channel: '#alerts-build'
          webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}
          message: |
            <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}| Documentation URLs check failed >
",47,1,3,"release, schedule, workflow_dispatch",3
n8n-io/n8n,check-pr-title.yml,"name: Check PR title

on:
  pull_request:
    types:
      - opened
      - edited
      - synchronize
    branches:
      - 'master'

jobs:
  check-pr-title:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
      - name: Check out branch
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1

      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 22.x

      - name: Setup corepack and pnpm
        run: |
          npm i -g corepack@0.33
          corepack enable

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Validate PR title
        id: validate_pr_title
        uses: n8n-io/validate-n8n-pull-request-title@c97ff722ac14ee0bda73766473bba764445db805 # v2.2.0
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
",36,1,1,pull_request,3
n8n-io/n8n,check-run-eligibility.yml,"# Determines if conditions are met for running subsequent jobs on a Pull Request.
#
# !! IMPORTANT !!
# This workflow RELIES on being called from a parent workflow triggered by
# a `pull_request` or `pull_request_target` event. It uses `github.event`
# to access PR details.
#
# It checks if all the following conditions are TRUE:
# 1. The PR is NOT from a fork (i.e., it's an internal PR).
# 2. The PR has been approved by a maintainer (`is_pr_approved_by_maintainer`).
# 3. The PR's source branch does NOT match an excluded pattern.
# 4. The PR includes relevant file changes (`paths_filter_patterns`).
#
# It outputs `should_run` as 'true' if ALL conditions pass, 'false' otherwise.

name: PR Eligibility Check

on:
  workflow_call:
    inputs:
      is_pr_approved_by_maintainer:
        required: true
        type: boolean
      paths_filter_patterns:
        description: ""Path filter patterns for 'paths-filter-action'.""
        required: false
        type: string
        default: |
          not_ignored:
            - '!.devcontainer/**'
            - '!.github/*'
            - '!.github/scripts/*'
            - '!.github/workflows/benchmark-*'
            - '!.github/workflows/check-*'
            - '!.vscode/**'
            - '!docker/**'
            - '!packages/@n8n/benchmark/**'
            - '!**/*.md'
      excluded_source_branch_patterns:
        description: ""Newline-separated list of glob patterns for source branches to EXCLUDE.""
        required: false
        type: string
        default: |
          release/*
          master

    outputs:
      should_run:
        description: ""Outputs 'true' if all eligibility checks pass, otherwise 'false'.""
        value: ${{ jobs.evaluate_conditions.outputs.run_decision }}

jobs:
  evaluate_conditions:
    runs-on: ubuntu-latest
    outputs:
      run_decision: ${{ steps.evaluate.outputs.should_run }}
    steps:
      - name: Check out current commit
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
        with:
          ref: ${{ github.event.pull_request.head.sha }}

      - name: Determine changed files
        uses: tomi/paths-filter-action@32c62f5ca100c1110406e3477d5b3ecef4666fec # v3.0.2
        id: changed
        with:
          filters: ${{ inputs.paths_filter_patterns }}
          predicate-quantifier: 'every'

      - name: Evaluate Conditions & Set Output
        id: evaluate
        env:
          IS_FORK: ${{ github.event.pull_request.head.repo.fork }}
          IS_APPROVED: ${{ inputs.is_pr_approved_by_maintainer }}
          FILES_CHANGED: ${{ steps.changed.outputs.not_ignored == 'true' }}
          HEAD_REF: ${{ github.event.pull_request.head.ref }}
          EXCLUDED_PATTERNS: ${{ inputs.excluded_source_branch_patterns }}
        run: |
          if [[ ""$IS_FORK"" == ""true"" ]]; then
            is_community=""true""
          else
            is_community=""false""
          fi

          source_branch_excluded=""false""
          while IFS= read -r pattern; do
            if [[ -n ""$pattern"" && ""$HEAD_REF"" == $pattern ]]; then
                source_branch_excluded=""true""
                break
            fi
          done <<< ""$EXCLUDED_PATTERNS""

          echo ""--- Checking Conditions ---""
          echo ""Is NOT Community PR: $([[ ""$is_community"" == ""false"" ]] && echo true || echo false)""
          echo ""Files Changed: $FILES_CHANGED""
          echo ""Source Branch Excluded: $source_branch_excluded""
          echo ""Is Approved: $IS_APPROVED""
          echo ""-------------------------""

          if [[ ""$is_community"" == ""false"" && \
                ""$FILES_CHANGED"" == ""true"" && \
                ""$source_branch_excluded"" == ""false"" && \
                ""$IS_APPROVED"" == ""true"" ]]; then
            echo ""Decision: Conditions met. Setting should_run=true.""
            echo ""should_run=true"" >> $GITHUB_OUTPUT
          else
            echo ""Decision: Conditions not met. Setting should_run=false.""
            echo ""should_run=false"" >> $GITHUB_OUTPUT
          fi",109,1,1,workflow_call,2
n8n-io/n8n,chromatic.yml,"name: Chromatic

on:
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch:
  pull_request_review:
    types: [submitted]

concurrency:
  group: chromatic-${{ github.event.pull_request.number || github.ref }}-${{github.event.review.state}}
  cancel-in-progress: true

jobs:
  get-metadata:
    name: Get Metadata
    runs-on: ubuntu-latest
    if: github.event.review.state == 'approved'
    steps:
      - name: Check out current commit
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          fetch-depth: 2

      - name: Determine changed files
        uses: tomi/paths-filter-action@32c62f5ca100c1110406e3477d5b3ecef4666fec # v3.0.2
        id: changed
        if: github.event_name == 'pull_request_review'
        with:
          filters: |
            design_system:
              - packages/design-system/**
              - .github/workflows/chromatic.yml

    outputs:
      design_system_files_changed: ${{ steps.changed.outputs.design_system == 'true' }}
      is_community_pr: ${{ contains(github.event.pull_request.labels.*.name, 'community') }}
      is_pr_target_master: ${{ github.event.pull_request.base.ref == 'master' }}
      is_dispatch: ${{ github.event_name == 'workflow_dispatch' }}
      is_pr_approved: ${{ github.event.review.state == 'approved' }}

  chromatic:
    needs: [get-metadata]
    if: |
      needs.get-metadata.outputs.is_dispatch == 'true' ||
      (
       needs.get-metadata.outputs.design_system_files_changed == 'true' &&
       needs.get-metadata.outputs.is_community_pr == 'false' &&
       needs.get-metadata.outputs.is_pr_target_master == 'true' &&
       needs.get-metadata.outputs.is_pr_approved == 'true'
      )
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
        with:
          fetch-depth: 0

      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 22.x

      - name: Setup corepack and pnpm
        run: |
          npm i -g corepack@0.33
          corepack enable

      - run: pnpm install --frozen-lockfile

      - name: Publish to Chromatic
        uses: chromaui/action@c93e0bc3a63aa176e14a75b61a31847cbfdd341c # v11
        id: chromatic_tests
        continue-on-error: true
        with:
          workingDir: packages/design-system
          onlyChanged: true
          projectToken: ${{ secrets.CHROMATIC_PROJECT_TOKEN }}
          exitZeroOnChanges: false

      - name: Success comment
        if: steps.chromatic_tests.outcome == 'success' && github.ref != 'refs/heads/master'
        uses: peter-evans/create-or-update-comment@71345be0265236311c031f5c7866368bd1eff043 # v4.0.0
        with:
          issue-number: ${{ github.event.pull_request.number }}
          token: ${{ secrets.GITHUB_TOKEN }}
          edit-mode: replace
          body: |
            :white_check_mark: No visual regressions found.

      - name: Fail comment
        if: steps.chromatic_tests.outcome != 'success' && github.ref != 'refs/heads/master'
        uses: peter-evans/create-or-update-comment@71345be0265236311c031f5c7866368bd1eff043 # v4.0.0
        with:
          issue-number: ${{ github.event.pull_request.number }}
          token: ${{ secrets.GITHUB_TOKEN }}
          edit-mode: replace
          body: |
            [:warning: Visual regressions found](${{steps.chromatic_tests.outputs.url}}): ${{steps.chromatic_tests.outputs.changeCount}}
",98,2,3,"schedule, workflow_dispatch, pull_request_review",7
n8n-io/n8n,ci-master.yml,"name: Test Master

on:
  push:
    branches:
      - master

jobs:
  install-and-build:
    runs-on: blacksmith-2vcpu-ubuntu-2204
    env:
      NODE_OPTIONS: '--max-old-space-size=4096'

    timeout-minutes: 10

    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1

      - uses: useblacksmith/setup-node@65c6ca86fdeb0ab3d85e78f57e4f6a7e4780b391 # v5
        with:
          node-version: 22.x

      - name: Setup corepack and pnpm
        run: |
          npm i -g corepack@0.33
          corepack enable

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Setup build cache
        uses: useblacksmith/caching-for-turbo@bafb57e7ebdbf1185762286ec94d24648cd3938a # v1

      - name: Build
        run: pnpm build

      - name: Cache build artifacts
        uses: useblacksmith/cache/save@c5fe29eb0efdf1cf4186b9f7fcbbcbc0cf025662 # v5
        with:
          path: ./packages/**/dist
          key: ${{ github.sha }}-base:build

  unit-test:
    name: Unit tests
    uses: ./.github/workflows/units-tests-reusable.yml
    needs: install-and-build
    strategy:
      matrix:
        node-version: [20.x, 22.x, 24.x]
    with:
      ref: ${{ inputs.branch }}
      nodeVersion: ${{ matrix.node-version }}
      cacheKey: ${{ github.sha }}-base:build
      collectCoverage: ${{ matrix.node-version == '22.x' }}
      ignoreTurboCache: ${{ matrix.node-version == '22.x' }}
      skipFrontendTests: ${{ matrix.node-version != '22.x' }}
    secrets:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

  lint:
    name: Lint
    uses: ./.github/workflows/linting-reusable.yml
    needs: install-and-build
    with:
      ref: ${{ inputs.branch }}
      cacheKey: ${{ github.sha }}-base:build

  notify-on-failure:
    name: Notify Slack on failure
    runs-on: ubuntu-latest
    needs: [unit-test, lint]
    steps:
      - name: Notify Slack on failure
        uses: act10ns/slack@44541246747a30eb3102d87f7a4cc5471b0ffb7d # v2.1.0
        if: failure()
        with:
          status: ${{ job.status }}
          channel: '#alerts-build'
          webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}
          message: Master branch (build or test or lint) failed (${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
",80,4,1,push,7
n8n-io/n8n,ci-postgres-mysql.yml,"name: Test Postgres and MySQL schemas

on:
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch:
  pull_request:
    paths:
      - packages/cli/src/databases/**
      - packages/@n8n/db/**
      - packages/cli/src/modules/*/database/**
      - packages/cli/test/integration/**
      - packages/cli/test/shared/db/**
      - packages/@n8n/db/**
      - .github/workflows/ci-postgres-mysql.yml
      - .github/docker-compose.yml
  pull_request_review:
    types: [submitted]

concurrency:
  group: db-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: false

jobs:
  build:
    name: Install & Build
    runs-on: ubuntu-latest
    if: github.event_name != 'pull_request_review' || startsWith(github.event.pull_request.base.ref, 'release/')
    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1

      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 22.x

      - name: Setup corepack and pnpm
        run: |
          npm i -g corepack@0.33
          corepack enable

      - run: pnpm install --frozen-lockfile

      - name: Setup build cache
        uses: rharkor/caching-for-turbo@439abec0d28d21b192fa8817b744ffdf1ee5ac0d # v1.5

      - name: Build
        run: pnpm build

      - name: Cache build artifacts
        uses: actions/cache/save@1bd1e32a3bdc45362d1e726936510720a7c30a57 # v4.2.0
        with:
          path: ./packages/**/dist
          key: ${{ github.sha }}:db-tests

  sqlite-pooled:
    name: SQLite Pooled
    runs-on: ubuntu-latest
    needs: build
    timeout-minutes: 20
    env:
      DB_TYPE: sqlite
      DB_SQLITE_POOL_SIZE: 4
    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1

      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 22.x

      - name: Setup corepack and pnpm
        run: |
          npm i -g corepack@0.33
          corepack enable

      - run: pnpm install --frozen-lockfile

      - name: Setup build cache
        uses: rharkor/caching-for-turbo@439abec0d28d21b192fa8817b744ffdf1ee5ac0d # v1.5

      - name: Restore cached build artifacts
        uses: actions/cache/restore@1bd1e32a3bdc45362d1e726936510720a7c30a57 # v4.2.0
        with:
          path: ./packages/**/dist
          key: ${{ github.sha }}:db-tests

      - name: Test SQLite Pooled
        working-directory: packages/cli
        run: pnpm jest

  mariadb:
    name: MariaDB
    runs-on: ubuntu-latest
    needs: build
    timeout-minutes: 20
    env:
      DB_MYSQLDB_PASSWORD: password
    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1

      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 22.x

      - name: Setup corepack and pnpm
        run: |
          npm i -g corepack@0.33
          corepack enable

      - run: pnpm install --frozen-lockfile

      - name: Setup build cache
        uses: rharkor/caching-for-turbo@439abec0d28d21b192fa8817b744ffdf1ee5ac0d # v1.5

      - name: Restore cached build artifacts
        uses: actions/cache/restore@1bd1e32a3bdc45362d1e726936510720a7c30a57 # v4.2.0
        with:
          path: ./packages/**/dist
          key: ${{ github.sha }}:db-tests

      - name: Start MariaDB
        uses: isbang/compose-action@802a148945af6399a338c7906c267331b39a71af # v2.0.0
        with:
          compose-file: ./.github/docker-compose.yml
          services: |
            mariadb

      - name: Test MariaDB
        working-directory: packages/cli
        run: pnpm test:mariadb --testTimeout 60000

  mysql:
    name: MySQL (${{ matrix.service-name }})
    runs-on: ubuntu-latest
    needs: build
    timeout-minutes: 20
    strategy:
      matrix:
        service-name: ['mysql-8.0.13', 'mysql-8.4']
    env:
      DB_MYSQLDB_PASSWORD: password
    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1

      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 22.x

      - name: Setup corepack and pnpm
        run: |
          npm i -g corepack@0.33
          corepack enable

      - run: pnpm install --frozen-lockfile

      - name: Setup build cache
        uses: rharkor/caching-for-turbo@439abec0d28d21b192fa8817b744ffdf1ee5ac0d # v1.5

      - name: Restore cached build artifacts
        uses: actions/cache/restore@1bd1e32a3bdc45362d1e726936510720a7c30a57 # v4.2.0
        with:
          path: ./packages/**/dist
          key: ${{ github.sha }}:db-tests

      - name: Start MySQL
        uses: isbang/compose-action@802a148945af6399a338c7906c267331b39a71af # v2.0.0
        with:
          compose-file: ./.github/docker-compose.yml
          services: |
            ${{ matrix.service-name }}

      - name: Test MySQL
        working-directory: packages/cli
        run: pnpm test:mysql --testTimeout 60000

  postgres:
    name: Postgres
    runs-on: ubuntu-latest
    needs: build
    timeout-minutes: 20
    env:
      DB_POSTGRESDB_PASSWORD: password
      DB_POSTGRESDB_POOL_SIZE: 1 # Detect connection pooling deadlocks
    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1

      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 22.x

      - name: Setup corepack and pnpm
        run: |
          npm i -g corepack@0.33
          corepack enable

      - run: pnpm install --frozen-lockfile

      - name: Setup build cache
        uses: rharkor/caching-for-turbo@439abec0d28d21b192fa8817b744ffdf1ee5ac0d # v1.5

      - name: Restore cached build artifacts
        uses: actions/cache/restore@1bd1e32a3bdc45362d1e726936510720a7c30a57 # v4.2.0
        with:
          path: ./packages/**/dist
          key: ${{ github.sha }}:db-tests

      - name: Start Postgres
        uses: isbang/compose-action@802a148945af6399a338c7906c267331b39a71af # v2.0.0
        with:
          compose-file: ./.github/docker-compose.yml
          services: |
            postgres

      - name: Test Postgres
        working-directory: packages/cli
        run: pnpm test:postgres

  notify-on-failure:
    name: Notify Slack on failure
    runs-on: ubuntu-latest
    needs: [mariadb, postgres, mysql]
    steps:
      - name: Notify Slack on failure
        uses: act10ns/slack@44541246747a30eb3102d87f7a4cc5471b0ffb7d # v2.1.0
        if: failure() && github.ref == 'refs/heads/master'
        with:
          status: ${{ job.status }}
          channel: '#alerts-build'
          webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}
          message: Postgres, MariaDB or MySQL tests failed (${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
",229,6,4,"schedule, workflow_dispatch, pull_request, pull_request_review",24
n8n-io/n8n,ci-pull-requests.yml,"name: Build, unit test and lint branch

on:
  pull_request:
    branches:
      - '**'
      - '!release/*'

jobs:
  install-and-build:
    name: Install & Build
    runs-on: blacksmith-2vcpu-ubuntu-2204
    env:
      NODE_OPTIONS: '--max-old-space-size=4096'
    outputs:
      frontend_changed: ${{ steps.paths-filter.outputs.frontend == 'true' }}
    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
        with:
          ref: refs/pull/${{ github.event.pull_request.number }}/merge

      - name: Check for frontend changes
        uses: dorny/paths-filter@v3
        id: paths-filter
        with:
          filters: |
            frontend:
              - packages/frontend/**
              - packages/@n8n/design-system/**
              - packages/@n8n/chat/**
              - packages/@n8n/codemirror-lang/**
              - .bundlemonrc.json
              - .github/workflows/ci-pull-requests.yml

      - name: Setup Environment and Build Project
        uses: ./.github/actions/setup-and-build
        with:
          node-version: 22.x
          enable-caching: true

      - name: Run formatcheck
        run: pnpm format:check

      - name: Run typecheck
        run: pnpm typecheck

      - name: Upload Frontend Build Artifacts
        if: steps.paths-filter.outputs.frontend == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: editor-ui-dist
          path: packages/frontend/editor-ui/dist/
          retention-days: 1

  bundle-size-check:
    name: Bundle Size Check
    needs: install-and-build
    if: needs.install-and-build.outputs.frontend_changed == 'true'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
        with:
          ref: refs/pull/${{ github.event.pull_request.number }}/merge

      - name: Setup pnpm CLI
        uses: pnpm/action-setup@a7487c7e89a18df4991f7f222e4898a00d66ddda # v4.1.0

      - name: Setup Node.js
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: '22.x'
          cache: pnpm

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Download Frontend Build Artifacts
        uses: actions/download-artifact@v4
        with:
          name: editor-ui-dist
          path: packages/frontend/editor-ui/dist/

      - name: BundleMon
        uses: lironer/bundlemon-action@v1.2.0

  unit-test:
    name: Unit tests
    uses: ./.github/workflows/units-tests-reusable.yml
    needs: install-and-build
    with:
      ref: refs/pull/${{ github.event.pull_request.number }}/merge
      cacheKey: ${{ github.sha }}-base:build
      collectCoverage: true
    secrets:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

  lint:
    name: Lint
    uses: ./.github/workflows/linting-reusable.yml
    needs: install-and-build
    with:
      ref: refs/pull/${{ github.event.pull_request.number }}/merge
      cacheKey: ${{ github.sha }}-base:build
",103,4,1,pull_request,11
n8n-io/n8n,docker-base-image.yml,"name: Docker Base Image CI

on:
  workflow_dispatch:
    inputs:
      node_version:
        description: 'Node.js version to build this image with.'
        type: choice
        required: true
        default: '20'
        options:
          - '20'
          - '22'
          - '24'

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1

      - name: Set up QEMU
        uses: docker/setup-qemu-action@53851d14592bedcffcf25ea515637cff71ef929a # v3.3.0

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@6524bf65af31da8d45b59e8c27de4bd072b392f5 # v3.8.0

      - name: Login to GitHub Container Registry
        uses: docker/login-action@9780b0c442fbb1117ed29e0efdff1e18412f7567 # v3.3.0
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Login to DockerHub
        uses: docker/login-action@9780b0c442fbb1117ed29e0efdff1e18412f7567 # v3.3.0
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Build
        uses: docker/build-push-action@b32b51a8eda65d6793cd0494a773d4f6bcef32dc # v6.11.0
        env:
          DOCKER_BUILD_SUMMARY: false
        with:
          context: .
          file: ./docker/images/n8n-base/Dockerfile
          build-args: |
            NODE_VERSION=${{github.event.inputs.node_version}}
          platforms: linux/amd64,linux/arm64
          provenance: false
          push: true
          tags: |
            ${{ secrets.DOCKER_USERNAME }}/base:${{ github.event.inputs.node_version }}
            ghcr.io/${{ github.repository_owner }}/base:${{ github.event.inputs.node_version }}
",55,1,1,workflow_dispatch,6
n8n-io/n8n,docker-build-push.yml,"# This workflow is used to build and push the Docker image for n8n
# - build-and-push-docker: This builds on both an ARM64 and AMD64 runner so the builds are native to the platform. Uses blacksmith native runners and build-push-action
# - create_multi_arch_manifest: This creates the multi-arch manifest for the Docker image. Needed to recombine the images from the build-and-push-docker job since they are separate runners.
# - security-scan: This scans the Docker image for security vulnerabilities using Trivy.

name: 'Docker: Build and Push'

on:
  schedule:
    - cron: '0 0 * * *'

  workflow_call:
    inputs:
      n8n_version:
        description: 'N8N version to build'
        required: true
        type: string
      release_type:
        description: 'Release type (stable, nightly, dev)'
        required: true
        type: string
        default: 'dev'
      push_enabled:
        description: 'Whether to push the built images'
        required: false
        type: boolean
        default: true

  workflow_dispatch:
    inputs:
      release_type:
        description: 'Release type'
        required: true
        type: choice
        options:
          - nightly
          - dev
          - stable
          - branch
        default: 'dev'
      push_to_registry:
        description: 'Push image to registry'
        required: false
        type: boolean
        default: true

  pull_request:
    types:
      - opened
      - ready_for_review
    paths:
      - '.github/workflows/docker-build-push.yml'
      - 'docker/images/n8n/Dockerfile'

jobs:
  build-and-push-docker:
    strategy:
      matrix:
        platform: [amd64, arm64]
        include:
          - platform: amd64
            runner: blacksmith-4vcpu-ubuntu-2204
            docker_platform: linux/amd64
          - platform: arm64
            runner: blacksmith-4vcpu-ubuntu-2204-arm
            docker_platform: linux/arm64

    name: Build App, then Build and Push Docker Image (${{ matrix.platform }})
    runs-on: ${{ matrix.runner }}
    timeout-minutes: 15
    outputs:
      image_ref: ${{ steps.determine-tags.outputs.primary_tag }}
      primary_ghcr_manifest_tag: ${{ steps.determine-tags.outputs.primary_ghcr_manifest_tag }}
      push_enabled_status: ${{ steps.context.outputs.push_enabled }}
      release_type: ${{ steps.context.outputs.release_type }}
      n8n_version: ${{ steps.context.outputs.n8n_version }}
    steps:
      - name: Checkout code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          fetch-depth: 0

      - name: Setup pnpm
        uses: pnpm/action-setup@a7487c7e89a18df4991f7f222e4898a00d66ddda # v4.1.0
        with:
          run_install: false

      - name: Setup Node.js
        uses: useblacksmith/setup-node@65c6ca86fdeb0ab3d85e78f57e4f6a7e4780b391 # v5.0.4
        with:
          node-version: 22.x

      - name: Install dependencies
        run: pnpm install --frozen-lockfile
        shell: bash

      - name: Configure Turborepo Cache
        uses: useblacksmith/caching-for-turbo@bafb57e7ebdbf1185762286ec94d24648cd3938a # v1

      - name: Build n8n for Docker
        run: pnpm build:n8n
        shell: bash

      - name: Determine build context values
        id: context
        run: |
          if [[ ""${{ github.event_name }}"" == ""schedule"" ]]; then
            echo ""release_type=nightly"" >> $GITHUB_OUTPUT
            echo ""n8n_version=snapshot"" >> $GITHUB_OUTPUT
            echo ""push_enabled=true"" >> $GITHUB_OUTPUT
          elif [[ ""${{ github.event_name }}"" == ""workflow_call"" ]]; then
            echo ""release_type=${{ inputs.release_type }}"" >> $GITHUB_OUTPUT
            echo ""n8n_version=${{ inputs.n8n_version }}"" >> $GITHUB_OUTPUT
            echo ""push_enabled=${{ inputs.push_enabled }}"" >> $GITHUB_OUTPUT
          elif [[ ""${{ github.event_name }}"" == ""workflow_dispatch"" ]]; then
            if [[ ""${{ inputs.release_type }}"" == ""branch"" ]]; then
              BRANCH_NAME=""${{ github.ref_name }}""
              SAFE_BRANCH_NAME=$(echo ""$BRANCH_NAME"" | tr '/' '-' | tr -cd '[:alnum:]-_')
              echo ""release_type=branch"" >> $GITHUB_OUTPUT
              echo ""n8n_version=branch-${SAFE_BRANCH_NAME}"" >> $GITHUB_OUTPUT
              echo ""push_enabled=${{ inputs.push_to_registry }}"" >> $GITHUB_OUTPUT
            else
              echo ""release_type=${{ inputs.release_type }}"" >> $GITHUB_OUTPUT
              echo ""n8n_version=snapshot"" >> $GITHUB_OUTPUT
              echo ""push_enabled=true"" >> $GITHUB_OUTPUT
            fi
          elif [[ ""${{ github.event_name }}"" == ""push"" ]]; then
            echo ""release_type=dev"" >> $GITHUB_OUTPUT
            echo ""n8n_version=snapshot"" >> $GITHUB_OUTPUT
            echo ""push_enabled=true"" >> $GITHUB_OUTPUT
          elif [[ ""${{ github.event_name }}"" == ""pull_request"" ]]; then
            echo ""release_type=dev"" >> $GITHUB_OUTPUT
            echo ""n8n_version=pr-${{ github.event.pull_request.number }}"" >> $GITHUB_OUTPUT
            echo ""push_enabled=false"" >> $GITHUB_OUTPUT
          else
            echo ""release_type=dev"" >> $GITHUB_OUTPUT
            echo ""n8n_version=snapshot"" >> $GITHUB_OUTPUT
            echo ""push_enabled=false"" >> $GITHUB_OUTPUT
          fi

      - name: Determine Docker tags
        id: determine-tags
        run: |
          RELEASE_TYPE=""${{ steps.context.outputs.release_type }}""
          N8N_VERSION_TAG=""${{ steps.context.outputs.n8n_version }}""
          GHCR_BASE=""ghcr.io/${{ github.repository_owner }}/n8n""
          DOCKER_BASE=""${{ secrets.DOCKER_USERNAME }}/n8n""
          PLATFORM=""${{ matrix.platform }}""

          GHCR_TAGS_FOR_PUSH=""""
          DOCKER_TAGS_FOR_PUSH=""""

          PRIMARY_GHCR_MANIFEST_TAG_VALUE=""""
          PRIMARY_DOCKER_MANIFEST_TAG_VALUE=""""

          if [[ ""$RELEASE_TYPE"" == ""stable"" && -z ""$N8N_VERSION_TAG"" ]]; then
            echo ""Error: N8N_VERSION_TAG is empty for a stable release.""
            exit 1
          fi

          case ""$RELEASE_TYPE"" in
            ""stable"")
              PRIMARY_GHCR_MANIFEST_TAG_VALUE=""${GHCR_BASE}:${N8N_VERSION_TAG}""
              PRIMARY_DOCKER_MANIFEST_TAG_VALUE=""${DOCKER_BASE}:${N8N_VERSION_TAG}""
              GHCR_TAGS_FOR_PUSH=""${PRIMARY_GHCR_MANIFEST_TAG_VALUE}-${PLATFORM}""
              DOCKER_TAGS_FOR_PUSH=""${PRIMARY_DOCKER_MANIFEST_TAG_VALUE}-${PLATFORM}""
              ;;
            ""nightly"")
              PRIMARY_GHCR_MANIFEST_TAG_VALUE=""${GHCR_BASE}:nightly""
              PRIMARY_DOCKER_MANIFEST_TAG_VALUE=""${DOCKER_BASE}:nightly""
              GHCR_TAGS_FOR_PUSH=""${PRIMARY_GHCR_MANIFEST_TAG_VALUE}-${PLATFORM}""
              DOCKER_TAGS_FOR_PUSH=""${PRIMARY_DOCKER_MANIFEST_TAG_VALUE}-${PLATFORM}""
              ;;
            ""branch"")
              PRIMARY_GHCR_MANIFEST_TAG_VALUE=""${GHCR_BASE}:${N8N_VERSION_TAG}""
              GHCR_TAGS_FOR_PUSH=""${PRIMARY_GHCR_MANIFEST_TAG_VALUE}-${PLATFORM}""
              PRIMARY_DOCKER_MANIFEST_TAG_VALUE=""""
              DOCKER_TAGS_FOR_PUSH=""""
              ;;
            ""dev""|*)
              if [[ ""$N8N_VERSION_TAG"" == pr-* ]]; then
                PRIMARY_GHCR_MANIFEST_TAG_VALUE=""${GHCR_BASE}:${N8N_VERSION_TAG}""
                GHCR_TAGS_FOR_PUSH=""${PRIMARY_GHCR_MANIFEST_TAG_VALUE}-${PLATFORM}""
                PRIMARY_DOCKER_MANIFEST_TAG_VALUE=""""
                DOCKER_TAGS_FOR_PUSH=""""
              else
                PRIMARY_GHCR_MANIFEST_TAG_VALUE=""${GHCR_BASE}:dev""
                PRIMARY_DOCKER_MANIFEST_TAG_VALUE=""${DOCKER_BASE}:dev""
                GHCR_TAGS_FOR_PUSH=""${PRIMARY_GHCR_MANIFEST_TAG_VALUE}-${PLATFORM}""
                DOCKER_TAGS_FOR_PUSH=""${PRIMARY_DOCKER_MANIFEST_TAG_VALUE}-${PLATFORM}""
              fi
              ;;
          esac

          ALL_TAGS=""${GHCR_TAGS_FOR_PUSH}""
          if [[ -n ""$DOCKER_TAGS_FOR_PUSH"" ]]; then
            ALL_TAGS=""${ALL_TAGS}\n${DOCKER_TAGS_FOR_PUSH}""
          fi

          echo ""Generated Tags for push: $ALL_TAGS""
          echo ""tags<<EOF"" >> $GITHUB_OUTPUT
          echo -e ""$ALL_TAGS"" >> $GITHUB_OUTPUT
          echo ""EOF"" >> $GITHUB_OUTPUT

          echo ""ghcr_platform_tag=${GHCR_TAGS_FOR_PUSH}"" >> $GITHUB_OUTPUT
          echo ""dockerhub_platform_tag=${DOCKER_TAGS_FOR_PUSH}"" >> $GITHUB_OUTPUT
          echo ""primary_ghcr_manifest_tag=${PRIMARY_GHCR_MANIFEST_TAG_VALUE}"" >> $GITHUB_OUTPUT

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@b5ca514318bd6ebac0fb2aedd5d36ec1b5c232a2 # v3.10.0

      - name: Login to GitHub Container Registry
        if: steps.context.outputs.push_enabled == 'true'
        uses: docker/login-action@74a5d142397b4f367a81961eba4e8cd7edddf772 # v3.4.0
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Login to DockerHub
        if: steps.context.outputs.push_enabled == 'true'
        uses: docker/login-action@74a5d142397b4f367a81961eba4e8cd7edddf772 # v3.4.0
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Build and push Docker image
        uses: useblacksmith/build-push-action@6fe3b1c3665ca911656e8249f6195103b7dc9782 # v1.2
        with:
          context: .
          file: ./docker/images/n8n/Dockerfile
          build-args: |
            NODE_VERSION=22
            N8N_VERSION=${{ steps.context.outputs.n8n_version }}
            N8N_RELEASE_TYPE=${{ steps.context.outputs.release_type }}
          platforms: ${{ matrix.docker_platform }}
          provenance: false
          push: ${{ steps.context.outputs.push_enabled }}
          tags: ${{ steps.determine-tags.outputs.tags }}

  create_multi_arch_manifest:
    name: Create Multi-Arch Manifest
    needs: build-and-push-docker
    runs-on: ubuntu-latest
    if: |
      needs.build-and-push-docker.result == 'success' &&
      needs.build-and-push-docker.outputs.push_enabled_status == 'true'
    steps:
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@e468171a9de216ec08956ac3ada2f0791b6bd435 # v3.11.1

      - name: Login to GitHub Container Registry
        uses: docker/login-action@74a5d142397b4f367a81961eba4e8cd7edddf772 # v3.4.0
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Login to Docker Hub
        uses: docker/login-action@74a5d142397b4f367a81961eba4e8cd7edddf772 # v3.4.0
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Reconstruct Docker Hub Primary Tag
        id: reconstruct_dockerhub_tag
        run: |
          RELEASE_TYPE=""${{ needs.build-and-push-docker.outputs.release_type }}""
          N8N_VERSION=""${{ needs.build-and-push-docker.outputs.n8n_version }}""
          DOCKER_BASE=""${{ secrets.DOCKER_USERNAME }}/n8n""

          PRIMARY_DOCKER_MANIFEST_TAG=""""

          case ""$RELEASE_TYPE"" in
            ""stable"")
              PRIMARY_DOCKER_MANIFEST_TAG=""${DOCKER_BASE}:${N8N_VERSION}""
              ;;
            ""nightly"")
              PRIMARY_DOCKER_MANIFEST_TAG=""${DOCKER_BASE}:nightly""
              ;;
            ""dev"")
              if [[ ""$N8N_VERSION"" != pr-* ]]; then
                PRIMARY_DOCKER_MANIFEST_TAG=""${DOCKER_BASE}:dev""
              fi
              ;;
          esac

          if [[ -n ""$PRIMARY_DOCKER_MANIFEST_TAG"" ]]; then
            echo ""PRIMARY_DOCKER_MANIFEST_TAG=$PRIMARY_DOCKER_MANIFEST_TAG"" >> ""$GITHUB_ENV""
          else
            echo ""::notice::No Docker Hub primary tag to reconstruct for release type '$RELEASE_TYPE' and version '$N8N_VERSION'. Skipping Docker Hub manifest creation.""
          fi

      - name: Create GHCR multi-arch manifest
        if: needs.build-and-push-docker.outputs.primary_ghcr_manifest_tag != ''
        run: |
          MANIFEST_TAG=""${{ needs.build-and-push-docker.outputs.primary_ghcr_manifest_tag }}""

          echo ""Creating GHCR manifest: $MANIFEST_TAG""

          # Create and push the multi-arch manifest using buildx
          docker buildx imagetools create \
            --tag $MANIFEST_TAG \
            ${MANIFEST_TAG}-amd64 \
            ${MANIFEST_TAG}-arm64

      # Create Docker Hub multi-arch manifest
      - name: Create Docker Hub multi-arch manifest
        if: env.PRIMARY_DOCKER_MANIFEST_TAG != ''
        run: |
          MANIFEST_TAG=""${{ env.PRIMARY_DOCKER_MANIFEST_TAG }}""

          echo ""Creating Docker Hub manifest: $MANIFEST_TAG""

          # Create and push the multi-arch manifest using buildx
          docker buildx imagetools create \
            --tag $MANIFEST_TAG \
            ${MANIFEST_TAG}-amd64 \
            ${MANIFEST_TAG}-arm64

  security-scan:
    name: Security Scan
    needs: [build-and-push-docker]
    if: |
      success() &&
      (github.event_name == 'schedule' ||
       (github.event_name == 'workflow_call' && inputs.release_type == 'stable'))
    uses: ./.github/workflows/security-trivy-scan-callable.yml
    with:
      image_ref: ${{ needs.build-and-push-docker.outputs.image_ref }}
    secrets:
      SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
",332,3,4,"schedule, workflow_call, workflow_dispatch, pull_request",12
n8n-io/n8n,docker-images-benchmark.yml,"name: Benchmark Docker Image CI

on:
  workflow_dispatch:
  push:
    branches:
      - master
    paths:
      - 'packages/@n8n/benchmark/**'
      - 'pnpm-lock.yaml'
      - 'pnpm-workspace.yaml'
      - '.github/workflows/docker-images-benchmark.yml'

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1

      - name: Set up QEMU
        uses: docker/setup-qemu-action@53851d14592bedcffcf25ea515637cff71ef929a # v3.3.0

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@6524bf65af31da8d45b59e8c27de4bd072b392f5 # v3.8.0

      - name: Login to GitHub Container Registry
        uses: docker/login-action@9780b0c442fbb1117ed29e0efdff1e18412f7567 # v3.3.0
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build
        uses: docker/build-push-action@b32b51a8eda65d6793cd0494a773d4f6bcef32dc # v6.11.0
        env:
          DOCKER_BUILD_SUMMARY: false
        with:
          context: .
          file: ./packages/@n8n/benchmark/Dockerfile
          platforms: linux/amd64
          provenance: false
          push: true
          tags: |
            ghcr.io/${{ github.repository_owner }}/n8n-benchmark:latest
",45,1,2,"workflow_dispatch, push",5
n8n-io/n8n,e2e-flaky.yml,"name: Debug Flaky E2E Test

on:
  workflow_dispatch:
    inputs:
      test_name:
        description: 'The name of the test to filter.'
        required: true
        type: string
      burn_count:
        description: 'Number of times to run the test.'
        required: false
        type: number
        default: 50
      branch:
        description: 'Optional: GitHub branch, tag, or SHA to test. Defaults to the branch selected in UI.'
        required: false
        type: string

jobs:
  debug-test:
    runs-on: blacksmith-4vcpu-ubuntu-2204
    timeout-minutes: 60

    steps:
      - name: Checkout code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          ref: ${{ github.event.inputs.branch }}

      - name: Setup PNPM
        uses: pnpm/action-setup@a7487c7e89a18df4991f7f222e4898a00d66ddda # v4.1.0

      - name: Setup Node.js
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 22.x
          cache: 'pnpm'

      - name: Cache build artifacts
        id: cache-build-artifacts
        uses: useblacksmith/cache@c5fe29eb0efdf1cf4186b9f7fcbbcbc0cf025662 # v5.0.2
        with:
          path: |
            /home/runner/.cache/Cypress
            ./packages/**/dist
          key: ${{ github.ref }}-${{ github.sha }}-debug-build
          restore-keys: |
            ${{ github.ref }}-debug-build-

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Build application
        if: steps.cache-build-artifacts.outputs.cache-hit != 'true'
        run: pnpm build

      - name: Cypress install
        if: steps.cache-build-artifacts.outputs.cache-hit != 'true'
        working-directory: cypress
        run: pnpm cypress:install

      - name: Run Flaky Debug Command
        run: pnpm run debug:flaky:e2e ""${{ github.event.inputs.test_name }}"" ${{ github.event.inputs.burn_count }}
        env:
          NODE_OPTIONS: --dns-result-order=ipv4first
          E2E_TESTS: true
          SHELL: /bin/sh
",68,1,1,workflow_dispatch,4
n8n-io/n8n,e2e-reusable.yml,"name: Reusable e2e workflow

on:
  workflow_call:
    inputs:
      branch:
        description: 'GitHub branch to test.'
        required: false
        type: string
      user:
        description: 'User who kicked this off.'
        required: false
        type: string
        default: 'schedule'
      spec:
        description: 'Specify specs.'
        required: false
        default: 'e2e/*'
        type: string
      record:
        description: 'Record test run.'
        required: false
        default: true
        type: boolean
      parallel:
        description: 'Run tests in parallel.'
        required: false
        default: true
        type: boolean
      containers:
        description: 'Number of containers to run tests in.'
        required: false
        default: '[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]'
        type: string
      pr_number:
        description: 'PR number to run tests for.'
        required: false
        type: number
    secrets:
      CYPRESS_RECORD_KEY:
        description: 'Cypress record key.'
        required: true
    outputs:
      tests_passed:
        description: 'True if all E2E tests passed, otherwise false'
        value: ${{ jobs.check_testing_matrix.outputs.all_tests_passed }}

jobs:
  # single job that generates and outputs a common id
  prepare:
    runs-on: ubuntu-latest
    outputs:
      uuid: ${{ steps.uuid.outputs.value }}
    steps:
      - name: Generate unique ID 💎
        id: uuid
        # take the current commit + timestamp together
        # the typical value would be something like
        # ""sha-5d3fe...35d3-time-1620841214""
        run: echo ""value=sha-$GITHUB_SHA-time-$(date +""%s"")"" >> $GITHUB_OUTPUT

      - name: Calculate Git Ref 🤔
        id: calculate_ref
        run: |
          if [ -n ""${{ inputs.pr_number }}"" ]; then
            echo ""value=refs/pull/${{ inputs.pr_number }}/head"" >> $GITHUB_OUTPUT
          else
            echo ""value=${{ inputs.branch }}"" >> $GITHUB_OUTPUT
          fi
  install:
    runs-on: blacksmith-4vcpu-ubuntu-2204
    needs: ['prepare']

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          ref: ${{ steps.calculate_ref.outputs.value }}

      - uses: pnpm/action-setup@a7487c7e89a18df4991f7f222e4898a00d66ddda # v4.0.0

      - name: Setup Node.js
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 22.x
          cache: 'pnpm'

      - name: Cache build artifacts
        id: cache-build-artifacts
        uses: useblacksmith/cache@c5fe29eb0efdf1cf4186b9f7fcbbcbc0cf025662 # v5
        with:
          path: |
            /home/runner/.cache/Cypress
            /github/home/.pnpm-store
            ./packages/**/dist
          key: ${{ github.sha }}-ui

      - name: Install dependencies
        if: steps.cache-build-artifacts.outputs.cache-hit != 'true'
        run: pnpm install --frozen-lockfile

      - name: Cypress build
        if: steps.cache-build-artifacts.outputs.cache-hit != 'true'
        uses: cypress-io/github-action@be1bab96b388bbd9ce3887e397d373c8557e15af # v6.9.2
        with:
          # Disable running of tests within install job
          runTests: false
          install: false
          build: pnpm build

      - name: Cypress install
        if: steps.cache-build-artifacts.outputs.cache-hit != 'true'
        working-directory: cypress
        run: pnpm cypress:install

  testing:
    runs-on: blacksmith-2vcpu-ubuntu-2204
    needs: ['prepare', 'install']
    strategy:
      fail-fast: false
      matrix:
        # If spec is not e2e/* then we run only one container to prevent
        # running the same tests multiple times
        containers: ${{ fromJSON( inputs.spec == 'e2e/*' && inputs.containers || '[1]' ) }}
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          ref: ${{ steps.calculate_ref.outputs.value }}

      - uses: pnpm/action-setup@a7487c7e89a18df4991f7f222e4898a00d66ddda # v4.0.0

      - name: Setup Node.js
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 22.x
          cache: 'pnpm'

      - name: Restore cached pnpm modules
        id: cache-build-artifacts
        uses: useblacksmith/cache@c5fe29eb0efdf1cf4186b9f7fcbbcbc0cf025662 # v5
        with:
          path: |
            /home/runner/.cache/Cypress
            /github/home/.pnpm-store
            ./packages/**/dist
          key: ${{ github.sha }}-ui

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Cypress run
        uses: cypress-io/github-action@be1bab96b388bbd9ce3887e397d373c8557e15af # v6.9.2
        with:
          working-directory: cypress
          install: false
          start: pnpm start
          wait-on: 'http://localhost:5678'
          wait-on-timeout: 120
          record: ${{ inputs.record }}
          parallel: ${{ fromJSON( inputs.spec == 'e2e/*' && inputs.parallel || false ) }}
          # We have to provide custom ci-build-id key to make sure that this workflow could be run multiple times
          # in the same parent workflow
          ci-build-id: ${{ needs.prepare.outputs.uuid }}
          spec: '${{ inputs.spec }}'
        env:
          NODE_OPTIONS: --dns-result-order=ipv4first
          CYPRESS_RECORD_KEY: ${{ secrets.CYPRESS_RECORD_KEY }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          E2E_TESTS: true
          COMMIT_INFO_MESSAGE: 🌳 ${{ inputs.branch }} 🤖 ${{ inputs.user }} 🗃️ ${{ inputs.spec }}
          SHELL: /bin/sh

  # Check if all tests passed and set the output variable
  check_testing_matrix:
    runs-on: ubuntu-latest
    needs: [testing]
    outputs:
      all_tests_passed: ${{ steps.all_tests_passed.outputs.result }}
    steps:
      - name: Check all tests passed
        id: all_tests_passed
        run: |
          success=true
          for status in ${{ needs.testing.result }}; do
            if [ $status != ""success"" ]; then
              success=false
              break
            fi
          done
          echo ""::set-output name=result::$success""
",189,4,1,workflow_call,10
n8n-io/n8n,e2e-tests-pr.yml,"name: PR E2E

on:
  pull_request_review:
    types: [submitted]

concurrency:
  group: e2e-${{ github.event.pull_request.number || github.ref }}-${{github.event.review.state}}
  cancel-in-progress: true

jobs:
  eligibility_check:
    name: Check Eligibility for Test Run
    if: github.event.review.state == 'approved'
    uses: ./.github/workflows/check-run-eligibility.yml
    with:
      is_pr_approved_by_maintainer: true

  run-e2e-tests:
    name: E2E
    uses: ./.github/workflows/e2e-reusable.yml
    needs: [eligibility_check]
    if: needs.eligibility_check.outputs.should_run == 'true'
    with:
      pr_number: ${{ github.event.pull_request.number }}
      user: ${{ github.event.pull_request.user.login || 'PR User' }}
    secrets:
      CYPRESS_RECORD_KEY: ${{ secrets.CYPRESS_RECORD_KEY }}

  post-e2e-tests:
    name: E2E - Checks
    runs-on: ubuntu-latest
    needs: [eligibility_check, run-e2e-tests]
    if: always() && needs.eligibility_check.result != 'skipped'
    steps:
      - name: Determine Outcome and Comment Message
        id: determine_outcome
        run: |
          JOB_OUTCOME=""success""
          COMMENT_BODY=""""
          SHOULD_POST_COMMENT=""false""

          if [[ ""${{ needs.eligibility_check.outputs.should_run }}"" == ""false"" ]]; then
            COMMENT_BODY=""ℹ️ E2E tests were not run for this PR based on the eligibility criteria.""
            SHOULD_POST_COMMENT=""true""
            JOB_OUTCOME=""success""
          elif [[ ""${{ needs.run-e2e-tests.result }}"" == ""success"" ]]; then
            COMMENT_BODY="":white_check_mark: All Cypress E2E specs passed""
            SHOULD_POST_COMMENT=""true""
            JOB_OUTCOME=""success""
          elif [[ ""${{ needs.run-e2e-tests.result }}"" == ""failure"" ]]; then
            COMMENT_BODY="":warning: Some Cypress E2E specs are failing, please fix them before merging""
            SHOULD_POST_COMMENT=""true""
            JOB_OUTCOME=""failure""
          else
            COMMENT_BODY=""ℹ️ E2E tests were scheduled but did not complete as expected (Result: ${{ needs.run-e2e-tests.result }}).""
            SHOULD_POST_COMMENT=""true""
            JOB_OUTCOME=""failure""
          fi

          echo ""comment_body=$COMMENT_BODY"" >> $GITHUB_OUTPUT
          echo ""should_post_comment=$SHOULD_POST_COMMENT"" >> $GITHUB_OUTPUT
          echo ""job_outcome=$JOB_OUTCOME"" >> $GITHUB_OUTPUT

      - name: Create or Update PR Comment
        if: steps.determine_outcome.outputs.should_post_comment == 'true' && needs.eligibility_check.outputs.should_run == 'true'
        uses: peter-evans/create-or-update-comment@71345be0265236311c031f5c7866368bd1eff043
        with:
          issue-number: ${{ github.event.pull_request.number }}
          body: ${{ steps.determine_outcome.outputs.comment_body }}
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Finalize Job Status
        run: |
          if [[ ""${{ steps.determine_outcome.outputs.job_outcome }}"" == ""failure"" ]]; then
            exit 1
          else
            exit 0
          fi
",79,3,1,pull_request_review,3
n8n-io/n8n,e2e-tests.yml,"name: End-to-End tests
run-name: E2E Tests ${{ inputs.branch }} - ${{ inputs.user }}

on:
  schedule:
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      branch:
        description: 'GitHub branch to test.'
        required: false
        default: 'master'
      spec:
        description: 'Specify specs.'
        required: false
        default: 'e2e/*'
        type: string
      user:
        description: 'User who kicked this off.'
        required: false
        default: 'schedule'
      start-url:
        description: 'URL to call after workflow is kicked off.'
        required: false
        default: ''
      success-url:
        description: 'URL to call after workflow is done.'
        required: false
        default: ''

jobs:
  calls-start-url:
    name: Calls start URL
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.start-url != '' }}
    steps:
      - name: Calls start URL
        run: |
          [[ ""${{github.event.inputs.start-url}}"" != """" ]] && curl -v -X POST -d 'url=${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}' ${{github.event.inputs.start-url}} || echo """"
        shell: bash

  run-e2e-tests:
    name: E2E
    uses: ./.github/workflows/e2e-reusable.yml
    with:
      branch: ${{ github.event.inputs.branch || 'master' }}
      user: ${{ github.event.inputs.user || 'PR User' }}
      spec: ${{ github.event.inputs.spec || 'e2e/*' }}
    secrets:
      CYPRESS_RECORD_KEY: ${{ secrets.CYPRESS_RECORD_KEY }}

  calls-success-url-notify:
    name: Calls success URL and notifies
    runs-on: ubuntu-latest
    needs: [run-e2e-tests]
    if: ${{ github.event.inputs.success-url != '' }}
    steps:
      - name: Notify Slack on failure
        uses: act10ns/slack@44541246747a30eb3102d87f7a4cc5471b0ffb7d # v2.1.0
        if: failure()
        with:
          status: ${{ job.status }}
          channel: '#alerts-build'
          webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}
          message: E2E failure for branch `${{ inputs.branch || 'master' }}` deployed by ${{ inputs.user || 'schedule' }} (${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

      - name: Call Success URL - optionally
        run: |
          [[ ""${{github.event.inputs.success-url}}"" != """" ]] && curl -v ${{github.event.inputs.success-url}} || echo """"
        shell: bash
",70,3,2,"schedule, workflow_dispatch",2
n8n-io/n8n,linting-reusable.yml,"name: Reusable linting workflow

on:
  workflow_call:
    inputs:
      ref:
        description: GitHub ref to lint.
        required: false
        type: string
        default: master
      nodeVersion:
        description: Version of node to use.
        required: false
        type: string
        default: 22.x
      cacheKey:
        description: Cache key for modules and build artifacts.
        required: false
        type: string
        default: ''

jobs:
  lint:
    name: Lint
    runs-on: blacksmith-4vcpu-ubuntu-2204
    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
        with:
          ref: ${{ inputs.ref }}

      - name: Setup Environment
        uses: ./.github/actions/setup-and-build
        with:
          node-version: ${{ inputs.nodeVersion }}
          enable-caching: true
          skip-build: ${{ inputs.cacheKey != '' }}

      - name: Lint Backend
        run: pnpm lint:backend

      - name: Lint Nodes
        run: pnpm lint:nodes

      - name: Lint Frontend
        run: pnpm lint:frontend
",45,1,1,workflow_call,2
n8n-io/n8n,notify-pr-status.yml,"name: Notify PR status changed

on:
  pull_request_review:
    types: [submitted, dismissed]
  pull_request:
    types: [closed]

jobs:
  notify:
    runs-on: ubuntu-latest
    if: >-
      (github.event_name == 'pull_request_review' && github.event.review.state == 'approved') ||
      (github.event_name == 'pull_request_review' && github.event.review.state == 'dismissed') ||
      (github.event_name == 'pull_request' && github.event.pull_request.merged == true) ||
      (github.event_name == 'pull_request' && github.event.pull_request.merged == false && github.event.action == 'closed')
    steps:
      - uses: fjogeleit/http-request-action@bf78da14118941f7e940279dd58f67e863cbeff6 # v1
        if: ${{!contains(github.event.pull_request.labels.*.name, 'community')}}
        name: Notify
        env:
          PR_URL: ${{ github.event.pull_request.html_url }}
        with:
          url: ${{ secrets.N8N_NOTIFY_PR_STATUS_CHANGED_URL }}
          method: 'POST'
          customHeaders: '{ ""x-api-token"": ""${{ secrets.N8N_NOTIFY_PR_STATUS_CHANGED_TOKEN }}"" }'
          data: '{ ""event_name"": ""${{ github.event_name }}"", ""pr_url"": ""${{ env.PR_URL }}"",  ""event"": ${{ toJSON(github.event) }} }'
",27,1,2,"pull_request_review, pull_request",1
n8n-io/n8n,release-create-pr.yml,"name: 'Release: Create Pull Request'

on:
  workflow_dispatch:
    inputs:
      base-branch:
        description: 'The branch, tag, or commit to create this release PR from.'
        required: true
        default: 'master'

      release-type:
        description: 'A SemVer release type.'
        required: true
        type: choice
        default: 'minor'
        options:
          - patch
          - minor
          - major

jobs:
  create-release-pr:
    runs-on: ubuntu-latest

    permissions:
      contents: write
      pull-requests: write

    timeout-minutes: 5

    steps:
      - name: Checkout
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
        with:
          fetch-depth: 0
          ref: ${{ github.event.inputs.base-branch }}

      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 22.x

      - run: npm install --prefix=.github/scripts --no-package-lock

      - name: Setup corepack and pnpm
        run: |
          npm i -g corepack@0.33
          corepack enable

      - name: Bump package versions
        run: |
          echo ""NEXT_RELEASE=$(node .github/scripts/bump-versions.mjs)"" >> $GITHUB_ENV
        env:
          RELEASE_TYPE: ${{ github.event.inputs.release-type }}

      - name: Update Changelog
        run: node .github/scripts/update-changelog.mjs

      - name: Push the base branch
        run: |
          git push -f origin refs/remotes/origin/${{ github.event.inputs.base-branch }}:refs/heads/release/${{ env.NEXT_RELEASE }}

      - name: Push the release branch, and Create the PR
        uses: peter-evans/create-pull-request@c5a7806660adbe173f04e3e038b0ccdcd758773c # v6
        with:
          base: 'release/${{ env.NEXT_RELEASE }}'
          branch: 'release-pr/${{ env.NEXT_RELEASE }}'
          commit-message: ':rocket: Release ${{ env.NEXT_RELEASE }}'
          delete-branch: true
          labels: release,release:${{ github.event.inputs.release-type }}
          title: ':rocket: Release ${{ env.NEXT_RELEASE }}'
          body-path: 'CHANGELOG-${{ env.NEXT_RELEASE }}.md'
",71,1,1,workflow_dispatch,3
n8n-io/n8n,release-publish.yml,"name: 'Release: Publish'

on:
  pull_request:
    types:
      - closed
    branches:
      - 'release/*'

jobs:
  publish-to-npm:
    name: Publish to NPM
    runs-on: ubuntu-latest
    if: github.event.pull_request.merged == true
    timeout-minutes: 10
    permissions:
      id-token: write
    env:
      NPM_CONFIG_PROVENANCE: true
    outputs:
      release: ${{ steps.set-release.outputs.release }}
    steps:
      - name: Checkout
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
        with:
          fetch-depth: 0

      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 22.x

      - name: Setup corepack and pnpm
        run: |
          npm i -g corepack@0.33
          corepack enable

      - run: pnpm install --frozen-lockfile

      - name: Set release version in env
        run: echo ""RELEASE=$(node -e 'console.log(require(""./package.json"").version)')"" >> $GITHUB_ENV

      - name: Build
        run: pnpm build

      - name: Cache build artifacts
        uses: actions/cache/save@1bd1e32a3bdc45362d1e726936510720a7c30a57 # v4.2.0
        with:
          path: ./packages/**/dist
          key: ${{ github.sha }}-release:build

      - name: Dry-run publishing
        run: pnpm publish -r --no-git-checks --dry-run

      - name: Pre publishing changes
        run: |
          echo ""//registry.npmjs.org/:_authToken=${{ secrets.NPM_TOKEN }}"" > ~/.npmrc
          node .github/scripts/trim-fe-packageJson.js
          node .github/scripts/ensure-provenance-fields.mjs
          cp README.md packages/cli/README.md
          sed -i ""s/default: 'dev'/default: 'stable'/g"" packages/cli/dist/config/schema.js

      - name: Publish to NPM
        run: pnpm publish -r --publish-branch ${{github.event.pull_request.base.ref}} --access public --tag rc --no-git-checks

      - name: Cleanup rc tag
        run: npm dist-tag rm n8n rc
        continue-on-error: true

      - id: set-release
        run: echo ""release=${{ env.RELEASE }}"" >> $GITHUB_OUTPUT

  publish-to-docker-hub:
    name: Publish to DockerHub
    needs: [publish-to-npm]
    uses: ./.github/workflows/docker-build-push.yml
    with:
      n8n_version: ${{ needs.publish-to-npm.outputs.release }}
      release_type: stable
    secrets: inherit

  create-github-release:
    name: Create a GitHub Release
    needs: [publish-to-npm, publish-to-docker-hub]
    runs-on: ubuntu-latest
    if: github.event.pull_request.merged == true
    timeout-minutes: 5

    permissions:
      contents: write
      id-token: write

    steps:
      - name: Create a Release on GitHub
        uses: ncipollo/release-action@440c8c1cb0ed28b9f43e4d1d670870f059653174 # v1
        with:
          commit: ${{github.event.pull_request.base.ref}}
          tag: 'n8n@${{ needs.publish-to-npm.outputs.release }}'
          prerelease: true
          makeLatest: false
          body: ${{github.event.pull_request.body}}

  create-sentry-release:
    name: Create a Sentry Release
    needs: [publish-to-npm, publish-to-docker-hub]
    runs-on: ubuntu-latest
    if: github.event.pull_request.merged == true
    timeout-minutes: 5
    env:
      SENTRY_AUTH_TOKEN: ${{ secrets.SENTRY_AUTH_TOKEN }}
      SENTRY_ORG: ${{ secrets.SENTRY_ORG }}

    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
      - name: Restore cached build artifacts
        uses: actions/cache/restore@1bd1e32a3bdc45362d1e726936510720a7c30a57 # v4.2.0
        with:
          path: ./packages/**/dist
          key: ${{ github.sha }}-release:build

      - name: Create a frontend release
        uses: getsentry/action-release@e769183448303de84c5a06aaaddf9da7be26d6c7 # v1.7.0
        continue-on-error: true
        with:
          projects: ${{ secrets.SENTRY_FRONTEND_PROJECT }}
          version: n8n@${{ needs.publish-to-npm.outputs.release }}
          sourcemaps: packages/frontend/editor-ui/dist

      - name: Create a backend release
        uses: getsentry/action-release@e769183448303de84c5a06aaaddf9da7be26d6c7 # v1.7.0
        continue-on-error: true
        with:
          projects: ${{ secrets.SENTRY_BACKEND_PROJECT }}
          version: n8n@${{ needs.publish-to-npm.outputs.release }}
          sourcemaps: packages/cli/dist packages/core/dist packages/nodes-base/dist packages/@n8n/n8n-nodes-langchain/dist

      - name: Create a task runner release
        uses: getsentry/action-release@e769183448303de84c5a06aaaddf9da7be26d6c7 # v1.7.0
        continue-on-error: true
        with:
          projects: ${{ secrets.SENTRY_TASK_RUNNER_PROJECT }}
          version: n8n@${{ needs.publish-to-npm.outputs.release }}
          sourcemaps: packages/core/dist packages/workflow/dist packages/@n8n/task-runner/dist

  trigger-release-note:
    name: Trigger a release note
    needs: [publish-to-npm, create-github-release]
    if: github.event.pull_request.merged == true
    runs-on: ubuntu-latest
    steps:
      - name: Trigger a release note
        run: curl -u docsWorkflows:${{ secrets.N8N_WEBHOOK_DOCS_PASSWORD }} --request GET 'https://internal.users.n8n.cloud/webhook/trigger-release-note' --header 'Content-Type:application/json' --data '{""version"":""${{ needs.publish-to-npm.outputs.release }}""}'

  # merge-back-into-master:
  #   name: Merge back into master
  #   needs: [publish-to-npm, create-github-release]
  #   if: ${{ github.event.pull_request.merged == true && !contains(github.event.pull_request.labels.*.name, 'release:patch') }}
  #   runs-on: ubuntu-latest
  #   steps:
  #     - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11
  # v4.1.1
  #         fetch-depth: 0
  #     - run: |
  #         git checkout --track origin/master
  #         git config user.name ""github-actions[bot]""
  #         git config user.email 41898282+github-actions[bot]@users.noreply.github.com
  #         git merge --ff n8n@${{ needs.publish-to-npm.outputs.release }}
  #         git push origin master
  #         git push origin :${{github.event.pull_request.base.ref}}
",168,5,1,pull_request,11
n8n-io/n8n,release-push-to-channel.yml,"name: 'Release: Push to Channel'

on:
  workflow_dispatch:
    inputs:
      version:
        description: 'n8n Release version to push to a channel (e.g., 1.2.3 or 1.2.3-beta.4)'
        required: true
        type: string

      release-channel:
        description: 'Release channel'
        required: true
        type: choice
        default: 'beta'
        options:
          - beta
          - stable

jobs:
  validate-inputs:
    name: Validate Inputs
    runs-on: ubuntu-latest
    outputs:
      version: ${{ steps.check_version.outputs.version }}
      release_channel: ${{ github.event.inputs.release-channel }}
    steps:
      - name: Check Version Format
        id: check_version
        run: |
          input_version=""${{ github.event.inputs.version }}""
          version_regex='^[0-9]+\.[0-9]+\.[0-9]+(-[a-zA-Z0-9.-]+)?$'

          if [[ ""$input_version"" =~ $version_regex ]]; then
            echo ""Version format is valid: $input_version""
            echo ""version=$input_version"" >> $GITHUB_OUTPUT
          else
            echo ""::error::Invalid version format provided: '$input_version'. Must match regex '$version_regex'.""
            exit 1
          fi

  release-to-npm:
    name: Release to NPM
    runs-on: ubuntu-latest
    needs: validate-inputs
    timeout-minutes: 5
    steps:
      - uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 22.x

      - run: echo ""//registry.npmjs.org/:_authToken=${{ secrets.NPM_TOKEN }}"" > ~/.npmrc

      - name: Add beta/next tags to NPM
        if: needs.validate-inputs.outputs.release_channel == 'beta'
        run: |
          npm dist-tag add ""n8n@${{ needs.validate-inputs.outputs.version }}"" next
          npm dist-tag add ""n8n@${{ needs.validate-inputs.outputs.version }}"" beta

      - name: Add latest/stable tags to NPM
        if: needs.validate-inputs.outputs.release_channel == 'stable'
        run: |
          npm dist-tag add ""n8n@${{ needs.validate-inputs.outputs.version }}"" latest
          npm dist-tag add ""n8n@${{ needs.validate-inputs.outputs.version }}"" stable

  release-to-docker-hub:
    name: Release to DockerHub
    runs-on: ubuntu-latest
    needs: validate-inputs
    timeout-minutes: 5
    steps:
      - uses: docker/login-action@9780b0c442fbb1117ed29e0efdff1e18412f7567 # v3.3.0
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Tag stable/latest Docker image
        if: needs.validate-inputs.outputs.release_channel == 'stable'
        run: |
          docker buildx imagetools create -t ""${{ secrets.DOCKER_USERNAME }}/n8n:stable"" ""${{ secrets.DOCKER_USERNAME }}/n8n:${{ needs.validate-inputs.outputs.version }}""
          docker buildx imagetools create -t ""${{ secrets.DOCKER_USERNAME }}/n8n:latest"" ""${{ secrets.DOCKER_USERNAME }}/n8n:${{ needs.validate-inputs.outputs.version }}""

      - name: Tag beta/next Docker image
        if: needs.validate-inputs.outputs.release_channel == 'beta'
        run: |
          docker buildx imagetools create -t ""${{ secrets.DOCKER_USERNAME }}/n8n:beta"" ""${{ secrets.DOCKER_USERNAME }}/n8n:${{ needs.validate-inputs.outputs.version }}""
          docker buildx imagetools create -t ""${{ secrets.DOCKER_USERNAME }}/n8n:next"" ""${{ secrets.DOCKER_USERNAME }}/n8n:${{ needs.validate-inputs.outputs.version }}""

  release-to-github-container-registry:
    name: Release to GitHub Container Registry
    runs-on: ubuntu-latest
    needs: validate-inputs
    timeout-minutes: 5
    steps:
      - uses: docker/login-action@9780b0c442fbb1117ed29e0efdff1e18412f7567 # v3.3.0
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Tag stable/latest GHCR image
        if: needs.validate-inputs.outputs.release_channel == 'stable'
        run: |
          docker buildx imagetools create -t ""ghcr.io/${{ github.repository_owner }}/n8n:stable"" ""ghcr.io/${{ github.repository_owner }}/n8n:${{ needs.validate-inputs.outputs.version }}""
          docker buildx imagetools create -t ""ghcr.io/${{ github.repository_owner }}/n8n:latest"" ""ghcr.io/${{ github.repository_owner }}/n8n:${{ needs.validate-inputs.outputs.version }}""

      - name: Tag beta/next GHCR image
        if: needs.validate-inputs.outputs.release_channel == 'beta'
        run: |
          docker buildx imagetools create -t ""ghcr.io/${{ github.repository_owner }}/n8n:beta"" ""ghcr.io/${{ github.repository_owner }}/n8n:${{ needs.validate-inputs.outputs.version }}""
          docker buildx imagetools create -t ""ghcr.io/${{ github.repository_owner }}/n8n:next"" ""ghcr.io/${{ github.repository_owner }}/n8n:${{ needs.validate-inputs.outputs.version }}""

  update-docs:
    name: Update latest and next in the docs
    runs-on: ubuntu-latest
    needs: [validate-inputs, release-to-npm, release-to-docker-hub]
    steps:
      - continue-on-error: true
        run: curl -u docsWorkflows:${{ secrets.N8N_WEBHOOK_DOCS_PASSWORD }} --request GET 'https://internal.users.n8n.cloud/webhook/update-latest-next'
",119,5,1,workflow_dispatch,3
n8n-io/n8n,security-trivy-scan-callable.yml,"name: Security - Scan Docker Image With Trivy

on:
  workflow_dispatch:
    inputs:
      image_ref:
        description: Full image reference to scan e.g ghcr.io/n8n-io/n8n:latest
        required: true
        default: 'ghcr.io/n8n-io/n8n:latest'
  workflow_call:
    inputs:
      image_ref:
        type: string
        description: Full image reference to scan e.g ghcr.io/n8n-io/n8n:latest
        required: true
    secrets:
      SLACK_WEBHOOK_URL:
        required: true

jobs:
  security_scan:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Pull Docker image
        run: |
          docker pull ${{ inputs.image_ref }}

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@6c175e9c4083a92bbca2f9724c8a5e33bc2d97a5 # v0.30.0
        with:
          image-ref: ${{ inputs.image_ref }}
          format: 'json'
          output: 'trivy-results.json'
          severity: 'CRITICAL,HIGH,MEDIUM,LOW'
          vuln-type: 'os,library'
          ignore-unfixed: false
          exit-code: '0'

      - name: Process vulnerability results
        id: process_vulns
        run: |
          if [ -f trivy-results.json ]; then
            CRITICAL_COUNT=$(jq '[.Results[]?.Vulnerabilities[]? | select(.Severity == ""CRITICAL"")] | length' trivy-results.json)
            HIGH_COUNT=$(jq '[.Results[]?.Vulnerabilities[]? | select(.Severity == ""HIGH"")] | length' trivy-results.json)
            MEDIUM_COUNT=$(jq '[.Results[]?.Vulnerabilities[]? | select(.Severity == ""MEDIUM"")] | length' trivy-results.json)
            LOW_COUNT=$(jq '[.Results[]?.Vulnerabilities[]? | select(.Severity == ""LOW"")] | length' trivy-results.json)

            TOTAL_VULNS=$((CRITICAL_COUNT + HIGH_COUNT + MEDIUM_COUNT + LOW_COUNT))

            echo ""critical_count=${CRITICAL_COUNT}"" >> $GITHUB_OUTPUT
            echo ""high_count=${HIGH_COUNT}"" >> $GITHUB_OUTPUT
            echo ""medium_count=${MEDIUM_COUNT}"" >> $GITHUB_OUTPUT
            echo ""low_count=${LOW_COUNT}"" >> $GITHUB_OUTPUT
            echo ""total_count=${TOTAL_VULNS}"" >> $GITHUB_OUTPUT

            if [ $TOTAL_VULNS -gt 0 ]; then
              echo ""vulnerabilities_found=true"" >> $GITHUB_OUTPUT

              # Extract top vulnerabilities for display (limit to 10 for readability)
              TOP_VULNS=$(jq -r '
                .Results[]?
                | .Vulnerabilities[]?
                | select(.Severity == ""CRITICAL"" or .Severity == ""HIGH"" or .Severity == ""MEDIUM"" or .Severity == ""LOW"")
                | ""• \(.VulnerabilityID): \(.Title // ""No title"") (\(.Severity))""
              ' trivy-results.json | head -10)

              echo ""top_vulnerabilities<<EOF"" >> $GITHUB_OUTPUT
              echo ""$TOP_VULNS"" >> $GITHUB_OUTPUT
              echo ""EOF"" >> $GITHUB_OUTPUT
            else
              echo ""vulnerabilities_found=false"" >> $GITHUB_OUTPUT
            fi
          else
            echo ""Trivy results file not found.""
            echo ""vulnerabilities_found=false"" >> $GITHUB_OUTPUT
          fi

      - name: Notify Slack - Vulnerabilities Found
        uses: act10ns/slack@44541246747a30eb3102d87f7a4cc5471b0ffb7d # v2.1.0
        if: steps.process_vulns.outputs.vulnerabilities_found == 'true'
        with:
          status: 'warning'
          channel: '#mission-security'
          webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}
          message: |
            :warning: *Trivy Scan: Vulnerabilities Detected*

            *Repository:* `${{ github.repository }}`
            *Image Ref:* `${{ inputs.image_ref }}`

            *Vulnerability Summary:*
            • *Critical:* ${{ steps.process_vulns.outputs.critical_count }}
            • *High:* ${{ steps.process_vulns.outputs.high_count }}
            • *Medium:* ${{ steps.process_vulns.outputs.medium_count }}
            • *Low:* ${{ steps.process_vulns.outputs.low_count }}
            • *Total:* ${{ steps.process_vulns.outputs.total_count }}

            *Top Vulnerabilities (showing first 10):*
            ```
            ${{ steps.process_vulns.outputs.top_vulnerabilities }}
            ```

            :point_right: <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Full Scan Results>

      - name: Notify Slack - No Vulnerabilities
        uses: act10ns/slack@44541246747a30eb3102d87f7a4cc5471b0ffb7d # v2.1.0
        if: steps.process_vulns.outputs.vulnerabilities_found == 'false'
        with:
          status: 'success'
          channel: '#mission-security'
          webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}
          message: |
            :white_check_mark: *Trivy Scan: All Clear*

            *Repository:* `${{ github.repository }}`
            *Image Ref:* `${{ inputs.image_ref }}`

            No vulnerabilities detected in the Docker image scan.

            :point_right: <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Scan Results>
",123,1,2,"workflow_dispatch, workflow_call",4
n8n-io/n8n,test-workflows-callable.yml,"name: Callable Test Workflows

on:
  workflow_call:
    inputs:
      git_ref:
        description: 'The Git ref (branch, tag, or SHA) to checkout and test.'
        required: true
        type: string
      send_webhook_report:
        description: 'Set to true to send test results to the webhook.'
        required: false
        type: boolean
        default: false
      pr_number:
        description: 'The PR number, if applicable (for context in webhook).'
        required: false
        type: string
        default: ''
    secrets:
      ENCRYPTION_KEY:
        description: 'Encryption key for n8n operations.'
        required: true
      CI_SENTRY_DSN:
        description: 'Sentry DSN for CI test runs.'
        required: false
      WORKFLOW_TESTS_RESULT_DESTINATION:
        description: 'Webhook URL to send test results to (if enabled).'
        required: false

jobs:
  build_and_test:
    name: Install, Build, and Test Workflows
    runs-on: blacksmith-2vcpu-ubuntu-2204
    timeout-minutes: 10
    env:
      N8N_ENCRYPTION_KEY: ${{ secrets.ENCRYPTION_KEY }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
        with:
          ref: ${{ inputs.git_ref }}

      - name: Setup Environment and Build Project
        uses: ./.github/actions/setup-and-build
        with:
          node-version: '22.x'
          cache-suffix: 'workflow-test'

      - name: Install OS dependencies
        run: |
          sudo apt update -y
          echo 'tzdata tzdata/Areas select Europe' | sudo debconf-set-selections
          echo 'tzdata tzdata/Zones/Europe select Paris' | sudo debconf-set-selections
          sudo DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends graphicsmagick
          sudo apt-get clean
          sudo rm -rf /var/lib/apt/lists/*

      - name: Import credentials
        run: ./packages/cli/bin/n8n import:credentials --input=test-workflows/credentials.json

      - name: Import workflows
        run: ./packages/cli/bin/n8n import:workflow --separate --input=test-workflows/workflows


      - name: Copy static assets
        run: |
          mkdir -p /tmp/testData/pdfs
          cp assets/n8n-logo.png /tmp/n8n-logo.png
          cp assets/n8n-screenshot.png /tmp/n8n-screenshot.png
          cp test-workflows/testData/pdfs/*.pdf /tmp/testData/pdfs/

      - name: Run tests
        id: tests
        run: ./packages/cli/bin/n8n executeBatch --shallow --skipList=test-workflows/skipList.json --githubWorkflow --shortOutput --output=test-results.json --concurrency=16 --compare=test-workflows/snapshots
        continue-on-error: true
        env:
          SKIP_STATISTICS_EVENTS: ""true""
          DB_SQLITE_POOL_SIZE: ""4""
          N8N_SENTRY_DSN: ${{ secrets.CI_SENTRY_DSN }}

      - name: Report test outcome
        if: always()
        run: |
          echo ""Test step outcome was: ${{ steps.tests.outcome }}""
          if [[ ""${{ steps.tests.outcome }}"" == ""failure"" ]]; then
            echo ""Workflow tests failed but the workflow will continue.""
          elif [[ ""${{ steps.tests.outcome }}"" == ""success"" ]]; then
            echo ""Workflow tests passed.""
          else
            echo ""Workflow tests outcome: ${{ steps.tests.outcome }}""
          fi

      - name: Prepare and Send Test Results to Webhook
        if: inputs.send_webhook_report == true
        shell: bash
        env:
          WEBHOOK_URL: ${{ secrets.WORKFLOW_TESTS_RESULT_DESTINATION }}
          TEST_RESULTS_FILE: ./test-results.json
          GH_REPOSITORY: ${{ github.repository }}
          GH_RUN_ID: ${{ github.run_id }}
          GH_RUN_ATTEMPT: ${{ github.run_attempt }}
          GH_REF_TESTED: ${{ inputs.git_ref }}
          GH_EVENT_NAME: ${{ github.event_name }}
          GH_PR_NUMBER_INPUT: ${{ inputs.pr_number }}
          GH_WORKFLOW_RUN_URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
          GH_ACTOR: ${{ github.actor }}
        run: |
          echo ""Attempting to send test results to webhook...""
          echo ""Test results file expected at: $TEST_RESULTS_FILE""

          if [ ! -f ""$TEST_RESULTS_FILE"" ]; then
            echo ""::warning::Test results file ($TEST_RESULTS_FILE) not found. Skipping webhook.""
            exit 0
          fi

          if ! command -v jq &> /dev/null; then
            echo ""jq not found. Installing jq...""
            sudo apt-get update -qq && sudo apt-get install -y -qq jq
            if ! command -v jq &> /dev/null; then
              echo ""::error::Failed to install jq. Cannot process JSON.""
              exit 1
            fi
          fi

          pr_number_to_send=""$GH_PR_NUMBER_INPUT""

          echo ""Preparing JSON payload...""
          if [ ! -s ""$TEST_RESULTS_FILE"" ]; then
            echo ""::warning::Test results file ($TEST_RESULTS_FILE) is empty. Sending only GitHub context.""
            enriched_payload=$(jq -n \
              --arg repository ""$GH_REPOSITORY"" \
              --arg run_id ""$GH_RUN_ID"" \
              --arg run_attempt ""$GH_RUN_ATTEMPT"" \
              --arg ref_tested ""$GH_REF_TESTED"" \
              --arg event_name ""$GH_EVENT_NAME"" \
              --arg pr_num ""$pr_number_to_send"" \
              --arg workflow_run_url ""$GH_WORKFLOW_RUN_URL"" \
              --arg actor ""$GH_ACTOR"" \
              '{
                githubWorkflowContext: {
                  repository: $repository,
                  runId: $run_id,
                  runAttempt: $run_attempt,
                  gitRefTested: $ref_tested,
                  triggeringEventName: $event_name,
                  prNumber: (if $pr_num == """" then null else $pr_num | tonumber? // $pr_num end),
                  workflowRunUrl: $workflow_run_url,
                  triggeredBy: $actor
                }
              }')
          else
            enriched_payload=$(jq \
              --arg repository ""$GH_REPOSITORY"" \
              --arg run_id ""$GH_RUN_ID"" \
              --arg run_attempt ""$GH_RUN_ATTEMPT"" \
              --arg ref_tested ""$GH_REF_TESTED"" \
              --arg event_name ""$GH_EVENT_NAME"" \
              --arg pr_num ""$pr_number_to_send"" \
              --arg workflow_run_url ""$GH_WORKFLOW_RUN_URL"" \
              --arg actor ""$GH_ACTOR"" \
              '. + {
                githubWorkflowContext: {
                  repository: $repository,
                  runId: $run_id,
                  runAttempt: $run_attempt,
                  gitRefTested: $ref_tested,
                  triggeringEventName: $event_name,
                  prNumber: (if $pr_num == """" then null else $pr_num | tonumber? // $pr_num end),
                  workflowRunUrl: $workflow_run_url,
                  triggeredBy: $actor
                }
              }' ""$TEST_RESULTS_FILE"")
          fi

          jq_exit_code=$?
          if [ $jq_exit_code -ne 0 ] || [ -z ""$enriched_payload"" ]; then
            echo ""::error::Failed to process JSON with jq (exit code: $jq_exit_code). Input file: $TEST_RESULTS_FILE""
            if [ -s ""$TEST_RESULTS_FILE"" ]; then
              echo ""Contents of $TEST_RESULTS_FILE that may have caused an error:""
              head -c 1000 ""$TEST_RESULTS_FILE"" # Print first 1000 chars
              echo """" # Newline after head
            elif [ -f ""$TEST_RESULTS_FILE"" ]; then
              echo ""$TEST_RESULTS_FILE exists but is empty.""
            fi
            exit 1
          fi

          echo ""Enriched payload to send (first 500 chars):""
          echo ""$enriched_payload"" | head -c 500
          echo """"

          echo ""Sending data to webhook: $WEBHOOK_URL""
          http_response_code=$(curl -s -w ""%{http_code}"" \
            -X POST \
            -H ""Content-Type: application/json"" \
            -H ""X-GitHub-Event: $GH_EVENT_NAME"" \
            -H ""X-GitHub-Run-Id: $GH_RUN_ID"" \
            --data ""$enriched_payload"" \
            ""$WEBHOOK_URL"" \
            -o curl_response_body.txt 2>curl_stderr.txt)

          curl_stderr_content=$(cat curl_stderr.txt)
          if [ -n ""$curl_stderr_content"" ]; then
            echo ""::warning::curl stderr: $curl_stderr_content""
          fi
          echo ""Webhook response code: $http_response_code""
          echo ""Webhook response body:""
          cat curl_response_body.txt
          if [[ ""$http_response_code"" -ge 200 && ""$http_response_code"" -lt 300 ]]; then
            echo ""Successfully sent data to webhook.""
          else
            echo ""::error::Webhook call failed with status code $http_response_code.""
          fi",215,1,1,workflow_call,2
n8n-io/n8n,test-workflows-nightly.yml,"name: Test Workflows Nightly and Manual

on:
  schedule:
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      git_ref_to_test:
        description: 'The Git ref (branch, tag, or SHA) to run tests against.'
        required: true
        type: string
        default: 'master'


permissions:
  contents: read

jobs:
  run_workflow_tests:
    name: Run Workflow Tests
    uses: ./.github/workflows/test-workflows-callable.yml
    with:
      git_ref: ${{ github.event_name == 'schedule' && 'master' || github.event.inputs.git_ref_to_test }}
      send_webhook_report: false
      pr_number: ''
    secrets: inherit",26,1,2,"schedule, workflow_dispatch",1
n8n-io/n8n,test-workflows-pr-approved.yml,"name: Test Workflows on PR Approval

on:
  pull_request_review:
    types: [submitted]

permissions:
  contents: read

jobs:
  eligibility_check:
    name: Check Eligibility for Test Run
    if: github.event.review.state == 'approved'
    uses: ./.github/workflows/check-run-eligibility.yml
    with:
      is_pr_approved_by_maintainer: true

  run_workflow_tests:
    name: Run Tests on Approved Internal PR
    needs: [eligibility_check]
    if: needs.eligibility_check.outputs.should_run == 'true'
    uses: ./.github/workflows/test-workflows-callable.yml
    with:
      git_ref: ${{ github.event.pull_request.head.sha }}
      send_webhook_report: true
      pr_number: ${{ github.event.pull_request.number }}
    secrets: inherit",27,2,1,pull_request_review,2
n8n-io/n8n,test-workflows-pr-comment.yml,"name: Test Workflows on PR Comment

on:
  issue_comment:
    types: [created]

permissions:
  pull-requests: read
  contents: read

jobs:
  handle_comment_command:
    name: Handle /test-workflows Command
    if: github.event.issue.pull_request && startsWith(github.event.comment.body, '/test-workflows')
    runs-on: ubuntu-latest
    outputs:
      permission_granted: ${{ steps.pr_check_and_details.outputs.permission_granted }}
      git_ref: ${{ steps.pr_check_and_details.outputs.head_sha }}
      pr_number: ${{ steps.pr_check_and_details.outputs.pr_number_string }}

    steps:
      - name: Validate User, Get PR Details, and React
        id: pr_check_and_details
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const commenter = context.actor;
            const issueOwner = context.repo.owner;
            const issueRepo = context.repo.repo;
            const commentId = context.payload.comment.id;
            const prNumber = context.issue.number; // In issue_comment on a PR, issue.number is the PR number

            // Function to add a reaction to the comment
            async function addReaction(content) {
              try {
                await github.rest.reactions.createForIssueComment({
                  owner: issueOwner,
                  repo: issueRepo,
                  comment_id: commentId,
                  content: content
                });
              } catch (reactionError) {
                // Log if reaction fails but don't fail the script for this
                console.log(`Failed to add reaction '${content}': ${reactionError.message}`);
              }
            }

            // Initialize outputs to a non-triggering state
            core.setOutput('permission_granted', 'false');
            core.setOutput('head_sha', '');
            core.setOutput('pr_number_string', '');

            // 1. Check user permissions
            try {
              const { data: permissions } = await github.rest.repos.getCollaboratorPermissionLevel({
                owner: issueOwner,
                repo: issueRepo,
                username: commenter
              });

              const allowedPermissions = ['admin', 'write', 'maintain'];
              if (!allowedPermissions.includes(permissions.permission)) {
                console.log(`User @${commenter} has '${permissions.permission}' permission. Needs 'admin', 'write', or 'maintain'.`);
                await addReaction('-1'); // User does not have permission
                return; // Exit script, tests will not be triggered
              }
              console.log(`User @${commenter} has '${permissions.permission}' permission.`);
            } catch (error) {
              console.log(`Could not verify permissions for @${commenter}: ${error.message}`);
              await addReaction('confused'); // Error checking permissions
              return; // Exit script
            }

            // 2. Fetch PR details (if permission check passed)
            let headSha;
            try {
              const { data: pr } = await github.rest.pulls.get({
                owner: issueOwner,
                repo: issueRepo,
                pull_number: prNumber,
              });
              headSha = pr.head.sha;
              console.log(`Workspaced PR details: SHA - ${headSha}, PR Number - ${prNumber}`);

              // Set outputs for the next job
              core.setOutput('permission_granted', 'true');
              core.setOutput('head_sha', headSha);
              core.setOutput('pr_number_string', prNumber.toString());
              await addReaction('+1'); // Command accepted, tests will be triggered

            } catch (error) {
              console.log(`Failed to fetch PR details for PR #${prNumber}: ${error.message}`);
              core.setOutput('permission_granted', 'false'); // Ensure this is false if PR fetch fails
              await addReaction('confused'); // Error fetching PR details
            }

  trigger_reusable_tests:
    name: Trigger Reusable Test Workflow
    needs: handle_comment_command

    if: >
      always() &&
      needs.handle_comment_command.result != 'skipped' &&
      needs.handle_comment_command.outputs.permission_granted == 'true' &&
      needs.handle_comment_command.outputs.git_ref != ''
    uses: ./.github/workflows/test-workflows-callable.yml
    with:
      git_ref: ${{ needs.handle_comment_command.outputs.git_ref }}
      send_webhook_report: true
      pr_number: ${{ needs.handle_comment_command.outputs.pr_number }}
    secrets: inherit
",112,2,1,issue_comment,2
n8n-io/n8n,units-tests-reusable.yml,"name: Reusable units test workflow

on:
  workflow_call:
    inputs:
      ref:
        description: GitHub ref to test.
        required: false
        type: string
        default: master
      nodeVersion:
        description: Version of node to use.
        required: false
        type: string
        default: 22.x
      cacheKey:
        description: Cache key for modules and build artifacts.
        required: false
        default: ''
        type: string
      collectCoverage:
        required: false
        default: false
        type: boolean
      ignoreTurboCache:
        required: false
        default: false
        type: boolean
      skipFrontendTests:
        required: false
        default: false
        type: boolean
    secrets:
      CODECOV_TOKEN:
        description: 'Codecov upload token.'
        required: false

jobs:
  unit-test:
    name: Unit tests
    runs-on: blacksmith-4vcpu-ubuntu-2204
    env:
      TURBO_FORCE: ${{ inputs.ignoreTurboCache }}
      COVERAGE_ENABLED: ${{ inputs.collectCoverage }}
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          ref: ${{ inputs.ref }}

      - name: Setup Environment and Build Project
        uses: ./.github/actions/setup-and-build
        with:
          node-version: ${{ inputs.nodeVersion }}
          skip-build: ${{ inputs.cacheKey != '' }}

      - name: Restore cached build artifacts only
        if: ${{ inputs.cacheKey != '' }}
        uses: useblacksmith/cache/restore@c5fe29eb0efdf1cf4186b9f7fcbbcbc0cf025662 # v5.0.2
        with:
          path: ./packages/**/dist
          key: ${{ inputs.cacheKey }}
          fail-on-cache-miss: true

      - name: Test Backend
        run: pnpm test:backend

      - name: Test Nodes
        run: pnpm test:nodes

      - name: Test Frontend
        if: ${{ !inputs.skipFrontendTests }}
        run: pnpm test:frontend

      - name: Upload test results to Codecov
        if: ${{ !cancelled() }}
        uses: codecov/test-results-action@47f89e9acb64b76debcd5ea40642d25a4adced9f # v1.1.1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@18283e04ce6e62d37312384ff67231eb8fd56d24 # v5.4.3
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
",83,1,1,workflow_call,5
nodejs/node,auto-start-ci.yml,"name: Auto Start CI

on:
  schedule:
    # Runs every five minutes (fastest the scheduler can run). Five minutes is
    # optimistic, it can take longer to run.
    # To understand why `schedule` is used instead of other events, refer to
    # ./doc/contributing/commit-queue.md
    - cron: '*/5 * * * *'

concurrency: ${{ github.workflow }}

env:
  NODE_VERSION: lts/*

permissions:
  contents: read

jobs:
  get-prs-for-ci:
    permissions:
      pull-requests: read
    if: github.repository == 'nodejs/node'
    runs-on: ubuntu-latest
    outputs:
      numbers: ${{ steps.get_prs_for_ci.outputs.numbers }}
    steps:
      - name: Get Pull Requests
        id: get_prs_for_ci
        run: >
          numbers=$(gh pr list \
                  --repo ${{ github.repository }} \
                  --label 'request-ci' \
                  --json 'number' \
                  -t '{{ range . }}{{ .number }} {{ end }}' \
                  --limit 5)
          echo ""numbers=$numbers"" >> $GITHUB_OUTPUT
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  start-ci:
    permissions:
      contents: read
      pull-requests: write
    needs: get-prs-for-ci
    if: needs.get-prs-for-ci.outputs.numbers != ''
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false

      - name: Install Node.js
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020  # v4.4.0
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install @node-core/utils
        run: npm install -g @node-core/utils

      - name: Setup @node-core/utils
        run: |
          ncu-config set username ""$USERNAME""
          ncu-config set token ""$GH_TOKEN""
          ncu-config set jenkins_token ""$JENKINS_TOKEN""
          ncu-config set owner ""${{ github.repository_owner }}""
          ncu-config set repo ""$(echo ${{ github.repository }} | cut -d/ -f2)""
        env:
          USERNAME: ${{ secrets.JENKINS_USER }}
          GH_TOKEN: ${{ secrets.GH_USER_TOKEN }}
          JENKINS_TOKEN: ${{ secrets.JENKINS_TOKEN }}

      - name: Start the CI
        run: ./tools/actions/start-ci.sh ${{ needs.get-prs-for-ci.outputs.numbers }}
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
",75,2,1,schedule,2
nodejs/node,build-tarball.yml,"name: Build from tarball

on:
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
    paths-ignore:
      - .mailmap
      - '**.md'
      - AUTHORS
      - doc/**
      - .github/**
      - '!.github/workflows/build-tarball.yml'
  push:
    branches:
      - main
      - v[0-9]+.x-staging
      - v[0-9]+.x
    paths-ignore:
      - .mailmap
      - '**.md'
      - AUTHORS
      - doc/**
      - .github/**
      - '!.github/workflows/build-tarball.yml'

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.12'
  FLAKY_TESTS: keep_retrying
  CC: sccache clang
  CXX: sccache clang++
  SCCACHE_GHA_ENABLED: 'true'

permissions:
  contents: read

jobs:
  build-tarball:
    if: github.event.pull_request.draft == false
    runs-on: ubuntu-24.04
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Set up sccache
        uses: Mozilla-Actions/sccache-action@7d986dd989559c6ecdb630a3fd2557667be217ad  # v0.0.9
        with:
          version: v0.10.0
      - name: Environment Information
        run: npx envinfo
      - name: Make tarball
        run: |
          export DISTTYPE=nightly
          export DATESTRING=`date ""+%Y-%m-%d""`
          export COMMIT=$(git rev-parse --short=10 ""$GITHUB_SHA"")
          ./configure && make tar -j8 SKIP_XZ=1
          mkdir tarballs
          mv *.tar.gz tarballs
      - name: Upload tarball artifact
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # v4.6.2
        with:
          name: tarballs
          path: tarballs
  test-tarball-linux:
    needs: build-tarball
    runs-on: ubuntu-24.04
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Set up sccache
        uses: Mozilla-Actions/sccache-action@7d986dd989559c6ecdb630a3fd2557667be217ad  # v0.0.9
        with:
          version: v0.10.0
      - name: Environment Information
        run: npx envinfo
      - name: Download tarball
        uses: actions/download-artifact@d3f86a106a0bac45b974a628896c90dbdf5c8093  # v4.3.0
        with:
          name: tarballs
          path: tarballs
      - name: Extract tarball
        run: |
          tar xzf tarballs/*.tar.gz -C $RUNNER_TEMP
          echo ""TAR_DIR=$RUNNER_TEMP/`basename tarballs/*.tar.gz .tar.gz`"" >> $GITHUB_ENV
      - name: Copy directories needed for testing
        run: |
          cp -r tools/eslint $TAR_DIR/tools
          cp -r tools/eslint-rules $TAR_DIR/tools
      - name: Build
        run: |
          cd $TAR_DIR
          make build-ci -j4 V=1
      - name: Test
        run: |
          cd $TAR_DIR
          make run-ci -j4 V=1 TEST_CI_ARGS=""-p dots  --measure-flakiness 9""
",108,2,2,"pull_request, push",8
nodejs/node,close-stale-feature-requests.yml,"name: Close stale feature requests
on:
  workflow_dispatch:
  schedule:
    # Run every day at 1:00 AM UTC.
    - cron: 0 1 * * *

# yamllint disable rule:empty-lines
env:
  CLOSE_MESSAGE: >
    There has been no activity on this feature request
    and it is being closed. If you feel closing this issue is not the
    right thing to do, please leave a comment.


    For more information on how the project manages
    feature requests, please consult the
    [feature request management document](https://github.com/nodejs/node/blob/HEAD/doc/contributing/feature-request-management.md).

  WARN_MESSAGE: >
    There has been no activity on this feature request for
    5 months. To help maintain relevant open issues, please
    add the https://github.com/nodejs/node/labels/never-stale
    label or close this issue if it should be closed. If not,
    the issue will be automatically closed 6 months after the
    last non-automated comment.

    For more information on how the project manages
    feature requests, please consult the
    [feature request management document](https://github.com/nodejs/node/blob/HEAD/doc/contributing/feature-request-management.md).
# yamllint enable

permissions:
  contents: read

jobs:
  stale:
    permissions:
      issues: write  # for actions/stale to close stale issues
      pull-requests: write  # for actions/stale to close stale PRs
    if: github.repository == 'nodejs/node'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/stale@5bef64f19d7facfb25b37b414482c7164d639639  # v9.1.0
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          days-before-stale: 180
          days-before-close: 30
          stale-issue-label: stale
          close-issue-message: ${{ env.CLOSE_MESSAGE }}
          stale-issue-message: ${{ env.WARN_MESSAGE }}
          only-labels: feature request
          exempt-issue-labels: never-stale
          # max requests it will send per run to the GitHub API before it deliberately exits to avoid hitting API rate limits
          operations-per-run: 500
          remove-stale-when-updated: true
",56,1,2,"workflow_dispatch, schedule",1
nodejs/node,close-stale-pull-requests.yml,"name: Close stale pull requests
on:
  workflow_dispatch:
    inputs:
      endDate:
        description: stop processing PRs after this date
        required: false
        type: string

# yamllint disable rule:empty-lines
env:
  CLOSE_MESSAGE: >
    This pull request was opened more than a year ago and there has
    been no activity in the last 6 months. We value your contribution
    but since it has not progressed in the last 6 months it is being
    closed. If you feel closing this pull request is not the right thing
    to do, please leave a comment.

  WARN_MESSAGE: >
    This pull request was opened more than a year ago and there has
    been no activity in the last 5 months. We value your contribution
    but since it has not progressed in the last 5 months it is being
    marked stale and will be closed if there is no progress in the
    next month. If you feel that is not the right thing to do please
    comment on the pull request.
# yamllint enable

permissions:
  contents: read

jobs:
  stale:
    permissions:
      pull-requests: write  # for actions/stale to close stale PRs
    if: github.repository == 'nodejs/node'
    runs-on: ubuntu-latest
    steps:
      - name: Set default end date which is 1 year ago
        run: echo ""END_DATE=$(date --date='525600 minutes ago' --rfc-2822)""  >> ""$GITHUB_ENV""
      - name: if date set in event override the default end date
        env:
          END_DATE_INPUT_VALUE: ${{ github.event.inputs.endDate }}
        if: ${{ github.event.inputs.endDate != '' }}
        run: echo ""END_DATE=$END_DATE_INPUT_VALUE""  >> ""$GITHUB_ENV""
      - uses: mhdawson/stale@453d6581568dc43dbe345757f24408d7b451c651  # PR to add support for endDate
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          end-date: ${{ env.END_DATE }}
          days-before-issue-stale: -1
          days-before-issue-close: -1
          days-before-stale: 150
          days-before-close: 30
          stale-issue-label: stale
          close-issue-message: ${{ env.CLOSE_MESSAGE }}
          stale-issue-message: ${{ env.WARN_MESSAGE }}
          exempt-pr-labels: never-stale
          # max requests it will send per run to the GitHub API before it deliberately exits to avoid hitting API rate limits
          operations-per-run: 500
          remove-stale-when-updated: true
",59,1,1,workflow_dispatch,1
nodejs/node,close-stalled.yml,"name: Close stalled issues and PRs
on:
  schedule:
    - cron: 0 0 * * *

env:
  CLOSE_MESSAGE: >
    Closing this because it has stalled. Feel free to reopen if this issue/PR
    is still relevant, or to ping the collaborator who labelled it stalled if
    you have any questions.

permissions:
  contents: read

jobs:
  stale:
    permissions:
      issues: write  # for actions/stale to close stale issues
      pull-requests: write  # for actions/stale to close stale PRs
    if: github.repository == 'nodejs/node'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/stale@5bef64f19d7facfb25b37b414482c7164d639639  # v9.1.0
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          days-before-close: 30
          stale-pr-label: stalled
          stale-issue-label: stalled
          close-issue-message: ${{ env.CLOSE_MESSAGE }}
          close-pr-message: ${{ env.CLOSE_MESSAGE }}
          # used to filter issues to check whether or not should be closed, avoids hitting maximum operations allowed if needing to paginate through all open issues
          only-labels: stalled
          # max requests it will send per run to the GitHub API before it deliberately exits to avoid hitting API rate limits
          operations-per-run: 500
          # deactivates automatic stale labelling as we prefer to do that manually
          days-before-stale: -1
",36,1,1,schedule,1
nodejs/node,codeql.yml,"name: Run CodeQL

on:
  schedule:
    - cron: 0 0 * * *

permissions:
  contents: read

jobs:
  analyze:
    name: Analyze
    runs-on: ubuntu-latest
    permissions:
      actions: read
      contents: read
      security-events: write

    strategy:
      fail-fast: false
      matrix:
        language: [cpp, javascript, python]

    steps:
      - name: Checkout repository
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2

      # Initializes the CodeQL tools for scanning.
      - name: Initialize CodeQL
        uses: github/codeql-action/init@ff0a06e83cb2de871e5a09832bc6a81e7276941f  # v3.28.18
        with:
          languages: ${{ matrix.language }}
          config-file: ./.github/codeql-config.yml

      - name: Autobuild
        uses: github/codeql-action/autobuild@ff0a06e83cb2de871e5a09832bc6a81e7276941f  # v3.28.18

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@ff0a06e83cb2de871e5a09832bc6a81e7276941f  # v3.28.18
        with:
          category: /language:${{matrix.language}}
",41,1,1,schedule,4
nodejs/node,comment-labeled.yml,"name: Comment on issues and PRs when labeled
on:
  issues:
    types: [labeled]
  pull_request_target:
    types: [labeled]

env:
  STALE_MESSAGE: >
    This issue/PR was marked as stalled, it will be automatically closed in 30 days.
    If it should remain open, please leave a comment explaining why it should remain open.
  FAST_TRACK_MESSAGE: Fast-track has been requested by @${{ github.actor }}. Please 👍 to approve.
  NOTABLE_CHANGE_MESSAGE: |
    The https://github.com/nodejs/node/labels/notable-change label has been added by @${{ github.actor }}.

    Please suggest a text for the release notes if you'd like to include a more detailed summary, then proceed to update the PR description with the text or a link to the notable change suggested text comment. Otherwise, the commit will be placed in the _Other Notable Changes_ section.

permissions:
  contents: read

jobs:
  stale-comment:
    permissions:
      issues: write
      pull-requests: write
    if: github.repository == 'nodejs/node' && github.event.label.name == 'stalled'
    runs-on: ubuntu-latest
    steps:
      - name: Post stalled comment
        env:
          NUMBER: ${{ github.event.issue.number || github.event.pull_request.number }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: gh issue comment ""$NUMBER"" --repo ${{ github.repository }} --body ""$STALE_MESSAGE""

  fast-track:
    permissions:
      pull-requests: write
    if: github.repository == 'nodejs/node' && github.event_name == 'pull_request_target' && github.event.label.name == 'fast-track'
    runs-on: ubuntu-latest
    steps:
      - name: Request Fast-Track
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: gh pr comment ${{ github.event.pull_request.number }} --repo ${{ github.repository }} --body ""$FAST_TRACK_MESSAGE""

  notable-change:
    permissions:
      pull-requests: write
    if: github.repository == 'nodejs/node' && github.event_name == 'pull_request_target' && github.event.label.name == 'notable-change'
    runs-on: ubuntu-latest
    steps:
      - name: Add notable change description
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: gh pr comment ${{ github.event.pull_request.number }} --repo ${{ github.repository }} --body ""$NOTABLE_CHANGE_MESSAGE""
",55,3,2,"issues, pull_request_target",0
nodejs/node,commit-lint.yml,"name: First commit message adheres to guidelines

on: [pull_request]

env:
  NODE_VERSION: lts/*

permissions:
  contents: read

jobs:
  lint-commit-message:
    runs-on: ubuntu-latest
    steps:
      - name: Compute number of commits in the PR
        id: nb-of-commits
        run: |
          echo ""plusOne=$((${{ github.event.pull_request.commits }} + 1))"" >> $GITHUB_OUTPUT
          echo ""minusOne=$((${{ github.event.pull_request.commits }} - 1))"" >> $GITHUB_OUTPUT
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          fetch-depth: ${{ steps.nb-of-commits.outputs.plusOne }}
          persist-credentials: false
      - run: git reset HEAD^2
      - name: Install Node.js
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020  # v4.4.0
        with:
          node-version: ${{ env.NODE_VERSION }}
      - name: Validate commit message
        run: |
          echo ""::add-matcher::.github/workflows/commit-lint-problem-matcher.json""
          git rev-parse HEAD~${{ steps.nb-of-commits.outputs.minusOne }} | xargs npx -q core-validate-commit --no-validate-metadata --tap
",32,1,1,pull_request,2
nodejs/node,commit-queue.yml,"# This action requires the following secrets to be set on the repository:
#   GH_USER_TOKEN: GitHub user token, to be used by ncu and to push changes
#   JENKINS_USER: GitHub user whose Jenkins token is defined below
#   JENKINS_TOKEN: Jenkins token, to be used to check CI status

name: Commit Queue

on:
  # `schedule` event is used instead of `pull_request` because when a
  # `pull_request` event is triggered on a PR from a fork, GITHUB_TOKEN will
  # be read-only, and the Action won't have access to any other repository
  # secrets, which it needs to access Jenkins API.
  schedule:
    - cron: '*/5 * * * *'

concurrency: ${{ github.workflow }}

env:
  NODE_VERSION: lts/*

permissions:
  contents: read

jobs:
  get_mergeable_prs:
    permissions:
      pull-requests: read
    if: github.repository == 'nodejs/node'
    runs-on: ubuntu-latest
    outputs:
      numbers: ${{ steps.get_mergeable_prs.outputs.numbers }}
    steps:
      - name: Get Pull Requests
        id: get_mergeable_prs
        run: |
          prs=$(gh pr list \
                  --repo ""$GITHUB_REPOSITORY"" \
                  --base ""$GITHUB_REF_NAME"" \
                  --label 'commit-queue' \
                  --json 'number' \
                  --search ""created:<=$(date --date=""2 days ago""  +""%Y-%m-%dT%H:%M:%S%z"") -label:blocked"" \
                  -t '{{ range . }}{{ .number }} {{ end }}' \
                  --limit 100)
          fast_track_prs=$(gh pr list \
                  --repo ""$GITHUB_REPOSITORY"" \
                  --base ""$GITHUB_REF_NAME"" \
                  --label 'commit-queue' \
                  --label 'fast-track' \
                  --search ""-label:blocked"" \
                  --json 'number' \
                  -t '{{ range . }}{{ .number }} {{ end }}' \
                  --limit 100)
          numbers=$(echo $prs' '$fast_track_prs | jq -r -s 'unique | join("" "")')
          echo ""numbers=$numbers"" >> ""$GITHUB_OUTPUT""
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  commitQueue:
    needs: get_mergeable_prs
    if: needs.get_mergeable_prs.outputs.numbers != ''
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          # A personal token is required because pushing with GITHUB_TOKEN will
          # prevent commits from running CI after they land. It needs
          # to be set here because `checkout` configures GitHub authentication
          # for push as well.
          token: ${{ secrets.GH_USER_TOKEN }}

      # Install dependencies
      - name: Install Node.js
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020  # v4.4.0
        with:
          node-version: ${{ env.NODE_VERSION }}
      - name: Install @node-core/utils
        run: npm install -g @node-core/utils

      - name: Set variables
        run: |
          echo ""REPOSITORY=$(echo ""$GITHUB_REPOSITORY"" | cut -d/ -f2)"" >> ""$GITHUB_ENV""

      - name: Configure @node-core/utils
        run: |
          ncu-config set branch ""${GITHUB_REF_NAME}""
          ncu-config set upstream origin
          ncu-config set username ""$USERNAME""
          ncu-config set token ""$GITHUB_TOKEN""
          ncu-config set jenkins_token ""$JENKINS_TOKEN""
          ncu-config set repo ""${REPOSITORY}""
          ncu-config set owner ""${GITHUB_REPOSITORY_OWNER}""
        env:
          USERNAME: ${{ secrets.JENKINS_USER }}
          GITHUB_TOKEN: ${{ secrets.GH_USER_TOKEN }}
          JENKINS_TOKEN: ${{ secrets.JENKINS_TOKEN }}

      - name: Start the Commit Queue
        run: ./tools/actions/commit-queue.sh ""${GITHUB_REPOSITORY_OWNER}"" ""${REPOSITORY}"" ${{ needs.get_mergeable_prs.outputs.numbers }}
        env:
          GITHUB_TOKEN: ${{ secrets.GH_USER_TOKEN }}
",99,2,1,schedule,2
nodejs/node,coverage-linux-without-intl.yml,"name: Coverage Linux (without intl)

on:
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
    paths:
      - lib/**/*.js
      - Makefile
      - src/**/*.cc
      - src/**/*.h
      - test/**
      - tools/gyp/**
      - tools/test.py
      - .github/workflows/coverage-linux-without-intl.yml
      - codecov.yml
      - .nycrc
  push:
    branches:
      - main
    paths:
      - lib/**/*.js
      - Makefile
      - src/**/*.cc
      - src/**/*.h
      - test/**
      - tools/gyp/**
      - tools/test.py
      - .github/workflows/coverage-linux-without-intl.yml
      - codecov.yml
      - .nycrc

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.12'
  FLAKY_TESTS: keep_retrying
  CC: sccache clang
  CXX: sccache clang++
  SCCACHE_GHA_ENABLED: 'true'

permissions:
  contents: read

jobs:
  coverage-linux-without-intl:
    if: github.event.pull_request.draft == false
    runs-on: ubuntu-24.04
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Set up sccache
        uses: Mozilla-Actions/sccache-action@7d986dd989559c6ecdb630a3fd2557667be217ad  # v0.0.9
        with:
          version: v0.10.0
      - name: Environment Information
        run: npx envinfo
      - name: Install gcovr
        run: pip install gcovr==7.2
      - name: Build
        run: make build-ci -j4 V=1 CONFIG_FLAGS=""--error-on-warn --coverage --without-intl""
      # TODO(bcoe): fix the couple tests that fail with the inspector enabled.
      # The cause is most likely coverage's use of the inspector.
      - name: Test
        run: NODE_V8_COVERAGE=coverage/tmp make test-cov -j4 V=1 TEST_CI_ARGS=""-p dots  --measure-flakiness 9"" || exit 0
      - name: Report JS
        run: npx c8 report --check-coverage
        env:
          NODE_OPTIONS: --max-old-space-size=8192
      - name: Report C++
        run: gcovr --object-directory=out -v --filter src --xml -o ./coverage/coverage-cxx.xml --root=./ --gcov-executable=""llvm-cov-18 gcov""
      # Clean temporary output from gcov and c8, so that it's not uploaded:
      - name: Clean tmp
        run: rm -rf coverage/tmp && rm -rf out
      - name: Upload
        uses: codecov/codecov-action@18283e04ce6e62d37312384ff67231eb8fd56d24  # v5.4.3
        with:
          directory: ./coverage
",84,1,2,"pull_request, push",4
nodejs/node,coverage-linux.yml,"name: Coverage Linux

on:
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
    paths:
      - lib/**/*.js
      - Makefile
      - src/**/*.cc
      - src/**/*.h
      - test/**
      - tools/gyp/**
      - tools/test.py
      - .github/workflows/coverage-linux.yml
      - codecov.yml
      - .nycrc
  push:
    branches:
      - main
    paths:
      - lib/**/*.js
      - Makefile
      - src/**/*.cc
      - src/**/*.h
      - test/**
      - tools/gyp/**
      - tools/test.py
      - .github/workflows/coverage-linux.yml
      - codecov.yml
      - .nycrc

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.12'
  FLAKY_TESTS: keep_retrying
  CC: sccache clang
  CXX: sccache clang++
  SCCACHE_GHA_ENABLED: 'true'

permissions:
  contents: read

jobs:
  coverage-linux:
    if: github.event.pull_request.draft == false
    runs-on: ubuntu-24.04
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Set up sccache
        uses: Mozilla-Actions/sccache-action@7d986dd989559c6ecdb630a3fd2557667be217ad  # v0.0.9
        with:
          version: v0.10.0
      - name: Environment Information
        run: npx envinfo
      - name: Install gcovr
        run: pip install gcovr==7.2
      - name: Build
        run: make build-ci -j4 V=1 CONFIG_FLAGS=""--error-on-warn --coverage""
      # TODO(bcoe): fix the couple tests that fail with the inspector enabled.
      # The cause is most likely coverage's use of the inspector.
      - name: Test
        run: NODE_V8_COVERAGE=coverage/tmp make test-cov -j4 V=1 TEST_CI_ARGS=""-p dots  --measure-flakiness 9"" || exit 0
      - name: Report JS
        run: npx c8 report --check-coverage
        env:
          NODE_OPTIONS: --max-old-space-size=8192
      - name: Report C++
        run: gcovr --object-directory=out -v --filter src --xml -o ./coverage/coverage-cxx.xml --root=./ --gcov-executable=""llvm-cov-18 gcov""
      # Clean temporary output from gcov and c8, so that it's not uploaded:
      - name: Clean tmp
        run: rm -rf coverage/tmp && rm -rf out
      - name: Upload
        uses: codecov/codecov-action@18283e04ce6e62d37312384ff67231eb8fd56d24  # v5.4.3
        with:
          directory: ./coverage
",84,1,2,"pull_request, push",4
nodejs/node,coverage-windows.yml,"name: Coverage Windows

on:
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
    paths:
      - lib/**/*.js
      - vcbuild.bat
      - src/**/*.cc
      - src/**/*.h
      - test/**
      - tools/gyp/**
      - tools/test.py
      - .github/workflows/coverage-windows.yml
      - codecov.yml
      - .nycrc
  push:
    branches:
      - main
    paths:
      - lib/**/*.js
      - vcbuild.bat
      - src/**/*.cc
      - src/**/*.h
      - test/**
      - tools/gyp/**
      - tools/test.py
      - .github/workflows/coverage-windows.yml
      - codecov.yml
      - .nycrc

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.12'
  FLAKY_TESTS: keep_retrying

permissions:
  contents: read

jobs:
  coverage-windows:
    if: github.event.pull_request.draft == false
    runs-on: windows-2022
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install deps
        run: choco install nasm
      - name: Environment Information
        run: npx envinfo
      - name: Build
        run: ./vcbuild.bat clang-cl
      # TODO(bcoe): investigate tests that fail with coverage enabled
      # on Windows.
      - name: Test
        run: ./vcbuild.bat noprojgen nobuild test-ci-js; node -e 'process.exit(0)'
        env:
          NODE_V8_COVERAGE: ./coverage/tmp
      - name: Report
        run: npx c8 report
        env:
          NODE_OPTIONS: --max-old-space-size=8192
      - name: Clean tmp
        run: npx rimraf ./coverage/tmp
      - name: Upload
        uses: codecov/codecov-action@18283e04ce6e62d37312384ff67231eb8fd56d24  # v5.4.3
        with:
          directory: ./coverage
",76,1,2,"pull_request, push",3
nodejs/node,create-release-proposal.yml,"# This action requires the following secrets to be set on the repository:
#   GH_USER_TOKEN: GitHub user token, to be used by ncu and to push changes

name: Create Release Proposal

on:
  workflow_dispatch:
    inputs:
      release-line:
        required: true
        type: number
        description: 'The release line (without dots or prefix). e.g: 22'
      release-date:
        required: true
        type: string
        description: The release date in YYYY-MM-DD format

concurrency: ${{ github.workflow }}

env:
  NODE_VERSION: lts/*

permissions:
  contents: write
  pull-requests: write

jobs:
  releasePrepare:
    env:
      STAGING_BRANCH: v${{ inputs.release-line }}.x-staging
      RELEASE_BRANCH: v${{ inputs.release-line }}.x
      RELEASE_DATE: ${{ inputs.release-date }}
      RELEASE_LINE: ${{ inputs.release-line }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          ref: ${{ env.STAGING_BRANCH }}
          persist-credentials: false

      # Install dependencies
      - name: Install Node.js
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020  # v4.4.0
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install @node-core/utils
        run: npm install -g @node-core/utils

      - name: Configure @node-core/utils
        run: |
          ncu-config set branch ""${RELEASE_BRANCH}""
          ncu-config set upstream origin
          ncu-config set username ""$GITHUB_ACTOR""
          ncu-config set token ""$GH_TOKEN""
          ncu-config set repo ""$(echo ""$GITHUB_REPOSITORY"" | cut -d/ -f2)""
          ncu-config set owner ""${GITHUB_REPOSITORY_OWNER}""
        env:
          GH_TOKEN: ${{ github.token }}

      - name: Set up ghauth config (Ubuntu)
        run: |
          mkdir -p ""${XDG_CONFIG_HOME:-~/.config}/changelog-maker""
          jq --null-input '{user: env.GITHUB_ACTOR, token: env.TOKEN}' > ""${XDG_CONFIG_HOME:-~/.config}/changelog-maker/config.json""
        env:
          TOKEN: ${{ github.token }}

      - name: Setup git author
        run: |
          git config --local user.email ""github-bot@iojs.org""
          git config --local user.name ""Node.js GitHub Bot""

      - name: Start git node release prepare
        # The curl command is to make sure we run the version of the script corresponding to the current workflow.
        run: |
          curl -fsSL https://github.com/${GITHUB_REPOSITORY}/raw/${GITHUB_SHA}/tools/actions/create-release-proposal.sh |\
          sh -s -- ""${RELEASE_DATE}"" ""${RELEASE_LINE}"" ""${GITHUB_ACTOR}""
        env:
          GH_TOKEN: ${{ github.token }}
          # We want the bot to push the push the release commit so CI runs on it.
          BOT_TOKEN: ${{ secrets.GH_USER_TOKEN }}
",81,1,1,workflow_dispatch,2
nodejs/node,daily-wpt-fyi.yml,"# This workflow runs every night and tests various releases of Node.js
# (latest nightly, current, and two latest LTS release lines) against the
# `epochs/daily` branch of WPT.

name: Daily WPT report

on:
  workflow_dispatch:
  schedule:
    # This is 20 minutes after `epochs/daily` branch is triggered to be created
    # in WPT repo.
    # https://github.com/web-platform-tests/wpt/blob/master/.github/workflows/epochs.yml
    - cron: 30 0 * * *

env:
  PYTHON_VERSION: '3.12'

permissions:
  contents: read

jobs:
  collect-versions:
    if: github.repository == 'nodejs/node' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.query.outputs.matrix }}
    steps:
      - id: query
        run: |
          matrix=$(curl -s https://raw.githubusercontent.com/nodejs/Release/refs/heads/main/schedule.json | jq -c --arg now ""$(date +%Y-%m-%d)"" '[with_entries(select(.value.end > $now and .value.start < $now)) | keys[] | ltrimstr(""v"") | tonumber] + [""latest-nightly""]')
          echo ""matrix=$matrix"" >> ""$GITHUB_OUTPUT""
  report:
    needs:
      - collect-versions
    strategy:
      matrix:
        node-version: ${{ fromJSON(needs.collect-versions.outputs.matrix) }}
      fail-fast: false
    runs-on: ubuntu-latest
    steps:
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Environment Information
        run: npx envinfo

      # install a version and checkout
      - name: Get latest nightly
        if: matrix.node-version == 'latest-nightly'
        run: echo ""NIGHTLY=$(curl -s https://nodejs.org/download/nightly/index.json | jq -r '[.[] | select(.files[] | contains(""linux-x64""))][0].version')"" >> $GITHUB_ENV
      - name: Install Node.js
        id: setup-node
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020  # v4.4.0
        with:
          node-version: ${{ env.NIGHTLY || matrix.node-version }}
          check-latest: true
      - name: Get nightly ref
        if: contains(matrix.node-version, 'nightly')
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          SHORT_SHA=$(node -p 'process.version.split(/-nightly\d{8}/)[1]')
          echo ""NIGHTLY_REF=$(gh api /repos/nodejs/node/commits/$SHORT_SHA --jq '.sha')"" >> $GITHUB_ENV
      - name: Checkout ${{ steps.setup-node.outputs.node-version }}
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
          ref: ${{ env.NIGHTLY_REF || steps.setup-node.outputs.node-version }}
      - name: Set env.NODE
        run: echo ""NODE=$(which node)"" >> $GITHUB_ENV
      - name: Set env.WPT_REVISION
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: echo ""WPT_REVISION=$(gh api /repos/web-platform-tests/wpt/branches/epochs/daily --jq '.commit.sha')"" >> $GITHUB_ENV

      # replace checked out WPT with the synchronized branch
      - name: Remove stale WPT
        run: rm -rf wpt
        working-directory: test/fixtures
      - name: Checkout epochs/daily WPT
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          repository: web-platform-tests/wpt
          persist-credentials: false
          path: test/fixtures/wpt
          clean: false
          ref: ${{ env.WPT_REVISION }}

      # Node.js WPT Runner
      - name: Run WPT and generate report
        run: |
          make test-wpt-report || true
          if [ -e out/wpt/wptreport.json ]; then
            echo ""WPT_REPORT=$(pwd)/out/wpt/wptreport.json"" >> $GITHUB_ENV
          fi

      # undici WPT Runner
      - name: Set env.UNDICI_VERSION
        if: ${{ env.WPT_REPORT != '' }}
        run: echo ""UNDICI_VERSION=v$(jq -r '.version' < deps/undici/src/package.json)"" >> $GITHUB_ENV
      - name: Remove deps/undici
        if: ${{ env.WPT_REPORT != '' }}
        run: rm -rf deps/undici
      - name: Checkout undici
        if: ${{ env.WPT_REPORT != '' }}
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          repository: nodejs/undici
          persist-credentials: false
          path: deps/undici
          clean: false
          ref: ${{ env.UNDICI_VERSION }}
      - name: Add undici WPTs to the report
        if: ${{ env.WPT_REPORT != '' }}
        run: |
          rm -rf test/wpt/tests
          mv ../../test/fixtures/wpt/ test/wpt/tests/
          npm install
          npm run test:wpt || true
        working-directory: deps/undici

      # Upload artifacts
      - name: Clone report for upload
        if: ${{ env.WPT_REPORT != '' }}
        working-directory: out/wpt
        run: cp wptreport.json wptreport-${{ steps.setup-node.outputs.node-version }}.json
      - name: Upload GitHub Actions artifact
        if: ${{ env.WPT_REPORT != '' }}
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # v4.6.2
        with:
          path: out/wpt/wptreport-*.json
          name: WPT Report for ${{ steps.setup-node.outputs.node-version }}
          if-no-files-found: error
      - name: Upload WPT Report to wpt.fyi API
        if: ${{ env.WPT_REPORT != '' }}
        env:
          WPT_FYI_USERNAME: ${{ vars.WPT_FYI_USERNAME }}
          WPT_FYI_PASSWORD: ${{ secrets.WPT_FYI_PASSWORD }}
        working-directory: out/wpt
        run: |
          gzip wptreport.json
          echo ""## Node.js ${{ steps.setup-node.outputs.node-version }}"" >> $GITHUB_STEP_SUMMARY
          echo """" >> $GITHUB_STEP_SUMMARY
          echo ""WPT Revision: [\`${WPT_REVISION:0:7}\`](https://github.com/web-platform-tests/wpt/commits/$WPT_REVISION)"" >> $GITHUB_STEP_SUMMARY
          for WPT_FYI_ENDPOINT in ""https://wpt.fyi/api/results/upload"" ""https://staging.wpt.fyi/api/results/upload""
          do
            response=$(curl -sS \
              -u ""$WPT_FYI_USERNAME:$WPT_FYI_PASSWORD"" \
              -F ""result_file=@wptreport.json.gz"" \
              -F ""labels=master"" \
              $WPT_FYI_ENDPOINT)

            if [[ $response =~ Task\ ([0-9]+)\ added\ to\ queue ]]; then
              run_id=${BASH_REMATCH[1]}
              origin=${WPT_FYI_ENDPOINT%/api/results/upload}

              echo """" >> $GITHUB_STEP_SUMMARY
              echo ""Run ID [\`$run_id\`]($origin/api/runs/$run_id) added to the processor queue at ${origin:8}"" >> $GITHUB_STEP_SUMMARY
              echo ""- [View on the ${origin:8} dashboard]($origin/results?run_id=$run_id)"" >> $GITHUB_STEP_SUMMARY
            fi
          done
",162,2,2,"workflow_dispatch, schedule",6
nodejs/node,daily.yml,"name: Node.js daily job

on:
  workflow_dispatch:
  schedule:
    - cron: 0 0 * * *

env:
  NODE_VERSION: lts/*

permissions:
  contents: read

jobs:
  build-lto:
    runs-on: ubuntu-24.04
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
      - name: Use Node.js ${{ env.NODE_VERSION }}
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020  # v4.4.0
        with:
          node-version: ${{ env.NODE_VERSION }}
      - name: Environment Information
        run: npx envinfo
      - name: Build lto
        run: |
          sudo apt-get update && sudo apt-get install ninja-build -y
          ./configure --enable-lto --ninja
          ninja -C out/Release
",31,1,2,"workflow_dispatch, schedule",2
nodejs/node,doc.yml,"name: Test and upload documentation to artifacts

on:
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
  push:
    branches:
      - main
      - v[0-9]+.x-staging
      - v[0-9]+.x

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

env:
  NODE_VERSION: lts/*

permissions:
  contents: read

jobs:
  build-docs:
    if: github.event.pull_request.draft == false
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
      - name: Use Node.js ${{ env.NODE_VERSION }}
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020  # v4.4.0
        with:
          node-version: ${{ env.NODE_VERSION }}
      - name: Environment Information
        run: npx envinfo
      - name: Build
        run: NODE=$(command -v node) make doc-only
      - uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # v4.6.2
        with:
          name: docs
          path: out/doc
      - name: Test
        run: NODE=$(command -v node) make test-doc-ci TEST_CI_ARGS=""-p actions --measure-flakiness 9""
",43,1,2,"pull_request, push",3
nodejs/node,find-inactive-collaborators.yml,"name: Find inactive collaborators

on:
  schedule:
    # Run every Monday at 4:05 AM UTC.
    - cron: 5 4 * * 1

  workflow_dispatch:

env:
  NODE_VERSION: lts/*

permissions:
  contents: read

jobs:
  find:
    if: github.repository == 'nodejs/node'
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          fetch-depth: 0
          persist-credentials: false

      - name: Use Node.js ${{ env.NODE_VERSION }}
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020  # v4.4.0
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Find inactive collaborators
        run: tools/find-inactive-collaborators.mjs

      - name: Open pull request
        uses: gr2m/create-or-update-pull-request-action@77596e3166f328b24613f7082ab30bf2d93079d5
        # Creates a PR or update the Action's existing PR, or
        # no-op if the base branch is already up-to-date.
        env:
          GITHUB_TOKEN: ${{ secrets.GH_USER_TOKEN }}
        with:
          author: Node.js GitHub Bot <github-bot@iojs.org>
          branch: actions/inactive-collaborators
          body: |
            This PR was generated by the [`find-inactive-collaborators.yml` workflow](https://github.com/nodejs/node/blob/main/.github/workflows/find-inactive-collaborators.yml).

            @nodejs/tsc Please follow up with the [offboarding tasks](https://github.com/nodejs/node/blob/main/doc/contributing/offboarding.md).
          commit-message: 'meta: move one or more collaborators to emeritus'
          labels: meta
          title: 'meta: move one or more collaborators to emeritus'
",50,1,2,"schedule, workflow_dispatch",3
nodejs/node,find-inactive-tsc.yml,"name: Find inactive TSC voting members

on:
  schedule:
    # Run every Tuesday 12:05 AM UTC.
    - cron: 5 0 * * 2

  workflow_dispatch:

env:
  NODE_VERSION: lts/*

permissions:
  contents: read

jobs:
  find:
    if: github.repository == 'nodejs/node'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout the repo
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          fetch-depth: 0
          persist-credentials: false

      - name: Clone nodejs/TSC repository
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          fetch-depth: 0
          path: .tmp
          persist-credentials: false
          repository: nodejs/TSC

      - name: Use Node.js ${{ env.NODE_VERSION }}
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020  # v4.4.0
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Find inactive TSC voting members
        run: tools/find-inactive-tsc.mjs >> $GITHUB_ENV

      - name: Open pull request
        uses: gr2m/create-or-update-pull-request-action@77596e3166f328b24613f7082ab30bf2d93079d5
        # Creates a PR or update the Action's existing PR, or
        # no-op if the base branch is already up-to-date.
        env:
          GITHUB_TOKEN: ${{ secrets.GH_USER_TOKEN }}
        with:
          author: Node.js GitHub Bot <github-bot@iojs.org>
          branch: actions/inactive-tsc
          body: |
            This PR was generated by tools/find-inactive-tsc.yml.

            @nodejs/tsc ${{ env.INACTIVE_TSC_HANDLES }}

            ${{ env.DETAILS_FOR_COMMIT_BODY }}
          commit-message: 'meta: move TSC voting member(s) to regular member(s)'
          labels: meta
          title: 'meta: move TSC voting member(s) to regular member(s)'
          update-pull-request-title-and-body: true
",62,1,2,"schedule, workflow_dispatch",4
nodejs/node,label-flaky-test-issue.yml,"name: Label Flaky Test Issues

on:
  issues:
    types: [labeled]

permissions:
  contents: read

jobs:
  label:
    if: github.event.label.name == 'flaky-test'
    runs-on: ubuntu-latest
    permissions:
      issues: write
    steps:
      - name: Extract labels
        id: extract-labels
        env:
          BODY: ${{ github.event.issue.body }}
        run: |
          BODY=""${BODY//$'\n'/'\n'}""

          declare -A platform2label

          platform2label[""AIX""]=""aix"";
          platform2label[""FreeBSD""]=""freebsd"";
          platform2label[""Linux ARM64""]=""linux"";
          platform2label[""Linux PPC64LE""]=""ppc"";
          platform2label[""Linux s390x""]=""s390"";
          platform2label[""Linux x64""]=""linux"";
          platform2label[""macOS ARM64""]=""macos"";
          platform2label[""macOS x64""]=""macos"";
          platform2label[""SmartOS""]=""smartos"";
          platform2label[""Windows""]=""windows"";

          # sed is cleaning up the edges
          PLATFORMS=$(echo $BODY | sed 's/^.*Platform\\n\\n//' | sed 's/\(, Other\)\?\\n\\n.*$//') 2> /dev/null
          readarray -d , -t list <<< ""$PLATFORMS""
          labels=
          for row in ""${list[@]}""; do \
            platform=$(echo $row | xargs); \
            labels=""${labels}${platform2label[$platform]},""; \
          done;

          echo ""LABELS=${labels::-1}"" >> $GITHUB_OUTPUT

      - name: Add labels
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          NUMBER: ${{ github.event.issue.number }}
        run: gh issue edit ""$NUMBER"" --repo ${{ github.repository }} --add-label ""${{ steps.extract-labels.outputs.LABELS }}""
",52,1,1,issues,0
nodejs/node,label-pr.yml,"name: Label PRs

on:
  pull_request_target:
    types: [opened]

permissions:
  contents: read

jobs:
  label:
    runs-on: ubuntu-latest

    steps:
      - uses: nodejs/node-pr-labeler@d4cf1b8b9f23189c37917000e5e17e796c770a6b  # v1
        with:
          repo-token: ${{ secrets.GH_USER_TOKEN }}
          configuration-path: .github/label-pr-config.yml
",18,1,1,pull_request_target,1
nodejs/node,license-builder.yml,"name: License update
on:
  schedule:
    # 00:00:00 every Monday
    # https://crontab.guru/#0_0_*_*_1
    - cron: 0 0 * * 1
  workflow_dispatch:

permissions:
  contents: read

jobs:
  update_license:
    permissions:
      contents: write  # for gr2m/create-or-update-pull-request-action to push local changes
      pull-requests: write  # for gr2m/create-or-update-pull-request-action to create a PR
    if: github.repository == 'nodejs/node'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
      - run: ./tools/license-builder.sh  # Run the license builder tool
      - uses: gr2m/create-or-update-pull-request-action@86ec1766034c8173518f61d2075cc2a173fb8c97  # v1.9.4
        # Creates a PR or update the Action's existing PR, or
        # no-op if the base branch is already up-to-date.
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          author: Node.js GitHub Bot <github-bot@iojs.org>
          branch: actions/license-builder
          title: 'doc: run license-builder'
          body: >
            License is likely out of date. This is an automatically generated PR by
            the `license-builder.yml` GitHub Action, which runs `license-builder.sh`
            and submits a new PR or updates an existing PR.
          commit-message: 'doc: run license-builder'
          labels: meta
",38,1,2,"schedule, workflow_dispatch",2
nodejs/node,lint-release-proposal.yml,"name: Linters (release proposals)

on:
  push:
    branches:
      - v[0-9]+.[0-9]+.[0-9]+-proposal

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.12'
  NODE_VERSION: lts/*

permissions:
  contents: read

jobs:
  lint-release-commit:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: read
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
      - name: Lint release commit title format
        run: |
          EXPECTED_TITLE='^[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}, Version [[:digit:]]+\.[[:digit:]]+\.[[:digit:]]+ (\(Current|'.+' \(LTS)\)$'
          echo ""Expected commit title format: $EXPECTED_TITLE""
          COMMIT_SUBJECT=""$(git --no-pager log -1 --format=%s)""
          echo ""Actual: $ACTUAL""
          echo ""$COMMIT_SUBJECT"" | grep -q -E ""$EXPECTED_TITLE""
          echo ""COMMIT_SUBJECT=$COMMIT_SUBJECT"" >> ""$GITHUB_ENV""
      - name: Lint release commit message trailers
        run: |
          EXPECTED_TRAILER=""^$GITHUB_SERVER_URL/$GITHUB_REPOSITORY/pull/[[:digit:]]+\$""
          echo ""Expected trailer format: $EXPECTED_TRAILER""
          PR_URL=""$(git --no-pager log -1 --format='%(trailers:key=PR-URL,valueonly)')""
          echo ""Actual: $ACTUAL""
          echo ""$PR_URL"" | grep -E -q ""$EXPECTED_TRAILER""

          PR_HEAD=""$(gh pr view ""$PR_URL"" --json headRefOid -q .headRefOid)""
          echo ""Head of $PR_URL: $PR_HEAD""
          echo ""Current commit: $GITHUB_SHA""
          [ ""$PR_HEAD"" = ""$GITHUB_SHA"" ]
        env:
          GH_TOKEN: ${{ github.token }}
      - name: Verify it's release-ready
        run: |
          SKIP_XZ=1 make release-only
      - name: Validate CHANGELOG
        id: releaser-info
        run: |
          EXPECTED_CHANGELOG_TITLE_INTRO=""## $COMMIT_SUBJECT, @""
          echo ""Expected CHANGELOG section title: $EXPECTED_CHANGELOG_TITLE_INTRO""
          MAJOR=""$(awk '/^#define NODE_MAJOR_VERSION / { print $3 }' src/node_version.h)""
          CHANGELOG_PATH=""doc/changelogs/CHANGELOG_V${MAJOR}.md""
          CHANGELOG_TITLE=""$(grep ""$EXPECTED_CHANGELOG_TITLE_INTRO"" ""$CHANGELOG_PATH"")""
          echo ""Actual: $CHANGELOG_TITLE""
          [ ""${CHANGELOG_TITLE%%@*}@"" = ""$EXPECTED_CHANGELOG_TITLE_INTRO"" ]
          gh api \
            -H ""Accept: application/vnd.github+json"" \
            -H ""X-GitHub-Api-Version: 2022-11-28"" \
            --jq '.commits.[] | { smallSha: .sha[0:10] } + (.commit.message|capture(""^(?<title>.+)\n\n(.*\n)*PR-URL: (?<prURL>.+)\n""))' \
            ""/repos/${GITHUB_REPOSITORY}/compare/v${MAJOR}.x...$GITHUB_SHA"" --paginate \
          | node tools/actions/lint-release-proposal-commit-list.mjs ""$CHANGELOG_PATH"" ""$GITHUB_SHA"" \
          | while IFS= read -r PR_URL; do
            LABEL=""dont-land-on-v${MAJOR}.x"" gh pr view \
              --json labels,url \
              --jq 'if (.labels|map(.name==env.LABEL)|any) then error(""\(.url) has the \(env.LABEL) label, forbidding it to be in this release proposal"") end' \
              ""$PR_URL"" > /dev/null
          done
        shell: bash  # See https://docs.github.com/en/actions/writing-workflows/workflow-syntax-for-github-actions#exit-codes-and-error-action-preference, we want the pipefail option.
        env:
          GH_TOKEN: ${{ github.token }}
",78,1,1,push,1
nodejs/node,linters.yml,"name: Linters

on:
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
  push:
    branches:
      - main
      - v[0-9]+.x-staging
      - v[0-9]+.x

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.12'
  NODE_VERSION: lts/*

permissions:
  contents: read

jobs:
  lint-addon-docs:
    if: github.event.pull_request.draft == false
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
      - name: Use Node.js ${{ env.NODE_VERSION }}
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020  # v4.4.0
        with:
          node-version: ${{ env.NODE_VERSION }}
      - name: Environment Information
        run: npx envinfo
      - name: Lint addon docs
        run: NODE=$(command -v node) make lint-addon-docs
  lint-cpp:
    if: github.event.pull_request.draft == false
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Environment Information
        run: npx envinfo
      - name: Lint C/C++ files
        run: make lint-cpp
  format-cpp:
    if: ${{ github.event.pull_request && github.event.pull_request.draft == false && github.base_ref == github.event.repository.default_branch }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          fetch-depth: 0
          persist-credentials: false
      - name: Use Node.js ${{ env.NODE_VERSION }}
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020  # v4.4.0
        with:
          node-version: ${{ env.NODE_VERSION }}
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Environment Information
        run: npx envinfo
      - name: Format C/C++ files
        run: |
          make format-cpp-build
          # The `make format-cpp` error code is intentionally ignored here
          # because it is irrelevant. We already check if the formatter produced
          # a diff in the next line.
          # Refs: https://github.com/nodejs/node/pull/42764
          CLANG_FORMAT_START=""$(git merge-base HEAD refs/remotes/origin/$GITHUB_BASE_REF)"" \
            make format-cpp || true
          git --no-pager diff --exit-code && EXIT_CODE=""$?"" || EXIT_CODE=""$?""
          if [ ""$EXIT_CODE"" != ""0"" ]
          then
            echo
            echo 'ERROR: Please run:'
            echo
            echo ""  CLANG_FORMAT_START=""$\(git merge-base HEAD ${GITHUB_BASE_REF}\)"" make format-cpp""
            echo
            echo 'to format the commits in your branch.'
            exit ""$EXIT_CODE""
          fi
  lint-js-and-md:
    if: github.event.pull_request.draft == false
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
      - name: Use Node.js ${{ env.NODE_VERSION }}
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020  # v4.4.0
        with:
          node-version: ${{ env.NODE_VERSION }}
      - name: Environment Information
        run: npx envinfo
      - name: Lint JavaScript files
        run: NODE=$(command -v node) make lint-js
      - name: Get release version numbers
        if: ${{ github.event.pull_request && github.event.pull_request.base.ref == github.event.pull_request.base.repo.default_branch }}
        id: get-released-versions
        run: ./tools/lint-md/list-released-versions-from-changelogs.mjs >> $GITHUB_OUTPUT
      - name: Lint markdown files
        run: |
          echo ""::add-matcher::.github/workflows/remark-lint-problem-matcher.json""
          NODE=$(command -v node) make lint-md
        env:
          NODE_RELEASED_VERSIONS: ${{ steps.get-released-versions.outputs.NODE_RELEASED_VERSIONS }}
  lint-py:
    if: github.event.pull_request.draft == false
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Environment Information
        run: npx envinfo
      - name: Lint Python
        run: |
          make lint-py-build
          make lint-py
  lint-yaml:
    if: github.event.pull_request.draft == false
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
      - name: Use Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Environment Information
        run: npx envinfo
      - name: Lint YAML
        run: |
          make lint-yaml-build || true
          make lint-yaml

  lint-sh:
    if: github.event.pull_request.draft == false
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
      - run: shellcheck -V
      - name: Lint Shell scripts
        run: tools/lint-sh.mjs .
  lint-codeowners:
    if: github.event.pull_request.draft == false
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
      - uses: mszostok/codeowners-validator@7f3f5e28c6d7b8dfae5731e54ce2272ca384592f
        with:
          checks: files,duppatterns
  lint-pr-url:
    if: ${{ github.event.pull_request }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          fetch-depth: 2
          persist-credentials: false
      # GH Actions squashes all PR commits, HEAD^ refers to the base branch.
      - run: git diff HEAD^ HEAD -G""pr-url:"" -- ""*.md"" | ./tools/lint-pr-url.mjs ${{ github.event.pull_request.html_url }}
  lint-readme:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
      - name: Get team members if possible
        if: ${{ (github.event.pull_request && github.event.pull_request.base.ref == github.event.pull_request.base.repo.default_branch) || github.event.ref == github.event.repository.default_branch }}
        id: team_members
        run: |
          get_list_members() {
            TEAM=""$1""
            QUOTE='""'
            gh api ""/orgs/nodejs/teams/$TEAM/members"" -X GET -f per_page=100 --jq ""map(.login) | ${QUOTE}${TEAM}=\(tojson)${QUOTE}""
          }
          [ -z ""$GITHUB_TOKEN"" ] || (
            get_list_members ""collaborators""
            get_list_members ""issue-triage""
            get_list_members ""tsc""
          ) >> ""$GITHUB_OUTPUT""
        env:
          GITHUB_TOKEN: ${{ secrets.GH_USER_TOKEN }}
      - run: tools/lint-readme-lists.mjs ""$TEAMS""
        env:
          TEAMS: ${{ tojson(steps.team_members.outputs) }}
",206,10,2,"pull_request, push",18
nodejs/node,major-release.yml,"name: Major Release

on:
  schedule:
    - cron: 0 0 15 2,8 *  # runs at midnight UTC every 15 February and 15 August

permissions:
  contents: read

jobs:
  create-issue:
    if: github.repository == 'nodejs/node'
    runs-on: ubuntu-latest
    permissions:
      issues: write
    steps:
      - name: Check for release schedule
        id: check-date
        run: |
          # Get the current month and day
          MONTH=$(date +'%m')
          DAY=$(date +'%d')
          # We'll create the reminder issue two months prior the release
          if [[ ""$MONTH"" == ""02"" || ""$MONTH"" == ""08"" ]] && [[ ""$DAY"" == ""15"" ]]; then
            echo ""create_issue=true"" >> ""$GITHUB_ENV""
          fi
      - name: Retrieve next major release info from nodejs/Release
        if: env.create_issue == 'true'
        run: |
          curl -L https://github.com/nodejs/Release/raw/HEAD/schedule.json | \
          jq -r 'to_entries | map(select(.value.start | strptime(""%Y-%m-%d"") | mktime > now)) | first | ""VERSION="" + .key + ""\nRELEASE_DATE="" + .value.start' >> ""$GITHUB_ENV""
      - name: Compute max date for landing semver-major PRs
        if: env.create_issue == 'true'
        run: |
          echo ""PR_MAX_DATE=$(date -d ""$RELEASE_DATE -1 month"" +%Y-%m-%d)"" >> ""$GITHUB_ENV""
      - name: Create release announcement issue
        if: env.create_issue == 'true'
        run: |
         gh issue create --repo ""${GITHUB_REPOSITORY}"" \
           --title ""Upcoming Node.js Major Release ($VERSION)"" \
           --body-file -<<EOF
            A reminder that the next Node.js **SemVer Major release** is scheduled for **${RELEASE_DATE}**.
            All commits that were landed until **${PR_MAX_DATE}** (one month prior to the release) will be included in the next semver major release. Please ensure that any necessary preparations are made in advance.
            For more details on the release process, consult the [Node.js Release Working Group repository](https://github.com/nodejs/release).

            cc: @nodejs/collaborators
         EOF
        env:
          GH_TOKEN: ${{ github.token }}
",49,1,1,schedule,0
nodejs/node,notify-on-push.yml,"on:
  push:
    branches:
      - main

name: Notify on Push
permissions:
  contents: read

jobs:
  notifyOnForcePush:
    name: Notify on Force Push on `main`
    if: github.repository == 'nodejs/node' && github.event.forced
    runs-on: ubuntu-latest
    steps:
      - name: Slack Notification
        uses: rtCamp/action-slack-notify@e31e87e03dd19038e411e38ae27cbad084a90661  # 2.3.3
        env:
          SLACK_COLOR: '#DE512A'
          SLACK_ICON: https://github.com/nodejs.png?size=48
          SLACK_TITLE: ${{ github.actor }} force-pushed to ${{ github.ref }}
          SLACK_MESSAGE: |
            <!here> A commit was force-pushed to <https://github.com/${{ github.repository }}/tree/${{ github.ref_name }}|${{ github.repository }}@${{ github.ref_name }}> by <https://github.com/${{ github.actor }}|${{ github.actor }}>

            Before: <https://github.com/${{ github.repository }}/commit/${{ github.event.before }}|${{ github.event.before }}>
            After: <https://github.com/${{ github.repository }}/commit/${{ github.event.after }}|${{ github.event.after }}>
          SLACK_USERNAME: nodejs-bot
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}

  notifyOnMissingMetadata:
    name: Notify on Push on `main` that lacks metadata
    if: github.repository == 'nodejs/node'
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
      - name: Check commit message
        run: npx -q core-validate-commit ${{ github.event.after }} || echo ""INVALID_COMMIT_MESSAGE=1"" >> $GITHUB_ENV
      - name: Retrieve PR number if possible
        if: env.INVALID_COMMIT_MESSAGE
        run: |
          COMMIT_TITLE=$(git --no-pager log --oneline -1 --no-color) node <<'EOF' >> $GITHUB_ENV || true
          const invalidCommitMessageMatch = /\s\(\#(\d+)\)$/.exec(process.env.COMMIT_TITLE);
          if (invalidCommitMessageMatch == null) process.exit(1)
          console.log(`PR_ID=${invalidCommitMessageMatch[1]}`)
          EOF
      - name: Comment on the Pull Request
        if: ${{ env.PR_ID }}
        run: |
          gh pr comment ${{ env.PR_ID }} --repo ""${{ github.repository }}"" \
            --body ""A commit referencing this Pull Request was pushed to `${{ github.ref_name }}` by @${{ github.actor }} with an invalid commit message.""
        env:
          GH_TOKEN: ${{ github.token }}
      - name: Slack Notification
        if: ${{ env.INVALID_COMMIT_MESSAGE }}
        uses: rtCamp/action-slack-notify@e31e87e03dd19038e411e38ae27cbad084a90661  # 2.3.3
        env:
          SLACK_COLOR: '#DE512A'
          SLACK_ICON: https://github.com/nodejs.png?size=48
          SLACK_TITLE: Invalid commit was pushed to ${{ github.ref }}
          SLACK_MESSAGE: |
            <!here> A commit with an invalid message was pushed to <https://github.com/${{ github.repository }}/tree/${{ github.ref_name }}|${{ github.repository }}@${{ github.ref_name }}> by <https://github.com/${{ github.actor }}|${{ github.actor }}>.

            Before: <https://github.com/${{ github.repository }}/commit/${{ github.event.before }}|${{ github.event.before }}>
            After: <https://github.com/${{ github.repository }}/commit/${{ github.event.after }}|${{ github.event.after }}>
          SLACK_USERNAME: nodejs-bot
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
",71,2,1,push,3
nodejs/node,notify-on-review-wanted.yml,"name: Notify on Review Wanted
on:
  issues:
    types: [labeled]
  pull_request_target:
    types: [labeled]

permissions:
  contents: read

jobs:
  notifyOnReviewWanted:
    name: Notify on Review Wanted
    if: github.repository == 'nodejs/node' && github.event.label.name == 'review wanted'
    runs-on: ubuntu-latest
    steps:
      - name: Determine PR or Issue
        id: define-message
        env:
          TITLE_ISSUE: ${{ github.event.issue.title }}
          TITLE_PR: ${{ github.event.pull_request.title }}
        run: |
          if [[ -n ""${{ github.event.pull_request.number }}"" ]]; then
            number=""${{ github.event.pull_request.number }}""
            link=""https://github.com/${{ github.repository }}/pull/$number""
            echo ""message=The PR (#$number) requires review from Node.js maintainers. See: $link"" >> ""$GITHUB_OUTPUT""
            echo ""title=$TITLE_PR"" >> ""$GITHUB_OUTPUT""
          else
            number=""${{ github.event.issue.number }}""
            link=""https://github.com/${{ github.repository }}/issues/$number""
            echo ""message=The issue (#$number) requires review from Node.js maintainers. See: $link"" >> ""$GITHUB_OUTPUT""
            echo ""title=$TITLE_ISSUE"" >> ""$GITHUB_OUTPUT""
          fi

      - name: Slack Notification
        uses: rtCamp/action-slack-notify@e31e87e03dd19038e411e38ae27cbad084a90661  # 2.3.3
        env:
          MSG_MINIMAL: actions url
          SLACK_COLOR: '#3d85c6'
          SLACK_ICON: https://github.com/nodejs.png?size=48
          SLACK_TITLE: ${{ steps.define-message.outputs.title }}
          SLACK_MESSAGE: ${{ steps.define-message.outputs.message }}
          SLACK_USERNAME: nodejs-bot
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
",44,1,2,"issues, pull_request_target",1
nodejs/node,scorecard.yml,"# This workflow uses actions that are not certified by GitHub. They are provided
# by a third-party and are governed by separate terms of service, privacy
# policy, and support documentation.

name: Scorecard supply-chain security
on:
  # For Branch-Protection check. Only the default branch is supported. See
  # https://github.com/ossf/scorecard/blob/main/docs/checks.md#branch-protection
  branch_protection_rule:
  # To guarantee Maintained check is occasionally updated. See
  # https://github.com/ossf/scorecard/blob/main/docs/checks.md#maintained
  schedule:
    - cron: 16 21 * * 1
  push:
    branches: [main]
  workflow_dispatch:

# Declare default permissions as read only.
permissions: read-all

jobs:
  analysis:
    name: Scorecard analysis
    runs-on: ubuntu-latest
    permissions:
      # Needed to upload the results to code-scanning dashboard.
      security-events: write
      # Needed to publish results and get a badge (see publish_results below).
      id-token: write
      # Uncomment the permissions below if installing in a private repository.
      # contents: read
      # actions: read

    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@0634a2670c59f64b4a01f0f96f84700a4088b9f0  # v2.12.0
        with:
          egress-policy: audit  # TODO: change to 'egress-policy: block' after couple of runs

      - name: Checkout code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false

      - name: Run analysis
        uses: ossf/scorecard-action@05b42c624433fc40578a4040d5cf5e36ddca8cde  # v2.4.2
        with:
          results_file: results.sarif
          results_format: sarif
          # (Optional) ""write"" PAT token. Uncomment the `repo_token` line below if:
          # - you want to enable the Branch-Protection check on a *public* repository, or
          # - you are installing Scorecard on a *private* repository
          # To create the PAT, follow the steps in https://github.com/ossf/scorecard-action#authentication-with-pat.
          # repo_token: ${{ secrets.SCORECARD_TOKEN }}

          # Public repositories:
          #   - Publish results to OpenSSF REST API for easy access by consumers
          #   - Allows the repository to include the Scorecard badge.
          #   - See https://github.com/ossf/scorecard-action#publishing-results.
          # For private repositories:
          #   - `publish_results` will always be set to `false`, regardless
          #     of the value entered here.
          publish_results: true

      # Upload the results as artifacts (optional). Commenting out will disable uploads of run results in SARIF
      # format to the repository Actions tab.
      - name: Upload artifact
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # v4.6.2
        with:
          name: SARIF file
          path: results.sarif
          retention-days: 5

      # Upload the results to GitHub's code scanning dashboard.
      - name: Upload to code-scanning
        uses: github/codeql-action/upload-sarif@ff0a06e83cb2de871e5a09832bc6a81e7276941f  # v3.28.18
        with:
          sarif_file: results.sarif
",78,1,4,"branch_protection_rule, schedule, push, workflow_dispatch",5
nodejs/node,test-internet.yml,"name: Test internet

on:
  workflow_dispatch:
  schedule:
    - cron: 5 0 * * *

  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
    paths:
      - .github/workflows/test-internet.yml
      - test/internet/**
      - internal/dns/**
      - lib/dns.js
      - lib/net.js
  push:
    branches:
      - main
      - canary
      - v[0-9]+.x-staging
      - v[0-9]+.x
    paths:
      - .github/workflows/test-internet.yml
      - test/internet/**
      - internal/dns/**
      - lib/dns.js
      - lib/net.js

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.12'
  FLAKY_TESTS: keep_retrying
  CC: clang
  CXX: clang++

permissions:
  contents: read

jobs:
  test-internet:
    if: github.repository == 'nodejs/node' || github.event_name != 'schedule'
    runs-on: ubuntu-24.04
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Environment Information
        run: npx envinfo
      - name: Build
        run: make build-ci -j4 V=1 CONFIG_FLAGS=""--error-on-warn""
      - name: Test Internet
        run: make test-internet -j4 V=1;
",59,1,4,"workflow_dispatch, schedule, pull_request, push",2
nodejs/node,test-linux.yml,"name: Test Linux

on:
  pull_request:
    paths-ignore:
      - .mailmap
      - README.md
      - .github/**
      - '!.github/workflows/test-linux.yml'
    types: [opened, synchronize, reopened, ready_for_review]
  push:
    branches:
      - main
      - canary
      - v[0-9]+.x-staging
      - v[0-9]+.x
    paths-ignore:
      - .mailmap
      - README.md
      - .github/**
      - '!.github/workflows/test-linux.yml'

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.12'
  FLAKY_TESTS: keep_retrying
  CC: sccache clang
  CXX: sccache clang++
  SCCACHE_GHA_ENABLED: 'true'

permissions:
  contents: read

jobs:
  test-linux:
    if: github.event.pull_request.draft == false
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-24.04, ubuntu-24.04-arm]
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
          path: node
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Set up sccache
        uses: Mozilla-Actions/sccache-action@7d986dd989559c6ecdb630a3fd2557667be217ad  # v0.0.9
        with:
          version: v0.10.0
      - name: Environment Information
        run: npx envinfo
      - name: Build
        run: make -C node build-ci -j4 V=1 CONFIG_FLAGS=""--error-on-warn""
      - name: Test
        run: make -C node run-ci -j4 V=1 TEST_CI_ARGS=""-p actions --measure-flakiness 9""
      - name: Re-run test in a folder whose name contains unusual chars
        run: |
          mv node ""$DIR""
          cd ""$DIR""
          ./tools/test.py --flaky-tests keep_retrying -p actions -j 4
        env:
          DIR: dir%20with $unusual""chars?'åß∂ƒ©∆¬…`
",70,1,2,"pull_request, push",3
nodejs/node,test-macos.yml,"name: Test macOS

on:
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
    paths-ignore:
      - .mailmap
      - '**.md'
      - AUTHORS
      - doc/**
      - .github/**
      - '!.github/workflows/test-macos.yml'
  push:
    branches:
      - main
      - canary
      - v[0-9]+.x-staging
      - v[0-9]+.x
    paths-ignore:
      - .mailmap
      - '**.md'
      - AUTHORS
      - doc/**
      - .github/**
      - '!.github/workflows/test-macos.yml'

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.12'
  XCODE_VERSION: '16.1'
  FLAKY_TESTS: keep_retrying

permissions:
  contents: read

jobs:
  test-macOS:
    if: github.event.pull_request.draft == false
    strategy:
      fail-fast: false
    runs-on: macos-14
    env:
      CC: sccache gcc
      CXX: sccache g++
      SCCACHE_GHA_ENABLED: 'true'
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
          path: node
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Set up Xcode ${{ env.XCODE_VERSION }}
        run: sudo xcode-select -s /Applications/Xcode_${{ env.XCODE_VERSION }}.app
      - name: Set up sccache
        uses: Mozilla-Actions/sccache-action@7d986dd989559c6ecdb630a3fd2557667be217ad  # v0.0.9
        with:
          version: v0.10.0
      - name: Environment Information
        run: npx envinfo
      # The `npm ci` for this step fails a lot as part of the Test step. Run it
      # now so that we don't have to wait 2 hours for the Build step to pass
      # first before that failure happens. (And if there's something about
      # `make run-ci -j3` that is causing the failure and the failure doesn't
      # happen anymore running this step here first, that's also useful
      # information.)
      - name: tools/doc/node_modules workaround
        run: make -C node tools/doc/node_modules
      # This is needed due to https://github.com/nodejs/build/issues/3878
      - name: Cleanup
        run: |
          echo ""::group::Free space before cleanup""
          df -h
          echo ""::endgroup::""
          echo ""::group::Cleaned Files""

          sudo rm -rf /Users/runner/Library/Android/sdk

          echo ""::endgroup::""
          echo ""::group::Free space after cleanup""
          df -h
          echo ""::endgroup::""
      - name: Build
        run: make -C node build-ci -j$(getconf _NPROCESSORS_ONLN) V=1 CONFIG_FLAGS=""--error-on-warn""
      - name: Free Space After Build
        run: df -h
      - name: Test
        run: make -C node run-ci -j$(getconf _NPROCESSORS_ONLN) V=1 TEST_CI_ARGS=""-p actions --measure-flakiness 9""
      - name: Re-run test in a folder whose name contains unusual chars
        run: |
          mv node ""$DIR""
          cd ""$DIR""
          ./tools/test.py --flaky-tests keep_retrying -p actions -j 4
        env:
          DIR: dir%20with $unusual""chars?'åß∂ƒ©∆¬…`
",100,1,2,"pull_request, push",3
nodejs/node,timezone-update.yml,"name: Timezone update
on:
  schedule:
    # Run once a week at 00:05 AM UTC on Sunday.
    - cron: 5 0 * * 0

  workflow_dispatch:

permissions:
  contents: read

jobs:
  timezone_update:
    permissions:
      contents: write  # to push local changes (gr2m/create-or-update-pull-request-action)
      pull-requests: write  # to create a PR (gr2m/create-or-update-pull-request-action)

    if: github.repository == 'nodejs/node'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout nodejs/node
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false

      - name: Checkout unicode-org/icu-data
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          path: icu-data
          persist-credentials: false
          repository: unicode-org/icu-data

      - name: Record new version
        run: echo ""new_version=$(ls icu-data/tzdata/icunew | tail -1)"" >> $GITHUB_ENV

      - name: Record current version
        run: echo ""current_version=$(cat ./test/fixtures/tz-version.txt)"" >> $GITHUB_ENV

      - name: Compare versions
        run: |
          echo ""Comparing current version ${{ env.current_version }} to new version ${{ env.new_version }}""

      - run: ./tools/update-timezone.mjs
        if: ${{ env.new_version != env.current_version }}

      - name: Update the expected timezone version in test
        if: ${{ env.new_version != env.current_version }}
        run: echo ""${{ env.new_version }}"" > test/fixtures/tz-version.txt

      - name: Open Pull Request
        if: ${{ env.new_version != env.current_version }}
        uses: gr2m/create-or-update-pull-request-action@77596e3166f328b24613f7082ab30bf2d93079d5  # Create a PR or update the Action's existing PR
        env:
          GITHUB_TOKEN: ${{ secrets.GH_USER_TOKEN }}
        with:
          author: Node.js GitHub Bot <github-bot@iojs.org>
          body: |
            This PR was generated by tools/timezone-update.yml.

            Updates the ICU files as per the instructions present in https://github.com/nodejs/node/blob/main/doc/contributing/maintaining/maintaining-icu.md#time-zone-data

            To test, build node off this branch & log the version of tz using
            ```js
            console.log(process.versions.tz)
            ```
          branch: actions/timezone-update
          commit-message: 'deps: update timezone to ${{ env.new_version }}'
          labels: dependencies
          title: 'deps: update timezone to ${{ env.new_version }}'
          reviewers: \@nodejs/i18n-api
          update-pull-request-title-and-body: true
",72,1,2,"schedule, workflow_dispatch",3
nodejs/node,tools.yml,"name: Tools and deps update
on:
  schedule:
    # Run once a week at 00:05 AM UTC on Sunday.
    - cron: 5 0 * * 0

  workflow_dispatch:
    inputs:
      id:
        description: The ID of the job to run
        required: true
        default: all
        type: choice
        options:
          - all
          - acorn
          - acorn-walk
          - ada
          - amaro
          - brotli
          - c-ares
          - cjs-module-lexer
          - corepack
          - doc
          - googletest
          - gyp-next
          - histogram
          - icu
          - libuv
          - llhttp
          - minimatch
          - nbytes
          - nghttp2
          - nghttp3
          - ngtcp2
          - postject
          - root-certificates
          - simdjson
          - sqlite
          - undici
          - uvwasi
          - zlib
          - zstd

env:
  PYTHON_VERSION: '3.12'

permissions:
  contents: read

jobs:
  tools-deps-update:
    if: github.repository == 'nodejs/node' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false  # Prevent other jobs from aborting if one fails
      matrix:
        include:
          - id: acorn
            subsystem: deps
            label: dependencies
            run: |
              ./tools/dep_updaters/update-acorn.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: acorn-walk
            subsystem: deps
            label: dependencies
            run: |
              ./tools/dep_updaters/update-acorn-walk.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: ada
            subsystem: deps
            label: dependencies
            run: |
              ./tools/dep_updaters/update-ada.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: amaro
            subsystem: deps
            label: dependencies, strip-types
            run: |
              ./tools/dep_updaters/update-amaro.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: brotli
            subsystem: deps
            label: dependencies, zlib
            run: |
              ./tools/dep_updaters/update-brotli.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: c-ares
            subsystem: deps
            label: dependencies, cares
            run: |
              ./tools/dep_updaters/update-c-ares.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: cjs-module-lexer
            subsystem: deps
            label: dependencies
            run: |
              ./tools/dep_updaters/update-cjs-module-lexer.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: corepack
            subsystem: deps
            label: dependencies
            run: |
              make corepack-update
              echo ""NEW_VERSION=$(node deps/corepack/dist/corepack.js --version)"" >> $GITHUB_ENV
          - id: doc
            subsystem: tools
            label: tools
            run: |
              cd tools/doc
              npm ci
              NEW_VERSION=$(npm outdated --parseable | cut -d: -f4 | xargs)
              if [ ""$NEW_VERSION"" != """" ]; then
                echo ""NEW_VERSION=new version"" >> $GITHUB_ENV
                rm -rf package-lock.json node_modules
                # Include $NEW_VERSION to explicitly update the package.json
                # entry for the dependency and also so that semver-major updates
                # are not skipped.
                npm install --ignore-scripts $NEW_VERSION
                npm install --ignore-scripts
              fi
          - id: googletest
            subsystem: deps
            label: dependencies, test
            run: |
              ./tools/dep_updaters/update-googletest.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: gyp-next
            subsystem: tools
            label: tools, gyp
            run: |
              ./tools/dep_updaters/update-gyp-next.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: histogram
            subsystem: deps
            label: dependencies
            run: |
              ./tools/dep_updaters/update-histogram.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: icu
            subsystem: deps
            label: dependencies, test, icu
            run: |
              ./tools/dep_updaters/update-icu.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: libuv
            subsystem: deps
            label: dependencies
            run: |
              ./tools/dep_updaters/update-libuv.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: llhttp
            subsystem: deps
            label: dependencies
            run: |
              ./tools/dep_updaters/update-llhttp.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: minimatch
            subsystem: deps
            label: dependencies
            run: |
              ./tools/dep_updaters/update-minimatch.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: nbytes
            subsystem: deps
            label: dependencies
            run: |
              ./tools/dep_updaters/update-nbytes.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: nghttp2
            subsystem: deps
            label: dependencies
            run: |
              ./tools/dep_updaters/update-nghttp2.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: nghttp3
            subsystem: deps
            label: dependencies
            run: |
              ./tools/dep_updaters/update-nghttp3.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: ngtcp2
            subsystem: deps
            label: dependencies
            run: |
              ./tools/dep_updaters/update-ngtcp2.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: postject
            subsystem: deps,test
            label: test
            run: |
              ./tools/dep_updaters/update-postject.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: root-certificates
            subsystem: crypto
            label: crypto, notable-change
            run: |
              node ./tools/dep_updaters/update-root-certs.mjs -v -f ""$GITHUB_ENV""
          - id: simdjson
            subsystem: deps
            label: dependencies
            run: |
              ./tools/dep_updaters/update-simdjson.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: sqlite
            subsystem: deps
            label: dependencies, sqlite
            run: |
              ./tools/dep_updaters/update-sqlite.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: undici
            subsystem: deps
            label: dependencies
            run: |
              ./tools/dep_updaters/update-undici.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: uvwasi
            subsystem: deps
            label: dependencies
            run: |
              ./tools/dep_updaters/update-uvwasi.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: zlib
            subsystem: deps
            label: dependencies, zlib
            run: |
              ./tools/dep_updaters/update-zlib.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
          - id: zstd
            subsystem: deps
            label: dependencies, zlib
            run: |
              ./tools/dep_updaters/update-zstd.sh > temp-output
              cat temp-output
              tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
              rm temp-output
    steps:
      - name: Setup Git config
        run: |
           git config --global user.name ""Node.js GitHub Bot""
           git config --global user.email ""github-bot@iojs.org""
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        if: github.event_name == 'schedule' || inputs.id == 'all' || inputs.id == matrix.id
        with:
          persist-credentials: false
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        if: matrix.id == 'icu' && (github.event_name == 'schedule' || inputs.id == 'all' || inputs.id == matrix.id)
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - run: ${{ matrix.run }}
        if: github.event_name == 'schedule' || inputs.id == 'all' || inputs.id == matrix.id
        env:
          GITHUB_TOKEN: ${{ secrets.GH_USER_TOKEN }}
      - name: Generate commit message if not set
        if: env.COMMIT_MSG == '' && (github.event_name == 'schedule' || inputs.id == 'all' || inputs.id == matrix.id)
        run: |
          echo ""COMMIT_MSG=${{ matrix.subsystem }}: update ${{ matrix.id }} to ${{ env.NEW_VERSION }}"" >> ""$GITHUB_ENV""
      - uses: peter-evans/create-pull-request@271a8d0340265f705b14b6d32b9829c1cb33d45e  # v7.0.8
        if: github.event_name == 'schedule' || inputs.id == 'all' || inputs.id == matrix.id
        # Creates a PR or update the Action's existing PR, or
        # no-op if the base branch is already up-to-date.
        with:
          token: ${{ secrets.GH_USER_TOKEN }}
          branch: actions/tools-update-${{ matrix.id }}  # Custom branch *just* for this Action.
          delete-branch: true
          commit-message: ${{ env.COMMIT_MSG }}
          labels: ${{ matrix.label }}
          title: '${{ matrix.subsystem }}: update ${{ matrix.id }} to ${{ env.NEW_VERSION }}'
          body: This is an automated update of ${{ matrix.id }} to ${{ env.NEW_VERSION }}.
",319,1,2,"schedule, workflow_dispatch",3
nodejs/node,update-openssl.yml,"name: OpenSSL update
on:
  schedule:
    # Run once a week at 00:05 AM UTC on Sunday.
    - cron: 5 0 * * 0

  workflow_dispatch:

permissions:
  contents: read

jobs:
  openssl-update:
    if: github.repository == 'nodejs/node'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
      - name: Check and download new OpenSSL version
        run: |
          ./tools/dep_updaters/update-openssl.sh download > temp-output
          cat temp-output
          tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
          rm temp-output
        env:
          GITHUB_TOKEN: ${{ secrets.GH_USER_TOKEN }}
      - name: Create PR with first commit
        if: env.NEW_VERSION
        uses: gr2m/create-or-update-pull-request-action@77596e3166f328b24613f7082ab30bf2d93079d5
        # Creates a PR with the new OpenSSL source code committed
        env:
          GITHUB_TOKEN: ${{ secrets.GH_USER_TOKEN }}
        with:
          author: Node.js GitHub Bot <github-bot@iojs.org>
          body: This is an automated update of OpenSSL to ${{ env.NEW_VERSION }}.
          branch: actions/tools-update-openssl  # Custom branch *just* for this Action.
          commit-message: 'deps: upgrade openssl sources to openssl-${{ env.NEW_VERSION }}'
          labels: dependencies, openssl
          title: 'deps: update OpenSSL to ${{ env.NEW_VERSION }}'
          path: deps/openssl
          update-pull-request-title-and-body: true
      - name: Regenerate platform specific files
        if: env.NEW_VERSION
        run: |
          sudo apt install -y nasm libtext-template-perl
          ./tools/dep_updaters/update-openssl.sh regenerate
        env:
          GITHUB_TOKEN: ${{ secrets.GH_USER_TOKEN }}
      - name: Add second commit
        # Adds a second commit to the PR with the generated platform-dependent files
        if: env.NEW_VERSION
        uses: gr2m/create-or-update-pull-request-action@77596e3166f328b24613f7082ab30bf2d93079d5
        env:
          GITHUB_TOKEN: ${{ secrets.GH_USER_TOKEN }}
        with:
          author: Node.js GitHub Bot <github-bot@iojs.org>
          branch: actions/tools-update-openssl  # Custom branch *just* for this Action.
          commit-message: 'deps: update archs files for openssl-${{ env.NEW_VERSION }}'
          path: deps/openssl
",60,1,2,"schedule, workflow_dispatch",3
nodejs/node,update-release-links.yml,"name: Update release links

on:
  workflow_dispatch:
  release:
    types: [released]

permissions:
  contents: read

jobs:
  update-release-links:
    if: github.repository == 'nodejs/node'
    runs-on: ubuntu-latest
    steps:
      - name: Trigger update-links workflow on nodejs/release-cloudflare-worker
        run: |
          gh workflow run update-links.yml --repo nodejs/release-cloudflare-worker
        env:
          GITHUB_TOKEN: ${{ secrets.GH_USER_TOKEN }}
",20,1,2,"workflow_dispatch, release",0
nodejs/node,update-v8.yml,"name: V8 patch update
on:
  schedule:
    # Run once a week at 00:05 AM UTC on Sunday.
    - cron: 5 0 * * 0
  workflow_dispatch:

env:
  NODE_VERSION: lts/*

permissions:
  contents: read

jobs:
  v8-update:
    if: github.repository == 'nodejs/node'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false
      - name: Cache node modules and update-v8
        uses: actions/cache@5a3ec84eff668545956fd18022155c47e93e2684  # v4.2.3
        id: cache-v8-npm
        env:
          cache-name: cache-v8-npm
        with:
          path: |
            ~/.update-v8
            ~/.npm
          key: ${{ runner.os }}-build-${{ env.cache-name }}
      - name: Install Node.js
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020  # v4.4.0
        with:
          node-version: ${{ env.NODE_VERSION }}
      - name: Install @node-core/utils
        run: npm install -g @node-core/utils
      - name: Setup Git config
        run: |
          git config --global user.name ""Node.js GitHub Bot""
          git config --global user.email ""github-bot@iojs.org""
      - name: Check and download new V8 version
        run: |
            ./tools/dep_updaters/update-v8-patch.sh > temp-output
            cat temp-output
            tail -n1 temp-output | grep ""NEW_VERSION="" >> ""$GITHUB_ENV"" || true
            rm temp-output
      - uses: peter-evans/create-pull-request@271a8d0340265f705b14b6d32b9829c1cb33d45e  # v7.0.8
        # Creates a PR or update the Action's existing PR, or
        # no-op if the base branch is already up-to-date.
        with:
          token: ${{ secrets.GH_USER_TOKEN }}
          branch: actions/update-v8-patch  # Custom branch *just* for this Action.
          delete-branch: true
          title: 'deps: patch V8 to ${{ env.NEW_VERSION }}'
          body: This is an automated patch update of V8 to ${{ env.NEW_VERSION }}.
          labels: dependencies, v8 engine
",57,1,2,"schedule, workflow_dispatch",4
nodejs/node,update-wpt.yml,"name: WPT update

on:
  schedule:
    # Run once a week at 12:00 AM UTC on Sunday.
    - cron: 0 0 * * 0
  workflow_dispatch:
    inputs:
      subsystems:
        description: Subsystem to run the update for
        required: false
        default: '[""url"", ""urlpattern"", ""WebCryptoAPI""]'

permissions:
  contents: read

env:
  NODE_VERSION: lts/*

jobs:
  wpt-subsystem-update:
    if: github.repository == 'nodejs/node' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        subsystem: ${{ fromJSON(github.event.inputs.subsystems || '[""url"", ""urlpattern"", ""WebCryptoAPI""]') }}

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          persist-credentials: false

      - name: Install Node.js
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020  # v4.4.0
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install @node-core/utils
        run: npm install -g @node-core/utils

      - name: Setup @node-core/utils
        run: |
          ncu-config set username ""$USERNAME""
          ncu-config set token ""$GH_TOKEN""
          ncu-config set owner ""${GITHUB_REPOSITORY_OWNER}""
          ncu-config set repo ""$(echo ""$GITHUB_REPOSITORY"" | cut -d/ -f2)""
        env:
          USERNAME: ${{ secrets.JENKINS_USER }}
          GH_TOKEN: ${{ secrets.GH_USER_TOKEN }}

      - name: Update WPT for subsystem ${{ matrix.subsystem }}
        run: |
          git node wpt ""$SUBSYSTEM""
        env:
          SUBSYSTEM: ${{ matrix.subsystem }}

      - name: Retrieve new version commit
        run: |
          new_version=""$(
            node -p 'require(""./test/fixtures/wpt/versions.json"")[process.argv[1]].commit' ""$SUBSYSTEM""
          )""
          {
            echo ""long_version=$new_version""
            echo ""short_version=${new_version:0:10}""
          } >> ""$GITHUB_ENV""
        env:
          SUBSYSTEM: ${{ matrix.subsystem }}

      - name: Open or update PR for the subsystem update
        uses: gr2m/create-or-update-pull-request-action@77596e3166f328b24613f7082ab30bf2d93079d5
        with:
          # The create-or-update-pull-request-action matches the branch name by prefix,
          # which is why we need to add the -wpt suffix. If we dont do that, we risk matching wrong PRs,
          # like for example ""url"" mistakenly matching and updating the ""urlpattern"" PR
          # as seen in https://github.com/nodejs/node/pull/57368
          branch: actions/update-${{ matrix.subsystem }}-wpt
          author: Node.js GitHub Bot <github-bot@iojs.org>
          title: 'test: update WPT for ${{ matrix.subsystem }} to ${{ env.short_version }}'
          commit-message: 'test: update WPT for ${{ matrix.subsystem }} to ${{ env.short_version }}'
          labels: test
          update-pull-request-title-and-body: true
          body: >
            This is an automated update of the WPT for ${{ matrix.subsystem }} to
            https://github.com/web-platform-tests/wpt/commit/${{ env.long_version }}.
        env:
          GITHUB_TOKEN: ${{ secrets.GH_USER_TOKEN }}
",87,1,2,"schedule, workflow_dispatch",3
d3/d3,deploy.yml,"name: Deploy

on:
  workflow_dispatch: {}
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    permissions:
      pages: write
      id-token: write
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - uses: actions/checkout@v4
      - uses: actions/checkout@v4
        with:
          repository: d3/d3.github.com
          path: build/d3.github.com
      - uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: 'yarn'
      - run: yarn --frozen-lockfile
      - run: yarn prepublishOnly
      - run: yarn docs:build
      - uses: actions/configure-pages@v4
      - uses: actions/upload-pages-artifact@v3
        with:
          path: docs/.vitepress/dist
      - name: Deploy
        id: deployment
        uses: actions/deploy-pages@v4
",36,1,2,"workflow_dispatch, push",6
d3/d3,test.yml,"name: Test

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: 'yarn'
      - run: yarn --frozen-lockfile
      - run: |
          echo ::add-matcher::.github/eslint.json
          yarn run eslint src test --format=compact
      - run: yarn test
      - run: yarn prepublishOnly
      - run: yarn docs:build
",24,1,2,"push, pull_request",2
langchain-ai/langchain,_compile_integration_test.yml,"name: compile-integration-test

on:
  workflow_call:
    inputs:
      working-directory:
        required: true
        type: string
        description: ""From which folder this pipeline executes""
      python-version:
        required: true
        type: string
        description: ""Python version to use""

env:
  UV_FROZEN: ""true""

jobs:
  build:
    defaults:
      run:
        working-directory: ${{ inputs.working-directory }}
    runs-on: ubuntu-latest
    timeout-minutes: 20
    name: ""uv run pytest -m compile tests/integration_tests #${{ inputs.python-version }}""
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ inputs.python-version }} + uv
        uses: ""./.github/actions/uv_setup""
        with:
          python-version: ${{ inputs.python-version }}

      - name: Install integration dependencies
        shell: bash
        run: uv sync --group test --group test_integration

      - name: Check integration tests compile
        shell: bash
        run: uv run pytest -m compile tests/integration_tests

      - name: Ensure the tests did not create any additional files
        shell: bash
        run: |
          set -eu

          STATUS=""$(git status)""
          echo ""$STATUS""

          # grep will exit non-zero if the target message isn't found,
          # and `set -e` above will cause the step to fail.
          echo ""$STATUS"" | grep 'nothing to commit, working tree clean'
",52,1,1,workflow_call,2
langchain-ai/langchain,_integration_test.yml,"name: Integration tests

on:
  workflow_dispatch:
    inputs:
      working-directory:
        required: true
        type: string
        description: ""From which folder this pipeline executes""
      python-version:
        required: true
        type: string
        description: ""Python version to use""

env:
  UV_FROZEN: ""true""

jobs:
  build:
    defaults:
      run:
        working-directory: ${{ inputs.working-directory }}
    runs-on: ubuntu-latest
    name: Python ${{ inputs.python-version }}
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ inputs.python-version }} + uv
        uses: ""./.github/actions/uv_setup""
        with:
          python-version: ${{ inputs.python-version }}

      - name: Install dependencies
        shell: bash
        run: uv sync --group test --group test_integration

      - name: Run integration tests
        shell: bash
        env:
          AI21_API_KEY: ${{ secrets.AI21_API_KEY }}
          FIREWORKS_API_KEY: ${{ secrets.FIREWORKS_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          ANTHROPIC_FILES_API_IMAGE_ID: ${{ secrets.ANTHROPIC_FILES_API_IMAGE_ID }}
          ANTHROPIC_FILES_API_PDF_ID: ${{ secrets.ANTHROPIC_FILES_API_PDF_ID }}
          AZURE_OPENAI_API_VERSION: ${{ secrets.AZURE_OPENAI_API_VERSION }}
          AZURE_OPENAI_API_BASE: ${{ secrets.AZURE_OPENAI_API_BASE }}
          AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
          AZURE_OPENAI_CHAT_DEPLOYMENT_NAME: ${{ secrets.AZURE_OPENAI_CHAT_DEPLOYMENT_NAME }}
          AZURE_OPENAI_LEGACY_CHAT_DEPLOYMENT_NAME: ${{ secrets.AZURE_OPENAI_LEGACY_CHAT_DEPLOYMENT_NAME }}
          AZURE_OPENAI_LLM_DEPLOYMENT_NAME: ${{ secrets.AZURE_OPENAI_LLM_DEPLOYMENT_NAME }}
          AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME: ${{ secrets.AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME }}
          MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}
          TOGETHER_API_KEY: ${{ secrets.TOGETHER_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          NVIDIA_API_KEY: ${{ secrets.NVIDIA_API_KEY }}
          GOOGLE_SEARCH_API_KEY: ${{ secrets.GOOGLE_SEARCH_API_KEY }}
          GOOGLE_CSE_ID: ${{ secrets.GOOGLE_CSE_ID }}
          HUGGINGFACEHUB_API_TOKEN: ${{ secrets.HUGGINGFACEHUB_API_TOKEN }}
          EXA_API_KEY: ${{ secrets.EXA_API_KEY }}
          NOMIC_API_KEY: ${{ secrets.NOMIC_API_KEY }}
          WATSONX_APIKEY: ${{ secrets.WATSONX_APIKEY }}
          WATSONX_PROJECT_ID: ${{ secrets.WATSONX_PROJECT_ID }}
          ASTRA_DB_API_ENDPOINT: ${{ secrets.ASTRA_DB_API_ENDPOINT }}
          ASTRA_DB_APPLICATION_TOKEN: ${{ secrets.ASTRA_DB_APPLICATION_TOKEN }}
          ASTRA_DB_KEYSPACE: ${{ secrets.ASTRA_DB_KEYSPACE }}
          ES_URL: ${{ secrets.ES_URL }}
          ES_CLOUD_ID: ${{ secrets.ES_CLOUD_ID }}
          ES_API_KEY: ${{ secrets.ES_API_KEY }}
          MONGODB_ATLAS_URI: ${{ secrets.MONGODB_ATLAS_URI }}
          COHERE_API_KEY: ${{ secrets.COHERE_API_KEY }}
          UPSTAGE_API_KEY: ${{ secrets.UPSTAGE_API_KEY }}
          XAI_API_KEY: ${{ secrets.XAI_API_KEY }}
          PPLX_API_KEY: ${{ secrets.PPLX_API_KEY }}
        run: |
          make integration_tests

      - name: Ensure the tests did not create any additional files
        shell: bash
        run: |
          set -eu

          STATUS=""$(git status)""
          echo ""$STATUS""

          # grep will exit non-zero if the target message isn't found,
          # and `set -e` above will cause the step to fail.
          echo ""$STATUS"" | grep 'nothing to commit, working tree clean'
",89,1,1,workflow_dispatch,2
langchain-ai/langchain,_lint.yml,"name: lint

on:
  workflow_call:
    inputs:
      working-directory:
        required: true
        type: string
        description: ""From which folder this pipeline executes""
      python-version:
        required: true
        type: string
        description: ""Python version to use""

env:
  WORKDIR: ${{ inputs.working-directory == '' && '.' || inputs.working-directory }}

  # This env var allows us to get inline annotations when ruff has complaints.
  RUFF_OUTPUT_FORMAT: github

  UV_FROZEN: ""true""

jobs:
  build:
    name: ""make lint #${{ inputs.python-version }}""
    runs-on: ubuntu-latest
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ inputs.python-version }} + uv
        uses: ""./.github/actions/uv_setup""
        with:
          python-version: ${{ inputs.python-version }}

      - name: Install dependencies
        # Also installs dev/lint/test/typing dependencies, to ensure we have
        # type hints for as many of our libraries as possible.
        # This helps catch errors that require dependencies to be spotted, for example:
        # https://github.com/langchain-ai/langchain/pull/10249/files#diff-935185cd488d015f026dcd9e19616ff62863e8cde8c0bee70318d3ccbca98341
        #
        # If you change this configuration, make sure to change the `cache-key`
        # in the `poetry_setup` action above to stop using the old cache.
        # It doesn't matter how you change it, any change will cause a cache-bust.
        working-directory: ${{ inputs.working-directory }}
        run: |
          uv sync --group lint --group typing

      - name: Analysing the code with our lint
        working-directory: ${{ inputs.working-directory }}
        run: |
          make lint_package

      - name: Install unit test dependencies
        # Also installs dev/lint/test/typing dependencies, to ensure we have
        # type hints for as many of our libraries as possible.
        # This helps catch errors that require dependencies to be spotted, for example:
        # https://github.com/langchain-ai/langchain/pull/10249/files#diff-935185cd488d015f026dcd9e19616ff62863e8cde8c0bee70318d3ccbca98341
        #
        # If you change this configuration, make sure to change the `cache-key`
        # in the `poetry_setup` action above to stop using the old cache.
        # It doesn't matter how you change it, any change will cause a cache-bust.
        if: ${{ ! startsWith(inputs.working-directory, 'libs/partners/') }}
        working-directory: ${{ inputs.working-directory }}
        run: |
          uv sync --inexact --group test
      - name: Install unit+integration test dependencies
        if: ${{ startsWith(inputs.working-directory, 'libs/partners/') }}
        working-directory: ${{ inputs.working-directory }}
        run: |
          uv sync --inexact --group test --group test_integration

      - name: Analysing the code with our lint
        working-directory: ${{ inputs.working-directory }}
        run: |
          make lint_tests
",76,1,1,workflow_call,2
langchain-ai/langchain,_release.yml,"name: release
run-name: Release ${{ inputs.working-directory }} by @${{ github.actor }}
on:
  workflow_call:
    inputs:
      working-directory:
        required: true
        type: string
        description: ""From which folder this pipeline executes""
  workflow_dispatch:
    inputs:
      working-directory:
        required: true
        type: string
        description: ""From which folder this pipeline executes""
        default: 'libs/langchain'
      dangerous-nonmaster-release:
        required: false
        type: boolean
        default: false
        description: ""Release from a non-master branch (danger!)""

env:
  PYTHON_VERSION: ""3.11""
  UV_FROZEN: ""true""
  UV_NO_SYNC: ""true""

jobs:
  build:
    if: github.ref == 'refs/heads/master' || inputs.dangerous-nonmaster-release
    environment: Scheduled testing
    runs-on: ubuntu-latest

    outputs:
      pkg-name: ${{ steps.check-version.outputs.pkg-name }}
      version: ${{ steps.check-version.outputs.version }}

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python + uv
        uses: ""./.github/actions/uv_setup""
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      # We want to keep this build stage *separate* from the release stage,
      # so that there's no sharing of permissions between them.
      # The release stage has trusted publishing and GitHub repo contents write access,
      # and we want to keep the scope of that access limited just to the release job.
      # Otherwise, a malicious `build` step (e.g. via a compromised dependency)
      # could get access to our GitHub or PyPI credentials.
      #
      # Per the trusted publishing GitHub Action:
      # > It is strongly advised to separate jobs for building [...]
      # > from the publish job.
      # https://github.com/pypa/gh-action-pypi-publish#non-goals
      - name: Build project for distribution
        run: uv build
        working-directory: ${{ inputs.working-directory }}

      - name: Upload build
        uses: actions/upload-artifact@v4
        with:
          name: dist
          path: ${{ inputs.working-directory }}/dist/

      - name: Check Version
        id: check-version
        shell: python
        working-directory: ${{ inputs.working-directory }}
        run: |
          import os
          import tomllib
          with open(""pyproject.toml"", ""rb"") as f:
              data = tomllib.load(f)
          pkg_name = data[""project""][""name""]
          version = data[""project""][""version""]
          with open(os.environ[""GITHUB_OUTPUT""], ""a"") as f:
              f.write(f""pkg-name={pkg_name}\n"")
              f.write(f""version={version}\n"")
  release-notes:
    needs:
      - build
    runs-on: ubuntu-latest
    outputs:
      release-body: ${{ steps.generate-release-body.outputs.release-body }}
    steps:
      - uses: actions/checkout@v4
        with:
          repository: langchain-ai/langchain
          path: langchain
          sparse-checkout: | # this only grabs files for relevant dir
            ${{ inputs.working-directory }}
          ref: ${{ github.ref }} # this scopes to just ref'd branch
          fetch-depth: 0 # this fetches entire commit history
      - name: Check Tags
        id: check-tags
        shell: bash
        working-directory: langchain/${{ inputs.working-directory }}
        env:
          PKG_NAME: ${{ needs.build.outputs.pkg-name }}
          VERSION: ${{ needs.build.outputs.version }}
        run: |
          # Handle regular versions and pre-release versions differently
          if [[ ""$VERSION"" == *""-""* ]]; then
            # This is a pre-release version (contains a hyphen)
            # Extract the base version without the pre-release suffix
            BASE_VERSION=${VERSION%%-*}
            # Look for the latest release of the same base version
            REGEX=""^$PKG_NAME==$BASE_VERSION\$""
            PREV_TAG=$(git tag --sort=-creatordate | (grep -P ""$REGEX"" || true) | head -1)
            
            # If no exact base version match, look for the latest release of any kind
            if [ -z ""$PREV_TAG"" ]; then
              REGEX=""^$PKG_NAME==\\d+\\.\\d+\\.\\d+\$""
              PREV_TAG=$(git tag --sort=-creatordate | (grep -P ""$REGEX"" || true) | head -1)
            fi
          else
            # Regular version handling
            PREV_TAG=""$PKG_NAME==${VERSION%.*}.$(( ${VERSION##*.} - 1 ))""; [[ ""${VERSION##*.}"" -eq 0 ]] && PREV_TAG=""""

            # backup case if releasing e.g. 0.3.0, looks up last release
            # note if last release (chronologically) was e.g. 0.1.47 it will get 
            # that instead of the last 0.2 release
            if [ -z ""$PREV_TAG"" ]; then
              REGEX=""^$PKG_NAME==\\d+\\.\\d+\\.\\d+\$""
              echo $REGEX
              PREV_TAG=$(git tag --sort=-creatordate | (grep -P $REGEX || true) | head -1)
            fi
          fi

          # if PREV_TAG is empty, let it be empty
          if [ -z ""$PREV_TAG"" ]; then
            echo ""No previous tag found - first release""
          else
            # confirm prev-tag actually exists in git repo with git tag
            GIT_TAG_RESULT=$(git tag -l ""$PREV_TAG"")
            if [ -z ""$GIT_TAG_RESULT"" ]; then
              echo ""Previous tag $PREV_TAG not found in git repo""
              exit 1
            fi
          fi


          TAG=""${PKG_NAME}==${VERSION}""
          if [ ""$TAG"" == ""$PREV_TAG"" ]; then
            echo ""No new version to release""
            exit 1
          fi
          echo tag=""$TAG"" >> $GITHUB_OUTPUT
          echo prev-tag=""$PREV_TAG"" >> $GITHUB_OUTPUT
      - name: Generate release body
        id: generate-release-body
        working-directory: langchain
        env:
          WORKING_DIR: ${{ inputs.working-directory }}
          PKG_NAME: ${{ needs.build.outputs.pkg-name }}
          TAG: ${{ steps.check-tags.outputs.tag }}
          PREV_TAG: ${{ steps.check-tags.outputs.prev-tag }}
        run: |
          PREAMBLE=""Changes since $PREV_TAG""
          # if PREV_TAG is empty, then we are releasing the first version
          if [ -z ""$PREV_TAG"" ]; then
            PREAMBLE=""Initial release""
            PREV_TAG=$(git rev-list --max-parents=0 HEAD)
          fi
          {
            echo 'release-body<<EOF'
            echo $PREAMBLE
            echo
            git log --format=""%s"" ""$PREV_TAG""..HEAD -- $WORKING_DIR
            echo EOF
          } >> ""$GITHUB_OUTPUT""

  test-pypi-publish:
    needs:
      - build
      - release-notes
    uses:
      ./.github/workflows/_test_release.yml
    permissions: write-all
    with:
      working-directory: ${{ inputs.working-directory }}
      dangerous-nonmaster-release: ${{ inputs.dangerous-nonmaster-release }}
    secrets: inherit

  pre-release-checks:
    needs:
      - build
      - release-notes
      - test-pypi-publish
    runs-on: ubuntu-latest
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4

      # We explicitly *don't* set up caching here. This ensures our tests are
      # maximally sensitive to catching breakage.
      #
      # For example, here's a way that caching can cause a falsely-passing test:
      # - Make the langchain package manifest no longer list a dependency package
      #   as a requirement. This means it won't be installed by `pip install`,
      #   and attempting to use it would cause a crash.
      # - That dependency used to be required, so it may have been cached.
      #   When restoring the venv packages from cache, that dependency gets included.
      # - Tests pass, because the dependency is present even though it wasn't specified.
      # - The package is published, and it breaks on the missing dependency when
      #   used in the real world.

      - name: Set up Python + uv
        uses: ""./.github/actions/uv_setup""
        id: setup-python
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - uses: actions/download-artifact@v4
        with:
          name: dist
          path: ${{ inputs.working-directory }}/dist/

      - name: Import dist package
        shell: bash
        working-directory: ${{ inputs.working-directory }}
        env:
          PKG_NAME: ${{ needs.build.outputs.pkg-name }}
          VERSION: ${{ needs.build.outputs.version }}
        # Here we use:
        # - The default regular PyPI index as the *primary* index, meaning
        #   that it takes priority (https://pypi.org/simple)
        # - The test PyPI index as an extra index, so that any dependencies that
        #   are not found on test PyPI can be resolved and installed anyway.
        #   (https://test.pypi.org/simple). This will include the PKG_NAME==VERSION
        #   package because VERSION will not have been uploaded to regular PyPI yet.
        # - attempt install again after 5 seconds if it fails because there is
        #   sometimes a delay in availability on test pypi
        run: |
          uv venv
          VIRTUAL_ENV=.venv uv pip install dist/*.whl

          # Replace all dashes in the package name with underscores,
          # since that's how Python imports packages with dashes in the name.
          # also remove _official suffix
          IMPORT_NAME=""$(echo ""$PKG_NAME"" | sed s/-/_/g | sed s/_official//g)""

          uv run python -c ""import $IMPORT_NAME; print(dir($IMPORT_NAME))""

      - name: Import test dependencies
        run: uv sync --group test
        working-directory: ${{ inputs.working-directory }}

      # Overwrite the local version of the package with the built version
      - name: Import published package (again)
        working-directory: ${{ inputs.working-directory }}
        shell: bash
        env:
          PKG_NAME: ${{ needs.build.outputs.pkg-name }}
          VERSION: ${{ needs.build.outputs.version }}
        run: |
          VIRTUAL_ENV=.venv uv pip install dist/*.whl

      - name: Run unit tests
        run: make tests
        working-directory: ${{ inputs.working-directory }}

      - name: Check for prerelease versions
        working-directory: ${{ inputs.working-directory }}
        run: |
          uv run python $GITHUB_WORKSPACE/.github/scripts/check_prerelease_dependencies.py pyproject.toml

      - name: Get minimum versions
        working-directory: ${{ inputs.working-directory }}
        id: min-version
        run: |
          VIRTUAL_ENV=.venv uv pip install packaging requests
          python_version=""$(uv run python --version | awk '{print $2}')""
          min_versions=""$(uv run python $GITHUB_WORKSPACE/.github/scripts/get_min_versions.py pyproject.toml release $python_version)""
          echo ""min-versions=$min_versions"" >> ""$GITHUB_OUTPUT""
          echo ""min-versions=$min_versions""

      - name: Run unit tests with minimum dependency versions
        if: ${{ steps.min-version.outputs.min-versions != '' }}
        env:
          MIN_VERSIONS: ${{ steps.min-version.outputs.min-versions }}
        run: |
          VIRTUAL_ENV=.venv uv pip install --force-reinstall $MIN_VERSIONS --editable .
          make tests
        working-directory: ${{ inputs.working-directory }}

      - name: Import integration test dependencies
        run: uv sync --group test --group test_integration
        working-directory: ${{ inputs.working-directory }}

      - name: Run integration tests
        if: ${{ startsWith(inputs.working-directory, 'libs/partners/') }}
        env:
          AI21_API_KEY: ${{ secrets.AI21_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}
          TOGETHER_API_KEY: ${{ secrets.TOGETHER_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          AZURE_OPENAI_API_VERSION: ${{ secrets.AZURE_OPENAI_API_VERSION }}
          AZURE_OPENAI_API_BASE: ${{ secrets.AZURE_OPENAI_API_BASE }}
          AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
          AZURE_OPENAI_CHAT_DEPLOYMENT_NAME: ${{ secrets.AZURE_OPENAI_CHAT_DEPLOYMENT_NAME }}
          AZURE_OPENAI_LEGACY_CHAT_DEPLOYMENT_NAME: ${{ secrets.AZURE_OPENAI_LEGACY_CHAT_DEPLOYMENT_NAME }}
          AZURE_OPENAI_LLM_DEPLOYMENT_NAME: ${{ secrets.AZURE_OPENAI_LLM_DEPLOYMENT_NAME }}
          AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME: ${{ secrets.AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME }}
          NVIDIA_API_KEY: ${{ secrets.NVIDIA_API_KEY }}
          GOOGLE_SEARCH_API_KEY: ${{ secrets.GOOGLE_SEARCH_API_KEY }}
          GOOGLE_CSE_ID: ${{ secrets.GOOGLE_CSE_ID }}
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          HUGGINGFACEHUB_API_TOKEN: ${{ secrets.HUGGINGFACEHUB_API_TOKEN }}
          EXA_API_KEY: ${{ secrets.EXA_API_KEY }}
          NOMIC_API_KEY: ${{ secrets.NOMIC_API_KEY }}
          WATSONX_APIKEY: ${{ secrets.WATSONX_APIKEY }}
          WATSONX_PROJECT_ID: ${{ secrets.WATSONX_PROJECT_ID }}
          ASTRA_DB_API_ENDPOINT: ${{ secrets.ASTRA_DB_API_ENDPOINT }}
          ASTRA_DB_APPLICATION_TOKEN: ${{ secrets.ASTRA_DB_APPLICATION_TOKEN }}
          ASTRA_DB_KEYSPACE: ${{ secrets.ASTRA_DB_KEYSPACE }}
          ES_URL: ${{ secrets.ES_URL }}
          ES_CLOUD_ID: ${{ secrets.ES_CLOUD_ID }}
          ES_API_KEY: ${{ secrets.ES_API_KEY }}
          MONGODB_ATLAS_URI: ${{ secrets.MONGODB_ATLAS_URI }}
          UPSTAGE_API_KEY: ${{ secrets.UPSTAGE_API_KEY }}
          FIREWORKS_API_KEY: ${{ secrets.FIREWORKS_API_KEY }}
          XAI_API_KEY: ${{ secrets.XAI_API_KEY }}
          DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
          PPLX_API_KEY: ${{ secrets.PPLX_API_KEY }}
        run: make integration_tests
        working-directory: ${{ inputs.working-directory }}

  # Test select published packages against new core
  test-prior-published-packages-against-new-core:
    needs:
      - build
      - release-notes
      - test-pypi-publish
      - pre-release-checks
    runs-on: ubuntu-latest
    strategy:
      matrix:
        partner: [openai, anthropic]
      fail-fast: false  # Continue testing other partners if one fails
    env:
      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      ANTHROPIC_FILES_API_IMAGE_ID: ${{ secrets.ANTHROPIC_FILES_API_IMAGE_ID }}
      ANTHROPIC_FILES_API_PDF_ID: ${{ secrets.ANTHROPIC_FILES_API_PDF_ID }}
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      AZURE_OPENAI_API_VERSION: ${{ secrets.AZURE_OPENAI_API_VERSION }}
      AZURE_OPENAI_API_BASE: ${{ secrets.AZURE_OPENAI_API_BASE }}
      AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
      AZURE_OPENAI_CHAT_DEPLOYMENT_NAME: ${{ secrets.AZURE_OPENAI_CHAT_DEPLOYMENT_NAME }}
      AZURE_OPENAI_LEGACY_CHAT_DEPLOYMENT_NAME: ${{ secrets.AZURE_OPENAI_LEGACY_CHAT_DEPLOYMENT_NAME }}
      AZURE_OPENAI_LLM_DEPLOYMENT_NAME: ${{ secrets.AZURE_OPENAI_LLM_DEPLOYMENT_NAME }}
      AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME: ${{ secrets.AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME }}
    steps:
      - uses: actions/checkout@v4

      # We implement this conditional as Github Actions does not have good support
      # for conditionally needing steps. https://github.com/actions/runner/issues/491
      - name: Check if libs/core
        run: |
          if [ ""${{ startsWith(inputs.working-directory, 'libs/core') }}"" != ""true"" ]; then
            echo ""Not in libs/core. Exiting successfully.""
            exit 0
          fi

      - name: Set up Python + uv
        if: startsWith(inputs.working-directory, 'libs/core')
        uses: ""./.github/actions/uv_setup""
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - uses: actions/download-artifact@v4
        if: startsWith(inputs.working-directory, 'libs/core')
        with:
          name: dist
          path: ${{ inputs.working-directory }}/dist/

      - name: Test against ${{ matrix.partner }}
        if: startsWith(inputs.working-directory, 'libs/core')
        run: |
          # Identify latest tag
          LATEST_PACKAGE_TAG=""$(
            git ls-remote --tags origin ""langchain-${{ matrix.partner }}*"" \
            | awk '{print $2}' \
            | sed 's|refs/tags/||' \
            | sort -Vr \
            | head -n 1
          )""
          echo ""Latest package tag: $LATEST_PACKAGE_TAG""

          # Shallow-fetch just that single tag
          git fetch --depth=1 origin tag ""$LATEST_PACKAGE_TAG""

          # Checkout the latest package files
          rm -rf $GITHUB_WORKSPACE/libs/partners/${{ matrix.partner }}/*
          rm -rf $GITHUB_WORKSPACE/libs/standard-tests/*
          cd $GITHUB_WORKSPACE/libs/
          git checkout ""$LATEST_PACKAGE_TAG"" -- standard-tests/
          git checkout ""$LATEST_PACKAGE_TAG"" -- partners/${{ matrix.partner }}/
          cd partners/${{ matrix.partner }}

          # Print as a sanity check
          echo ""Version number from pyproject.toml: ""
          cat pyproject.toml | grep ""version = ""

          # Run tests
          uv sync --group test --group test_integration
          uv pip install ../../core/dist/*.whl
          make integration_tests

  publish:
    needs:
      - build
      - release-notes
      - test-pypi-publish
      - pre-release-checks
      - test-prior-published-packages-against-new-core
    runs-on: ubuntu-latest
    permissions:
      # This permission is used for trusted publishing:
      # https://blog.pypi.org/posts/2023-04-20-introducing-trusted-publishers/
      #
      # Trusted publishing has to also be configured on PyPI for each package:
      # https://docs.pypi.org/trusted-publishers/adding-a-publisher/
      id-token: write

    defaults:
      run:
        working-directory: ${{ inputs.working-directory }}

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python + uv
        uses: ""./.github/actions/uv_setup""
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - uses: actions/download-artifact@v4
        with:
          name: dist
          path: ${{ inputs.working-directory }}/dist/

      - name: Publish package distributions to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          packages-dir: ${{ inputs.working-directory }}/dist/
          verbose: true
          print-hash: true
          # Temp workaround since attestations are on by default as of gh-action-pypi-publish v1.11.0
          attestations: false

  mark-release:
    needs:
      - build
      - release-notes
      - test-pypi-publish
      - pre-release-checks
      - publish
    runs-on: ubuntu-latest
    permissions:
      # This permission is needed by `ncipollo/release-action` to
      # create the GitHub release.
      contents: write

    defaults:
      run:
        working-directory: ${{ inputs.working-directory }}

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python + uv
        uses: ""./.github/actions/uv_setup""
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - uses: actions/download-artifact@v4
        with:
          name: dist
          path: ${{ inputs.working-directory }}/dist/
          
      - name: Create Tag
        uses: ncipollo/release-action@v1
        with:
          artifacts: ""dist/*""
          token: ${{ secrets.GITHUB_TOKEN }}
          generateReleaseNotes: false
          tag: ${{needs.build.outputs.pkg-name}}==${{ needs.build.outputs.version }}
          body: ${{ needs.release-notes.outputs.release-body }}
          commit: ${{ github.sha }}
          makeLatest: ${{ needs.build.outputs.pkg-name == 'langchain-core'}}
",495,7,2,"workflow_call, workflow_dispatch",19
langchain-ai/langchain,_test.yml,"name: test

on:
  workflow_call:
    inputs:
      working-directory:
        required: true
        type: string
        description: ""From which folder this pipeline executes""
      python-version:
        required: true
        type: string
        description: ""Python version to use""

env:
  UV_FROZEN: ""true""
  UV_NO_SYNC: ""true""

jobs:
  build:
    defaults:
      run:
        working-directory: ${{ inputs.working-directory }}
    runs-on: ubuntu-latest
    timeout-minutes: 20
    name: ""make test #${{ inputs.python-version }}""
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ inputs.python-version }} + uv
        uses: ""./.github/actions/uv_setup""
        id: setup-python
        with:
          python-version: ${{ inputs.python-version }}
      - name: Install dependencies
        shell: bash
        run: uv sync --group test --dev

      - name: Run core tests
        shell: bash
        run: |
          make test

      - name: Get minimum versions
        working-directory: ${{ inputs.working-directory }}
        id: min-version
        shell: bash
        run: |
          VIRTUAL_ENV=.venv uv pip install packaging tomli requests
          python_version=""$(uv run python --version | awk '{print $2}')""
          min_versions=""$(uv run python $GITHUB_WORKSPACE/.github/scripts/get_min_versions.py pyproject.toml pull_request $python_version)""
          echo ""min-versions=$min_versions"" >> ""$GITHUB_OUTPUT""
          echo ""min-versions=$min_versions""

      - name: Run unit tests with minimum dependency versions
        if: ${{ steps.min-version.outputs.min-versions != '' }}
        env:
          MIN_VERSIONS: ${{ steps.min-version.outputs.min-versions }}
        run: |
          VIRTUAL_ENV=.venv uv pip install $MIN_VERSIONS
          make tests
        working-directory: ${{ inputs.working-directory }}

      - name: Ensure the tests did not create any additional files
        shell: bash
        run: |
          set -eu

          STATUS=""$(git status)""
          echo ""$STATUS""

          # grep will exit non-zero if the target message isn't found,
          # and `set -e` above will cause the step to fail.
          echo ""$STATUS"" | grep 'nothing to commit, working tree clean'
          
",75,1,1,workflow_call,2
langchain-ai/langchain,_test_doc_imports.yml,"name: test_doc_imports

on:
  workflow_call:
    inputs:
      python-version:
        required: true
        type: string
        description: ""Python version to use""

env:
  UV_FROZEN: ""true""

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    name: ""check doc imports #${{ inputs.python-version }}""
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ inputs.python-version }} + uv
        uses: ""./.github/actions/uv_setup""
        with:
          python-version: ${{ inputs.python-version }}

      - name: Install dependencies
        shell: bash
        run: uv sync --group test

      - name: Install langchain editable
        run: |
          VIRTUAL_ENV=.venv uv pip install langchain-experimental langchain-community -e libs/core libs/langchain

      - name: Check doc imports
        shell: bash
        run: |
          uv run python docs/scripts/check_imports.py

      - name: Ensure the test did not create any additional files
        shell: bash
        run: |
          set -eu

          STATUS=""$(git status)""
          echo ""$STATUS""

          # grep will exit non-zero if the target message isn't found,
          # and `set -e` above will cause the step to fail.
          echo ""$STATUS"" | grep 'nothing to commit, working tree clean'
",50,1,1,workflow_call,2
langchain-ai/langchain,_test_pydantic.yml,"name: test pydantic intermediate versions

on:
  workflow_call:
    inputs:
      working-directory:
        required: true
        type: string
        description: ""From which folder this pipeline executes""
      python-version:
        required: false
        type: string
        description: ""Python version to use""
        default: ""3.11""
      pydantic-version:
        required: true
        type: string
        description: ""Pydantic version to test.""

env:
  UV_FROZEN: ""true""
  UV_NO_SYNC: ""true""

jobs:
  build:
    defaults:
      run:
        working-directory: ${{ inputs.working-directory }}
    runs-on: ubuntu-latest
    timeout-minutes: 20
    name: ""make test # pydantic: ~=${{ inputs.pydantic-version }}, python: ${{ inputs.python-version }}, ""
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ inputs.python-version }} + uv
        uses: ""./.github/actions/uv_setup""
        with:
          python-version: ${{ inputs.python-version }}

      - name: Install dependencies
        shell: bash
        run: uv sync --group test

      - name: Overwrite pydantic version
        shell: bash
        run: VIRTUAL_ENV=.venv uv pip install pydantic~=${{ inputs.pydantic-version }}

      - name: Run core tests
        shell: bash
        run: |
          make test

      - name: Ensure the tests did not create any additional files
        shell: bash
        run: |
          set -eu

          STATUS=""$(git status)""
          echo ""$STATUS""

          # grep will exit non-zero if the target message isn't found,
          # and `set -e` above will cause the step to fail.
          echo ""$STATUS"" | grep 'nothing to commit, working tree clean'",63,1,1,workflow_call,2
langchain-ai/langchain,_test_release.yml,"name: test-release

on:
  workflow_call:
    inputs:
      working-directory:
        required: true
        type: string
        description: ""From which folder this pipeline executes""
      dangerous-nonmaster-release:
        required: false
        type: boolean
        default: false
        description: ""Release from a non-master branch (danger!)""

env:
  PYTHON_VERSION: ""3.11""
  UV_FROZEN: ""true""

jobs:
  build:
    if: github.ref == 'refs/heads/master' || inputs.dangerous-nonmaster-release
    runs-on: ubuntu-latest

    outputs:
      pkg-name: ${{ steps.check-version.outputs.pkg-name }}
      version: ${{ steps.check-version.outputs.version }}

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python + uv
        uses: ""./.github/actions/uv_setup""
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      # We want to keep this build stage *separate* from the release stage,
      # so that there's no sharing of permissions between them.
      # The release stage has trusted publishing and GitHub repo contents write access,
      # and we want to keep the scope of that access limited just to the release job.
      # Otherwise, a malicious `build` step (e.g. via a compromised dependency)
      # could get access to our GitHub or PyPI credentials.
      #
      # Per the trusted publishing GitHub Action:
      # > It is strongly advised to separate jobs for building [...]
      # > from the publish job.
      # https://github.com/pypa/gh-action-pypi-publish#non-goals
      - name: Build project for distribution
        run: uv build
        working-directory: ${{ inputs.working-directory }}

      - name: Upload build
        uses: actions/upload-artifact@v4
        with:
          name: test-dist
          path: ${{ inputs.working-directory }}/dist/

      - name: Check Version
        id: check-version
        shell: python
        working-directory: ${{ inputs.working-directory }}
        run: |
          import os
          import tomllib
          with open(""pyproject.toml"", ""rb"") as f:
              data = tomllib.load(f)
          pkg_name = data[""project""][""name""]
          version = data[""project""][""version""]
          with open(os.environ[""GITHUB_OUTPUT""], ""a"") as f:
              f.write(f""pkg-name={pkg_name}\n"")
              f.write(f""version={version}\n"")

  publish:
    needs:
      - build
    runs-on: ubuntu-latest
    permissions:
      # This permission is used for trusted publishing:
      # https://blog.pypi.org/posts/2023-04-20-introducing-trusted-publishers/
      #
      # Trusted publishing has to also be configured on PyPI for each package:
      # https://docs.pypi.org/trusted-publishers/adding-a-publisher/
      id-token: write

    steps:
      - uses: actions/checkout@v4

      - uses: actions/download-artifact@v4
        with:
          name: test-dist
          path: ${{ inputs.working-directory }}/dist/

      - name: Publish to test PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          packages-dir: ${{ inputs.working-directory }}/dist/
          verbose: true
          print-hash: true
          repository-url: https://test.pypi.org/legacy/

          # We overwrite any existing distributions with the same name and version.
          # This is *only for CI use* and is *extremely dangerous* otherwise!
          # https://github.com/pypa/gh-action-pypi-publish#tolerating-release-package-file-duplicates
          skip-existing: true
          # Temp workaround since attestations are on by default as of gh-action-pypi-publish v1.11.0
          attestations: false
",106,2,1,workflow_call,6
langchain-ai/langchain,api_doc_build.yml,"name: API docs build

on:
  workflow_dispatch:
  schedule:
    - cron:  '0 13 * * *'
env:
  PYTHON_VERSION: ""3.11""

jobs:
  build:
    if: github.repository == 'langchain-ai/langchain' || github.event_name != 'schedule'
    runs-on: ubuntu-latest
    permissions: write-all
    steps:
      - uses: actions/checkout@v4
        with:
          path: langchain
      - uses: actions/checkout@v4
        with:
          repository: langchain-ai/langchain-api-docs-html
          path: langchain-api-docs-html
          token: ${{ secrets.TOKEN_GITHUB_API_DOCS_HTML }}

      - name: Get repos with yq
        id: get-unsorted-repos
        uses: mikefarah/yq@master
        with:
          cmd: |
            yq '
              .packages[]
              | select(
                  (
                    (.repo | test(""^langchain-ai/""))
                    and
                    (.repo != ""langchain-ai/langchain"")
                  )
                  or
                  (.include_in_api_ref // false)
                )
              | .repo
            ' langchain/libs/packages.yml

      - name: Parse YAML and checkout repos
        env:
          REPOS_UNSORTED: ${{ steps.get-unsorted-repos.outputs.result }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Get unique repositories
          REPOS=$(echo ""$REPOS_UNSORTED"" | sort -u)
          
          # Checkout each unique repository that is in langchain-ai org
          for repo in $REPOS; do
            REPO_NAME=$(echo $repo | cut -d'/' -f2)
            echo ""Checking out $repo to $REPO_NAME""
            git clone --depth 1 https://github.com/$repo.git $REPO_NAME
          done

      - name: Setup python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        id: setup-python
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install initial py deps
        working-directory: langchain
        run: |
          python -m pip install -U uv
          python -m uv pip install --upgrade --no-cache-dir pip setuptools pyyaml

      - name: Move libs with script
        run: python langchain/.github/scripts/prep_api_docs_build.py
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Rm old html
        run:
          rm -rf langchain-api-docs-html/api_reference_build/html

      - name: Install dependencies
        working-directory: langchain
        run: |
          python -m uv pip install $(ls ./libs/partners | xargs -I {} echo ""./libs/partners/{}"") --overrides ./docs/vercel_overrides.txt
          python -m uv pip install libs/core libs/langchain libs/text-splitters libs/community libs/experimental libs/standard-tests
          python -m uv pip install -r docs/api_reference/requirements.txt

      - name: Set Git config
        working-directory: langchain
        run: |
          git config --local user.email ""actions@github.com""
          git config --local user.name ""Github Actions""

      - name: Build docs
        working-directory: langchain
        run: |
          python docs/api_reference/create_api_rst.py
          python -m sphinx -T -E -b html -d ../langchain-api-docs-html/_build/doctrees -c docs/api_reference docs/api_reference ../langchain-api-docs-html/api_reference_build/html -j auto
          python docs/api_reference/scripts/custom_formatter.py ../langchain-api-docs-html/api_reference_build/html
          # Default index page is blank so we copy in the actual home page.
          cp ../langchain-api-docs-html/api_reference_build/html/{reference,index}.html
          rm -rf ../langchain-api-docs-html/_build/

      # https://github.com/marketplace/actions/add-commit
      - uses: EndBug/add-and-commit@v9
        with:
          cwd: langchain-api-docs-html
          message: 'Update API docs build'
",107,1,2,"workflow_dispatch, schedule",5
langchain-ai/langchain,check-broken-links.yml,"name: Check Broken Links

on:
  workflow_dispatch:
  schedule:
    - cron:  '0 13 * * *'

jobs:
  check-links:
    if: github.repository_owner == 'langchain-ai' || github.event_name != 'schedule'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Use Node.js 18.x
        uses: actions/setup-node@v4
        with:
          node-version: 18.x
          cache: ""yarn""
          cache-dependency-path: ./docs/yarn.lock
      - name: Install dependencies
        run: yarn install --immutable --mode=skip-build
        working-directory: ./docs
      - name: Check broken links
        run: yarn check-broken-links
        working-directory: ./docs
",25,1,2,"workflow_dispatch, schedule",2
langchain-ai/langchain,check_core_versions.yml,"name: Check `langchain-core` version equality

on:
  pull_request:
    paths:
      - 'libs/core/pyproject.toml'
      - 'libs/core/langchain_core/version.py'

jobs:
  check_version_equality:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Check version equality
        run: |
          PYPROJECT_VERSION=$(grep -Po '(?<=^version = "")[^""]*' libs/core/pyproject.toml)
          VERSION_PY_VERSION=$(grep -Po '(?<=^VERSION = "")[^""]*' libs/core/langchain_core/version.py)

          # Compare the two versions
          if [ ""$PYPROJECT_VERSION"" != ""$VERSION_PY_VERSION"" ]; then
            echo ""langchain-core versions in pyproject.toml and version.py do not match!""
            echo ""pyproject.toml version: $PYPROJECT_VERSION""
            echo ""version.py version: $VERSION_PY_VERSION""
            exit 1
          else
            echo ""Versions match: $PYPROJECT_VERSION""
          fi
",29,1,1,pull_request,1
langchain-ai/langchain,check_diffs.yml,"name: CI

on:
  push:
    branches: [master]
  pull_request:
  merge_group:

# If another push to the same PR or branch happens while this workflow is still running,
# cancel the earlier run in favor of the next run.
#
# There's no point in testing an outdated version of the code. GitHub only allows
# a limited number of job runners to be active at the same time, so it's better to cancel
# pointless jobs early so that more useful jobs can run sooner.
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  UV_FROZEN: ""true""
  UV_NO_SYNC: ""true""

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - id: files
        uses: Ana06/get-changed-files@v2.3.0
      - id: set-matrix
        run: |
          python -m pip install packaging requests
          python .github/scripts/check_diff.py ${{ steps.files.outputs.all }} >> $GITHUB_OUTPUT
    outputs:
      lint: ${{ steps.set-matrix.outputs.lint }}
      test: ${{ steps.set-matrix.outputs.test }}
      extended-tests: ${{ steps.set-matrix.outputs.extended-tests }}
      compile-integration-tests: ${{ steps.set-matrix.outputs.compile-integration-tests }}
      dependencies: ${{ steps.set-matrix.outputs.dependencies }}
      test-doc-imports: ${{ steps.set-matrix.outputs.test-doc-imports }}
      test-pydantic: ${{ steps.set-matrix.outputs.test-pydantic }}
  lint:
    name: cd ${{ matrix.job-configs.working-directory }}
    needs: [ build ]
    if: ${{ needs.build.outputs.lint != '[]' }}
    strategy:
      matrix:
        job-configs: ${{ fromJson(needs.build.outputs.lint) }}
      fail-fast: false
    uses: ./.github/workflows/_lint.yml
    with:
      working-directory: ${{ matrix.job-configs.working-directory }}
      python-version: ${{ matrix.job-configs.python-version }}
    secrets: inherit

  test:
    name: cd ${{ matrix.job-configs.working-directory }}
    needs: [ build ]
    if: ${{ needs.build.outputs.test != '[]' }}
    strategy:
      matrix:
        job-configs: ${{ fromJson(needs.build.outputs.test) }}
      fail-fast: false
    uses: ./.github/workflows/_test.yml
    with:
      working-directory: ${{ matrix.job-configs.working-directory }}
      python-version: ${{ matrix.job-configs.python-version }}
    secrets: inherit

  test-pydantic:
    name: cd ${{ matrix.job-configs.working-directory }}
    needs: [ build ]
    if: ${{ needs.build.outputs.test-pydantic != '[]' }}
    strategy:
      matrix:
        job-configs: ${{ fromJson(needs.build.outputs.test-pydantic) }}
      fail-fast: false
    uses: ./.github/workflows/_test_pydantic.yml
    with:
      working-directory: ${{ matrix.job-configs.working-directory }}
      pydantic-version: ${{ matrix.job-configs.pydantic-version }}
    secrets: inherit

  test-doc-imports:
    needs: [ build ]
    if: ${{ needs.build.outputs.test-doc-imports != '[]' }}
    strategy:
      matrix:
        job-configs: ${{ fromJson(needs.build.outputs.test-doc-imports) }}
      fail-fast: false
    uses: ./.github/workflows/_test_doc_imports.yml
    secrets: inherit
    with:
      python-version: ${{ matrix.job-configs.python-version }}

  compile-integration-tests:
    name: cd ${{ matrix.job-configs.working-directory }}
    needs: [ build ]
    if: ${{ needs.build.outputs.compile-integration-tests != '[]' }}
    strategy:
      matrix:
        job-configs: ${{ fromJson(needs.build.outputs.compile-integration-tests) }}
      fail-fast: false
    uses: ./.github/workflows/_compile_integration_test.yml
    with:
      working-directory: ${{ matrix.job-configs.working-directory }}
      python-version: ${{ matrix.job-configs.python-version }}
    secrets: inherit

  extended-tests:
    name: ""cd ${{ matrix.job-configs.working-directory }} / make extended_tests #${{ matrix.job-configs.python-version }}""
    needs: [ build ]
    if: ${{ needs.build.outputs.extended-tests != '[]' }}
    strategy:
      matrix:
        # note different variable for extended test dirs
        job-configs: ${{ fromJson(needs.build.outputs.extended-tests) }}
      fail-fast: false
    runs-on: ubuntu-latest
    timeout-minutes: 20
    defaults:
      run:
        working-directory: ${{ matrix.job-configs.working-directory }}
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.job-configs.python-version }} + uv
        uses: ""./.github/actions/uv_setup""
        with:
          python-version: ${{ matrix.job-configs.python-version }}

      - name: Install dependencies and run extended tests
        shell: bash
        run: |
          echo ""Running extended tests, installing dependencies with uv...""
          uv venv
          uv sync --group test
          VIRTUAL_ENV=.venv uv pip install -r extended_testing_deps.txt
          VIRTUAL_ENV=.venv make extended_tests

      - name: Ensure the tests did not create any additional files
        shell: bash
        run: |
          set -eu

          STATUS=""$(git status)""
          echo ""$STATUS""

          # grep will exit non-zero if the target message isn't found,
          # and `set -e` above will cause the step to fail.
          echo ""$STATUS"" | grep 'nothing to commit, working tree clean'

  ci_success:
    name: ""CI Success""
    needs: [build, lint, test, compile-integration-tests, extended-tests, test-doc-imports, test-pydantic]
    if: |
      always()
    runs-on: ubuntu-latest
    env:
      JOBS_JSON: ${{ toJSON(needs) }}
      RESULTS_JSON: ${{ toJSON(needs.*.result) }}
      EXIT_CODE: ${{!contains(needs.*.result, 'failure') && !contains(needs.*.result, 'cancelled') && '0' || '1'}}
    steps:
      - name: ""CI Success""
        run: |
          echo $JOBS_JSON
          echo $RESULTS_JSON
          echo ""Exiting with $EXIT_CODE""
          exit $EXIT_CODE
",172,8,3,"push, pull_request, merge_group",10
langchain-ai/langchain,check_new_docs.yml,"name: Integration docs lint

on:
  push:
    branches: [master]
  pull_request:

# If another push to the same PR or branch happens while this workflow is still running,
# cancel the earlier run in favor of the next run.
#
# There's no point in testing an outdated version of the code. GitHub only allows
# a limited number of job runners to be active at the same time, so it's better to cancel
# pointless jobs early so that more useful jobs can run sooner.
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      - id: files
        uses: Ana06/get-changed-files@v2.3.0
        with:
          filter: |
            *.ipynb
            *.md
            *.mdx
      - name: Check new docs
        run: |
          python docs/scripts/check_templates.py ${{ steps.files.outputs.added }}
",35,1,2,"push, pull_request",3
langchain-ai/langchain,codespell.yml,"name: CI / cd . / make spell_check

on:
  push:
    branches: [master, v0.1, v0.2]
  pull_request:

permissions:
  contents: read

jobs:
  codespell:
    name: (Check for spelling errors)
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install Dependencies
        run: |
          pip install toml

      - name: Extract Ignore Words List
        run: |
          # Use a Python script to extract the ignore words list from pyproject.toml
          python .github/workflows/extract_ignored_words_list.py
        id: extract_ignore_words

#      - name: Codespell
#        uses: codespell-project/actions-codespell@v2
#        with:
#          skip: guide_imports.json,*.ambr,./cookbook/data/imdb_top_1000.csv,*.lock
#          ignore_words_list: ${{ steps.extract_ignore_words.outputs.ignore_words_list }}
#          exclude_file: ./.github/workflows/codespell-exclude
",35,1,2,"push, pull_request",2
langchain-ai/langchain,codspeed.yml,"name: CodSpeed

on:
  push:
    branches:
        - master
  pull_request:
  workflow_dispatch:

env:
  AZURE_OPENAI_CHAT_DEPLOYMENT_NAME: foo
  AZURE_OPENAI_LEGACY_CHAT_DEPLOYMENT_NAME: foo
  DEEPSEEK_API_KEY: foo
  FIREWORKS_API_KEY: foo

jobs:
  codspeed:
    name: Run benchmarks
    runs-on: ubuntu-latest
    strategy:
      matrix:
        include:
          - working-directory: libs/core
            mode: walltime
          - working-directory: libs/partners/openai
          - working-directory: libs/partners/anthropic
          - working-directory: libs/partners/deepseek
          - working-directory: libs/partners/fireworks
          - working-directory: libs/partners/xai
          - working-directory: libs/partners/mistralai
          - working-directory: libs/partners/groq
      fail-fast: false

    steps:
      - uses: actions/checkout@v4

      # We have to use 3.12 as 3.13 is not yet supported
      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          python-version: ""3.12""

      - uses: actions/setup-python@v5
        with:
          python-version: ""3.12""

      - name: Install dependencies
        run: uv sync --group test
        working-directory: ${{ matrix.working-directory }}

      - name: Run benchmarks ${{ matrix.working-directory }}
        uses: CodSpeedHQ/action@v3
        with:
          token: ${{ secrets.CODSPEED_TOKEN }}
          run: |
            cd ${{ matrix.working-directory }}
            if [ ""${{ matrix.working-directory }}"" = ""libs/core"" ]; then
              uv run --no-sync pytest ./tests/benchmarks --codspeed
            else
              uv run --no-sync pytest ./tests/ --codspeed
            fi
          mode: ${{ matrix.mode || 'instrumentation' }}
",62,1,3,"push, pull_request, workflow_dispatch",4
langchain-ai/langchain,people.yml,"name: LangChain People

on:
  schedule:
    - cron: ""0 14 1 * *""
  push:
    branches: [jacob/people]
  workflow_dispatch:

jobs:
  langchain-people:
    if: github.repository_owner == 'langchain-ai' || github.event_name != 'schedule'
    runs-on: ubuntu-latest
    permissions: write-all
    steps:
      - name: Dump GitHub context
        env:
          GITHUB_CONTEXT: ${{ toJson(github) }}
        run: echo ""$GITHUB_CONTEXT""
      - uses: actions/checkout@v4
      # Ref: https://github.com/actions/runner/issues/2033
      - name: Fix git safe.directory in container
        run: mkdir -p /home/runner/work/_temp/_github_home && printf ""[safe]\n\tdirectory = /github/workspace"" > /home/runner/work/_temp/_github_home/.gitconfig
      - uses: ./.github/actions/people
        with:
          token: ${{ secrets.LANGCHAIN_PEOPLE_GITHUB_TOKEN }}",26,1,3,"schedule, push, workflow_dispatch",2
langchain-ai/langchain,run_notebooks.yml,"name: Run notebooks

on:
  workflow_dispatch:
    inputs:
      python_version:
        description: 'Python version'
        required: false
        default: '3.11'
      working-directory:
        description: 'Working directory or subset (e.g., docs/docs/tutorials/llm_chain.ipynb or docs/docs/how_to)'
        required: false
        default: 'all'
  schedule:
    - cron: '0 13 * * *'

env:
  UV_FROZEN: ""true""

jobs:
  build:
    runs-on: ubuntu-latest
    if: github.repository == 'langchain-ai/langchain' || github.event_name != 'schedule'
    name: ""Test docs""
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python + uv
        uses: ""./.github/actions/uv_setup""
        with:
          python-version: ${{ github.event.inputs.python_version || '3.11' }}

      - name: 'Authenticate to Google Cloud'
        id: 'auth'
        uses: google-github-actions/auth@v2
        with:
          credentials_json: '${{ secrets.GOOGLE_CREDENTIALS }}'

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Install dependencies
        run: |
          uv sync --group dev --group test

      - name: Pre-download files
        run: |
          uv run python docs/scripts/cache_data.py
          curl -s https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql | sqlite3 docs/docs/how_to/Chinook.db
          cp docs/docs/how_to/Chinook.db docs/docs/tutorials/Chinook.db

      - name: Prepare notebooks
        run: |
          uv run python docs/scripts/prepare_notebooks_for_ci.py --comment-install-cells --working-directory ${{ github.event.inputs.working-directory || 'all' }}

      - name: Run notebooks
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          FIREWORKS_API_KEY: ${{ secrets.FIREWORKS_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY }}
          TOGETHER_API_KEY: ${{ secrets.TOGETHER_API_KEY }}
          WORKING_DIRECTORY: ${{ github.event.inputs.working-directory || 'all' }}
        run: |
          ./docs/scripts/execute_notebooks.sh $WORKING_DIRECTORY
",72,1,2,"workflow_dispatch, schedule",4
langchain-ai/langchain,scheduled_test.yml,"name: Scheduled tests

on:
  workflow_dispatch:  # Allows to trigger the workflow manually in GitHub UI
    inputs:
      working-directory-force:
        type: string
        description: ""From which folder this pipeline executes - defaults to all in matrix - example value: libs/partners/anthropic""
      python-version-force:
        type: string
        description: ""Python version to use - defaults to 3.9 and 3.11 in matrix - example value: 3.9""
  schedule:
    - cron:  '0 13 * * *'

env:
  POETRY_VERSION: ""1.8.4""
  UV_FROZEN: ""true""
  DEFAULT_LIBS: '[""libs/partners/openai"", ""libs/partners/anthropic"", ""libs/partners/fireworks"", ""libs/partners/groq"", ""libs/partners/mistralai"", ""libs/partners/xai"", ""libs/partners/google-vertexai"", ""libs/partners/google-genai"", ""libs/partners/aws""]'
  POETRY_LIBS: (""libs/partners/google-vertexai"" ""libs/partners/google-genai"" ""libs/partners/aws"")

jobs:
  compute-matrix:
    if: github.repository_owner == 'langchain-ai' || github.event_name != 'schedule'
    runs-on: ubuntu-latest
    name: Compute matrix
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Set matrix
        id: set-matrix
        env:
          DEFAULT_LIBS: ${{ env.DEFAULT_LIBS }}
          WORKING_DIRECTORY_FORCE: ${{ github.event.inputs.working-directory-force || '' }}
          PYTHON_VERSION_FORCE: ${{ github.event.inputs.python-version-force || '' }}
        run: |
          # echo ""matrix=..."" where matrix is a json formatted str with keys python-version and working-directory
          # python-version should default to 3.9 and 3.11, but is overridden to [PYTHON_VERSION_FORCE] if set
          # working-directory should default to DEFAULT_LIBS, but is overridden to [WORKING_DIRECTORY_FORCE] if set
          python_version='[""3.9"", ""3.11""]'
          working_directory=""$DEFAULT_LIBS""
          if [ -n ""$PYTHON_VERSION_FORCE"" ]; then
            python_version=""[\""$PYTHON_VERSION_FORCE\""]""
          fi
          if [ -n ""$WORKING_DIRECTORY_FORCE"" ]; then
            working_directory=""[\""$WORKING_DIRECTORY_FORCE\""]""
          fi
          matrix=""{\""python-version\"": $python_version, \""working-directory\"": $working_directory}""
          echo $matrix
          echo ""matrix=$matrix"" >> $GITHUB_OUTPUT
  build:
    if: github.repository_owner == 'langchain-ai' || github.event_name != 'schedule'
    name: Python ${{ matrix.python-version }} - ${{ matrix.working-directory }}
    runs-on: ubuntu-latest
    needs: [compute-matrix]
    timeout-minutes: 20
    strategy:
      fail-fast: false
      matrix:
        python-version: ${{ fromJSON(needs.compute-matrix.outputs.matrix).python-version }}
        working-directory: ${{ fromJSON(needs.compute-matrix.outputs.matrix).working-directory }}

    steps:
      - uses: actions/checkout@v4
        with:
          path: langchain
      - uses: actions/checkout@v4
        with:
          repository: langchain-ai/langchain-google
          path: langchain-google
      - uses: actions/checkout@v4
        with:
          repository: langchain-ai/langchain-aws
          path: langchain-aws

      - name: Move libs
        run: |
          rm -rf \
            langchain/libs/partners/google-genai \
            langchain/libs/partners/google-vertexai
          mv langchain-google/libs/genai langchain/libs/partners/google-genai
          mv langchain-google/libs/vertexai langchain/libs/partners/google-vertexai
          mv langchain-aws/libs/aws langchain/libs/partners/aws

      - name: Set up Python ${{ matrix.python-version }} with poetry
        if: contains(env.POETRY_LIBS, matrix.working-directory)
        uses: ""./langchain/.github/actions/poetry_setup""
        with:
          python-version: ${{ matrix.python-version }}
          poetry-version: ${{ env.POETRY_VERSION }}
          working-directory: langchain/${{ matrix.working-directory }}
          cache-key: scheduled

      - name: Set up Python ${{ matrix.python-version }} + uv
        if: ""!contains(env.POETRY_LIBS, matrix.working-directory)""
        uses: ""./langchain/.github/actions/uv_setup""
        with:
          python-version: ${{ matrix.python-version }}

      - name: 'Authenticate to Google Cloud'
        id: 'auth'
        uses: google-github-actions/auth@v2
        with:
          credentials_json: '${{ secrets.GOOGLE_CREDENTIALS }}'

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Install dependencies (poetry)
        if: contains(env.POETRY_LIBS, matrix.working-directory)
        run: |
          echo ""Running scheduled tests, installing dependencies with poetry...""
          cd langchain/${{ matrix.working-directory }}
          poetry install --with=test_integration,test

      - name: Install dependencies (uv)
        if: ""!contains(env.POETRY_LIBS, matrix.working-directory)""
        run: |
          echo ""Running scheduled tests, installing dependencies with uv...""
          cd langchain/${{ matrix.working-directory }}
          uv sync --group test --group test_integration

      - name: Run integration tests
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          ANTHROPIC_FILES_API_IMAGE_ID: ${{ secrets.ANTHROPIC_FILES_API_IMAGE_ID }}
          ANTHROPIC_FILES_API_PDF_ID: ${{ secrets.ANTHROPIC_FILES_API_PDF_ID }}
          AZURE_OPENAI_API_VERSION: ${{ secrets.AZURE_OPENAI_API_VERSION }}
          AZURE_OPENAI_API_BASE: ${{ secrets.AZURE_OPENAI_API_BASE }}
          AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
          AZURE_OPENAI_CHAT_DEPLOYMENT_NAME: ${{ secrets.AZURE_OPENAI_CHAT_DEPLOYMENT_NAME }}
          AZURE_OPENAI_LEGACY_CHAT_DEPLOYMENT_NAME: ${{ secrets.AZURE_OPENAI_LEGACY_CHAT_DEPLOYMENT_NAME }}
          AZURE_OPENAI_LLM_DEPLOYMENT_NAME: ${{ secrets.AZURE_OPENAI_LLM_DEPLOYMENT_NAME }}
          AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME: ${{ secrets.AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME }}
          DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
          FIREWORKS_API_KEY: ${{ secrets.FIREWORKS_API_KEY }}
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          HUGGINGFACEHUB_API_TOKEN: ${{ secrets.HUGGINGFACEHUB_API_TOKEN }}
          MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}
          XAI_API_KEY: ${{ secrets.XAI_API_KEY }}
          COHERE_API_KEY: ${{ secrets.COHERE_API_KEY }}
          NVIDIA_API_KEY: ${{ secrets.NVIDIA_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          GOOGLE_SEARCH_API_KEY: ${{ secrets.GOOGLE_SEARCH_API_KEY }}
          GOOGLE_CSE_ID: ${{ secrets.GOOGLE_CSE_ID }}
          PPLX_API_KEY: ${{ secrets.PPLX_API_KEY }}
        run: |
          cd langchain/${{ matrix.working-directory }}
          make integration_tests

      - name: Remove external libraries
        run: | 
          rm -rf \
            langchain/libs/partners/google-genai \
            langchain/libs/partners/google-vertexai \
            langchain/libs/partners/aws

      - name: Ensure the tests did not create any additional files
        working-directory: langchain
        run: |
          set -eu

          STATUS=""$(git status)""
          echo ""$STATUS""

          # grep will exit non-zero if the target message isn't found,
          # and `set -e` above will cause the step to fail.
          echo ""$STATUS"" | grep 'nothing to commit, working tree clean'
",172,2,2,"workflow_dispatch, schedule",7
axios/axios,ci.yml,"name: 'CI'

on:
  push:
    branches:
      - '*'
      - '*/*'
      - '**'
      - '!sponsors'
  pull_request:
    branches:
      - '*'
      - '*/*'
      - '**'
      - '!sponsors'

permissions:
  contents: read

jobs:
  build:
    runs-on: ubuntu-latest

    strategy:
      matrix:
        node-version: [12.x, 14.x, 16.x, 18.x, 20.x, 22.x, 24.x]
      fail-fast: false

    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: true
      - name: Get changed files
        id: changed-files
        uses: tj-actions/changed-files@v46
      - name: List all changed files
        run: |
          for file in ${{ steps.changed-files.outputs.all_changed_files }}; do
            echo ""$file was changed""
          done
      - name: Check changes
        id: changed-ignored
        uses: tj-actions/changed-files@v46
        with:
          files: |
            **.md
            sandbox/**
            examples/**
            .github/**
            templates/**
            bin/**
      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: npm
        if: steps.changed-ignored.outputs.only_modified == 'false'
      - run: npm ci
        if: steps.changed-ignored.outputs.only_modified == 'false'
      - run: npm test
        if: steps.changed-ignored.outputs.only_modified == 'false'
",61,1,2,"push, pull_request",4
axios/axios,codeql-analysis.yml,"name: 'CodeQL'

on:
  push:
    branches:
      - '*'
      - '*/*'
      - '**'
      - '!sponsors'
  pull_request:
    branches:
      - '*'
      - '*/*'
      - '**'
      - '!sponsors'
  schedule:
    - cron: '21 23 * * 5'

jobs:
  analyze:
    name: Analyze
    runs-on: ubuntu-latest
    permissions:
      actions: read
      contents: read
      security-events: write

    strategy:
      fail-fast: false
      matrix:
        language: [ 'javascript' ]

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        persist-credentials: false

    # Initializes the CodeQL tools for scanning.
    - name: Initialize CodeQL
      uses: github/codeql-action/init@v3
      with:
        languages: ${{ matrix.language }}
        queries: security-extended,security-and-quality

    - name: Perform CodeQL Analysis
      uses: github/codeql-action/analyze@v3
",47,1,3,"push, pull_request, schedule",3
axios/axios,depsreview.yaml,"name: 'Dependency Review'
on: [pull_request]

permissions:
  contents: read

jobs:
  dependency-review:
    runs-on: ubuntu-latest
    steps:
      - name: 'Checkout Repository'
        uses: actions/checkout@v4
        with:
          persist-credentials: false
      - name: 'Dependency Review'
        uses: actions/dependency-review-action@v4
",16,1,1,pull_request,2
axios/axios,labeler.yml,"name: ""PR Labeler""
on:
  pull_request_target:
  workflow_dispatch:
    inputs:
      prs:
        required: false
        description: ""pr number""

jobs:
  labeler:
    permissions:
      contents: read
      pull-requests: write
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: git config
        run: |
          git config user.name ""${GITHUB_ACTOR}""
          git config user.email ""${GITHUB_ACTOR}@users.noreply.github.com""
      - name: Set PR number
        id: set-pr
        run: |
            echo ""Using PR number: ${{ github.event.inputs.prs || github.event.pull_request.number }}""
            echo ""pr=${{ github.event.inputs.prs || github.event.pull_request.number }}"" >> $GITHUB_OUTPUT

      - uses: actions/labeler@v5
        with:
           pr-number: ${{ steps.set-pr.outputs.pr }}
",32,1,2,"pull_request_target, workflow_dispatch",2
axios/axios,notify.yml,"name: notify

on:
  #workflow_run:
  #  workflows: [""publish""]
  #  types:
  #    - completed
  #repository_dispatch:
  #  types: [ notify ]
  #release:
  #  types: [published]
  # branches:
  #    - main
  #    - 'v**'
  #push:
  #  tags:
  #    - 'v[0-9]+.[0-9]+.[0-9]+'
  #  branches:
  #    - main
  #    - 'v**'

  workflow_dispatch:
    inputs:
      tag:
        required: false
jobs:
  notify:
    runs-on: ubuntu-latest
    #if: ${{ github.event_name == 'workflow_dispatch' || github.event.workflow_run.conclusion == 'success' }}
    steps:
      #- name: Dump GitHub context
      #  env:
      #    GITHUB_CONTEXT: ${{ toJson(github) }}
      #  run: echo ""$GITHUB_CONTEXT""
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: git config
        run: |
          git config user.name ""${GITHUB_ACTOR}""
          git config user.email ""${GITHUB_ACTOR}@users.noreply.github.com""
      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: 18
          cache: npm
      - run: npm ci
      - name: Notify published PRs
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: node ./bin/actions/notify_published.js --tag ${{ github.event.inputs.tag || github.event.release.tag_name }}
",51,1,1,workflow_dispatch,2
axios/axios,npm-tag.yml,"name: NPM Tag
on:
  workflow_dispatch:
    inputs:
      version:
        required: true
      tag:
        required: true
        default: ""latest""
jobs:
  publish:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      id-token: write
    steps:
      - uses: actions/checkout@v4
      - name: git config
        run: |
          git config user.name ""${GITHUB_ACTOR}""
          git config user.email ""${GITHUB_ACTOR}@users.noreply.github.com""
      - uses: actions/setup-node@v4
        with:
          node-version: 18
          registry-url: https://registry.npmjs.org/
      ############# TAG RELEASE ##############
      - name: Tag release
        run: npm dist-tag add axios@${{ github.event.inputs.version }} ${{ github.event.inputs.tag }}
        env:
          NODE_AUTH_TOKEN: ${{secrets.npm_token}}
",30,1,1,workflow_dispatch,2
axios/axios,pr-guard.yml,"name: ""PR guard""

on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  check-dist:
    if: github.event.pull_request.head.repo.full_name != github.repository
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
    steps:
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            dist:
              - 'dist/**'

      - name: Comment if dist/ was changed
        if: steps.filter.outputs.dist == 'true'
        uses: peter-evans/create-or-update-comment@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          issue-number: ${{ github.event.pull_request.number }}
          body: |
            ⚠️ **Warning!** You have made changes to the `dist/` directory. 
            These files should not be edited manually as they will be built by our CI/CD pipeline during release. 
            Please remove these changes from the pull request.
",30,1,1,pull_request,2
axios/axios,pr.yml,"name: Release PR
on:
  workflow_dispatch:
    inputs:
      type:
        type: choice
        description: Choose release type
        options:
          - auto
          - patch
          - minor
          - major
        default: auto
      beta:
        type: boolean
        description: Prerelease
        default: false
jobs:
  releaseIt:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: git config
        run: |
          git config user.name ""${GITHUB_ACTOR}""
          git config user.email ""${GITHUB_ACTOR}@users.noreply.github.com""
      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: 16
          cache: npm
      - run: npm ci
      - name: Prepare release
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          TYPE_ARG: ${{ fromJSON('{""auto"":"""", ""patch"":""patch"", ""minor"":""minor"", ""major"":""major""}')[github.event.inputs.type] }}
          BETA_ARG: ${{ github.event.inputs.beta == 'true' && '--preRelease=beta' || '' }}
        run: npm run release -- $TYPE_ARG --ci --verbose --no-git.push --no-git.commit --no-git.tag --no-github $BETA_ARG $DRY_ARG
      - name: Show git status
        if: failure()
        run: git status && git diff
      - name: Add contributors list to CHANGELOG.md
        run: npm run release:changelog:fix
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: get-npm-version
        id: package-version
        uses: martinbeentjes/npm-get-version-action@main
      - name: Extract release notes
        id: extract-release-notes
        uses: ffurrer2/extract-release-notes@v2
      - name: Generate PR body
        id: body
        uses: mathiasvr/command-output@v1
        with:
          run: node ./bin/pr.js
      - name: Create pull request
        uses: peter-evans/create-pull-request@v7
        id: cpr
        with:
          branch: release
          delete-branch: true
          commit-message: 'chore(release): v${{ steps.package-version.outputs.current-version}}'
          title: '[Release] v${{ steps.package-version.outputs.current-version}}'
          body: |
            ${{ steps.body.outputs.stdout }}
            ## Release notes:
            ${{ steps.extract-release-notes.outputs.release_notes }}
          labels: |
            release
            bot
          signoff: false
          #team-reviewers: |
          #  owners
          #  maintainers
          #assignees: jasonsaayman
          #reviewers: jasonsaayman
          draft: false
      - name: Show PR link
        if: ${{ steps.cpr.outputs.pull-request-url }}
        run: |
          echo ""Axios Release v${{ steps.package-version.outputs.current-version}}' pull request - ${{ steps.cpr.outputs.pull-request-url }}""
",84,1,1,workflow_dispatch,6
axios/axios,publish.yml,"name: Publish
on:
  pull_request:
    types:
      - closed
    branches:
      - main
      - 'v**'
  workflow_dispatch:
jobs:
  publish:
    if: github.event_name == 'workflow_dispatch' || (github.event.pull_request.merged == true && github.event.pull_request.head.label == 'axios:release')
    runs-on: ubuntu-latest
    permissions:
      contents: write
      id-token: write
    steps:
      - name: ""Release PR info""
        if: github.event_name != 'workflow_dispatch'
        run: echo ""PR ${{ github.event.number }}""
      - uses: actions/checkout@v4
      - name: git config
        run: |
          git config user.name ""${GITHUB_ACTOR}""
          git config user.email ""${GITHUB_ACTOR}@users.noreply.github.com""
      - uses: actions/setup-node@v4
        with:
          node-version: 18
          registry-url: https://registry.npmjs.org/
      - run: npm ci
      - name: get-npm-version
        id: package-version
        uses: martinbeentjes/npm-get-version-action@main
      - name: Extract release notes
        id: extract-release-notes
        uses: ffurrer2/extract-release-notes@v2
      - name: Check versions
        run: node ./bin/check-build-version.js
      ############# TAG RELEASE ##############
      - name: ""Push tag v${{ steps.package-version.outputs.current-version }}""
        uses: rickstaa/action-create-tag@v1
        id: tag_version
        with:
          tag: ""v${{ steps.package-version.outputs.current-version }}""
      ############# RESOLVE NPM TAG ##############
      - name: NPM TAG
        id: 'npm_tag'
        run: node ./bin/resolveNPMTag.js
      ############# GITHUB RELEASE ##############
      - name: ""Create a GitHub release v${{ steps.package-version.outputs.current-version }}""
        uses: ncipollo/release-action@v1
        with:
          tag: ""v${{ steps.package-version.outputs.current-version }}""
          name: ""Release v${{ steps.package-version.outputs.current-version }}""
          body: |
            ## Release notes:
            ${{ steps.extract-release-notes.outputs.release_notes }}
      ############# NPM RELEASE ##############
      - name: Publish the release to NPM
        run: npm publish --provenance --access public --tag ${{ steps.npm_tag.outputs.tag || 'latest' }}
        env:
          NODE_AUTH_TOKEN: ${{secrets.npm_token}}
  notify:
    needs: [publish]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: git config
        run: |
          git config user.name ""${GITHUB_ACTOR}""
          git config user.email ""${GITHUB_ACTOR}@users.noreply.github.com""
      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: 18
          cache: npm
      - run: npm ci
      ############# Add release comments and tags to published PRs ##############
      - name: Notify published PRs
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: node ./bin/actions/notify_published.js --tag ${{ github.event.inputs.tag || github.event.release.tag_name }}
",84,2,2,"pull_request, workflow_dispatch",8
axios/axios,sponsors.yml,"name: Update Readme sponsor list
on:
  workflow_dispatch:
  repository_dispatch:
    types:
      - webhook
  schedule:
    # Run at 0000 daily
    - cron: '0 1 * * *'
jobs:
  sponsors:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: git config
        run: |
          git config user.name ""${GITHUB_ACTOR}""
          git config user.email ""${GITHUB_ACTOR}@users.noreply.github.com""
      - name: Setup node
        uses: actions/setup-node@v4
        with:
          node-version: 16
          cache: npm
      - run: npm ci
      - name: Check sponsors updates
        id: sponsors
        run: node ./bin/sponsors.js
      - name: Notify status
        if: ${{ steps.sponsors.outputs.changed == 'true'}}
        run: |
          echo ""Sponsor block has changed. Creating PR with updates...""
      - name: Read sponsors.md file content
        if: ${{ steps.sponsors.outputs.changed == 'true'}}
        id: read_file
        run: |
          echo 'CONTENT<<EOF' >> $GITHUB_ENV
          cat ./temp/sponsors.md >> $GITHUB_ENV
          echo 'EOF' >> $GITHUB_ENV
        shell: bash
      - name: Echo
        run: |
          echo ""$CONTENT""
      - name: Create pull request
        if: ${{ steps.sponsors.outputs.changed == 'true'}}
        uses: peter-evans/create-pull-request@v7
        id: cpr
        with:
          branch: sponsors
          delete-branch: true
          commit-message: 'chore(sponsor): update sponsor block'
          title: '[Chore] Update sponsor block'
          body: |
            **New sponsor block update:**
            ${{ env.CONTENT }}
          labels: |
            pr::docs
            bot
            automerge
          signoff: false
          #team-reviewers: |
          #  owners
          #  maintainers
          #assignees: jasonsaayman
          #reviewers: jasonsaayman
          draft: false
      - name: Show PR link
        if: ${{ steps.sponsors.outputs.changed == 'true'}}
        run: |
          echo ""Sponsor block has changed. Creating PR...""
          echo ""Axios Release v${{ steps.package-version.outputs.current-version}}' pull request - ${{ steps.cpr.outputs.pull-request-url }}""
",72,1,3,"workflow_dispatch, repository_dispatch, schedule",3
axios/axios,stale.yml,"name: 'Close Stale'

on:
  schedule:
    - cron: '0 0 * * 1'

jobs:
  stale:
    permissions:
      issues: write  # for actions/stale to close stale issues
      pull-requests: write  # for actions/stale to close stale PRs
    runs-on: ubuntu-latest
    steps:
      - name: Close Stale Issues
        uses: actions/stale@v9
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          stale-issue-message: |
            Hello! :wave:

            This issue is being automatically marked as stale because it has not been updated in a while. Please confirm that the issue is still present and reproducible. If no updates or new comments are received the issue will be closed in a few days.

            Thanks.
          stale-pr-message: |
            Hello! :wave:

            This pull request is being automatically marked as stale because it has not been updated in a while. Please confirm that the issue is still present and reproducible. If no updates or new comments are received the pull request will be closed in a few days.

            Thanks.
          stale-issue-label: 'status:stale'
          stale-pr-label: 'status:stale'
          only-labels: 'status:more info needed'
          days-before-stale: 30
          days-before-close: 14 
",34,1,1,schedule,1
mrdoob/three.js,ci.yml,"name: CI

on:
  pull_request:
    paths-ignore:
      - 'build/**'
      - 'docs/**'
      - 'files/**'

permissions:
  contents: read

jobs:
  lint:
    name: Lint testing
    runs-on: ubuntu-latest
    steps:
      - name: Git checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
      - name: Install Node
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4
        with:
          node-version: 18
          cache: 'npm'
      - name: Install dependencies
        run: npm ci

      - name: === Lint testing ===
        run: npm run lint

  unit:
    name: Unit testing
    runs-on: ubuntu-latest
    steps:
      - name: Git checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
      - name: Install Node
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4
        with:
          node-version: 18
          cache: 'npm'
      - name: Install dependencies
        run: npm ci

      - name: === Unit testing ===
        run: npm run test-unit

  circular:
    name: Circular dependencies testing
    runs-on: ubuntu-latest
    steps:
      - name: Git checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
      - name: Install Node
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4
        with:
          node-version: 18
          cache: 'npm'
      - name: Install dependencies
        run: npm ci

      - name: === Circular dependencies testing ===
        run: npm run test-circular-deps

  e2e:
    name: E2E testing
    runs-on: ${{ matrix.os }}
    timeout-minutes: 20
    strategy:
      fail-fast: false
      matrix:
        os: [ windows-latest ]
        CI: [ 0, 1, 2, 3 ]
    env:
      CI: ${{ matrix.CI }}
    steps:
      - name: Git checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
      - name: Install Node
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4
        with:
          node-version: 18
          cache: 'npm'
      - name: Install dependencies
        run: npm ci
      - name: Build
        run: npm run build-module

      - name: === E2E testing ===
        run: npm run test-e2e
      - name: Upload output screenshots
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4
        if: always()
        with:
          name: Output screenshots-${{ matrix.os }}-${{ matrix.CI }}
          path: test/e2e/output-screenshots
          if-no-files-found: ignore

  e2e-cov:
    name: Examples ready for release
    runs-on: ubuntu-latest
    steps:
      - name: Git checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
      - name: Install Node
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4
        with:
          node-version: 18
          cache: 'npm'
      - name: Install dependencies
        run: npm ci

      - name: === Examples ready for release ===
        run: npm run test-e2e-cov
",114,5,1,pull_request,11
mrdoob/three.js,codeql-code-scanning.yml,"name: ""CodeQL""

on:
  push:
    branches: [ ""dev"" ]
  pull_request:
    # The branches below must be a subset of the branches above
    branches: [ ""dev"" ]
  schedule:
    - cron: '29 23 * * 0'
  workflow_dispatch:

jobs:
  analyze:
    name: Analyze
    runs-on: ubuntu-latest
    permissions:
      actions: read
      contents: read
      security-events: write

    strategy:
      fail-fast: false
      matrix:
        language: [ 'javascript' ]

    steps:
    - name: Checkout repository
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4

    # Initializes the CodeQL tools for scanning.
    - name: Initialize CodeQL
      uses: github/codeql-action/init@ce28f5bb42b7a9f2c824e633a3f6ee835bab6858 # v3
      with:
        languages: ${{ matrix.language }}
        config-file: ./.github/codeql-config.yml
        queries: security-and-quality

    - name: Autobuild
      uses: github/codeql-action/autobuild@ce28f5bb42b7a9f2c824e633a3f6ee835bab6858 # v3

    - name: Perform CodeQL Analysis
      uses: github/codeql-action/analyze@ce28f5bb42b7a9f2c824e633a3f6ee835bab6858 # v3
      with:
        category: ""/language:${{matrix.language}}""
",45,1,4,"push, pull_request, schedule, workflow_dispatch",4
mrdoob/three.js,read-size.yml,"name: Read size

on:
  pull_request:
    paths:
      - 'src/**'
      - 'package.json'
      - 'utils/build/**'

# This workflow runs in a read-only environment. We can safely checkout
# the PR code here.
# Reference:
# https://securitylab.github.com/research/github-actions-preventing-pwn-requests/
permissions:
  contents: read

jobs:
  read-size:
    name: Tree-shaking
    runs-on: ubuntu-latest
    steps:
      - name: Git checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
      - name: Install Node
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4
        with:
          node-version: 20
          cache: 'npm'
      - name: Install dependencies
        run: npm ci
      - name: Build
        run: npm run build
      - name: === Test tree-shaking ===
        run: npm run test-treeshake
      - name: Read bundle sizes
        id: read-size
        run: |
          WEBGL_FILESIZE=$(stat --format=%s build/three.module.min.js)
          gzip -k build/three.module.min.js
          WEBGL_FILESIZE_GZIP=$(stat --format=%s build/three.module.min.js.gz)
          WEBGL_TREESHAKEN=$(stat --format=%s test/treeshake/index.bundle.min.js)
          gzip -k test/treeshake/index.bundle.min.js
          WEBGL_TREESHAKEN_GZIP=$(stat --format=%s test/treeshake/index.bundle.min.js.gz)

          WEBGPU_FILESIZE=$(stat --format=%s build/three.webgpu.min.js)
          gzip -k build/three.webgpu.min.js
          WEBGPU_FILESIZE_GZIP=$(stat --format=%s build/three.webgpu.min.js.gz)
          WEBGPU_TREESHAKEN=$(stat --format=%s test/treeshake/index.webgpu.bundle.min.js)
          gzip -k test/treeshake/index.webgpu.bundle.min.js
          WEBGPU_TREESHAKEN_GZIP=$(stat --format=%s test/treeshake/index.webgpu.bundle.min.js.gz)

          WEBGPU_NODES_FILESIZE=$(stat --format=%s build/three.webgpu.nodes.min.js)
          gzip -k build/three.webgpu.nodes.min.js
          WEBGPU_NODES_FILESIZE_GZIP=$(stat --format=%s build/three.webgpu.nodes.min.js.gz)
          WEBGPU_NODES_TREESHAKEN=$(stat --format=%s test/treeshake/index.webgpu.nodes.bundle.min.js)
          gzip -k test/treeshake/index.webgpu.nodes.bundle.min.js
          WEBGPU_NODES_TREESHAKEN_GZIP=$(stat --format=%s test/treeshake/index.webgpu.nodes.bundle.min.js.gz)

          PR=${{ github.event.pull_request.number }}

          # write the output in a json file to upload it as artifact
          node -pe ""JSON.stringify({ filesize: $WEBGL_FILESIZE, gzip: $WEBGL_FILESIZE_GZIP, treeshaken: $WEBGL_TREESHAKEN, treeshakenGzip: $WEBGL_TREESHAKEN_GZIP, filesize2: $WEBGPU_FILESIZE, gzip2: $WEBGPU_FILESIZE_GZIP, treeshaken2: $WEBGPU_TREESHAKEN, treeshakenGzip2: $WEBGPU_TREESHAKEN_GZIP, filesize3: $WEBGPU_NODES_FILESIZE, gzip3: $WEBGPU_NODES_FILESIZE_GZIP, treeshaken3: $WEBGPU_NODES_TREESHAKEN, treeshakenGzip3: $WEBGPU_NODES_TREESHAKEN_GZIP, pr: $PR })"" > sizes.json
      - name: Upload artifact
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4
        with:
          name: sizes
          path: sizes.json
",67,1,1,pull_request,3
mrdoob/three.js,report-size.yml,"name: Report size

on:
  workflow_run:
    workflows: [""Read size""]
    types:
      - completed

# This workflow needs to be run with ""pull-requests: write"" permissions to
# be able to comment on the pull request. We can't checkout the PR code
# in this workflow.
# Reference:
# https://securitylab.github.com/research/github-actions-preventing-pwn-requests/
permissions:
  pull-requests: write

jobs:
  report-size:
    name: Comment on PR
    runs-on: ubuntu-latest
    if: github.event.workflow_run.event == 'pull_request' &&
      github.event.workflow_run.conclusion == 'success'
    steps:
      - name: Log GitHub context
        env:
          GITHUB_CONTEXT: ${{ toJson(github) }}
        run: echo ""$GITHUB_CONTEXT""

      # Using actions/download-artifact doesn't work here
      # https://github.com/actions/download-artifact/issues/60
      - name: Download artifact
        uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea # v7
        id: download-artifact
        with:
          result-encoding: string
          script: |
            const fs = require('fs/promises');

            const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
               owner: context.repo.owner,
               repo: context.repo.repo,
               run_id: context.payload.workflow_run.id,
            });
            const matchArtifact = artifacts.data.artifacts.find((artifact) => artifact.name === 'sizes');
            const download = await github.rest.actions.downloadArtifact({
               owner: context.repo.owner,
               repo: context.repo.repo,
               artifact_id: matchArtifact.id,
               archive_format: 'zip',
            });

            await fs.writeFile('sizes.zip', Buffer.from(download.data));
            await exec.exec('unzip sizes.zip');
            const json = await fs.readFile('sizes.json', 'utf8');
            return json;

      # This runs on the base branch of the PR, meaning ""dev""
      - name: Git checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
      - name: Install Node
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4
        with:
          node-version: 20
          cache: 'npm'
      - name: Install dependencies
        run: npm ci
      - name: Build
        run: npm run build
      - name: === Test tree-shaking ===
        run: npm run test-treeshake
      - name: Read sizes
        id: read-size
        run: |
          WEBGL_FILESIZE_BASE=$(stat --format=%s build/three.module.min.js)
          gzip -k build/three.module.min.js
          WEBGL_FILESIZE_BASE_GZIP=$(stat --format=%s build/three.module.min.js.gz)
          WEBGL_TREESHAKEN_BASE=$(stat --format=%s test/treeshake/index.bundle.min.js)
          gzip -k test/treeshake/index.bundle.min.js
          WEBGL_TREESHAKEN_BASE_GZIP=$(stat --format=%s test/treeshake/index.bundle.min.js.gz)

          WEBGPU_FILESIZE_BASE=$(stat --format=%s build/three.webgpu.min.js)
          gzip -k build/three.webgpu.min.js
          WEBGPU_FILESIZE_BASE_GZIP=$(stat --format=%s build/three.webgpu.min.js.gz)
          WEBGPU_TREESHAKEN_BASE=$(stat --format=%s test/treeshake/index.webgpu.bundle.min.js)
          gzip -k test/treeshake/index.webgpu.bundle.min.js
          WEBGPU_TREESHAKEN_BASE_GZIP=$(stat --format=%s test/treeshake/index.webgpu.bundle.min.js.gz)

          WEBGPU_NODES_FILESIZE_BASE=$(stat --format=%s build/three.webgpu.nodes.min.js)
          gzip -k build/three.webgpu.nodes.min.js
          WEBGPU_NODES_FILESIZE_BASE_GZIP=$(stat --format=%s build/three.webgpu.nodes.min.js.gz)
          WEBGPU_NODES_TREESHAKEN_BASE=$(stat --format=%s test/treeshake/index.webgpu.nodes.bundle.min.js)
          gzip -k test/treeshake/index.webgpu.nodes.bundle.min.js
          WEBGPU_NODES_TREESHAKEN_BASE_GZIP=$(stat --format=%s test/treeshake/index.webgpu.nodes.bundle.min.js.gz)

          # log to console
          echo ""WEBGL_FILESIZE_BASE=$WEBGL_FILESIZE_BASE""
          echo ""WEBGL_FILESIZE_BASE_GZIP=$WEBGL_FILESIZE_BASE_GZIP""
          echo ""WEBGL_TREESHAKEN_BASE=$WEBGL_TREESHAKEN_BASE""
          echo ""WEBGL_TREESHAKEN_BASE_GZIP=$WEBGL_TREESHAKEN_BASE_GZIP""

          echo ""WEBGL_FILESIZE_BASE=$WEBGL_FILESIZE_BASE"" >> $GITHUB_OUTPUT
          echo ""WEBGL_FILESIZE_BASE_GZIP=$WEBGL_FILESIZE_BASE_GZIP"" >> $GITHUB_OUTPUT
          echo ""WEBGL_TREESHAKEN_BASE=$WEBGL_TREESHAKEN_BASE"" >> $GITHUB_OUTPUT
          echo ""WEBGL_TREESHAKEN_BASE_GZIP=$WEBGL_TREESHAKEN_BASE_GZIP"" >> $GITHUB_OUTPUT

          echo ""WEBGPU_FILESIZE_BASE=$WEBGPU_FILESIZE_BASE""
          echo ""WEBGPU_FILESIZE_BASE_GZIP=$WEBGPU_FILESIZE_BASE_GZIP""
          echo ""WEBGPU_TREESHAKEN_BASE=$WEBGPU_TREESHAKEN_BASE""
          echo ""WEBGPU_TREESHAKEN_BASE_GZIP=$WEBGPU_TREESHAKEN_BASE_GZIP""

          echo ""WEBGPU_FILESIZE_BASE=$WEBGPU_FILESIZE_BASE"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_FILESIZE_BASE_GZIP=$WEBGPU_FILESIZE_BASE_GZIP"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_TREESHAKEN_BASE=$WEBGPU_TREESHAKEN_BASE"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_TREESHAKEN_BASE_GZIP=$WEBGPU_TREESHAKEN_BASE_GZIP"" >> $GITHUB_OUTPUT

          echo ""WEBGPU_NODES_FILESIZE_BASE=$WEBGPU_NODES_FILESIZE_BASE""
          echo ""WEBGPU_NODES_FILESIZE_BASE_GZIP=$WEBGPU_NODES_FILESIZE_BASE_GZIP""
          echo ""WEBGPU_NODES_TREESHAKEN_BASE=$WEBGPU_NODES_TREESHAKEN_BASE""
          echo ""WEBGPU_NODES_TREESHAKEN_BASE_GZIP=$WEBGPU_NODES_TREESHAKEN_BASE_GZIP""

          echo ""WEBGPU_NODES_FILESIZE_BASE=$WEBGPU_NODES_FILESIZE_BASE"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_NODES_FILESIZE_BASE_GZIP=$WEBGPU_NODES_FILESIZE_BASE_GZIP"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_NODES_TREESHAKEN_BASE=$WEBGPU_NODES_TREESHAKEN_BASE"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_NODES_TREESHAKEN_BASE_GZIP=$WEBGPU_NODES_TREESHAKEN_BASE_GZIP"" >> $GITHUB_OUTPUT

      - name: Format sizes
        id: format
        # It's important these are passed as env variables.
        # https://securitylab.github.com/research/github-actions-untrusted-input/
        env:
          WEBGL_FILESIZE: ${{ fromJSON(steps.download-artifact.outputs.result).filesize }}
          WEBGL_FILESIZE_GZIP: ${{ fromJSON(steps.download-artifact.outputs.result).gzip }}
          WEBGL_FILESIZE_BASE: ${{ steps.read-size.outputs.WEBGL_FILESIZE_BASE }}
          WEBGL_FILESIZE_BASE_GZIP: ${{ steps.read-size.outputs.WEBGL_FILESIZE_BASE_GZIP }}
          WEBGL_TREESHAKEN: ${{ fromJSON(steps.download-artifact.outputs.result).treeshaken }}
          WEBGL_TREESHAKEN_GZIP: ${{ fromJSON(steps.download-artifact.outputs.result).treeshakenGzip }}
          WEBGL_TREESHAKEN_BASE: ${{ steps.read-size.outputs.WEBGL_TREESHAKEN_BASE }}
          WEBGL_TREESHAKEN_BASE_GZIP: ${{ steps.read-size.outputs.WEBGL_TREESHAKEN_BASE_GZIP }}
          WEBGPU_FILESIZE: ${{ fromJSON(steps.download-artifact.outputs.result).filesize2 }}
          WEBGPU_FILESIZE_GZIP: ${{ fromJSON(steps.download-artifact.outputs.result).gzip2 }}
          WEBGPU_FILESIZE_BASE: ${{ steps.read-size.outputs.WEBGPU_FILESIZE_BASE }}
          WEBGPU_FILESIZE_BASE_GZIP: ${{ steps.read-size.outputs.WEBGPU_FILESIZE_BASE_GZIP }}
          WEBGPU_TREESHAKEN: ${{ fromJSON(steps.download-artifact.outputs.result).treeshaken2 }}
          WEBGPU_TREESHAKEN_GZIP: ${{ fromJSON(steps.download-artifact.outputs.result).treeshakenGzip2 }}
          WEBGPU_TREESHAKEN_BASE: ${{ steps.read-size.outputs.WEBGPU_TREESHAKEN_BASE }}
          WEBGPU_TREESHAKEN_BASE_GZIP: ${{ steps.read-size.outputs.WEBGPU_TREESHAKEN_BASE_GZIP }}
          WEBGPU_NODES_FILESIZE: ${{ fromJSON(steps.download-artifact.outputs.result).filesize3 }}
          WEBGPU_NODES_FILESIZE_GZIP: ${{ fromJSON(steps.download-artifact.outputs.result).gzip3 }}
          WEBGPU_NODES_FILESIZE_BASE: ${{ steps.read-size.outputs.WEBGPU_NODES_FILESIZE_BASE }}
          WEBGPU_NODES_FILESIZE_BASE_GZIP: ${{ steps.read-size.outputs.WEBGPU_NODES_FILESIZE_BASE_GZIP }}
          WEBGPU_NODES_TREESHAKEN: ${{ fromJSON(steps.download-artifact.outputs.result).treeshaken3 }}
          WEBGPU_NODES_TREESHAKEN_GZIP: ${{ fromJSON(steps.download-artifact.outputs.result).treeshakenGzip3 }}
          WEBGPU_NODES_TREESHAKEN_BASE: ${{ steps.read-size.outputs.WEBGPU_NODES_TREESHAKEN_BASE }}
          WEBGPU_NODES_TREESHAKEN_BASE_GZIP: ${{ steps.read-size.outputs.WEBGPU_NODES_TREESHAKEN_BASE_GZIP }}
        run: |
          WEBGL_FILESIZE_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGL_FILESIZE"")
          WEBGL_FILESIZE_GZIP_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGL_FILESIZE_GZIP"")
          WEBGL_FILESIZE_BASE_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGL_FILESIZE_BASE"")
          WEBGL_FILESIZE_BASE_GZIP_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGL_FILESIZE_BASE_GZIP"")
          WEBGL_FILESIZE_DIFF=$(node ./test/treeshake/utils/format-diff.js ""$WEBGL_FILESIZE"" ""$WEBGL_FILESIZE_BASE"")
          WEBGL_FILESIZE_DIFF_GZIP=$(node ./test/treeshake/utils/format-diff.js ""$WEBGL_FILESIZE_GZIP"" ""$WEBGL_FILESIZE_BASE_GZIP"")

          WEBGL_TREESHAKEN_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGL_TREESHAKEN"")
          WEBGL_TREESHAKEN_GZIP_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGL_TREESHAKEN_GZIP"")
          WEBGL_TREESHAKEN_BASE_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGL_TREESHAKEN_BASE"")
          WEBGL_TREESHAKEN_BASE_GZIP_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGL_TREESHAKEN_BASE_GZIP"")
          WEBGL_TREESHAKEN_DIFF=$(node ./test/treeshake/utils/format-diff.js ""$WEBGL_TREESHAKEN"" ""$WEBGL_TREESHAKEN_BASE"")
          WEBGL_TREESHAKEN_DIFF_GZIP=$(node ./test/treeshake/utils/format-diff.js ""$WEBGL_TREESHAKEN_GZIP"" ""$WEBGL_TREESHAKEN_BASE_GZIP"")

          WEBGPU_FILESIZE_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGPU_FILESIZE"")
          WEBGPU_FILESIZE_GZIP_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGPU_FILESIZE_GZIP"")
          WEBGPU_FILESIZE_BASE_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGPU_FILESIZE_BASE"")
          WEBGPU_FILESIZE_BASE_GZIP_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGPU_FILESIZE_BASE_GZIP"")
          WEBGPU_FILESIZE_DIFF=$(node ./test/treeshake/utils/format-diff.js ""$WEBGPU_FILESIZE"" ""$WEBGPU_FILESIZE_BASE"")
          WEBGPU_FILESIZE_DIFF_GZIP=$(node ./test/treeshake/utils/format-diff.js ""$WEBGPU_FILESIZE_GZIP"" ""$WEBGPU_FILESIZE_BASE_GZIP"")

          WEBGPU_TREESHAKEN_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGPU_TREESHAKEN"")
          WEBGPU_TREESHAKEN_GZIP_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGPU_TREESHAKEN_GZIP"")
          WEBGPU_TREESHAKEN_BASE_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGPU_TREESHAKEN_BASE"")
          WEBGPU_TREESHAKEN_BASE_GZIP_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGPU_TREESHAKEN_BASE_GZIP"")
          WEBGPU_TREESHAKEN_DIFF=$(node ./test/treeshake/utils/format-diff.js ""$WEBGPU_TREESHAKEN"" ""$WEBGPU_TREESHAKEN_BASE"")
          WEBGPU_TREESHAKEN_DIFF_GZIP=$(node ./test/treeshake/utils/format-diff.js ""$WEBGPU_TREESHAKEN_GZIP"" ""$WEBGPU_TREESHAKEN_BASE_GZIP"")

          WEBGPU_NODES_FILESIZE_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGPU_NODES_FILESIZE"")
          WEBGPU_NODES_FILESIZE_GZIP_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGPU_NODES_FILESIZE_GZIP"")
          WEBGPU_NODES_FILESIZE_BASE_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGPU_NODES_FILESIZE_BASE"")
          WEBGPU_NODES_FILESIZE_BASE_GZIP_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGPU_NODES_FILESIZE_BASE_GZIP"")
          WEBGPU_NODES_FILESIZE_DIFF=$(node ./test/treeshake/utils/format-diff.js ""$WEBGPU_NODES_FILESIZE"" ""$WEBGPU_NODES_FILESIZE_BASE"")
          WEBGPU_NODES_FILESIZE_DIFF_GZIP=$(node ./test/treeshake/utils/format-diff.js ""$WEBGPU_NODES_FILESIZE_GZIP"" ""$WEBGPU_NODES_FILESIZE_BASE_GZIP"")

          WEBGPU_NODES_TREESHAKEN_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGPU_NODES_TREESHAKEN"")
          WEBGPU_NODES_TREESHAKEN_GZIP_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGPU_NODES_TREESHAKEN_GZIP"")
          WEBGPU_NODES_TREESHAKEN_BASE_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGPU_NODES_TREESHAKEN_BASE"")
          WEBGPU_NODES_TREESHAKEN_BASE_GZIP_FORM=$(node ./test/treeshake/utils/format-size.js ""$WEBGPU_NODES_TREESHAKEN_BASE_GZIP"")
          WEBGPU_NODES_TREESHAKEN_DIFF=$(node ./test/treeshake/utils/format-diff.js ""$WEBGPU_NODES_TREESHAKEN"" ""$WEBGPU_NODES_TREESHAKEN_BASE"")
          WEBGPU_NODES_TREESHAKEN_DIFF_GZIP=$(node ./test/treeshake/utils/format-diff.js ""$WEBGPU_NODES_TREESHAKEN_GZIP"" ""$WEBGPU_NODES_TREESHAKEN_BASE_GZIP"")

          echo ""WEBGL_FILESIZE=$WEBGL_FILESIZE_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGL_FILESIZE_GZIP=$WEBGL_FILESIZE_GZIP_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGL_FILESIZE_BASE=$WEBGL_FILESIZE_BASE_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGL_FILESIZE_BASE_GZIP=$WEBGL_FILESIZE_BASE_GZIP_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGL_FILESIZE_DIFF=$WEBGL_FILESIZE_DIFF"" >> $GITHUB_OUTPUT
          echo ""WEBGL_FILESIZE_DIFF_GZIP=$WEBGL_FILESIZE_DIFF_GZIP"" >> $GITHUB_OUTPUT

          echo ""WEBGL_TREESHAKEN=$WEBGL_TREESHAKEN_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGL_TREESHAKEN_GZIP=$WEBGL_TREESHAKEN_GZIP_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGL_TREESHAKEN_BASE=$WEBGL_TREESHAKEN_BASE_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGL_TREESHAKEN_BASE_GZIP=$WEBGL_TREESHAKEN_BASE_GZIP_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGL_TREESHAKEN_DIFF=$WEBGL_TREESHAKEN_DIFF"" >> $GITHUB_OUTPUT
          echo ""WEBGL_TREESHAKEN_DIFF_GZIP=$WEBGL_TREESHAKEN_DIFF_GZIP"" >> $GITHUB_OUTPUT

          echo ""WEBGPU_FILESIZE=$WEBGPU_FILESIZE_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_FILESIZE_GZIP=$WEBGPU_FILESIZE_GZIP_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_FILESIZE_BASE=$WEBGPU_FILESIZE_BASE_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_FILESIZE_BASE_GZIP=$WEBGPU_FILESIZE_BASE_GZIP_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_FILESIZE_DIFF=$WEBGPU_FILESIZE_DIFF"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_FILESIZE_DIFF_GZIP=$WEBGPU_FILESIZE_DIFF_GZIP"" >> $GITHUB_OUTPUT
          
          echo ""WEBGPU_TREESHAKEN=$WEBGPU_TREESHAKEN_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_TREESHAKEN_GZIP=$WEBGPU_TREESHAKEN_GZIP_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_TREESHAKEN_BASE=$WEBGPU_TREESHAKEN_BASE_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_TREESHAKEN_BASE_GZIP=$WEBGPU_TREESHAKEN_BASE_GZIP_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_TREESHAKEN_DIFF=$WEBGPU_TREESHAKEN_DIFF"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_TREESHAKEN_DIFF_GZIP=$WEBGPU_TREESHAKEN_DIFF_GZIP"" >> $GITHUB_OUTPUT

          echo ""WEBGPU_NODES_FILESIZE=$WEBGPU_NODES_FILESIZE_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_NODES_FILESIZE_GZIP=$WEBGPU_NODES_FILESIZE_GZIP_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_NODES_FILESIZE_BASE=$WEBGPU_NODES_FILESIZE_BASE_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_NODES_FILESIZE_BASE_GZIP=$WEBGPU_NODES_FILESIZE_BASE_GZIP_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_NODES_FILESIZE_DIFF=$WEBGPU_NODES_FILESIZE_DIFF"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_NODES_FILESIZE_DIFF_GZIP=$WEBGPU_NODES_FILESIZE_DIFF_GZIP"" >> $GITHUB_OUTPUT
          
          echo ""WEBGPU_NODES_TREESHAKEN=$WEBGPU_NODES_TREESHAKEN_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_NODES_TREESHAKEN_GZIP=$WEBGPU_NODES_TREESHAKEN_GZIP_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_NODES_TREESHAKEN_BASE=$WEBGPU_NODES_TREESHAKEN_BASE_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_NODES_TREESHAKEN_BASE_GZIP=$WEBGPU_NODES_TREESHAKEN_BASE_GZIP_FORM"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_NODES_TREESHAKEN_DIFF=$WEBGPU_NODES_TREESHAKEN_DIFF"" >> $GITHUB_OUTPUT
          echo ""WEBGPU_NODES_TREESHAKEN_DIFF_GZIP=$WEBGPU_NODES_TREESHAKEN_DIFF_GZIP"" >> $GITHUB_OUTPUT

      - name: Find existing comment
        uses: peter-evans/find-comment@3eae4d37986fb5a8592848f6a574fdf654e61f9e # v3
        id: find-comment
        with:
          issue-number: ${{ fromJSON(steps.download-artifact.outputs.result).pr }}
          comment-author: 'github-actions[bot]'
          body-includes: Bundle size
      - name: Comment on PR
        uses: peter-evans/create-or-update-comment@71345be0265236311c031f5c7866368bd1eff043 # v4
        with:
          issue-number: ${{ fromJSON(steps.download-artifact.outputs.result).pr }}
          comment-id: ${{ steps.find-comment.outputs.comment-id }}
          edit-mode: replace
          body: |
            ### 📦 Bundle size

            _Full ESM build, minified and gzipped._

            || Before | After | Diff |
            |:-:|:-:|:-:|:-:|
            | WebGL | ${{ steps.format.outputs.WEBGL_FILESIZE_BASE }} <br> **${{ steps.format.outputs.WEBGL_FILESIZE_BASE_GZIP }}** | ${{ steps.format.outputs.WEBGL_FILESIZE }} <br> **${{ steps.format.outputs.WEBGL_FILESIZE_GZIP }}** | ${{ steps.format.outputs.WEBGL_FILESIZE_DIFF }} <br> **${{ steps.format.outputs.WEBGL_FILESIZE_DIFF_GZIP }}** |
            | WebGPU | ${{ steps.format.outputs.WEBGPU_FILESIZE_BASE }} <br> **${{ steps.format.outputs.WEBGPU_FILESIZE_BASE_GZIP }}** | ${{ steps.format.outputs.WEBGPU_FILESIZE }} <br> **${{ steps.format.outputs.WEBGPU_FILESIZE_GZIP }}** | ${{ steps.format.outputs.WEBGPU_FILESIZE_DIFF }} <br> **${{ steps.format.outputs.WEBGPU_FILESIZE_DIFF_GZIP }}** |
            | WebGPU Nodes | ${{ steps.format.outputs.WEBGPU_NODES_FILESIZE_BASE }} <br> **${{ steps.format.outputs.WEBGPU_NODES_FILESIZE_BASE_GZIP }}** | ${{ steps.format.outputs.WEBGPU_NODES_FILESIZE }} <br> **${{ steps.format.outputs.WEBGPU_NODES_FILESIZE_GZIP }}** | ${{ steps.format.outputs.WEBGPU_NODES_FILESIZE_DIFF }} <br> **${{ steps.format.outputs.WEBGPU_NODES_FILESIZE_DIFF_GZIP }}** |

            ### 🌳 Bundle size after tree-shaking

            _Minimal build including a renderer, camera, empty scene, and dependencies._

            || Before | After | Diff |
            |:-:|:-:|:-:|:-:|
            | WebGL | ${{ steps.format.outputs.WEBGL_TREESHAKEN_BASE }} <br> **${{ steps.format.outputs.WEBGL_TREESHAKEN_BASE_GZIP }}** | ${{ steps.format.outputs.WEBGL_TREESHAKEN }} <br> **${{ steps.format.outputs.WEBGL_TREESHAKEN_GZIP }}** | ${{ steps.format.outputs.WEBGL_TREESHAKEN_DIFF }} <br> **${{ steps.format.outputs.WEBGL_TREESHAKEN_DIFF_GZIP }}** |
            | WebGPU | ${{ steps.format.outputs.WEBGPU_TREESHAKEN_BASE }} <br> **${{ steps.format.outputs.WEBGPU_TREESHAKEN_BASE_GZIP }}** | ${{ steps.format.outputs.WEBGPU_TREESHAKEN }} <br> **${{ steps.format.outputs.WEBGPU_TREESHAKEN_GZIP }}** | ${{ steps.format.outputs.WEBGPU_TREESHAKEN_DIFF }} <br> **${{ steps.format.outputs.WEBGPU_TREESHAKEN_DIFF_GZIP }}** |
            | WebGPU Nodes | ${{ steps.format.outputs.WEBGPU_NODES_TREESHAKEN_BASE }} <br> **${{ steps.format.outputs.WEBGPU_NODES_TREESHAKEN_BASE_GZIP }}** | ${{ steps.format.outputs.WEBGPU_NODES_TREESHAKEN }} <br> **${{ steps.format.outputs.WEBGPU_NODES_TREESHAKEN_GZIP }}** | ${{ steps.format.outputs.WEBGPU_NODES_TREESHAKEN_DIFF }} <br> **${{ steps.format.outputs.WEBGPU_NODES_TREESHAKEN_DIFF_GZIP }}** |
",272,1,1,workflow_run,5
langgenius/dify,api-tests.yml,"name: Run Pytest

on:
  pull_request:
    branches:
      - main
    paths:
      - api/**
      - docker/**
      - .github/workflows/api-tests.yml

concurrency:
  group: api-tests-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  test:
    name: API Tests
    runs-on: ubuntu-latest
    defaults:
      run:
        shell: bash
    strategy:
      matrix:
        python-version:
          - ""3.11""
          - ""3.12""

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Setup UV and Python
        uses: ./.github/actions/setup-uv
        with:
          python-version: ${{ matrix.python-version }}
          uv-lockfile: api/uv.lock

      - name: Check UV lockfile
        run: uv lock --project api --check

      - name: Install dependencies
        run: uv sync --project api --dev

      - name: Run Unit tests
        run: |
          uv run --project api bash dev/pytest/pytest_unit_tests.sh
          # Extract coverage percentage and create a summary
          TOTAL_COVERAGE=$(python -c 'import json; print(json.load(open(""coverage.json""))[""totals""][""percent_covered_display""])')

          # Create a detailed coverage summary
          echo ""### Test Coverage Summary :test_tube:"" >> $GITHUB_STEP_SUMMARY
          echo ""Total Coverage: ${TOTAL_COVERAGE}%"" >> $GITHUB_STEP_SUMMARY
          echo ""\`\`\`"" >> $GITHUB_STEP_SUMMARY
          uv run --project api coverage report >> $GITHUB_STEP_SUMMARY
          echo ""\`\`\`"" >> $GITHUB_STEP_SUMMARY

      - name: Run dify config tests
        run: uv run --project api dev/pytest/pytest_config_tests.py

      - name: MyPy Cache
        uses: actions/cache@v4
        with:
          path: api/.mypy_cache
          key: mypy-${{ matrix.python-version }}-${{ runner.os }}-${{ hashFiles('api/uv.lock') }}

      - name: Run MyPy Checks
        run: dev/mypy-check

      - name: Set up dotenvs
        run: |
          cp docker/.env.example docker/.env
          cp docker/middleware.env.example docker/middleware.env

      - name: Expose Service Ports
        run: sh .github/workflows/expose_service_ports.sh

      - name: Set up Sandbox
        uses: hoverkraft-tech/compose-action@v2.0.2
        with:
          compose-file: |
            docker/docker-compose.middleware.yaml
          services: |
            db
            redis
            sandbox
            ssrf_proxy

      - name: setup test config
        run: |
          cp api/tests/integration_tests/.env.example api/tests/integration_tests/.env

      - name: Run Workflow
        run: uv run --project api bash dev/pytest/pytest_workflow.sh

      - name: Run Tool
        run: uv run --project api bash dev/pytest/pytest_tools.sh
",99,1,1,pull_request,4
langgenius/dify,build-push.yml,"name: Build and Push API & Web

on:
  push:
    branches:
      - ""main""
      - ""deploy/dev""
      - ""deploy/enterprise""
    tags:
      - ""*""

concurrency:
  group: build-push-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

env:
  DOCKERHUB_USER: ${{ secrets.DOCKERHUB_USER }}
  DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
  DIFY_WEB_IMAGE_NAME: ${{ vars.DIFY_WEB_IMAGE_NAME || 'langgenius/dify-web' }}
  DIFY_API_IMAGE_NAME: ${{ vars.DIFY_API_IMAGE_NAME || 'langgenius/dify-api' }}

jobs:
  build:
    runs-on: ${{ matrix.platform == 'linux/arm64' && 'arm64_runner' || 'ubuntu-latest' }}
    if: github.repository == 'langgenius/dify'
    strategy:
      matrix:
        include:
          - service_name: ""build-api-amd64""
            image_name_env: ""DIFY_API_IMAGE_NAME""
            context: ""api""
            platform: linux/amd64
          - service_name: ""build-api-arm64""
            image_name_env: ""DIFY_API_IMAGE_NAME""
            context: ""api""
            platform: linux/arm64
          - service_name: ""build-web-amd64""
            image_name_env: ""DIFY_WEB_IMAGE_NAME""
            context: ""web""
            platform: linux/amd64
          - service_name: ""build-web-arm64""
            image_name_env: ""DIFY_WEB_IMAGE_NAME""
            context: ""web""
            platform: linux/arm64

    steps:
      - name: Prepare
        run: |
          platform=${{ matrix.platform }}
          echo ""PLATFORM_PAIR=${platform//\//-}"" >> $GITHUB_ENV

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ env.DOCKERHUB_USER }}
          password: ${{ env.DOCKERHUB_TOKEN }}

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Extract metadata for Docker
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env[matrix.image_name_env] }}

      - name: Build Docker image
        id: build
        uses: docker/build-push-action@v6
        with:
          context: ""{{defaultContext}}:${{ matrix.context }}""
          platforms: ${{ matrix.platform }}
          build-args: COMMIT_SHA=${{ fromJSON(steps.meta.outputs.json).labels['org.opencontainers.image.revision'] }}
          labels: ${{ steps.meta.outputs.labels }}
          outputs: type=image,name=${{ env[matrix.image_name_env] }},push-by-digest=true,name-canonical=true,push=true
          cache-from: type=gha,scope=${{ matrix.service_name }}
          cache-to: type=gha,mode=max,scope=${{ matrix.service_name }}

      - name: Export digest
        env:
          DIGEST: ${{ steps.build.outputs.digest }}
        run: |
          mkdir -p /tmp/digests
          sanitized_digest=${DIGEST#sha256:}
          touch ""/tmp/digests/${sanitized_digest}""

      - name: Upload digest
        uses: actions/upload-artifact@v4
        with:
          name: digests-${{ matrix.context }}-${{ env.PLATFORM_PAIR }}
          path: /tmp/digests/*
          if-no-files-found: error
          retention-days: 1

  create-manifest:
    needs: build
    runs-on: ubuntu-latest
    if: github.repository == 'langgenius/dify'
    strategy:
      matrix:
        include:
          - service_name: ""merge-api-images""
            image_name_env: ""DIFY_API_IMAGE_NAME""
            context: ""api""
          - service_name: ""merge-web-images""
            image_name_env: ""DIFY_WEB_IMAGE_NAME""
            context: ""web""
    steps:
      - name: Download digests
        uses: actions/download-artifact@v4
        with:
          path: /tmp/digests
          pattern: digests-${{ matrix.context }}-*
          merge-multiple: true

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ env.DOCKERHUB_USER }}
          password: ${{ env.DOCKERHUB_TOKEN }}

      - name: Extract metadata for Docker
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env[matrix.image_name_env] }}
          tags: |
            type=raw,value=latest,enable=${{ startsWith(github.ref, 'refs/tags/') && !contains(github.ref, '-') }}
            type=ref,event=branch
            type=sha,enable=true,priority=100,prefix=,suffix=,format=long
            type=raw,value=${{ github.ref_name }},enable=${{ startsWith(github.ref, 'refs/tags/') }}

      - name: Create manifest list and push
        working-directory: /tmp/digests
        env:
          IMAGE_NAME: ${{ env[matrix.image_name_env] }}
        run: |
          docker buildx imagetools create $(jq -cr '.tags | map(""-t "" + .) | join("" "")' <<< ""$DOCKER_METADATA_OUTPUT_JSON"") \
            $(printf ""$IMAGE_NAME@sha256:%s "" *)

      - name: Inspect image
        env:
          IMAGE_NAME: ${{ env[matrix.image_name_env] }}
          IMAGE_VERSION: ${{ steps.meta.outputs.version }}
        run: |
          docker buildx imagetools inspect ""$IMAGE_NAME:$IMAGE_VERSION""
",149,2,1,push,9
langgenius/dify,db-migration-test.yml,"name: DB Migration Test

on:
  pull_request:
    branches:
      - main
      - plugins/beta
    paths:
      - api/migrations/**
      - .github/workflows/db-migration-test.yml

concurrency:
  group: db-migration-test-${{ github.ref }}
  cancel-in-progress: true

jobs:
  db-migration-test:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: false

      - name: Setup UV and Python
        uses: ./.github/actions/setup-uv
        with:
          uv-lockfile: api/uv.lock

      - name: Install dependencies
        run: uv sync --project api

      - name: Prepare middleware env
        run: |
          cd docker
          cp middleware.env.example middleware.env

      - name: Set up Middlewares
        uses: hoverkraft-tech/compose-action@v2.0.2
        with:
          compose-file: |
            docker/docker-compose.middleware.yaml
          services: |
            db
            redis

      - name: Prepare configs
        run: |
          cd api
          cp .env.example .env

      - name: Run DB Migration
        env:
          DEBUG: true
        run: uv run --directory api flask upgrade-db
",57,1,1,pull_request,3
langgenius/dify,deploy-dev.yml,"name: Deploy Dev

on:
  workflow_run:
    workflows: [""Build and Push API & Web""]
    branches:
      - ""deploy/dev""
    types:
      - completed

jobs:
  deploy:
    runs-on: ubuntu-latest
    if: |
      github.event.workflow_run.conclusion == 'success'
    steps:
      - name: Deploy to server
        uses: appleboy/ssh-action@v0.1.8
        with:
          host: ${{ secrets.SSH_HOST }}
          username: ${{ secrets.SSH_USER }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          script: |
            ${{ vars.SSH_SCRIPT || secrets.SSH_SCRIPT }}
",24,1,1,workflow_run,1
langgenius/dify,deploy-enterprise.yml,"name: Deploy Enterprise

permissions:
  contents: read

on:
  workflow_run:
    workflows: [""Build and Push API & Web""]
    branches:
      - ""deploy/enterprise""
    types:
      - completed

jobs:
  deploy:
    runs-on: ubuntu-latest
    if: |
      github.event.workflow_run.conclusion == 'success' &&
      github.event.workflow_run.head_branch == 'deploy/enterprise'

    steps:
      - name: Deploy to server
        uses: appleboy/ssh-action@v0.1.8
        with:
          host: ${{ secrets.ENTERPRISE_SSH_HOST }}
          username: ${{ secrets.ENTERPRISE_SSH_USER }}
          password: ${{ secrets.ENTERPRISE_SSH_PASSWORD }}
          script: |
            ${{ vars.ENTERPRISE_SSH_SCRIPT || secrets.ENTERPRISE_SSH_SCRIPT }}
",29,1,1,workflow_run,1
langgenius/dify,deploy-rag-dev.yml,"name: Deploy RAG Dev

permissions:
  contents: read

on:
  workflow_run:
    workflows: [""Build and Push API & Web""]
    branches:
      - ""deploy/rag-dev""
    types:
      - completed

jobs:
  deploy:
    runs-on: ubuntu-latest
    if: |
      github.event.workflow_run.conclusion == 'success' &&
      github.event.workflow_run.head_branch == 'deploy/rag-dev'
    steps:
      - name: Deploy to server
        uses: appleboy/ssh-action@v0.1.8
        with:
          host: ${{ secrets.RAG_SSH_HOST }}
          username: ${{ secrets.SSH_USER }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          script: |
            ${{ vars.SSH_SCRIPT || secrets.SSH_SCRIPT }}
",28,1,1,workflow_run,1
langgenius/dify,docker-build.yml,"name: Build docker image

on:
  pull_request:
    branches:
      - ""main""
    paths:
      - api/Dockerfile
      - web/Dockerfile

concurrency:
  group: docker-build-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  build-docker:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        include:
          - service_name: ""api-amd64""
            platform: linux/amd64
            context: ""api""
          - service_name: ""api-arm64""
            platform: linux/arm64
            context: ""api""
          - service_name: ""web-amd64""
            platform: linux/amd64
            context: ""web""
          - service_name: ""web-arm64""
            platform: linux/arm64
            context: ""web""
    steps:
      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Docker Image
        uses: docker/build-push-action@v6
        with:
          push: false
          context: ""{{defaultContext}}:${{ matrix.context }}""
          file: ""${{ matrix.file }}""
          platforms: ${{ matrix.platform }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
",48,1,1,pull_request,3
langgenius/dify,stale.yml,"# This workflow warns and then closes issues and PRs that have had no activity for a specified amount of time.
#
# You can adjust the behavior by modifying this file.
# For more information, see:
# https://github.com/actions/stale
name: Mark stale issues and pull requests

on:
  schedule:
    - cron: '0 3 * * *'

jobs:
  stale:

    runs-on: ubuntu-latest
    permissions:
      issues: write
      pull-requests: write

    steps:
      - uses: actions/stale@v5
        with:
          days-before-issue-stale: 15
          days-before-issue-close: 3
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          stale-issue-message: ""Close due to it's no longer active, if you have any questions, you can reopen it.""
          stale-pr-message: ""Close due to it's no longer active, if you have any questions, you can reopen it.""
          stale-issue-label: 'no-issue-activity'
          stale-pr-label: 'no-pr-activity'
          any-of-labels: 'duplicate,question,invalid,wontfix,no-issue-activity,no-pr-activity,enhancement,cant-reproduce,help-wanted'
",30,1,1,schedule,1
langgenius/dify,style.yml,"name: Style check

on:
  pull_request:
    branches:
      - main

concurrency:
  group: style-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

permissions:
  checks: write
  statuses: write
  contents: read


jobs:
  python-style:
    name: Python Style
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Check changed files
        id: changed-files
        uses: tj-actions/changed-files@v45
        with:
          files: |
            api/**
            .github/workflows/style.yml

      - name: Setup UV and Python
        if: steps.changed-files.outputs.any_changed == 'true'
        uses: ./.github/actions/setup-uv
        with:
          uv-lockfile: api/uv.lock
          enable-cache: false

      - name: Install dependencies
        if: steps.changed-files.outputs.any_changed == 'true'
        run: uv sync --project api --dev

      - name: Ruff check
        if: steps.changed-files.outputs.any_changed == 'true'
        run: |
          uv run --directory api ruff --version
          uv run --directory api ruff check --diff ./
          uv run --directory api ruff format --check --diff ./

      - name: Dotenv check
        if: steps.changed-files.outputs.any_changed == 'true'
        run: uv run --project api dotenv-linter ./api/.env.example ./web/.env.example

      - name: Lint hints
        if: failure()
        run: echo ""Please run 'dev/reformat' to fix the fixable linting errors.""

  web-style:
    name: Web Style
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ./web

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Check changed files
        id: changed-files
        uses: tj-actions/changed-files@v45
        with:
          files: web/**

      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 10
          run_install: false

      - name: Setup NodeJS
        uses: actions/setup-node@v4
        if: steps.changed-files.outputs.any_changed == 'true'
        with:
          node-version: 22
          cache: pnpm
          cache-dependency-path: ./web/package.json

      - name: Web dependencies
        if: steps.changed-files.outputs.any_changed == 'true'
        run: pnpm install --frozen-lockfile

      - name: Web style check
        if: steps.changed-files.outputs.any_changed == 'true'
        run: pnpm run lint

  docker-compose-template:
    name: Docker Compose Template
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Check changed files
        id: changed-files
        uses: tj-actions/changed-files@v45
        with:
          files: |
            docker/generate_docker_compose
            docker/.env.example
            docker/docker-compose-template.yaml
            docker/docker-compose.yaml

      - name: Generate Docker Compose
        if: steps.changed-files.outputs.any_changed == 'true'
        run: |
          cd docker
          ./generate_docker_compose

      - name: Check for changes
        if: steps.changed-files.outputs.any_changed == 'true'
        run: git diff --exit-code

  superlinter:
    name: SuperLinter
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: false

      - name: Check changed files
        id: changed-files
        uses: tj-actions/changed-files@v45
        with:
          files: |
            **.sh
            **.yaml
            **.yml
            **Dockerfile
            dev/**

      - name: Super-linter
        uses: super-linter/super-linter/slim@v7
        if: steps.changed-files.outputs.any_changed == 'true'
        env:
          BASH_SEVERITY: warning
          DEFAULT_BRANCH: main
          FILTER_REGEX_INCLUDE: pnpm-lock.yaml
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          IGNORE_GENERATED_FILES: true
          IGNORE_GITIGNORED_FILES: true
          VALIDATE_BASH: true
          VALIDATE_BASH_EXEC: true
          # FIXME: temporarily disabled until api-docker.yaml's run script is fixed for shellcheck
          # VALIDATE_GITHUB_ACTIONS: true
          VALIDATE_DOCKERFILE_HADOLINT: true
          VALIDATE_XML: true
          VALIDATE_YAML: true

      - name: EditorConfig checks
        uses: super-linter/super-linter/slim@v7
        env:
          DEFAULT_BRANCH: main
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          IGNORE_GENERATED_FILES: true
          IGNORE_GITIGNORED_FILES: true
          # EditorConfig validation
          VALIDATE_EDITORCONFIG: true
          EDITORCONFIG_FILE_NAME: editorconfig-checker.json
",183,4,1,pull_request,14
langgenius/dify,tool-test-sdks.yaml,"name: Run Unit Test For SDKs

on:
  pull_request:
    branches:
      - main
    paths:
      - sdks/**

concurrency:
  group: sdk-tests-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  build:
    name: unit test for Node.js SDK
    runs-on: ubuntu-latest

    strategy:
      matrix:
        node-version: [16, 18, 20, 22]

    defaults:
      run:
        working-directory: sdks/nodejs-client

    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: ''
          cache-dependency-path: 'pnpm-lock.yaml'

      - name: Install Dependencies
        run: pnpm install --frozen-lockfile

      - name: Test
        run: pnpm test
",43,1,1,pull_request,2
langgenius/dify,translate-i18n-base-on-english.yml,"name: Check i18n Files and Create PR

on:
  pull_request:
    types: [closed]
    branches: [main]

jobs:
  check-and-update:
    if: github.event.pull_request.merged == true
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: web
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 2 # last 2 commits
          persist-credentials: false

      - name: Check for file changes in i18n/en-US
        id: check_files
        run: |
          recent_commit_sha=$(git rev-parse HEAD)
          second_recent_commit_sha=$(git rev-parse HEAD~1)
          changed_files=$(git diff --name-only $recent_commit_sha $second_recent_commit_sha -- 'i18n/en-US/*.ts')
          echo ""Changed files: $changed_files""
          if [ -n ""$changed_files"" ]; then
            echo ""FILES_CHANGED=true"" >> $GITHUB_ENV
          else
            echo ""FILES_CHANGED=false"" >> $GITHUB_ENV
          fi

      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 10
          run_install: false

      - name: Set up Node.js
        if: env.FILES_CHANGED == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: 'lts/*'
          cache: pnpm
          cache-dependency-path: ./web/package.json

      - name: Install dependencies
        if: env.FILES_CHANGED == 'true'
        run: pnpm install --frozen-lockfile

      - name: Run npm script
        if: env.FILES_CHANGED == 'true'
        run: pnpm run auto-gen-i18n

      - name: Create Pull Request
        if: env.FILES_CHANGED == 'true'
        uses: peter-evans/create-pull-request@v6
        with:
          commit-message: Update i18n files based on en-US changes
          title: 'chore: translate i18n files'
          body: This PR was automatically created to update i18n files based on changes in en-US locale.
          branch: chore/automated-i18n-updates
",63,1,1,pull_request,4
langgenius/dify,vdb-tests.yml,"name: Run VDB Tests

on:
  pull_request:
    branches:
      - main
    paths:
      - api/core/rag/datasource/**
      - docker/**
      - .github/workflows/vdb-tests.yml
      - api/uv.lock
      - api/pyproject.toml

concurrency:
  group: vdb-tests-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  test:
    name: VDB Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version:
          - ""3.11""
          - ""3.12""

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Free Disk Space
        uses: endersonmenezes/free-disk-space@v2
        with:
          remove_dotnet: true
          remove_haskell: true
          remove_tool_cache: true

      - name: Setup UV and Python
        uses: ./.github/actions/setup-uv
        with:
          python-version: ${{ matrix.python-version }}
          uv-lockfile: api/uv.lock

      - name: Check UV lockfile
        run: uv lock --project api --check

      - name: Install dependencies
        run: uv sync --project api --dev

      - name: Set up dotenvs
        run: |
          cp docker/.env.example docker/.env
          cp docker/middleware.env.example docker/middleware.env

      - name: Expose Service Ports
        run: sh .github/workflows/expose_service_ports.sh

      - name: Set up Vector Store (TiDB)
        uses: hoverkraft-tech/compose-action@v2.0.2
        with:
          compose-file: docker/tidb/docker-compose.yaml
          services: |
            tidb
            tiflash

      - name: Set up Vector Stores (Weaviate, Qdrant, PGVector, Milvus, PgVecto-RS, Chroma, MyScale, ElasticSearch, Couchbase, OceanBase)
        uses: hoverkraft-tech/compose-action@v2.0.2
        with:
          compose-file: |
            docker/docker-compose.yaml
          services: |
            weaviate
            qdrant
            couchbase-server
            etcd
            minio
            milvus-standalone
            pgvecto-rs
            pgvector
            chroma
            elasticsearch
            oceanbase

      - name: setup test config
        run: |
          echo $(pwd)
          ls -lah .
          cp api/tests/integration_tests/.env.example api/tests/integration_tests/.env

      - name: Check VDB Ready (TiDB)
        run: uv run --project api python api/tests/integration_tests/vdb/tidb_vector/check_tiflash_ready.py

      - name: Test Vector Stores
        run: uv run --project api bash dev/pytest/pytest_vdb.sh
",97,1,1,pull_request,5
langgenius/dify,web-tests.yml,"name: Web Tests

on:
  pull_request:
    branches:
      - main
    paths:
      - web/**

concurrency:
  group: web-tests-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  test:
    name: Web Tests
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ./web

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Check changed files
        id: changed-files
        uses: tj-actions/changed-files@v45
        with:
          files: web/**

      - name: Install pnpm
        if: steps.changed-files.outputs.any_changed == 'true'
        uses: pnpm/action-setup@v4
        with:
          version: 10
          run_install: false

      - name: Setup Node.js
        uses: actions/setup-node@v4
        if: steps.changed-files.outputs.any_changed == 'true'
        with:
          node-version: 22
          cache: pnpm
          cache-dependency-path: ./web/package.json

      - name: Install dependencies
        if: steps.changed-files.outputs.any_changed == 'true'
        run: pnpm install --frozen-lockfile

      - name: Run tests
        if: steps.changed-files.outputs.any_changed == 'true'
        run: pnpm test
",55,1,1,pull_request,4
rust-lang/rust,ci.yml,"# This file defines our primary CI workflow that runs on pull requests
# and also on pushes to special branches (auto, try).
#
# The actual definition of the executed jobs is calculated by the
# `src/ci/citool` crate, which
# uses job definition data from src/ci/github-actions/jobs.yml.
# You should primarily modify the `jobs.yml` file if you want to modify
# what jobs are executed in CI.

name: CI
on:
  push:
    branches:
      # CI on master only serves for caching citool builds for the `calculate_matrix` job.
      # In order to use GHA cache on PR CI (and auto/try) jobs, we need to write to it
      # from the default branch.
      - master
      - auto
      - try
      - try-perf
      - automation/bors/try
  pull_request:
    branches:
      - ""**""

permissions:
  contents: read
  packages: write

defaults:
  run:
    # On Linux, macOS, and Windows, use the system-provided bash as the default
    # shell. (This should only make a difference on Windows, where the default
    # shell is PowerShell.)
    shell: bash

concurrency:
  # For a given workflow, if we push to the same branch, cancel all previous builds on that branch.
  # We add an exception for try builds (try branch) and unrolled rollup builds (try-perf), which
  # are all triggered on the same branch, but which should be able to run concurrently.
  group: ${{ github.workflow }}-${{ ((github.ref == 'refs/heads/try' || github.ref == 'refs/heads/try-perf' || github.ref == 'refs/heads/automation/bors/try') && github.sha) || github.ref }}
  cancel-in-progress: true
env:
  TOOLSTATE_REPO: ""https://github.com/rust-lang-nursery/rust-toolstate""
  # This will be empty in PR jobs.
  TOOLSTATE_REPO_ACCESS_TOKEN: ${{ secrets.TOOLSTATE_REPO_ACCESS_TOKEN }}
jobs:
  # The job matrix for `calculate_matrix` is defined in src/ci/github-actions/jobs.yml.
  # It calculates which jobs should be executed, based on the data of the ${{ github }} context.
  # If you want to modify CI jobs, take a look at src/ci/github-actions/jobs.yml.
  calculate_matrix:
    name: Calculate job matrix
    runs-on: ubuntu-24.04-arm
    outputs:
      jobs: ${{ steps.jobs.outputs.jobs }}
      run_type: ${{ steps.jobs.outputs.run_type }}
    steps:
      - name: Checkout the source code
        uses: actions/checkout@v4
      # Cache citool to make its build faster, as it's in the critical path.
      # The rust-cache doesn't bleed into the main `job`, so it should not affect any other
      # Rust compilation.
      - name: Cache citool
        uses: Swatinem/rust-cache@9d47c6ad4b02e050fd481d890b2ea34778fd09d6 # v2.7.8
        with:
          workspaces: src/ci/citool
      - name: Test citool
        # Only test citool on the auto branch, to reduce latency of the calculate matrix job
        # on PR/try builds.
        if: ${{ github.ref == 'refs/heads/auto' }}
        run: |
          cd src/ci/citool
          CARGO_INCREMENTAL=0 cargo test
      - name: Calculate the CI job matrix
        env:
          COMMIT_MESSAGE: ${{ github.event.head_commit.message }}
        run: |
          cd src/ci/citool
          CARGO_INCREMENTAL=0 cargo run calculate-job-matrix >> $GITHUB_OUTPUT
        id: jobs
  job:
    name: ${{ matrix.full_name }}
    needs: [ calculate_matrix ]
    runs-on: ""${{ matrix.os }}""
    timeout-minutes: 360
    # The bors environment contains secrets required for elevated workflows (try and auto builds),
    # which need to access e.g. S3 and upload artifacts. We want to provide access to that
    # environment only on the try/auto branches, which are only accessible to bors.
    # This also ensures that PR CI (which doesn't get write access to S3) works, as it cannot
    # access the environment.
    #
    # We only enable the environment for the rust-lang/rust repository, so that CI works on forks.
    environment: ${{ ((github.repository == 'rust-lang/rust' && (github.ref == 'refs/heads/try' || github.ref == 'refs/heads/try-perf' || github.ref == 'refs/heads/automation/bors/try' || github.ref == 'refs/heads/auto')) && 'bors') || '' }}
    env:
      CI_JOB_NAME: ${{ matrix.name }}
      CI_JOB_DOC_URL: ${{ matrix.doc_url }}
      GITHUB_WORKFLOW_RUN_ID: ${{ github.run_id }}
      GITHUB_REPOSITORY: ${{ github.repository }}
      CARGO_REGISTRIES_CRATES_IO_PROTOCOL: sparse
      # commit of PR sha or commit sha. `GITHUB_SHA` is not accurate for PRs.
      HEAD_SHA: ${{ github.event.pull_request.head.sha || github.sha }}
      DOCKER_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      SCCACHE_BUCKET: rust-lang-ci-sccache2
      SCCACHE_REGION: us-west-1
      CACHE_DOMAIN: ci-caches.rust-lang.org
    continue-on-error: ${{ matrix.continue_on_error || false }}
    strategy:
      matrix:
        # Check the `calculate_matrix` job to see how is the matrix defined.
        include: ${{ fromJSON(needs.calculate_matrix.outputs.jobs) }}
    steps:
      - name: Install cargo in AWS CodeBuild
        if: matrix.codebuild
        run: |
          # Check if cargo is installed
          if ! command -v cargo &> /dev/null; then
            echo ""Cargo not found, installing Rust...""
            curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --profile=minimal
            # Make cargo available in PATH
            echo ""$HOME/.cargo/bin"" >> $GITHUB_PATH
          fi

      - name: disable git crlf conversion
        run: git config --global core.autocrlf false

      - name: checkout the source code
        uses: actions/checkout@v4
        with:
          fetch-depth: 2

      # Free up disk space on Linux by removing preinstalled components that
      # we do not need. We do this to enable some of the less resource
      # intensive jobs to run on free runners, which however also have
      # less disk space.
      - name: free up disk space
        run: src/ci/scripts/free-disk-space.sh
        if: matrix.free_disk

      # Rust Log Analyzer can't currently detect the PR number of a GitHub
      # Actions build on its own, so a hint in the log message is needed to
      # point it in the right direction.
      - name: configure the PR in which the error message will be posted
        run: echo ""[CI_PR_NUMBER=$num]""
        env:
          num: ${{ github.event.number }}
        if: needs.calculate_matrix.outputs.run_type == 'pr'

      - name: add extra environment variables
        run: src/ci/scripts/setup-environment.sh
        env:
          # Since it's not possible to merge `${{ matrix.env }}` with the other
          # variables in `job.<name>.env`, the variables defined in the matrix
          # are passed to the `setup-environment.sh` script encoded in JSON,
          # which then uses log commands to actually set them.
          EXTRA_VARIABLES: ${{ toJson(matrix.env) }}

      - name: ensure the channel matches the target branch
        run: src/ci/scripts/verify-channel.sh

      - name: collect CPU statistics
        run: src/ci/scripts/collect-cpu-stats.sh

      - name: show the current environment
        run: src/ci/scripts/dump-environment.sh

      - name: install awscli
        run: src/ci/scripts/install-awscli.sh

      - name: install sccache
        run: src/ci/scripts/install-sccache.sh

      - name: select Xcode
        run: src/ci/scripts/select-xcode.sh

      - name: install clang
        run: src/ci/scripts/install-clang.sh

      - name: install tidy
        run: src/ci/scripts/install-tidy.sh

      - name: install WIX
        run: src/ci/scripts/install-wix.sh

      - name: disable git crlf conversion
        run: src/ci/scripts/disable-git-crlf-conversion.sh

      - name: checkout submodules
        run: src/ci/scripts/checkout-submodules.sh

      - name: install MinGW
        run: src/ci/scripts/install-mingw.sh

      - name: install ninja
        run: src/ci/scripts/install-ninja.sh

      - name: enable ipv6 on Docker
        # Don't run on codebuild because systemctl is not available
        if: ${{ !matrix.codebuild }}
        run: src/ci/scripts/enable-docker-ipv6.sh

      # Disable automatic line ending conversion (again). On Windows, when we're
      # installing dependencies, something switches the git configuration directory or
      # re-enables autocrlf. We've not tracked down the exact cause -- and there may
      # be multiple -- but this should ensure submodules are checked out with the
      # appropriate line endings.
      - name: disable git crlf conversion
        run: src/ci/scripts/disable-git-crlf-conversion.sh

      - name: ensure line endings are correct
        run: src/ci/scripts/verify-line-endings.sh

      - name: ensure backported commits are in upstream branches
        run: src/ci/scripts/verify-backported-commits.sh

      - name: ensure the stable version number is correct
        run: src/ci/scripts/verify-stable-version-number.sh

      # Show the environment just before we run the build
      # This makes it easier to diagnose problems with the above install scripts.
      - name: show the current environment
        run: src/ci/scripts/dump-environment.sh

      # Pre-build citool before the following step uninstalls rustup
      # Build it into the build directory, to avoid modifying sources
      - name: build citool
        run: |
          cd src/ci/citool
          CARGO_INCREMENTAL=0 CARGO_TARGET_DIR=../../../build/citool cargo build

      - name: run the build
        run: |
          set +e
          # Redirect stderr to stdout to avoid reordering the two streams in the GHA logs.
          src/ci/scripts/run-build-from-ci.sh 2>&1
          STATUS=$?
          set -e

          if [[ ""$STATUS"" -ne 0 && -n ""$CI_JOB_DOC_URL"" ]]; then
            echo ""****************************************************************************""
            echo ""To find more information about this job, visit the following URL:""
            echo ""$CI_JOB_DOC_URL""
            echo ""****************************************************************************""
          fi
          exit ${STATUS}
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.CACHES_AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.CACHES_AWS_SECRET_ACCESS_KEY }}

      - name: create github artifacts
        run: src/ci/scripts/create-doc-artifacts.sh

      - name: print disk usage
        run: |
          echo ""disk usage:""
          df -h

      - name: upload artifacts to github
        uses: actions/upload-artifact@v4
        with:
          # name is set in previous step
          name: ${{ env.DOC_ARTIFACT_NAME }}
          path: obj/artifacts/doc
          if-no-files-found: ignore
          retention-days: 5

      - name: upload artifacts to S3
        run: src/ci/scripts/upload-artifacts.sh
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.ARTIFACTS_AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.ARTIFACTS_AWS_SECRET_ACCESS_KEY }}
        # Adding a condition on DEPLOY=1 or DEPLOY_ALT=1 is not needed as all deploy
        # builders *should* have the AWS credentials available. Still, explicitly
        # adding the condition is helpful as this way CI will not silently skip
        # deploying artifacts from a dist builder if the variables are misconfigured,
        # erroring about invalid credentials instead.
        if: github.event_name == 'push' || env.DEPLOY == '1' || env.DEPLOY_ALT == '1'

      - name: postprocess metrics into the summary
        # This step is not critical, and if some I/O problem happens, we don't want
        # to cancel the build.
        continue-on-error: true
        run: |
          if [ -f build/metrics.json ]; then
            METRICS=build/metrics.json
          elif [ -f obj/build/metrics.json ]; then
            METRICS=obj/build/metrics.json
          else
            echo ""No metrics.json found""
            exit 0
          fi

          # Get closest bors merge commit
          PARENT_COMMIT=`git rev-list --author='bors <bors@rust-lang.org>' -n1 --first-parent HEAD^1`

          ./build/citool/debug/citool postprocess-metrics \
              --job-name ${CI_JOB_NAME} \
              --parent ${PARENT_COMMIT} \
              ${METRICS} >> ${GITHUB_STEP_SUMMARY}

      - name: upload job metrics to DataDog
        # This step is not critical, and if some I/O problem happens, we don't want
        # to cancel the build.
        continue-on-error: true
        if: needs.calculate_matrix.outputs.run_type != 'pr'
        env:
          DATADOG_API_KEY: ${{ secrets.DATADOG_API_KEY }}
          DD_GITHUB_JOB_NAME: ${{ matrix.full_name }}
        run: ./build/citool/debug/citool upload-build-metrics build/cpu-usage.csv

  # This job isused to tell bors the final status of the build, as there is no practical way to detect
  # when a workflow is successful listening to webhooks only in our current bors implementation (homu).
  outcome:
    name: bors build finished
    runs-on: ubuntu-24.04
    needs: [ calculate_matrix, job ]
    # !cancelled() executes the job regardless of whether the previous jobs passed or failed
    if: ${{ !cancelled() && contains(fromJSON('[""auto"", ""try""]'), needs.calculate_matrix.outputs.run_type) }}
    steps:
      - name: checkout the source code
        uses: actions/checkout@v4
        with:
          fetch-depth: 2
      # Calculate the exit status of the whole CI workflow.
      # If all dependent jobs were successful, this exits with 0 (and the outcome job continues successfully).
      # If a some dependent job has failed, this exits with 1.
      - name: calculate the correct exit status
        run: jq --exit-status 'all(.result == ""success"" or .result == ""skipped"")' <<< '${{ toJson(needs) }}'
      # Publish the toolstate if an auto build succeeds (just before push to master)
      - name: publish toolstate
        run: src/ci/publish_toolstate.sh
        shell: bash
        if: needs.calculate_matrix.outputs.run_type == 'auto'
        env:
          TOOLSTATE_ISSUES_API_URL: https://api.github.com/repos/rust-lang/rust/issues
          TOOLSTATE_PUBLISH: 1
",335,3,2,"push, pull_request",5
rust-lang/rust,dependencies.yml,"# Automatically run `cargo update` periodically

---
name: Bump dependencies in Cargo.lock
on:
  schedule:
    # Run weekly
    - cron: '0 0 * * Sun'
  workflow_dispatch:
    # Needed so we can run it manually
permissions:
  contents: read
defaults:
  run:
    shell: bash
env:
  # So cargo doesn't complain about unstable features
  RUSTC_BOOTSTRAP: 1
  PR_TITLE: Weekly `cargo update`
  PR_MESSAGE: |
    Automation to keep dependencies in `Cargo.lock` current.

    The following is the output from `cargo update`:
  COMMIT_MESSAGE: ""cargo update \n\n""

jobs:
  not-waiting-on-bors:
    if: github.repository_owner == 'rust-lang'
    name: skip if S-waiting-on-bors
    runs-on: ubuntu-24.04
    steps:
      - env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Fetch state and labels of PR
          # Or exit successfully if PR does not exist
          JSON=$(gh pr view cargo_update --repo $GITHUB_REPOSITORY --json labels,state || exit 0)
          STATE=$(echo ""$JSON"" | jq -r '.state')
          WAITING_ON_BORS=$(echo ""$JSON"" | jq '.labels[] | any(.name == ""S-waiting-on-bors""; .)')

          # Exit with error if open and S-waiting-on-bors
          if [[ ""$STATE"" == ""OPEN"" && ""$WAITING_ON_BORS"" == ""true"" ]]; then
            exit 1
          fi

  update:
    if: github.repository_owner == 'rust-lang'
    name: update dependencies
    needs: not-waiting-on-bors
    runs-on: ubuntu-24.04
    steps:
      - name: checkout the source code
        uses: actions/checkout@v4
        with:
          submodules: recursive
      - name: install the bootstrap toolchain
        run: |
          # Extract the stage0 version
          TOOLCHAIN=$(awk -F= '{a[$1]=$2} END {print(a[""compiler_version""] ""-"" a[""compiler_date""])}' src/stage0)
          # Install and set as default
          rustup toolchain install --no-self-update --profile minimal $TOOLCHAIN
          rustup default $TOOLCHAIN

      - name: cargo update compiler & tools
        # Remove first line that always just says ""Updating crates.io index""
        run: |
          echo -e ""\ncompiler & tools dependencies:"" >> cargo_update.log
          cargo update 2>&1 | sed '/crates.io index/d' | tee -a cargo_update.log
      - name: cargo update library
        run: |
          echo -e ""\nlibrary dependencies:"" >> cargo_update.log
          cargo update --manifest-path library/Cargo.toml 2>&1 | sed '/crates.io index/d' | tee -a cargo_update.log
      - name: cargo update rustbook
        run: |
          echo -e ""\nrustbook dependencies:"" >> cargo_update.log
          cargo update --manifest-path src/tools/rustbook/Cargo.toml 2>&1 | sed '/crates.io index/d' | tee -a cargo_update.log
      - name: upload Cargo.lock artifact for use in PR
        uses: actions/upload-artifact@v4
        with:
          name: Cargo-lock
          path: |
            Cargo.lock
            library/Cargo.lock
            src/tools/rustbook/Cargo.lock
          retention-days: 1
      - name: upload cargo-update log artifact for use in PR
        uses: actions/upload-artifact@v4
        with:
          name: cargo-updates
          path: cargo_update.log
          retention-days: 1

  pr:
    if: github.repository_owner == 'rust-lang'
    name: amend PR
    needs: update
    runs-on: ubuntu-24.04
    permissions:
      contents: write
      pull-requests: write
    steps:
      - name: checkout the source code
        uses: actions/checkout@v4

      - name: download Cargo.lock from update job
        uses: actions/download-artifact@v4
        with:
          name: Cargo-lock
      - name: download cargo-update log from update job
        uses: actions/download-artifact@v4
        with:
          name: cargo-updates

      - name: craft PR body and commit message
        run: |
          echo ""${COMMIT_MESSAGE}"" > commit.txt
          cat cargo_update.log >> commit.txt

          echo ""${PR_MESSAGE}"" > body.md
          echo '```txt' >> body.md
          cat cargo_update.log >> body.md
          echo '```' >> body.md

      - name: commit
        run: |
          git config user.name github-actions
          git config user.email github-actions@github.com
          git switch --force-create cargo_update
          git add ./Cargo.lock ./library/Cargo.lock ./src/tools/rustbook/Cargo.lock
          git commit --no-verify --file=commit.txt

      - name: push
        run: git push --no-verify --force --set-upstream origin cargo_update

      - name: edit existing open pull request
        id: edit
        # Don't fail job if we need to open new PR
        continue-on-error: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Exit with error if PR is closed
          STATE=$(gh pr view cargo_update --repo $GITHUB_REPOSITORY --json state --jq '.state')
          if [[ ""$STATE"" != ""OPEN"" ]]; then
            exit 1
          fi

          gh pr edit cargo_update --title ""${PR_TITLE}"" --body-file body.md --repo $GITHUB_REPOSITORY

      - name: open new pull request
        # Only run if there wasn't an existing PR
        if: steps.edit.outcome != 'success'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: gh pr create --title ""${PR_TITLE}"" --body-file body.md --repo $GITHUB_REPOSITORY
",155,3,2,"schedule, workflow_dispatch",6
rust-lang/rust,ghcr.yml,"# Mirror DockerHub images used by the Rust project to ghcr.io.
# Images are available at https://github.com/orgs/rust-lang/packages.
#
# In some CI jobs, we pull images from ghcr.io instead of Docker Hub because
# Docker Hub has a rate limit, while ghcr.io doesn't.
# Those images are pushed to ghcr.io by this job.
#
# While Docker Hub rate limit *shouldn't* be an issue on GitHub Actions,
# it certainly is for AWS codebuild.
#
# Note that authenticating to DockerHub or other registries isn't possible
# for PR jobs, because forks can't access secrets.
# That's why we use ghcr.io: it has no rate limit and it doesn't require authentication.

name: GHCR image mirroring

on:
  workflow_dispatch:
  schedule:
    # Run daily at midnight UTC
    - cron: '0 0 * * *'

jobs:
  mirror:
    name: DockerHub mirror
    runs-on: ubuntu-24.04
    if: github.repository == 'rust-lang/rust'
    permissions:
      # Needed to write to the ghcr.io registry
      packages: write
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Log in to registry
        run: echo ""${{ secrets.GITHUB_TOKEN }}"" | docker login ghcr.io -u ${{ github.repository_owner }} --password-stdin

      # Download crane in the current directory.
      # We use crane because it copies the docker image for all the architectures available in
      # DockerHub for the image.
      # Learn more about crane at
      # https://github.com/google/go-containerregistry/blob/main/cmd/crane/README.md
      - name: Download crane
        run: |
          curl -sL ""https://github.com/google/go-containerregistry/releases/download/${VERSION}/go-containerregistry_${OS}_${ARCH}.tar.gz"" | tar -xzf -
        env:
          VERSION: v0.20.2
          OS: Linux
          ARCH: x86_64

      - name: Mirror DockerHub
        run: |
          # List of DockerHub images to mirror to ghcr.io
          images=(
            # Mirrored because used by the mingw-check-tidy, which doesn't cache Docker images
            ""ubuntu:22.04""
            # Mirrored because used by all linux CI jobs, including mingw-check-tidy
            ""moby/buildkit:buildx-stable-1""
            # Mirrored because used when CI is running inside a Docker container
            ""alpine:3.4""
            # Mirrored because used by dist-x86_64-linux
            ""centos:7""
          )

          # Mirror each image from DockerHub to ghcr.io
          for img in ""${images[@]}""; do
            echo ""Mirroring ${img}...""
            # Remove namespace from the image if any.
            # E.g. ""moby/buildkit:buildx-stable-1"" becomes ""buildkit:buildx-stable-1""
            dest_image=$(echo ""${img}"" | cut -d'/' -f2-)
            ./crane copy \
              ""docker.io/${img}"" \
              ""ghcr.io/${{ github.repository_owner }}/${dest_image}""
          done
",75,1,2,"workflow_dispatch, schedule",1
rust-lang/rust,post-merge.yml,"# Workflow that runs after a merge to master, analyses changes in test executions
# and posts the result to the merged PR.

name: Post merge analysis

on:
  push:
    branches:
      - master

jobs:
  analysis:
    runs-on: ubuntu-24.04
    if: github.repository == 'rust-lang/rust'
    permissions:
      pull-requests: write
    steps:
      - uses: actions/checkout@v4
        with:
          # Make sure that we have enough commits to find the parent merge commit.
          # Since all merges should be through merge commits, fetching two commits
          # should be enough to get the parent bors merge commit.
          fetch-depth: 2
      - name: Perform analysis and send PR
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          # Give GitHub some time to propagate the information that the PR was merged
          sleep 60

          # Get closest bors merge commit
          PARENT_COMMIT=`git rev-list --author='bors <bors@rust-lang.org>' -n1 --first-parent HEAD^1`
          echo ""Parent: ${PARENT_COMMIT}""

          # Find PR for the current commit
          HEAD_PR=`gh pr list --search ""${{ github.sha }}"" --state merged --json number --jq '.[0].number'`
          if [ -z ""${HEAD_PR}"" ]; then
            echo ""PR for commit SHA ${{ github.sha }} not found, exiting""
            exit 1
          fi
          echo ""HEAD: ${{ github.sha }} (#${HEAD_PR})""

          cd src/ci/citool

          printf ""<details>\n<summary>What is this?</summary>\n"" >> output.log
          printf ""This is an experimental post-merge analysis report that shows differences in test outcomes between the merged PR and its parent PR.\n"" >> output.log
          printf ""</details>\n\n"" >> output.log

          cargo run --release post-merge-report ${PARENT_COMMIT} ${{ github.sha }} >> output.log

          cat output.log

          gh pr comment ${HEAD_PR} -F output.log
",53,1,1,push,1
denoland/deno,cargo_publish.yml,"name: cargo_publish

on: workflow_dispatch

# Ensures only one publish is running at a time
concurrency:
  group: ${{ github.workflow }}
  cancel-in-progress: true

jobs:
  build:
    name: cargo publish
    runs-on: ubuntu-24.04-xl
    timeout-minutes: 90

    env:
      CARGO_TERM_COLOR: always
      RUST_BACKTRACE: full
      RUSTC_FORCE_INCREMENTAL: 1

    steps:
      - name: Configure git
        run: |
          git config --global core.symlinks true
          git config --global fetch.parallel 32

      - name: Clone repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.DENOBOT_PAT }}
          submodules: recursive

      - uses: dsherret/rust-toolchain-file@v1

      - name: Install deno
        uses: denoland/setup-deno@v2
        with:
          deno-version: v2.x

      - name: Publish
        env:
          CARGO_REGISTRY_TOKEN: ${{ secrets.CARGO_REGISTRY_TOKEN }}
        run: ./tools/release/03_publish_crates.ts

      - name: Create release tag and check forward commit to main
        env:
          # the default secrets.GITHUB_TOKEN won't trigger a workflow run
          # when tagging, but it will if we provide a custom PAT
          GITHUB_TOKEN: ${{ secrets.DENOBOT_PAT }}
          GH_WORKFLOW_ACTOR: ${{ github.actor }}
        run: |
          git config user.email ""${{ github.actor }}@users.noreply.github.com""
          git config user.name ""${{ github.actor }}""
          ./tools/release/04_post_publish.ts
",55,1,1,workflow_dispatch,3
denoland/deno,ci.yml,"# GENERATED BY ./ci.generate.ts -- DO NOT DIRECTLY EDIT

name: ci
permissions:
  contents: write
  id-token: write
on:
  push:
    branches:
      - main
    tags:
      - '*'
  pull_request:
    types:
      - opened
      - reopened
      - synchronize
      - ready_for_review
concurrency:
  group: '${{ github.workflow }}-${{ !contains(github.event.pull_request.labels.*.name, ''ci-test-flaky'') && github.head_ref || github.run_id }}'
  cancel-in-progress: true
jobs:
  pre_build:
    name: pre-build
    runs-on: ubuntu-latest
    outputs:
      skip_build: '${{ steps.check.outputs.skip_build }}'
    steps:
      - name: Configure git
        run: |-
          git config --global core.symlinks true
          git config --global fetch.parallel 32
        if: github.event.pull_request.draft == true
      - name: Clone repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 5
          submodules: false
        if: github.event.pull_request.draft == true
      - id: check
        if: 'github.event.pull_request.draft == true && (!contains(github.event.pull_request.labels.*.name, ''ci-draft''))'
        run: |-
          GIT_MESSAGE=$(git log --format=%s -n 1 ${{github.event.after}})
          echo Commit message: $GIT_MESSAGE
          echo $GIT_MESSAGE | grep '\[ci\]' || (echo 'Exiting due to draft PR. Commit with [ci] to bypass or add the ci-draft label.' ; echo 'skip_build=true' >> $GITHUB_OUTPUT)
  build:
    name: '${{ matrix.job }} ${{ matrix.profile }} ${{ matrix.os }}-${{ matrix.arch }}'
    needs:
      - pre_build
    if: '${{ needs.pre_build.outputs.skip_build != ''true'' }}'
    runs-on: '${{ matrix.runner }}'
    environment:
      name: '${{ (github.ref == ''refs/heads/main'' || startsWith(github.ref, ''refs/tags/'')) && ''build'' || '''' }}'
    timeout-minutes: 240
    defaults:
      run:
        shell: bash
    strategy:
      matrix:
        include:
          - os: macos
            arch: x86_64
            runner: macos-13
            job: test
            profile: debug
          - os: macos
            arch: x86_64
            runner: '${{ (!contains(github.event.pull_request.labels.*.name, ''ci-full'') && (github.event_name == ''pull_request'')) && ''ubuntu-24.04'' || ''macos-13'' }}'
            job: test
            profile: release
            skip: '${{ !contains(github.event.pull_request.labels.*.name, ''ci-full'') && (github.event_name == ''pull_request'') }}'
          - os: macos
            arch: aarch64
            runner: macos-14
            job: test
            profile: debug
          - os: macos
            arch: aarch64
            runner: '${{ (!contains(github.event.pull_request.labels.*.name, ''ci-full'') && (github.event_name == ''pull_request'')) && ''ubuntu-24.04'' || github.repository == ''denoland/deno'' && (github.ref == ''refs/heads/main'' || startsWith(github.ref, ''refs/tags/'')) && ''ghcr.io/cirruslabs/macos-runner:sonoma'' || ''macos-14'' }}'
            job: test
            profile: release
            skip: '${{ !contains(github.event.pull_request.labels.*.name, ''ci-full'') && (github.event_name == ''pull_request'') }}'
          - os: windows
            arch: x86_64
            runner: windows-2022
            job: test
            profile: debug
          - os: windows
            arch: x86_64
            runner: '${{ (!contains(github.event.pull_request.labels.*.name, ''ci-full'') && (github.event_name == ''pull_request'')) && ''ubuntu-24.04'' || github.repository == ''denoland/deno'' && ''windows-2022-xl'' || ''windows-2022'' }}'
            job: test
            profile: release
            skip: '${{ !contains(github.event.pull_request.labels.*.name, ''ci-full'') && (github.event_name == ''pull_request'') }}'
          - os: linux
            arch: x86_64
            runner: '${{ github.repository == ''denoland/deno'' && ''ubuntu-24.04-xl'' || ''ubuntu-24.04'' }}'
            job: test
            profile: release
            use_sysroot: true
            wpt: '${{ !startsWith(github.ref, ''refs/tags/'') }}'
          - os: linux
            arch: x86_64
            runner: '${{ (!contains(github.event.pull_request.labels.*.name, ''ci-full'') && (github.event_name == ''pull_request'' && !contains(github.event.pull_request.labels.*.name, ''ci-bench''))) && ''ubuntu-24.04'' || github.repository == ''denoland/deno'' && ''ubuntu-24.04-xl'' || ''ubuntu-24.04'' }}'
            job: bench
            profile: release
            use_sysroot: true
            skip: '${{ !contains(github.event.pull_request.labels.*.name, ''ci-full'') && (github.event_name == ''pull_request'' && !contains(github.event.pull_request.labels.*.name, ''ci-bench'')) }}'
          - os: linux
            arch: x86_64
            runner: ubuntu-24.04
            job: test
            profile: debug
            use_sysroot: true
          - os: linux
            arch: x86_64
            runner: ubuntu-24.04
            job: lint
            profile: debug
          - os: linux
            arch: aarch64
            runner: ubicloud-standard-16-arm
            job: test
            profile: debug
          - os: linux
            arch: aarch64
            runner: '${{ (!contains(github.event.pull_request.labels.*.name, ''ci-full'') && (github.event_name == ''pull_request'')) && ''ubuntu-24.04'' || ''ubicloud-standard-16-arm'' }}'
            job: test
            profile: release
            use_sysroot: true
            skip: '${{ !contains(github.event.pull_request.labels.*.name, ''ci-full'') && (github.event_name == ''pull_request'') }}'
          - os: macos
            arch: x86_64
            runner: macos-13
            job: lint
            profile: debug
          - os: windows
            arch: x86_64
            runner: windows-2022
            job: lint
            profile: debug
      fail-fast: '${{ github.event_name == ''pull_request'' || (github.ref != ''refs/heads/main'' && !startsWith(github.ref, ''refs/tags/'')) }}'
    env:
      CARGO_TERM_COLOR: always
      RUST_BACKTRACE: full
      RUST_LIB_BACKTRACE: 0
    steps:
      - name: Configure git
        run: |-
          git config --global core.symlinks true
          git config --global fetch.parallel 32
        if: '!(matrix.skip)'
      - name: Clone repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 5
          submodules: false
        if: '!(matrix.skip)'
      - name: Clone submodule ./tests/util/std
        run: git submodule update --init --recursive --depth=1 -- ./tests/util/std
        if: '!(matrix.skip)'
      - name: Clone submodule ./tests/wpt/suite
        run: git submodule update --init --recursive --depth=1 -- ./tests/wpt/suite
        if: '!(matrix.skip) && (matrix.wpt)'
      - name: Clone submodule ./tests/node_compat/runner/suite
        run: git submodule update --init --recursive --depth=1 -- ./tests/node_compat/runner/suite
        if: '!(matrix.skip) && (matrix.job == ''lint'' && matrix.os == ''linux'')'
      - name: Clone submodule ./cli/bench/testdata/lsp_benchdata
        run: git submodule update --init --recursive --depth=1 -- ./cli/bench/testdata/lsp_benchdata
        if: '!(matrix.skip) && (matrix.job == ''bench'')'
      - name: 'Create source tarballs (release, linux)'
        if: |-
          !(matrix.skip) && (matrix.os == 'linux' &&
          matrix.profile == 'release' &&
          matrix.job == 'test' &&
          github.repository == 'denoland/deno' &&
          startsWith(github.ref, 'refs/tags/'))
        run: |-
          mkdir -p target/release
          tar --exclude="".git*"" --exclude=target --exclude=third_party/prebuilt \
              -czvf target/release/deno_src.tar.gz -C .. deno
      - name: Cache Cargo home
        uses: cirruslabs/cache@v4
        with:
          path: |-
            ~/.cargo/.crates.toml
            ~/.cargo/.crates2.json
            ~/.cargo/bin
            ~/.cargo/registry/index
            ~/.cargo/registry/cache
            ~/.cargo/git/db
          key: '63-cargo-home-${{ matrix.os }}-${{ matrix.arch }}-${{ hashFiles(''Cargo.lock'') }}'
          restore-keys: '63-cargo-home-${{ matrix.os }}-${{ matrix.arch }}-'
        if: '!(matrix.skip)'
      - uses: dsherret/rust-toolchain-file@v1
        if: '!(matrix.skip)'
      - if: '!(matrix.skip) && (matrix.job == ''lint'' || matrix.job == ''test'' || matrix.job == ''bench'')'
        name: Install Deno
        uses: denoland/setup-deno@v2
        with:
          deno-version: v2.x
      - name: Install Python
        uses: actions/setup-python@v5
        with:
          python-version: 3.11
        if: '!(matrix.skip) && (matrix.job != ''lint'' && (matrix.os != ''linux'' || matrix.arch != ''aarch64''))'
      - name: Remove unused versions of Python
        if: '!(matrix.skip) && (matrix.job != ''lint'' && (matrix.os != ''linux'' || matrix.arch != ''aarch64'') && (matrix.os == ''windows''))'
        shell: pwsh
        run: |-
          $env:PATH -split "";"" |
            Where-Object { Test-Path ""$_\python.exe"" } |
            Select-Object -Skip 1 |
            ForEach-Object { Move-Item ""$_"" ""$_.disabled"" }
      - if: '!(matrix.skip) && (matrix.job == ''bench'' || matrix.job == ''test'')'
        name: Install Node
        uses: actions/setup-node@v4
        with:
          node-version: 18
      - if: |-
          !(matrix.skip) && (matrix.profile == 'release' &&
          matrix.job == 'test' &&
          github.repository == 'denoland/deno' &&
          (github.ref == 'refs/heads/main' ||
          startsWith(github.ref, 'refs/tags/')))
        name: Authenticate with Google Cloud
        uses: google-github-actions/auth@v2
        with:
          project_id: denoland
          credentials_json: '${{ secrets.GCP_SA_KEY }}'
          export_environment_variables: true
          create_credentials_file: true
      - name: Setup gcloud (unix)
        if: |-
          !(matrix.skip) && (matrix.os != 'windows' &&
          matrix.profile == 'release' &&
          matrix.job == 'test' &&
          github.repository == 'denoland/deno' &&
          (github.ref == 'refs/heads/main' ||
          startsWith(github.ref, 'refs/tags/')))
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: denoland
      - name: Setup gcloud (windows)
        if: |-
          !(matrix.skip) && (matrix.os == 'windows' &&
          matrix.profile == 'release' &&
          matrix.job == 'test' &&
          github.repository == 'denoland/deno' &&
          (github.ref == 'refs/heads/main' ||
          startsWith(github.ref, 'refs/tags/')))
        uses: google-github-actions/setup-gcloud@v2
        env:
          CLOUDSDK_PYTHON: '${{env.pythonLocation}}\python.exe'
        with:
          project_id: denoland
      - name: Configure canary build
        if: |-
          !(matrix.skip) && (matrix.job == 'test' &&
          matrix.profile == 'release' &&
          github.repository == 'denoland/deno' &&
          github.ref == 'refs/heads/main')
        run: echo ""DENO_CANARY=true"" >> $GITHUB_ENV
      - if: '!(matrix.skip) && (matrix.use_sysroot)'
        name: Set up incremental LTO and sysroot build
        run: |-
          # Setting up sysroot
          export DEBIAN_FRONTEND=noninteractive
          # Avoid running man-db triggers, which sometimes takes several minutes
          # to complete.
          sudo apt-get -qq remove --purge -y man-db  > /dev/null 2> /dev/null
          # Remove older clang before we install
          sudo apt-get -qq remove   'clang-12*' 'clang-13*' 'clang-14*' 'clang-15*' 'clang-16*' 'clang-17*' 'clang-18*' 'llvm-12*' 'llvm-13*' 'llvm-14*' 'llvm-15*' 'llvm-16*' 'llvm-17*' 'llvm-18*' 'lld-12*' 'lld-13*' 'lld-14*' 'lld-15*' 'lld-16*' 'lld-17*' 'lld-18*' > /dev/null 2> /dev/null

          # Install clang-XXX, lld-XXX, and debootstrap.
          echo ""deb http://apt.llvm.org/jammy/ llvm-toolchain-jammy-19 main"" |
            sudo dd of=/etc/apt/sources.list.d/llvm-toolchain-jammy-19.list
          curl https://apt.llvm.org/llvm-snapshot.gpg.key |
            gpg --dearmor                                 |
          sudo dd of=/etc/apt/trusted.gpg.d/llvm-snapshot.gpg
          sudo apt-get update
          # this was unreliable sometimes, so try again if it fails
          sudo apt-get install --no-install-recommends clang-19 lld-19 clang-tools-19 clang-format-19 clang-tidy-19 || echo 'Failed. Trying again.' && sudo apt-get clean && sudo apt-get update && sudo apt-get install --no-install-recommends clang-19 lld-19 clang-tools-19 clang-format-19 clang-tidy-19
          # Fix alternatives
          (yes '' | sudo update-alternatives --force --all) > /dev/null 2> /dev/null || true

          clang-19 -c -o /tmp/memfd_create_shim.o tools/memfd_create_shim.c -fPIC

          echo ""Decompressing sysroot...""
          wget -q https://github.com/denoland/deno_sysroot_build/releases/download/sysroot-20250207/sysroot-`uname -m`.tar.xz -O /tmp/sysroot.tar.xz
          cd /
          xzcat /tmp/sysroot.tar.xz | sudo tar -x
          sudo mount --rbind /dev /sysroot/dev
          sudo mount --rbind /sys /sysroot/sys
          sudo mount --rbind /home /sysroot/home
          sudo mount -t proc /proc /sysroot/proc
          cd

          echo ""Done.""

          # Configure the build environment. Both Rust and Clang will produce
          # llvm bitcode only, so we can use lld's incremental LTO support.

          # Load the sysroot's env vars
          echo ""sysroot env:""
          cat /sysroot/.env
          . /sysroot/.env

          # Important notes:
          #   1. -ldl seems to be required to avoid a failure in FFI tests. This flag seems
          #      to be in the Rust default flags in the smoketest, so uncertain why we need
          #      to be explicit here.
          #   2. RUSTFLAGS and RUSTDOCFLAGS must be specified, otherwise the doctests fail
          #      to build because the object formats are not compatible.
          echo ""
          CARGO_PROFILE_BENCH_INCREMENTAL=false
          CARGO_PROFILE_RELEASE_INCREMENTAL=false
          RUSTFLAGS<<__1
            -C linker-plugin-lto=true
            -C linker=clang-19
            -C link-arg=-fuse-ld=lld-19
            -C link-arg=-ldl
            -C link-arg=-Wl,--allow-shlib-undefined
            -C link-arg=-Wl,--thinlto-cache-dir=$(pwd)/target/release/lto-cache
            -C link-arg=-Wl,--thinlto-cache-policy,cache_size_bytes=700m
            -C link-arg=/tmp/memfd_create_shim.o
            --cfg tokio_unstable
            $RUSTFLAGS
          __1
          RUSTDOCFLAGS<<__1
            -C linker-plugin-lto=true
            -C linker=clang-19
            -C link-arg=-fuse-ld=lld-19
            -C link-arg=-ldl
            -C link-arg=-Wl,--allow-shlib-undefined
            -C link-arg=-Wl,--thinlto-cache-dir=$(pwd)/target/release/lto-cache
            -C link-arg=-Wl,--thinlto-cache-policy,cache_size_bytes=700m
            -C link-arg=/tmp/memfd_create_shim.o
            --cfg tokio_unstable
            $RUSTFLAGS
          __1
          CC=/usr/bin/clang-19
          CFLAGS=$CFLAGS
          "" > $GITHUB_ENV
      - name: Remove macOS cURL --ipv4 flag
        run: |-
          curl --version
          which curl
          cat /etc/hosts
          rm ~/.curlrc || true
        if: '!(matrix.skip) && (matrix.os == ''macos'')'
      - name: Install macOS aarch64 lld
        env:
          GITHUB_TOKEN: '${{ secrets.GITHUB_TOKEN }}'
        run: ./tools/install_prebuilt.js ld64.lld
        if: '!(matrix.skip) && (matrix.os == ''macos'' && matrix.arch == ''aarch64'')'
      - name: Install rust-codesign
        env:
          GITHUB_TOKEN: '${{ secrets.GITHUB_TOKEN }}'
        run: |-
          ./tools/install_prebuilt.js rcodesign
          echo $GITHUB_WORKSPACE/third_party/prebuilt/mac >> $GITHUB_PATH
        if: '!(matrix.skip) && (matrix.os == ''macos'')'
      - name: Log versions
        run: |-
          echo '*** Python'
          command -v python && python --version || echo 'No python found or bad executable'
          echo '*** Rust'
          command -v rustc && rustc --version || echo 'No rustc found or bad executable'
          echo '*** Cargo'
          command -v cargo && cargo --version || echo 'No cargo found or bad executable'
          echo '*** Deno'
          command -v deno && deno --version || echo 'No deno found or bad executable'
          echo '*** Node'
          command -v node && node --version || echo 'No node found or bad executable'
          echo '*** Installed packages'
          command -v dpkg && dpkg -l || echo 'No dpkg found or bad executable'
        if: '!(matrix.skip)'
      - name: Install benchmark tools
        if: '!(matrix.skip) && (matrix.job == ''bench'')'
        env:
          GITHUB_TOKEN: '${{ secrets.GITHUB_TOKEN }}'
        run: ./tools/install_prebuilt.js wrk hyperfine
      - name: Restore cache build output (PR)
        uses: actions/cache/restore@v4
        if: '!(matrix.skip) && (github.ref != ''refs/heads/main'' && !startsWith(github.ref, ''refs/tags/''))'
        with:
          path: |-
            ./target
            !./target/*/gn_out
            !./target/*/gn_root
            !./target/*/*.zip
            !./target/*/*.tar.gz
          key: never_saved
          restore-keys: '63-cargo-target-${{ matrix.os }}-${{ matrix.arch }}-${{ matrix.profile }}-${{ matrix.job }}-'
      - name: Apply and update mtime cache
        if: '!(matrix.skip) && (!startsWith(github.ref, ''refs/tags/''))'
        uses: ./.github/mtime_cache
        with:
          cache-path: ./target
      - name: Set up playwright cache
        uses: actions/cache@v4
        with:
          path: ./.ms-playwright
          key: 'playwright-${{ runner.os }}-${{ runner.arch }}'
        if: '!(matrix.skip)'
      - name: test_format.js
        if: '!(matrix.skip) && (matrix.job == ''lint'' && matrix.os == ''linux'')'
        run: deno run --allow-write --allow-read --allow-run --allow-net ./tools/format.js --check
      - name: Lint PR title
        if: '!(matrix.skip) && (matrix.job == ''lint'' && github.event_name == ''pull_request'' && matrix.os == ''linux'')'
        env:
          PR_TITLE: '${{ github.event.pull_request.title }}'
        run: deno run ./tools/verify_pr_title.js ""$PR_TITLE""
      - name: lint.js
        if: '!(matrix.skip) && (matrix.job == ''lint'')'
        env:
          GITHUB_TOKEN: '${{ secrets.GITHUB_TOKEN }}'
        run: deno run --allow-write --allow-read --allow-run --allow-net --allow-env ./tools/lint.js
      - name: jsdoc_checker.js
        if: '!(matrix.skip) && (matrix.job == ''lint'')'
        run: deno run --allow-read --allow-env --allow-sys ./tools/jsdoc_checker.js
      - name: node_compat/setup.ts --check
        if: '!(matrix.skip) && (matrix.job == ''lint'' && matrix.os == ''linux'')'
        run: deno run --allow-write --allow-read --allow-run=git ./tests/node_compat/runner/setup.ts --check
      - name: Check tracing build
        if: '!(matrix.skip) && (matrix.job == ''test'' && matrix.profile == ''debug'' && matrix.os == ''linux'' && matrix.arch == ''x86_64'')'
        run: cargo check -p deno --features=lsp-tracing
        env:
          CARGO_PROFILE_DEV_DEBUG: 0
      - name: Build debug
        if: '!(matrix.skip) && (matrix.job == ''test'' && matrix.profile == ''debug'')'
        run: cargo build --locked --all-targets --features=panic-trace
        env:
          CARGO_PROFILE_DEV_DEBUG: 0
      - name: Build release
        if: |-
          !(matrix.skip) && ((matrix.job == 'test' || matrix.job == 'bench') &&
          matrix.profile == 'release' && (matrix.use_sysroot ||
          github.repository == 'denoland/deno'))
        run: |-
          df -h
          cargo build --release --locked --all-targets --features=panic-trace
          df -h
      - name: Check deno binary
        if: '!(matrix.skip) && (matrix.job == ''test'')'
        run: 'target/${{ matrix.profile }}/deno eval ""console.log(1+2)"" | grep 3'
        env:
          NO_COLOR: 1
      - name: Check deno binary (in sysroot)
        if: '!(matrix.skip) && (matrix.job == ''test'' && matrix.use_sysroot)'
        run: 'sudo chroot /sysroot ""$(pwd)/target/${{ matrix.profile }}/deno"" --version'
      - name: Generate symcache
        if: |-
          !(matrix.skip) && ((matrix.job == 'test' || matrix.job == 'bench') &&
          matrix.profile == 'release' && (matrix.use_sysroot ||
          github.repository == 'denoland/deno'))
        run: |-
          target/release/deno -A tools/release/create_symcache.ts ./deno.symcache
          du -h deno.symcache
          du -h target/release/deno
        env:
          NO_COLOR: 1
      - name: Upload PR artifact (linux)
        if: |-
          !(matrix.skip) && (matrix.job == 'test' &&
          matrix.profile == 'release' && (matrix.use_sysroot ||
          (github.repository == 'denoland/deno' &&
          (github.ref == 'refs/heads/main' ||
          startsWith(github.ref, 'refs/tags/')))))
        uses: actions/upload-artifact@v4
        with:
          name: 'deno-${{ matrix.os }}-${{ matrix.arch }}-${{ github.event.number }}'
          path: target/release/deno
      - name: Pre-release (linux)
        if: |-
          !(matrix.skip) && (matrix.os == 'linux' &&
          (matrix.job == 'test' || matrix.job == 'bench') &&
          matrix.profile == 'release' &&
          github.repository == 'denoland/deno')
        run: |-
          cd target/release
          ./deno -A ../../tools/release/create_symcache.ts deno-${{ matrix.arch }}-unknown-linux-gnu.symcache
          strip ./deno
          zip -r deno-${{ matrix.arch }}-unknown-linux-gnu.zip deno
          shasum -a 256 deno-${{ matrix.arch }}-unknown-linux-gnu.zip > deno-${{ matrix.arch }}-unknown-linux-gnu.zip.sha256sum
          strip ./denort
          zip -r denort-${{ matrix.arch }}-unknown-linux-gnu.zip denort
          shasum -a 256 denort-${{ matrix.arch }}-unknown-linux-gnu.zip > denort-${{ matrix.arch }}-unknown-linux-gnu.zip.sha256sum
          ./deno types > lib.deno.d.ts
      - name: Pre-release (mac)
        if: |-
          !(matrix.skip) && (matrix.os == 'macos' &&
          matrix.job == 'test' &&
          matrix.profile == 'release' &&
          github.repository == 'denoland/deno')
        env:
          APPLE_CODESIGN_KEY: '${{ secrets.APPLE_CODESIGN_KEY }}'
          APPLE_CODESIGN_PASSWORD: '${{ secrets.APPLE_CODESIGN_PASSWORD }}'
        run: |-
          target/release/deno -A tools/release/create_symcache.ts target/release/deno-${{ matrix.arch }}-apple-darwin.symcache
          strip -x -S target/release/deno
          echo ""Key is $(echo $APPLE_CODESIGN_KEY | base64 -d | wc -c) bytes""
          rcodesign sign target/release/deno --code-signature-flags=runtime --p12-password=""$APPLE_CODESIGN_PASSWORD"" --p12-file=<(echo $APPLE_CODESIGN_KEY | base64 -d) --entitlements-xml-file=cli/entitlements.plist
          cd target/release
          zip -r deno-${{ matrix.arch }}-apple-darwin.zip deno
          shasum -a 256 deno-${{ matrix.arch }}-apple-darwin.zip > deno-${{ matrix.arch }}-apple-darwin.zip.sha256sum
          strip -x -S ./denort
          zip -r denort-${{ matrix.arch }}-apple-darwin.zip denort
          shasum -a 256 denort-${{ matrix.arch }}-apple-darwin.zip > denort-${{ matrix.arch }}-apple-darwin.zip.sha256sum
      - name: Authenticate with Azure (windows)
        if: |-
          !(matrix.skip) && (matrix.os == 'windows' &&
          matrix.job == 'test' &&
          matrix.profile == 'release' &&
          github.repository == 'denoland/deno' &&
          (github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/')))
        uses: azure/login@v1
        with:
          client-id: '${{ secrets.AZURE_CLIENT_ID }}'
          tenant-id: '${{ secrets.AZURE_TENANT_ID }}'
          subscription-id: '${{ secrets.AZURE_SUBSCRIPTION_ID }}'
          enable-AzPSSession: true
      - name: Code sign deno.exe (windows)
        if: |-
          !(matrix.skip) && (matrix.os == 'windows' &&
          matrix.job == 'test' &&
          matrix.profile == 'release' &&
          github.repository == 'denoland/deno' &&
          (github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/')))
        uses: azure/trusted-signing-action@v0
        with:
          endpoint: 'https://eus.codesigning.azure.net/'
          trusted-signing-account-name: deno-cli-code-signing
          certificate-profile-name: deno-cli-code-signing-cert
          files-folder: target/release
          files-folder-filter: deno.exe
          file-digest: SHA256
          timestamp-rfc3161: 'http://timestamp.acs.microsoft.com'
          timestamp-digest: SHA256
          exclude-environment-credential: true
          exclude-workload-identity-credential: true
          exclude-managed-identity-credential: true
          exclude-shared-token-cache-credential: true
          exclude-visual-studio-credential: true
          exclude-visual-studio-code-credential: true
          exclude-azure-cli-credential: false
      - name: Verify signature (windows)
        if: |-
          !(matrix.skip) && (matrix.os == 'windows' &&
          matrix.job == 'test' &&
          matrix.profile == 'release' &&
          github.repository == 'denoland/deno' &&
          (github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/')))
        shell: pwsh
        run: |-
          $SignTool = Get-ChildItem -Path ""C:\Program Files*\Windows Kits\*\bin\*\x64\signtool.exe"" -Recurse -ErrorAction SilentlyContinue | Select-Object -First 1
          $SignToolPath = $SignTool.FullName
          & $SignToolPath verify /pa /v target\release\deno.exe
      - name: Pre-release (windows)
        if: |-
          !(matrix.skip) && (matrix.os == 'windows' &&
          matrix.job == 'test' &&
          matrix.profile == 'release' &&
          github.repository == 'denoland/deno')
        shell: pwsh
        run: |-
          Compress-Archive -CompressionLevel Optimal -Force -Path target/release/deno.exe -DestinationPath target/release/deno-${{ matrix.arch }}-pc-windows-msvc.zip
          Get-FileHash target/release/deno-${{ matrix.arch }}-pc-windows-msvc.zip -Algorithm SHA256 | Format-List > target/release/deno-${{ matrix.arch }}-pc-windows-msvc.zip.sha256sum
          Compress-Archive -CompressionLevel Optimal -Force -Path target/release/denort.exe -DestinationPath target/release/denort-${{ matrix.arch }}-pc-windows-msvc.zip
          Get-FileHash target/release/denort-${{ matrix.arch }}-pc-windows-msvc.zip -Algorithm SHA256 | Format-List > target/release/denort-${{ matrix.arch }}-pc-windows-msvc.zip.sha256sum
          target/release/deno.exe -A tools/release/create_symcache.ts target/release/deno-${{ matrix.arch }}-pc-windows-msvc.symcache
      - name: Upload canary to dl.deno.land
        if: |-
          !(matrix.skip) && (matrix.job == 'test' &&
          matrix.profile == 'release' &&
          github.repository == 'denoland/deno' &&
          github.ref == 'refs/heads/main')
        run: |-
          gsutil -h ""Cache-Control: public, max-age=3600"" cp ./target/release/*.zip gs://dl.deno.land/canary/$(git rev-parse HEAD)/
          gsutil -h ""Cache-Control: public, max-age=3600"" cp ./target/release/*.sha256sum gs://dl.deno.land/canary/$(git rev-parse HEAD)/
          gsutil -h ""Cache-Control: public, max-age=3600"" cp ./target/release/*.symcache gs://dl.deno.land/canary/$(git rev-parse HEAD)/
          echo ${{ github.sha }} > canary-latest.txt
          gsutil -h ""Cache-Control: no-cache"" cp canary-latest.txt gs://dl.deno.land/canary-$(rustc -vV | sed -n ""s|host: ||p"")-latest.txt
          rm canary-latest.txt gha-creds-*.json
      - name: Autobahn testsuite
        if: |-
          !(matrix.skip) && ((matrix.os == 'linux' && matrix.arch != 'aarch64') &&
          matrix.job == 'test' &&
          matrix.profile == 'release' &&
          !startsWith(github.ref, 'refs/tags/'))
        run: target/release/deno run -A --config tests/config/deno.json ext/websocket/autobahn/fuzzingclient.js
      - name: 'Test (full, debug)'
        if: |-
          !(matrix.skip) && (matrix.job == 'test' &&
          matrix.profile == 'debug' &&
          !startsWith(github.ref, 'refs/tags/') &&
          matrix.os == 'linux')
        run: cargo test --locked --features=panic-trace
        env:
          CARGO_PROFILE_DEV_DEBUG: 0
      - name: 'Test (fast, debug)'
        if: |-
          !(matrix.skip) && (matrix.job == 'test' &&
          matrix.profile == 'debug' &&
          (startsWith(github.ref, 'refs/tags/') || matrix.os != 'linux'))
        run: |-
          cargo test --locked --lib --features=panic-trace
          cargo test --locked --tests --features=panic-trace
        env:
          CARGO_PROFILE_DEV_DEBUG: 0
      - name: Test (release)
        if: |-
          !(matrix.skip) && (matrix.job == 'test' &&
          matrix.profile == 'release' &&
          (matrix.use_sysroot || (
          github.repository == 'denoland/deno' &&
          !startsWith(github.ref, 'refs/tags/'))))
        run: cargo test --release --locked --features=panic-trace
      - name: Ensure no git changes
        if: '!(matrix.skip) && (matrix.job == ''test'' && github.event_name == ''pull_request'')'
        run: ""if [[ -n \""$(git status --porcelain)\"" ]]; then\necho \""❌ Git working directory is dirty. Ensure `cargo test` is not modifying git tracked files.\""\necho \""\""\necho \""\U0001F4CB Status:\""\ngit status\necho \""\""\nexit 1\nfi""
      - name: Configure hosts file for WPT
        if: '!(matrix.skip) && (matrix.wpt)'
        run: ./wpt make-hosts-file | sudo tee -a /etc/hosts
        working-directory: tests/wpt/suite/
      - name: Run web platform tests (debug)
        if: '!(matrix.skip) && (matrix.wpt && matrix.profile == ''debug'')'
        env:
          DENO_BIN: ./target/debug/deno
        run: |-
          deno run -RWNE --allow-run --lock=tools/deno.lock.json --config tests/config/deno.json \
              ./tests/wpt/wpt.ts setup
          deno run -RWNE --allow-run --lock=tools/deno.lock.json --config tests/config/deno.json --unsafely-ignore-certificate-errors \
              ./tests/wpt/wpt.ts run --quiet --binary=""$DENO_BIN""
      - name: Run web platform tests (release)
        if: '!(matrix.skip) && (matrix.wpt && matrix.profile == ''release'')'
        env:
          DENO_BIN: ./target/release/deno
        run: |-
          deno run -RWNE --allow-run --lock=tools/deno.lock.json --config tests/config/deno.json \
              ./tests/wpt/wpt.ts setup
          deno run -RWNE --allow-run --lock=tools/deno.lock.json --config tests/config/deno.json --unsafely-ignore-certificate-errors \
              ./tests/wpt/wpt.ts run --quiet --release --binary=""$DENO_BIN"" --json=wpt.json --wptreport=wptreport.json
      - name: Upload wpt results to dl.deno.land
        continue-on-error: true
        if: |-
          !(matrix.skip) && (matrix.wpt &&
          matrix.os == 'linux' &&
          matrix.profile == 'release' &&
          github.repository == 'denoland/deno' &&
          github.ref == 'refs/heads/main' && !startsWith(github.ref, 'refs/tags/'))
        run: |-
          gzip ./wptreport.json
          gsutil -h ""Cache-Control: public, max-age=3600"" cp ./wpt.json gs://dl.deno.land/wpt/$(git rev-parse HEAD).json
          gsutil -h ""Cache-Control: public, max-age=3600"" cp ./wptreport.json.gz gs://dl.deno.land/wpt/$(git rev-parse HEAD)-wptreport.json.gz
          echo $(git rev-parse HEAD) > wpt-latest.txt
          gsutil -h ""Cache-Control: no-cache"" cp wpt-latest.txt gs://dl.deno.land/wpt-latest.txt
      - name: Upload wpt results to wpt.fyi
        continue-on-error: true
        if: |-
          !(matrix.skip) && (matrix.wpt &&
          matrix.os == 'linux' &&
          matrix.profile == 'release' &&
          github.repository == 'denoland/deno' &&
          github.ref == 'refs/heads/main' && !startsWith(github.ref, 'refs/tags/'))
        env:
          WPT_FYI_USER: deno
          WPT_FYI_PW: '${{ secrets.WPT_FYI_PW }}'
          GITHUB_TOKEN: '${{ secrets.DENOBOT_PAT }}'
        run: |-
          ./target/release/deno run --allow-all --lock=tools/deno.lock.json \
              ./tools/upload_wptfyi.js $(git rev-parse HEAD) --ghstatus
      - name: Run benchmarks
        if: '!(matrix.skip) && (matrix.job == ''bench'' && !startsWith(github.ref, ''refs/tags/''))'
        run: cargo bench --locked
      - name: Post Benchmarks
        if: |-
          !(matrix.skip) && (matrix.job == 'bench' &&
          github.repository == 'denoland/deno' &&
          github.ref == 'refs/heads/main' && !startsWith(github.ref, 'refs/tags/'))
        env:
          DENOBOT_PAT: '${{ secrets.DENOBOT_PAT }}'
        run: |-
          git clone --depth 1 --branch gh-pages                             \
              https://${DENOBOT_PAT}@github.com/denoland/benchmark_data.git \
              gh-pages
          ./target/release/deno run --allow-all ./tools/build_benchmark_jsons.js --release
          cd gh-pages
          git config user.email ""propelml@gmail.com""
          git config user.name ""denobot""
          git add .
          git commit --message ""Update benchmarks""
          git push origin gh-pages
      - name: Build product size info
        if: '!(matrix.skip) && (matrix.job != ''lint'' && matrix.profile != ''debug'' && github.repository == ''denoland/deno'' && (github.ref == ''refs/heads/main'' || startsWith(github.ref, ''refs/tags/'')))'
        run: |-
          du -hd1 ""./target/${{ matrix.profile }}""
          du -ha  ""./target/${{ matrix.profile }}/deno""
          du -ha  ""./target/${{ matrix.profile }}/denort""
      - name: Worker info
        if: '!(matrix.skip) && (matrix.job == ''bench'')'
        run: |-
          cat /proc/cpuinfo
          cat /proc/meminfo
      - name: Upload release to dl.deno.land (unix)
        if: |-
          !(matrix.skip) && (matrix.os != 'windows' &&
          matrix.job == 'test' &&
          matrix.profile == 'release' &&
          github.repository == 'denoland/deno' &&
          startsWith(github.ref, 'refs/tags/'))
        run: |-
          gsutil -h ""Cache-Control: public, max-age=3600"" cp ./target/release/*.zip gs://dl.deno.land/release/${GITHUB_REF#refs/*/}/
          gsutil -h ""Cache-Control: public, max-age=3600"" cp ./target/release/*.sha256sum gs://dl.deno.land/release/${GITHUB_REF#refs/*/}/
          gsutil -h ""Cache-Control: public, max-age=3600"" cp ./target/release/*.symcache gs://dl.deno.land/release/${GITHUB_REF#refs/*/}/
      - name: Upload release to dl.deno.land (windows)
        if: |-
          !(matrix.skip) && (matrix.os == 'windows' &&
          matrix.job == 'test' &&
          matrix.profile == 'release' &&
          github.repository == 'denoland/deno' &&
          startsWith(github.ref, 'refs/tags/'))
        env:
          CLOUDSDK_PYTHON: '${{env.pythonLocation}}\python.exe'
        run: |-
          gsutil -h ""Cache-Control: public, max-age=3600"" cp ./target/release/*.zip gs://dl.deno.land/release/${GITHUB_REF#refs/*/}/
          gsutil -h ""Cache-Control: public, max-age=3600"" cp ./target/release/*.sha256sum gs://dl.deno.land/release/${GITHUB_REF#refs/*/}/
          gsutil -h ""Cache-Control: public, max-age=3600"" cp ./target/release/*.symcache gs://dl.deno.land/release/${GITHUB_REF#refs/*/}/
      - name: Create release notes
        if: |-
          !(matrix.skip) && (matrix.job == 'test' &&
          matrix.profile == 'release' &&
          github.repository == 'denoland/deno' &&
          startsWith(github.ref, 'refs/tags/'))
        run: |-
          export PATH=$PATH:$(pwd)/target/release
          ./tools/release/05_create_release_notes.ts
      - name: Upload release to GitHub
        uses: softprops/action-gh-release@v0.1.15
        if: |-
          !(matrix.skip) && (matrix.job == 'test' &&
          matrix.profile == 'release' &&
          github.repository == 'denoland/deno' &&
          startsWith(github.ref, 'refs/tags/'))
        env:
          GITHUB_TOKEN: '${{ secrets.GITHUB_TOKEN }}'
        with:
          files: |-
            target/release/deno-x86_64-pc-windows-msvc.zip
            target/release/deno-x86_64-pc-windows-msvc.zip.sha256sum
            target/release/denort-x86_64-pc-windows-msvc.zip
            target/release/denort-x86_64-pc-windows-msvc.zip.sha256sum
            target/release/deno-x86_64-unknown-linux-gnu.zip
            target/release/deno-x86_64-unknown-linux-gnu.zip.sha256sum
            target/release/denort-x86_64-unknown-linux-gnu.zip
            target/release/denort-x86_64-unknown-linux-gnu.zip.sha256sum
            target/release/deno-x86_64-apple-darwin.zip
            target/release/deno-x86_64-apple-darwin.zip.sha256sum
            target/release/denort-x86_64-apple-darwin.zip
            target/release/denort-x86_64-apple-darwin.zip.sha256sum
            target/release/deno-aarch64-unknown-linux-gnu.zip
            target/release/deno-aarch64-unknown-linux-gnu.zip.sha256sum
            target/release/denort-aarch64-unknown-linux-gnu.zip
            target/release/denort-aarch64-unknown-linux-gnu.zip.sha256sum
            target/release/deno-aarch64-apple-darwin.zip
            target/release/deno-aarch64-apple-darwin.zip.sha256sum
            target/release/denort-aarch64-apple-darwin.zip
            target/release/denort-aarch64-apple-darwin.zip.sha256sum
            target/release/deno_src.tar.gz
            target/release/lib.deno.d.ts
          body_path: target/release/release-notes.md
          draft: true
      - name: Save cache build output (main)
        uses: actions/cache/save@v4
        if: '!(matrix.skip) && ((matrix.job == ''test'' || matrix.job == ''lint'') && github.ref == ''refs/heads/main'')'
        with:
          path: |-
            ./target
            !./target/*/gn_out
            !./target/*/gn_root
            !./target/*/*.zip
            !./target/*/*.tar.gz
          key: '63-cargo-target-${{ matrix.os }}-${{ matrix.arch }}-${{ matrix.profile }}-${{ matrix.job }}-${{ github.sha }}'
  libs:
    name: build libs
    needs:
      - pre_build
    if: '${{ needs.pre_build.outputs.skip_build != ''true'' }}'
    runs-on: ubuntu-24.04
    timeout-minutes: 30
    steps:
      - name: Configure git
        run: |-
          git config --global core.symlinks true
          git config --global fetch.parallel 32
        if: '!(matrix.skip)'
      - name: Clone repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 5
          submodules: false
        if: '!(matrix.skip)'
      - uses: dsherret/rust-toolchain-file@v1
        if: '!(matrix.skip)'
      - name: Install wasm target
        run: rustup target add wasm32-unknown-unknown
        if: '!(matrix.skip)'
      - name: Cargo check (deno_resolver)
        run: cargo check --target wasm32-unknown-unknown -p deno_resolver && cargo check --target wasm32-unknown-unknown -p deno_resolver --features graph && cargo check --target wasm32-unknown-unknown -p deno_resolver --features graph --features deno_ast
        if: '!(matrix.skip)'
      - name: Cargo check (deno_npm_installer)
        run: cargo check --target wasm32-unknown-unknown -p deno_npm_installer
        if: '!(matrix.skip)'
      - name: Cargo check (deno_config)
        run: |-
          cargo check --no-default-features -p deno_config
          cargo check --no-default-features --features workspace -p deno_config
          cargo check --no-default-features --features package_json -p deno_config
          cargo check --no-default-features --features workspace --features sync -p deno_config
          cargo check --target wasm32-unknown-unknown --all-features -p deno_config
        if: '!(matrix.skip)'
  publish-canary:
    name: publish canary
    runs-on: ubuntu-24.04
    needs:
      - build
    if: github.repository == 'denoland/deno' && github.ref == 'refs/heads/main'
    steps:
      - name: Authenticate with Google Cloud
        uses: google-github-actions/auth@v2
        with:
          project_id: denoland
          credentials_json: '${{ secrets.GCP_SA_KEY }}'
          export_environment_variables: true
          create_credentials_file: true
      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: denoland
      - name: Upload canary version file to dl.deno.land
        run: |-
          echo ${{ github.sha }} > canary-latest.txt
          gsutil -h ""Cache-Control: no-cache"" cp canary-latest.txt gs://dl.deno.land/canary-latest.txt
",843,4,2,"push, pull_request",22
denoland/deno,node_compat_test.yml,"name: node_compat_test

on:
  schedule:
    - cron: '0 10 * * *'
  workflow_dispatch:

jobs:
  test:
    runs-on: '${{ matrix.runner }}'
    strategy:
      matrix:
        include:
          - os: linux
            runner: ubuntu-latest
          - os: windows
            runner: windows-latest
          - os: darwin
            runner: macos-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          submodules: true
      - name: Setup Deno
        uses: denoland/setup-deno@v2
        with:
          deno-version: canary
      - name: Install Python
        uses: actions/setup-python@v5
        with:
          python-version: 3.11
      - name: Authenticate with Google Cloud
        uses: google-github-actions/auth@v2
        with:
          project_id: denoland
          credentials_json: '${{ secrets.GCP_SA_KEY }}'
          export_environment_variables: true
          create_credentials_file: true
      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: denoland
      - name: Run tests
        run: deno -A tools/node_compat_tests.js
      - name: Gzip the report
        run: gzip tests/node_compat/report.json
      - name: Upload the report to dl.deno.land
        run: |-
          gsutil -h ""Cache-Control: public, max-age=3600"" cp tests/node_compat/report.json.gz gs://dl.deno.land/node-compat-test/$(date +%F)/report-${{matrix.os}}.json.gz
  summary:
    runs-on: ubuntu-latest
    needs: test
    if: ${{ always() }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          submodules: true
      - name: Setup Deno
        uses: denoland/setup-deno@v2
      - name: Install Python
        uses: actions/setup-python@v5
        with:
          python-version: 3.11
      - name: Authenticate with Google Cloud
        uses: google-github-actions/auth@v2
        with:
          project_id: denoland
          credentials_json: '${{ secrets.GCP_SA_KEY }}'
          export_environment_variables: true
          create_credentials_file: true
      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: denoland
      - name: Add the day summary to the month summary
        run: deno -A --config tests/config/deno.json tests/node_compat/add_day_summary_to_month_summary.ts
      - name: Gzip the month summary
        run: gzip tests/node_compat/summary.json -k
      - name: Upload the month summary
        run: |-
          gsutil -h ""Cache-Control: public, max-age=3600"" cp tests/node_compat/summary.json.gz gs://dl.deno.land/node-compat-test/summary-$(date +%Y-%m).json.gz
      - name: Post message to slack channel
        run: deno -A --config tests/config/deno.json tests/node_compat/slack.ts
        env:
          SLACK_TOKEN: ${{ secrets.NODE_COMPAT_SLACK_TOKEN }} # NodeCompat bot
          SLACK_CHANNEL: ${{ secrets.NODE_COMPAT_SLACK_CHANNEL }} # #node-compat channel
",88,2,2,"schedule, workflow_dispatch",10
denoland/deno,npm_publish.yml,"name: npm_publish

on:
  workflow_dispatch:
    inputs:
      version:
        description: 'Version'
        type: string
  release:
    types: [published]

permissions:
  id-token: write

jobs:
  build:
    name: npm publish
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Configure git
        run: |
          git config --global core.symlinks true
          git config --global fetch.parallel 32

      - name: Clone repository
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Install Deno
        uses: denoland/setup-deno@v2
        with:
          deno-version: v2.x
      - name: Install Node
        uses: actions/setup-node@v4
        with:
          node-version: '22.x'
          registry-url: 'https://registry.npmjs.org'

      - name: Publish
        env:
          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}
        run: ./tools/release/npm/build.ts ${{ github.event.inputs.version }} --publish
",45,1,2,"workflow_dispatch, release",3
denoland/deno,post_publish.yml,"name: post_publish

on:
  release:
    types: [published]

jobs:
  update-dl-version:
    name: update dl.deno.land version
    runs-on: ubuntu-latest
    if: github.repository == 'denoland/deno'
    steps:
      - name: Clone repository
        uses: actions/checkout@v4
      - uses: denoland/setup-deno@v2
        with:
          deno-version: v2.x
      - name: Authenticate with Google Cloud
        uses: google-github-actions/auth@v1
        with:
          project_id: denoland
          credentials_json: ${{ secrets.GCP_SA_KEY }}
          export_environment_variables: true
          create_credentials_file: true

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v1
        with:
          project_id: denoland

      - name: Upload version file to dl.deno.land
        run: |
          echo ${GITHUB_REF#refs/*/} > release-latest.txt
          (deno run --allow-net tools/release/version_greater_latest.ts ${GITHUB_REF#refs/*/} || exit 0) && gsutil -h ""Cache-Control: no-cache"" cp release-latest.txt gs://dl.deno.land/release-latest.txt
",34,1,1,release,4
denoland/deno,promote_to_release.yml,"name: promote_to_release

on:
  workflow_dispatch:
    inputs:
      releaseKind:
        description: 'Kind of release'
        type: choice
        options:
          - rc
          - lts
        required: true
      commitHash:
        description: Commit to promote to release
        required: true

jobs:
  # Handle Windows binary patching and re-signing in a separate job
  promote-to-release-windows:
    name: Promote Windows to Release
    runs-on: windows-latest
    if: github.repository == 'denoland/deno'
    permissions:
      contents: write
      id-token: write # Required for GitHub OIDC with Azure for code signing
    environment:
      name: build
    steps:
      - name: Clone repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.DENOBOT_PAT }}
          submodules: recursive

      - name: Install deno
        uses: denoland/setup-deno@v2
        with:
          deno-version: v2.x

      - name: Download Windows binaries
        run: |
          $CANARY_URL=""https://dl.deno.land/canary/${{github.event.inputs.commitHash}}""
          Invoke-WebRequest -Uri ""$CANARY_URL/deno-x86_64-pc-windows-msvc.zip"" -OutFile ""deno-windows.zip""
          Invoke-WebRequest -Uri ""$CANARY_URL/denort-x86_64-pc-windows-msvc.zip"" -OutFile ""denort-windows.zip""
          Expand-Archive -Path ""deno-windows.zip"" -DestinationPath "".""
          Expand-Archive -Path ""denort-windows.zip"" -DestinationPath "".""

      - name: Run patchver for Windows
        shell: pwsh
        run: |
          deno install -A -n patchver https://deno.land/x/patchver@0.2.0/cli.ts
          $CHANNEL=""${{github.event.inputs.releaseKind}}""
          # Patch deno.exe
          Move-Item -Path ""deno.exe"" -Destination ""deno_original.exe""
          patchver ""deno_original.exe"" ""deno.exe"" $CHANNEL
          # Patch denort.exe
          Move-Item -Path ""denort.exe"" -Destination ""denort_original.exe""
          patchver ""denort_original.exe"" ""denort.exe"" $CHANNEL

          # Rename files to match expected pattern
          Move-Item -Path ""deno.exe"" -Destination ""deno-x86_64-pc-windows-msvc-$CHANNEL.exe""
          Move-Item -Path ""denort.exe"" -Destination ""denort-x86_64-pc-windows-msvc-$CHANNEL.exe""

      - name: Authenticate with Azure
        uses: azure/login@v1
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          enable-AzPSSession: true

      - name: Code sign deno.exe
        uses: azure/trusted-signing-action@v0
        with:
          endpoint: https://eus.codesigning.azure.net/
          trusted-signing-account-name: deno-cli-code-signing
          certificate-profile-name: deno-cli-code-signing-cert
          files-folder: .
          files-folder-filter: deno-x86_64-pc-windows-msvc-${{github.event.inputs.releaseKind}}.exe
          file-digest: SHA256
          timestamp-rfc3161: http://timestamp.acs.microsoft.com
          timestamp-digest: SHA256
          exclude-environment-credential: true
          exclude-workload-identity-credential: true
          exclude-managed-identity-credential: true
          exclude-shared-token-cache-credential: true
          exclude-visual-studio-credential: true
          exclude-visual-studio-code-credential: true
          exclude-azure-cli-credential: false

      - name: Verify signature
        shell: pwsh
        run: |
          $SignTool = Get-ChildItem -Path ""C:\Program Files*\Windows Kits\*\bin\*\x64\signtool.exe"" -Recurse -ErrorAction SilentlyContinue | Select-Object -First 1
          $SignToolPath = $SignTool.FullName
          & $SignToolPath verify /pa /v ""deno-x86_64-pc-windows-msvc-${{github.event.inputs.releaseKind}}.exe""

      - name: Create archives
        run: |
          Compress-Archive -Path ""deno-x86_64-pc-windows-msvc-${{github.event.inputs.releaseKind}}.exe"" -DestinationPath ""deno-x86_64-pc-windows-msvc.zip"" -Force
          Compress-Archive -Path ""denort-x86_64-pc-windows-msvc-${{github.event.inputs.releaseKind}}.exe"" -DestinationPath ""denort-x86_64-pc-windows-msvc.zip"" -Force

      - name: Upload Windows archives
        uses: actions/upload-artifact@v4
        with:
          name: windows-binaries
          path: |
            deno-x86_64-pc-windows-msvc.zip
            denort-x86_64-pc-windows-msvc.zip

  # Handle all other binaries on macOS
  promote-to-release:
    name: Promote to Release
    runs-on: macOS-latest
    if: github.repository == 'denoland/deno'
    needs: promote-to-release-windows
    steps:
      - name: Clone repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.DENOBOT_PAT }}
          submodules: recursive

      - name: Authenticate with Google Cloud
        uses: google-github-actions/auth@v1
        with:
          project_id: denoland
          credentials_json: ${{ secrets.GCP_SA_KEY }}
          export_environment_variables: true
          create_credentials_file: true

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v1
        with:
          project_id: denoland

      - name: Install deno
        uses: denoland/setup-deno@v2
        with:
          deno-version: v2.x

      - name: Install rust-codesign
        run: |-
          ./tools/install_prebuilt.js rcodesign
          echo $GITHUB_WORKSPACE/third_party/prebuilt/mac >> $GITHUB_PATH

      - name: Promote to Release (non-Windows)
        env:
          APPLE_CODESIGN_KEY: '${{ secrets.APPLE_CODESIGN_KEY }}'
          APPLE_CODESIGN_PASSWORD: '${{ secrets.APPLE_CODESIGN_PASSWORD }}'
        run: |
          deno run -A ./tools/release/promote_to_release.ts ${{github.event.inputs.releaseKind}} ${{github.event.inputs.commitHash}}

      - name: Download Windows binaries
        uses: actions/download-artifact@v4
        with:
          name: windows-binaries
          path: .

      - name: Create version file
        run: |
          # Unzip a binary to get the version
          unzip -o deno-x86_64-apple-darwin.zip
          DENO_VERSION=$(./deno -V | cut -d ' ' -f 2 | cut -d '+' -f 1)
          echo ""v${DENO_VERSION}"" > release-${{github.event.inputs.releaseKind}}-latest.txt
          rm -f ./deno

      - name: Upload archives to dl.deno.land
        run: |
          gsutil -h ""Cache-Control: public, max-age=3600"" cp ./*.zip gs://dl.deno.land/release/$(cat release-${{github.event.inputs.releaseKind}}-latest.txt)/
          gsutil -h ""Cache-Control: no-cache"" cp release-${{github.event.inputs.releaseKind}}-latest.txt gs://dl.deno.land/release-${{github.event.inputs.releaseKind}}-latest.txt
",171,2,1,workflow_dispatch,10
denoland/deno,start_release.yml,"name: start_release

on:
  workflow_dispatch:
    inputs:
      releaseKind:
        description: 'Kind of release'
        default: 'patch'
        type: choice
        options:
          - patch
          - minor
          - major
        required: true

jobs:
  build:
    name: start release
    runs-on: ubuntu-24.04
    timeout-minutes: 30

    env:
      CARGO_TERM_COLOR: always
      RUST_BACKTRACE: full
      RUSTC_FORCE_INCREMENTAL: 1

    steps:
      - name: Configure git
        run: |
          git config --global core.symlinks true
          git config --global fetch.parallel 32

      - name: Clone repository
        uses: actions/checkout@v4

      - name: Install deno
        uses: denoland/setup-deno@v2
        with:
          deno-version: v2.x

      - name: Create Gist URL
        env:
          GITHUB_TOKEN: ${{ secrets.DENOBOT_GIST_PAT }}
          GH_WORKFLOW_ACTOR: ${{ github.actor }}
        run: ./tools/release/00_start_release.ts --${{github.event.inputs.releaseKind}}
",45,1,1,workflow_dispatch,2
denoland/deno,version_bump.yml,"name: version_bump

on:
  workflow_dispatch:
    inputs:
      releaseKind:
        description: 'Kind of version bump'
        default: 'patch'
        type: choice
        options:
          - patch
          - minor
          - major
          - rc
        required: true

jobs:
  build:
    name: version bump
    runs-on: ubuntu-24.04
    timeout-minutes: 90

    env:
      CARGO_TERM_COLOR: always
      RUST_BACKTRACE: full
      RUSTC_FORCE_INCREMENTAL: 1

    steps:
      - name: Configure git
        run: |
          git config --global core.symlinks true
          git config --global fetch.parallel 32

      - name: Clone repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.DENOBOT_PAT }}
          submodules: recursive

      - uses: dsherret/rust-toolchain-file@v1

      - name: Install deno
        uses: denoland/setup-deno@v2
        with:
          deno-version: v2.x

      - name: Run version bump
        run: |
          git remote add upstream https://github.com/denoland/deno
          ./tools/release/01_bump_crate_versions.ts --${{github.event.inputs.releaseKind}}

      - name: Create PR
        env:
          GITHUB_TOKEN: ${{ secrets.DENOBOT_PAT }}
          GH_WORKFLOW_ACTOR: ${{ github.actor }}
        run: |
          git config user.email ""${{ github.actor }}@users.noreply.github.com""
          git config user.name ""${{ github.actor }}""
          ./tools/release/02_create_pr.ts
",59,1,1,workflow_dispatch,3
denoland/deno,wpt_epoch.yml,"# This CI job runs every night and tests all versions of Deno (canary and latest
# stable) across all OSes we support against the `epochs/daily` branch of WPT.

name: wpt_epoch

on:
  schedule:
    # Every night at 0:30 UTC. This is 20 minutes after `epochs/daily` branch is
    # triggered to be created in WPT repo.
    - cron: 30 0 * * *
  workflow_dispatch:

jobs:
  wpt:
    name: wpt / ${{ matrix.os }} / ${{ matrix.deno-version }}
    if: github.repository == 'denoland/deno'
    runs-on: ${{ matrix.os }}
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        deno-version: [v1.x, canary]
        os: [ubuntu-24.04-xl]

    steps:
      - name: Clone repository
        uses: actions/checkout@v4
        with:
          submodules: true
          persist-credentials: false

      - name: Setup Deno
        uses: denoland/setup-deno@v2
        with:
          deno-version: ${{ matrix.deno-version }}

      - name: Install Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Log versions
        run: |
          python --version
          deno --version

      - name: Switch WPT submodule to epochs/daily
        working-directory: tests/wpt/suite/
        shell: bash
        run: |
          git remote set-branches origin '*'
          git fetch origin
          git checkout $(./wpt rev-list --epoch 1d)
          git checkout -b epochs/daily

      - name: Configure hosts file for WPT (unix)
        if: runner.os != 'Windows'
        working-directory: tests/wpt/suite/
        run: ./wpt make-hosts-file | sudo tee -a /etc/hosts

      - name: Configure hosts file for WPT (windows)
        if: runner.os == 'Windows'
        working-directory: tests/wpt/suite/
        run: python wpt make-hosts-file | Out-File $env:SystemRoot\System32\drivers\etc\hosts -Encoding ascii -Append

      - name: Run web platform tests
        shell: bash
        run: |
          deno run -A --lock=tools/deno.lock.json --config=tests/config/deno.json \
            ./tests/wpt/wpt.ts setup
          deno run -A --lock=tools/deno.lock.json --config=tests/config/deno.json \
            ./tests/wpt/wpt.ts run                                             \                                                \
            --binary=$(which deno) --quiet --release --no-ignore --json=wpt.json --wptreport=wptreport.json --exit-zero

      - name: Upload wpt results to wpt.fyi
        env:
          WPT_FYI_USER: deno
          WPT_FYI_PW: ${{ secrets.WPT_FYI_PW }}
        run: |
          deno run -A --lock=tools/deno.lock.json ./tools/upload_wptfyi.js wptreport.json --from-raw-file --daily-run
",80,1,2,"schedule, workflow_dispatch",3
facebook/create-react-app,build-and-test.yml,"name: 'Build & Test'

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  build:
    name: 'Build (${{ matrix.os }}, Node ${{ matrix.node }})'
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os:
          - 'ubuntu-latest'
        node:
          - '16'
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: ${{ matrix.node }}
          cache: 'npm'
      - name: Install dependencies
        run: npm ci --prefer-offline
      - name: Build
        run: npm run build

  integration:
    name: 'Integration Tests (${{ matrix.os }}, Node ${{ matrix.node }})'
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os:
          - 'ubuntu-latest'
          - 'macos-latest'
          - 'windows-latest'
        node:
          - '16'
    steps:
      - uses: actions/checkout@v3
      - name: Setup node
        uses: actions/setup-node@v3
        with:
          node-version: ${{ matrix.node }}
          cache: 'npm'
      - name: Install dependencies
        run: npm ci --prefer-offline
      # The integration tests are run with yarn, so we need to install it.
      - name: Install yarn
        run: npm i -g yarn
      - name: Run integration tests
        run: npm run test:integration

  e2e-simple:
    name: E2E Simple
    uses: ./.github/workflows/e2e-base.yml
    with:
      testScript: 'tasks/e2e-simple.sh'

  e2e-installs:
    name: E2E Installs
    uses: ./.github/workflows/e2e-base.yml
    with:
      testScript: 'tasks/e2e-installs.sh'

  e2e-kitchensink:
    name: E2E Kitchensink
    uses: ./.github/workflows/e2e-base.yml
    with:
      testScript: 'tasks/e2e-kitchensink.sh'
",76,5,2,"push, pull_request",7
facebook/create-react-app,e2e-base.yml,"on:
  workflow_call:
    inputs:
      testScript:
        required: true
        type: string

name: E2E

jobs:
  test:
    name: 'Test (${{ matrix.os }}, Node ${{ matrix.node }})'
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os:
          - 'ubuntu-latest'
        node:
          - '16'
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: ${{ matrix.node }}
          cache: 'npm'
      - name: Install
        run: npm ci --prefer-offline
      - name: Initialize Global Git config
        run: |
          git config --global core.autocrlf false
          git config --global user.name ""Create React App""
          git config --global user.email ""cra@email.com""
      - name: Run tests
        run: ${{ inputs.testScript }}
",35,1,1,workflow_call,2
facebook/create-react-app,lint.yml,"name: Lint

on: [push, pull_request]

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: '16'
          cache: 'npm'
      - name: Install
        run: npm ci --prefer-offline
      - name: Alex
        run: npm run alex
      - name: Prettier
        run: npm run prettier -- --list-different
      - name: Eslint
        run: npm run eslint -- --max-warnings 0
",21,1,2,"push, pull_request",2
goldbergyoni/nodebestpractices,automerge-prs.yml,"name: automerge
on:
  pull_request:
    types:
      - labeled
      - unlabeled
      - synchronize
      - opened
      - edited
      - ready_for_review
      - reopened
      - unlocked
  pull_request_review:
    types:
      - submitted
  check_suite:
    types:
      - completed
  status: {}
jobs:
  automerge:
    runs-on: ubuntu-20.04
    steps:
      - name: automerge
        uses: ""pascalgn/automerge-action@v0.15.5""
        env:
          GITHUB_TOKEN: ""${{ secrets.GITHUB_TOKEN }}""
          MERGE_LABELS: ""auto-merge,!work in progress""
          MERGE_REMOVE_LABELS: ""auto-merge""
          MERGE_FORKS: ""false""
          MERGE_RETRIES: ""6""
          MERGE_RETRY_SLEEP: ""10000""
          MERGE_DELETE_BRANCH: ""true""
",33,1,4,"pull_request, pull_request_review, check_suite, status",1
goldbergyoni/nodebestpractices,lint-and-generate-html-from-markdown.yml,"name: Lint & Generate HTML from Markdown
on:
  push:
    branches:
      - master
  pull_request:

defaults:
  run:
    shell: bash
    working-directory: .operations

jobs:
  lint:
    name: Lint
    runs-on: ubuntu-20.04
    env:
      NODE_ENV: test

    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Setup Node.js environment
        uses: actions/setup-node@v3
        with:
          node-version: ""18""

      - run: npm install
      - run: npm run lint
",30,1,2,"push, pull_request",2
goldbergyoni/nodebestpractices,update-date-in-last-update-badge.yml,"name: Update date in last update badge
on:
  push:
    branches:
      - master

jobs:
  run:
    name: Update the date in last update badge to today
    runs-on: ubuntu-20.04

    # Limit this action to only run on the main repo and not on forks
    if: github.repository_owner == 'goldbergyoni'
    
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Update last update badge
        run: |
          # Make file runnable
          chmod +x ""${GITHUB_WORKSPACE}/.github/workflows/update-last-update-badge.sh""
          # Run script
          ""${GITHUB_WORKSPACE}/.github/workflows/update-last-update-badge.sh"" ""${GITHUB_WORKSPACE}/README.md""

      - name: Commit & Create Pull Request
        uses: peter-evans/create-pull-request@v4
        with:
          commit-message: update the last update badge to today [skip ci]
          author: Update Last Update Badge Action <${{ github.actor }}@users.noreply.github.com>
          branch: update-last-update-badge
          delete-branch: true
          title: 'Update last update badge to today [skip ci]'
          labels: |
            update-last-update-badge
            auto-merge
          
          # Force empty body as the action have default body
          body: ''
",39,1,1,push,2
excalidraw/excalidraw,autorelease-excalidraw.yml,"name: Auto release excalidraw next
on:
  push:
    branches:
      - release

jobs:
  Auto-release-excalidraw-next:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v2
        with:
          fetch-depth: 2
      - name: Setup Node.js 18.x
        uses: actions/setup-node@v2
        with:
          node-version: 18.x
      - name: Set up publish access
        run: |
          npm config set //registry.npmjs.org/:_authToken ${NPM_TOKEN}
        env:
          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}
      - name: Auto release
        run: |
          yarn add @actions/core -W
          yarn autorelease
",27,1,1,push,2
excalidraw/excalidraw,autorelease-preview.yml,"name: Auto release excalidraw preview
on:
  issue_comment:
    types: [created, edited]

jobs:
  Auto-release-excalidraw-preview:
    name: Auto release preview
    if: github.event.comment.body == '@excalibot trigger release' && github.event.issue.pull_request
    runs-on: ubuntu-latest
    steps:
      - name: React to release comment
        uses: peter-evans/create-or-update-comment@v1
        with:
          token: ${{ secrets.PUSH_TRANSLATIONS_COVERAGE_PAT }}
          comment-id: ${{ github.event.comment.id }}
          reactions: ""+1""
      - name: Get PR SHA
        id: sha
        uses: actions/github-script@v4
        with:
          result-encoding: string
          script: |
            const { owner, repo, number } = context.issue;
            const pr = await github.pulls.get({
              owner,
              repo,
              pull_number: number,
            });
            return pr.data.head.sha
      - uses: actions/checkout@v2
        with:
          ref: ${{ steps.sha.outputs.result }}
          fetch-depth: 2
      - name: Setup Node.js 18.x
        uses: actions/setup-node@v2
        with:
          node-version: 18.x
      - name: Set up publish access
        run: |
          npm config set //registry.npmjs.org/:_authToken ${NPM_TOKEN}
        env:
          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}
      - name: Auto release preview
        id: ""autorelease""
        run: |
          yarn add @actions/core -W
          yarn autorelease preview ${{ github.event.issue.number }}
      - name: Post comment post release
        if: always()
        uses: peter-evans/create-or-update-comment@v1
        with:
          token: ${{ secrets.PUSH_TRANSLATIONS_COVERAGE_PAT }}
          issue-number: ${{ github.event.issue.number }}
          body: ""@${{ github.event.comment.user.login }} ${{ steps.autorelease.outputs.result }}""
",55,1,1,issue_comment,5
excalidraw/excalidraw,build-docker.yml,"name: Build Docker image

on:
  push:
    branches:
      - release

jobs:
  build-docker:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - run: docker build -t excalidraw .
",13,1,1,push,1
excalidraw/excalidraw,cancel.yml,"name: Cancel previous runs

on:
  push:
    branches:
      - release
  pull_request:

jobs:
  cancel:
    runs-on: ubuntu-latest
    timeout-minutes: 3
    steps:
      - uses: styfle/cancel-workflow-action@0.6.0
        with:
          workflow_id: 400555, 400556, 905313, 1451724, 1710116, 3185001, 3438604
          access_token: ${{ secrets.GITHUB_TOKEN }}
",17,1,2,"push, pull_request",1
excalidraw/excalidraw,lint.yml,"name: Lint

on: pull_request

jobs:
  lint:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v2

      - name: Setup Node.js 18.x
        uses: actions/setup-node@v2
        with:
          node-version: 18.x

      - name: Install and lint
        run: |
          yarn install
          yarn test:other
          yarn test:code
          yarn test:typecheck
",22,1,1,pull_request,2
excalidraw/excalidraw,locales-coverage.yml,"name: Build locales coverage

on:
  push:
    branches:
      - l10n_master

jobs:
  locales:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.PUSH_TRANSLATIONS_COVERAGE_PAT }}

      - name: Setup Node.js 18.x
        uses: actions/setup-node@v2
        with:
          node-version: 18.x

      - name: Create report file
        run: |
          yarn locales-coverage
          FILE_CHANGED=$(git diff packages/excalidraw/locales/percentages.json)
          if [ ! -z ""${FILE_CHANGED}"" ]; then
            git config --global user.name 'Excalidraw Bot'
            git config --global user.email 'bot@excalidraw.com'
            git add packages/excalidraw/locales/percentages.json
            git commit -am ""Auto commit: Calculate translation coverage""
            git push
          fi
      - name: Construct comment body
        id: getCommentBody
        run: |
          body=$(npm run locales-coverage:description | grep '^[^>]')
          body=""${body//'%'/'%25'}""
          body=""${body//$'\n'/'%0A'}""
          body=""${body//$'\r'/'%0D'}""
          echo ::set-output name=body::$body

      - name: Update description with coverage
        uses: kt3k/update-pr-description@v1.0.1
        with:
          pr_body: ${{ steps.getCommentBody.outputs.body }}
          pr_title: ""chore: Update translations from Crowdin""
          github_token: ${{ secrets.PUSH_TRANSLATIONS_COVERAGE_PAT }}
",47,1,1,push,3
excalidraw/excalidraw,publish-docker.yml,"name: Publish Docker

on:
  push:
    branches:
      - release

jobs:
  publish-docker:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      - name: Login to DockerHub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}
      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: excalidraw/excalidraw:latest
          platforms: linux/amd64, linux/arm64, linux/arm/v7
",30,1,1,push,5
excalidraw/excalidraw,semantic-pr-title.yml,"name: Semantic PR title

on:
  pull_request:
    types:
      - opened
      - edited
      - synchronize

jobs:
  semantic:
    runs-on: ubuntu-latest
    steps:
      - uses: amannn/action-semantic-pull-request@v5
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
",16,1,1,pull_request,1
excalidraw/excalidraw,sentry-production.yml,"name: New Sentry production release

on:
  push:
    branches:
      - release

jobs:
  sentry:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Setup Node.js 18.x
        uses: actions/setup-node@v2
        with:
          node-version: 18.x
      - name: Install and build
        run: |
          yarn --frozen-lockfile
          yarn build:app
        env:
          CI: true
      - name: Install Sentry
        run: |
          curl -sL https://sentry.io/get-cli/ | bash
      - name: Create new Sentry release
        run: |
          export SENTRY_RELEASE=$(sentry-cli releases propose-version)
          sentry-cli releases new $SENTRY_RELEASE --project $SENTRY_PROJECT
          sentry-cli releases set-commits --auto $SENTRY_RELEASE
          sentry-cli releases files $SENTRY_RELEASE upload-sourcemaps --no-rewrite ./build/static/js/ --url-prefix ""~/static/js""
          sentry-cli releases finalize $SENTRY_RELEASE
          sentry-cli releases deploys $SENTRY_RELEASE new -e production
        env:
          SENTRY_AUTH_TOKEN: ${{ secrets.SENTRY_AUTH_TOKEN }}
          SENTRY_ORG: ${{ secrets.SENTRY_ORG }}
          SENTRY_PROJECT: ${{ secrets.SENTRY_PROJECT }}
",37,1,1,push,2
excalidraw/excalidraw,size-limit.yml,"name: ""Bundle Size check @excalidraw/excalidraw""
on:
  pull_request:
    branches:
      - master
jobs:
  size:
    runs-on: ubuntu-latest
    env:
      CI_JOB_NUMBER: 1
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      - name: Setup Node.js 18.x
        uses: actions/setup-node@v3
        with:
          node-version: 18.x
      - name: Install in packages/excalidraw
        run: yarn
        working-directory: packages/excalidraw
        env:
          CI: true
      - uses: andresz1/size-limit-action@v1
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          build_script: build:esm
          skip_step: install
          directory: packages/excalidraw
",28,1,1,pull_request,3
excalidraw/excalidraw,test-coverage-pr.yml,"name: Test Coverage PR
on:
  pull_request:

jobs:
  coverage:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write

    steps:
      - uses: actions/checkout@v2
      - name: ""Install Node""
        uses: actions/setup-node@v2
        with:
          node-version: ""18.x""
      - name: ""Install Deps""
        run: yarn install
      - name: ""Test Coverage""
        run: yarn test:coverage
      - name: ""Report Coverage""
        if: always() # Also generate the report if tests are failing
        uses: davelosert/vitest-coverage-report-action@v2
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
",26,1,1,pull_request,3
excalidraw/excalidraw,test.yml,"name: Tests

on:
  push:
    branches: master

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup Node.js 18.x
        uses: actions/setup-node@v4
        with:
          node-version: 18.x
      - name: Install and test
        run: |
          yarn install
          yarn test:app
",19,1,1,push,2
open-webui/open-webui,build-release.yml,"name: Release

on:
  push:
    branches:
      - main # or whatever branch you want to use

jobs:
  release:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Check for changes in package.json
        run: |
          git diff --cached --diff-filter=d package.json || {
            echo ""No changes to package.json""
            exit 1
          }

      - name: Get version number from package.json
        id: get_version
        run: |
          VERSION=$(jq -r '.version' package.json)
          echo ""::set-output name=version::$VERSION""

      - name: Extract latest CHANGELOG entry
        id: changelog
        run: |
          CHANGELOG_CONTENT=$(awk 'BEGIN {print_section=0;} /^## \[/ {if (print_section == 0) {print_section=1;} else {exit;}} print_section {print;}' CHANGELOG.md)
          CHANGELOG_ESCAPED=$(echo ""$CHANGELOG_CONTENT"" | sed ':a;N;$!ba;s/\n/%0A/g')
          echo ""Extracted latest release notes from CHANGELOG.md:"" 
          echo -e ""$CHANGELOG_CONTENT"" 
          echo ""::set-output name=content::$CHANGELOG_ESCAPED""

      - name: Create GitHub release
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const changelog = `${{ steps.changelog.outputs.content }}`;
            const release = await github.rest.repos.createRelease({
              owner: context.repo.owner,
              repo: context.repo.repo,
              tag_name: `v${{ steps.get_version.outputs.version }}`,
              name: `v${{ steps.get_version.outputs.version }}`,
              body: changelog,
            })
            console.log(`Created release ${release.data.html_url}`)

      - name: Upload package to GitHub release
        uses: actions/upload-artifact@v4
        with:
          name: package
          path: |
            .
            !.git
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Trigger Docker build workflow
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'docker-build.yaml',
              ref: 'v${{ steps.get_version.outputs.version }}',
            })
",72,1,1,push,4
open-webui/open-webui,deploy-to-hf-spaces.yml,"name: Deploy to HuggingFace Spaces

on:
  push:
    branches:
      - dev
      - main
  workflow_dispatch:

jobs:
  check-secret:
    runs-on: ubuntu-latest
    outputs:
      token-set: ${{ steps.check-key.outputs.defined }}
    steps:
      - id: check-key
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        if: ""${{ env.HF_TOKEN != '' }}""
        run: echo ""defined=true"" >> $GITHUB_OUTPUT

  deploy:
    runs-on: ubuntu-latest
    needs: [check-secret]
    if: needs.check-secret.outputs.token-set == 'true'
    env:
      HF_TOKEN: ${{ secrets.HF_TOKEN }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Remove git history
        run: rm -rf .git

      - name: Prepend YAML front matter to README.md
        run: |
          echo ""---"" > temp_readme.md
          echo ""title: Open WebUI"" >> temp_readme.md
          echo ""emoji: 🐳"" >> temp_readme.md
          echo ""colorFrom: purple"" >> temp_readme.md
          echo ""colorTo: gray"" >> temp_readme.md
          echo ""sdk: docker"" >> temp_readme.md
          echo ""app_port: 8080"" >> temp_readme.md
          echo ""---"" >> temp_readme.md
          cat README.md >> temp_readme.md
          mv temp_readme.md README.md

      - name: Configure git
        run: |
          git config --global user.email ""41898282+github-actions[bot]@users.noreply.github.com""
          git config --global user.name ""github-actions[bot]""
      - name: Set up Git and push to Space
        run: |
          git init --initial-branch=main
          git lfs install
          git lfs track ""*.ttf""
          git lfs track ""*.jpg""
          rm demo.gif
          git add .
          git commit -m ""GitHub deploy: ${{ github.sha }}""
          git push --force https://open-webui:${HF_TOKEN}@huggingface.co/spaces/open-webui/open-webui main
",63,2,2,"push, workflow_dispatch",1
open-webui/open-webui,docker-build.yaml,"name: Create and publish Docker images with specific build args

on:
  workflow_dispatch:
  push:
    branches:
      - main
      - dev
    tags:
      - v*

env:
  REGISTRY: ghcr.io

jobs:
  build-main-image:
    runs-on: ${{ matrix.runner }}
    permissions:
      contents: read
      packages: write
    strategy:
      fail-fast: false
      matrix:
        include:
          - platform: linux/amd64
            runner: ubuntu-latest
          - platform: linux/arm64
            runner: ubuntu-24.04-arm

    steps:
      # GitHub Packages requires the entire repository name to be in lowercase
      # although the repository owner has a lowercase username, this prevents some people from running actions after forking
      - name: Set repository and image name to lowercase
        run: |
          echo ""IMAGE_NAME=${IMAGE_NAME,,}"" >>${GITHUB_ENV}
          echo ""FULL_IMAGE_NAME=ghcr.io/${IMAGE_NAME,,}"" >>${GITHUB_ENV}
        env:
          IMAGE_NAME: '${{ github.repository }}'

      - name: Prepare
        run: |
          platform=${{ matrix.platform }}
          echo ""PLATFORM_PAIR=${platform//\//-}"" >> $GITHUB_ENV

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to the Container registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata for Docker images (default latest tag)
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.FULL_IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=tag
            type=sha,prefix=git-
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
          flavor: |
            latest=${{ github.ref == 'refs/heads/main' }}

      - name: Extract metadata for Docker cache
        id: cache-meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.FULL_IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            ${{ github.ref_type == 'tag' && 'type=raw,value=main' || '' }}
          flavor: |
            prefix=cache-${{ matrix.platform }}-
            latest=false

      - name: Build Docker image (latest)
        uses: docker/build-push-action@v5
        id: build
        with:
          context: .
          push: true
          platforms: ${{ matrix.platform }}
          labels: ${{ steps.meta.outputs.labels }}
          outputs: type=image,name=${{ env.FULL_IMAGE_NAME }},push-by-digest=true,name-canonical=true,push=true
          cache-from: type=registry,ref=${{ steps.cache-meta.outputs.tags }}
          cache-to: type=registry,ref=${{ steps.cache-meta.outputs.tags }},mode=max
          build-args: |
            BUILD_HASH=${{ github.sha }}

      - name: Export digest
        run: |
          mkdir -p /tmp/digests
          digest=""${{ steps.build.outputs.digest }}""
          touch ""/tmp/digests/${digest#sha256:}""

      - name: Upload digest
        uses: actions/upload-artifact@v4
        with:
          name: digests-main-${{ env.PLATFORM_PAIR }}
          path: /tmp/digests/*
          if-no-files-found: error
          retention-days: 1

  build-cuda-image:
    runs-on: ${{ matrix.runner }}
    permissions:
      contents: read
      packages: write
    strategy:
      fail-fast: false
      matrix:
        include:
          - platform: linux/amd64
            runner: ubuntu-latest
          - platform: linux/arm64
            runner: ubuntu-24.04-arm

    steps:
      # GitHub Packages requires the entire repository name to be in lowercase
      # although the repository owner has a lowercase username, this prevents some people from running actions after forking
      - name: Set repository and image name to lowercase
        run: |
          echo ""IMAGE_NAME=${IMAGE_NAME,,}"" >>${GITHUB_ENV}
          echo ""FULL_IMAGE_NAME=ghcr.io/${IMAGE_NAME,,}"" >>${GITHUB_ENV}
        env:
          IMAGE_NAME: '${{ github.repository }}'

      - name: Prepare
        run: |
          platform=${{ matrix.platform }}
          echo ""PLATFORM_PAIR=${platform//\//-}"" >> $GITHUB_ENV

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to the Container registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata for Docker images (cuda tag)
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.FULL_IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=tag
            type=sha,prefix=git-
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=raw,enable=${{ github.ref == 'refs/heads/main' }},prefix=,suffix=,value=cuda
          flavor: |
            latest=${{ github.ref == 'refs/heads/main' }}
            suffix=-cuda,onlatest=true

      - name: Extract metadata for Docker cache
        id: cache-meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.FULL_IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            ${{ github.ref_type == 'tag' && 'type=raw,value=main' || '' }}
          flavor: |
            prefix=cache-cuda-${{ matrix.platform }}-
            latest=false

      - name: Build Docker image (cuda)
        uses: docker/build-push-action@v5
        id: build
        with:
          context: .
          push: true
          platforms: ${{ matrix.platform }}
          labels: ${{ steps.meta.outputs.labels }}
          outputs: type=image,name=${{ env.FULL_IMAGE_NAME }},push-by-digest=true,name-canonical=true,push=true
          cache-from: type=registry,ref=${{ steps.cache-meta.outputs.tags }}
          cache-to: type=registry,ref=${{ steps.cache-meta.outputs.tags }},mode=max
          build-args: |
            BUILD_HASH=${{ github.sha }}
            USE_CUDA=true

      - name: Export digest
        run: |
          mkdir -p /tmp/digests
          digest=""${{ steps.build.outputs.digest }}""
          touch ""/tmp/digests/${digest#sha256:}""

      - name: Upload digest
        uses: actions/upload-artifact@v4
        with:
          name: digests-cuda-${{ env.PLATFORM_PAIR }}
          path: /tmp/digests/*
          if-no-files-found: error
          retention-days: 1

  build-cuda126-image:
    runs-on: ${{ matrix.runner }}
    permissions:
      contents: read
      packages: write
    strategy:
      fail-fast: false
      matrix:
        include:
          - platform: linux/amd64
            runner: ubuntu-latest
          - platform: linux/arm64
            runner: ubuntu-24.04-arm

    steps:
      # GitHub Packages requires the entire repository name to be in lowercase
      # although the repository owner has a lowercase username, this prevents some people from running actions after forking
      - name: Set repository and image name to lowercase
        run: |
          echo ""IMAGE_NAME=${IMAGE_NAME,,}"" >>${GITHUB_ENV}
          echo ""FULL_IMAGE_NAME=ghcr.io/${IMAGE_NAME,,}"" >>${GITHUB_ENV}
        env:
          IMAGE_NAME: '${{ github.repository }}'

      - name: Prepare
        run: |
          platform=${{ matrix.platform }}
          echo ""PLATFORM_PAIR=${platform//\//-}"" >> $GITHUB_ENV

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to the Container registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata for Docker images (cuda126 tag)
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.FULL_IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=tag
            type=sha,prefix=git-
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=raw,enable=${{ github.ref == 'refs/heads/main' }},prefix=,suffix=,value=cuda126
          flavor: |
            latest=${{ github.ref == 'refs/heads/main' }}
            suffix=-cuda126,onlatest=true

      - name: Extract metadata for Docker cache
        id: cache-meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.FULL_IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            ${{ github.ref_type == 'tag' && 'type=raw,value=main' || '' }}
          flavor: |
            prefix=cache-cuda126-${{ matrix.platform }}-
            latest=false

      - name: Build Docker image (cuda126)
        uses: docker/build-push-action@v5
        id: build
        with:
          context: .
          push: true
          platforms: ${{ matrix.platform }}
          labels: ${{ steps.meta.outputs.labels }}
          outputs: type=image,name=${{ env.FULL_IMAGE_NAME }},push-by-digest=true,name-canonical=true,push=true
          cache-from: type=registry,ref=${{ steps.cache-meta.outputs.tags }}
          cache-to: type=registry,ref=${{ steps.cache-meta.outputs.tags }},mode=max
          build-args: |
            BUILD_HASH=${{ github.sha }}
            USE_CUDA=true
            USE_CUDA_VER=cu126

      - name: Export digest
        run: |
          mkdir -p /tmp/digests
          digest=""${{ steps.build.outputs.digest }}""
          touch ""/tmp/digests/${digest#sha256:}""

      - name: Upload digest
        uses: actions/upload-artifact@v4
        with:
          name: digests-cuda126-${{ env.PLATFORM_PAIR }}
          path: /tmp/digests/*
          if-no-files-found: error
          retention-days: 1

  build-ollama-image:
    runs-on: ${{ matrix.runner }}
    permissions:
      contents: read
      packages: write
    strategy:
      fail-fast: false
      matrix:
        include:
          - platform: linux/amd64
            runner: ubuntu-latest
          - platform: linux/arm64
            runner: ubuntu-24.04-arm

    steps:
      # GitHub Packages requires the entire repository name to be in lowercase
      # although the repository owner has a lowercase username, this prevents some people from running actions after forking
      - name: Set repository and image name to lowercase
        run: |
          echo ""IMAGE_NAME=${IMAGE_NAME,,}"" >>${GITHUB_ENV}
          echo ""FULL_IMAGE_NAME=ghcr.io/${IMAGE_NAME,,}"" >>${GITHUB_ENV}
        env:
          IMAGE_NAME: '${{ github.repository }}'

      - name: Prepare
        run: |
          platform=${{ matrix.platform }}
          echo ""PLATFORM_PAIR=${platform//\//-}"" >> $GITHUB_ENV

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to the Container registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata for Docker images (ollama tag)
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.FULL_IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=tag
            type=sha,prefix=git-
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=raw,enable=${{ github.ref == 'refs/heads/main' }},prefix=,suffix=,value=ollama
          flavor: |
            latest=${{ github.ref == 'refs/heads/main' }}
            suffix=-ollama,onlatest=true

      - name: Extract metadata for Docker cache
        id: cache-meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.FULL_IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            ${{ github.ref_type == 'tag' && 'type=raw,value=main' || '' }}
          flavor: |
            prefix=cache-ollama-${{ matrix.platform }}-
            latest=false

      - name: Build Docker image (ollama)
        uses: docker/build-push-action@v5
        id: build
        with:
          context: .
          push: true
          platforms: ${{ matrix.platform }}
          labels: ${{ steps.meta.outputs.labels }}
          outputs: type=image,name=${{ env.FULL_IMAGE_NAME }},push-by-digest=true,name-canonical=true,push=true
          cache-from: type=registry,ref=${{ steps.cache-meta.outputs.tags }}
          cache-to: type=registry,ref=${{ steps.cache-meta.outputs.tags }},mode=max
          build-args: |
            BUILD_HASH=${{ github.sha }}
            USE_OLLAMA=true

      - name: Export digest
        run: |
          mkdir -p /tmp/digests
          digest=""${{ steps.build.outputs.digest }}""
          touch ""/tmp/digests/${digest#sha256:}""

      - name: Upload digest
        uses: actions/upload-artifact@v4
        with:
          name: digests-ollama-${{ env.PLATFORM_PAIR }}
          path: /tmp/digests/*
          if-no-files-found: error
          retention-days: 1

  merge-main-images:
    runs-on: ubuntu-latest
    needs: [build-main-image]
    steps:
      # GitHub Packages requires the entire repository name to be in lowercase
      # although the repository owner has a lowercase username, this prevents some people from running actions after forking
      - name: Set repository and image name to lowercase
        run: |
          echo ""IMAGE_NAME=${IMAGE_NAME,,}"" >>${GITHUB_ENV}
          echo ""FULL_IMAGE_NAME=ghcr.io/${IMAGE_NAME,,}"" >>${GITHUB_ENV}
        env:
          IMAGE_NAME: '${{ github.repository }}'

      - name: Download digests
        uses: actions/download-artifact@v4
        with:
          pattern: digests-main-*
          path: /tmp/digests
          merge-multiple: true

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to the Container registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata for Docker images (default latest tag)
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.FULL_IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=tag
            type=sha,prefix=git-
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
          flavor: |
            latest=${{ github.ref == 'refs/heads/main' }}

      - name: Create manifest list and push
        working-directory: /tmp/digests
        run: |
          docker buildx imagetools create $(jq -cr '.tags | map(""-t "" + .) | join("" "")' <<< ""$DOCKER_METADATA_OUTPUT_JSON"") \
            $(printf '${{ env.FULL_IMAGE_NAME }}@sha256:%s ' *)

      - name: Inspect image
        run: |
          docker buildx imagetools inspect ${{ env.FULL_IMAGE_NAME }}:${{ steps.meta.outputs.version }}

  merge-cuda-images:
    runs-on: ubuntu-latest
    needs: [build-cuda-image]
    steps:
      # GitHub Packages requires the entire repository name to be in lowercase
      # although the repository owner has a lowercase username, this prevents some people from running actions after forking
      - name: Set repository and image name to lowercase
        run: |
          echo ""IMAGE_NAME=${IMAGE_NAME,,}"" >>${GITHUB_ENV}
          echo ""FULL_IMAGE_NAME=ghcr.io/${IMAGE_NAME,,}"" >>${GITHUB_ENV}
        env:
          IMAGE_NAME: '${{ github.repository }}'

      - name: Download digests
        uses: actions/download-artifact@v4
        with:
          pattern: digests-cuda-*
          path: /tmp/digests
          merge-multiple: true

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to the Container registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata for Docker images (default latest tag)
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.FULL_IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=tag
            type=sha,prefix=git-
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=raw,enable=${{ github.ref == 'refs/heads/main' }},prefix=,suffix=,value=cuda
          flavor: |
            latest=${{ github.ref == 'refs/heads/main' }}
            suffix=-cuda,onlatest=true

      - name: Create manifest list and push
        working-directory: /tmp/digests
        run: |
          docker buildx imagetools create $(jq -cr '.tags | map(""-t "" + .) | join("" "")' <<< ""$DOCKER_METADATA_OUTPUT_JSON"") \
            $(printf '${{ env.FULL_IMAGE_NAME }}@sha256:%s ' *)

      - name: Inspect image
        run: |
          docker buildx imagetools inspect ${{ env.FULL_IMAGE_NAME }}:${{ steps.meta.outputs.version }}

  merge-cuda126-images:
    runs-on: ubuntu-latest
    needs: [build-cuda126-image]
    steps:
      # GitHub Packages requires the entire repository name to be in lowercase
      # although the repository owner has a lowercase username, this prevents some people from running actions after forking
      - name: Set repository and image name to lowercase
        run: |
          echo ""IMAGE_NAME=${IMAGE_NAME,,}"" >>${GITHUB_ENV}
          echo ""FULL_IMAGE_NAME=ghcr.io/${IMAGE_NAME,,}"" >>${GITHUB_ENV}
        env:
          IMAGE_NAME: '${{ github.repository }}'

      - name: Download digests
        uses: actions/download-artifact@v4
        with:
          pattern: digests-cuda126-*
          path: /tmp/digests
          merge-multiple: true

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to the Container registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata for Docker images (default latest tag)
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.FULL_IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=tag
            type=sha,prefix=git-
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=raw,enable=${{ github.ref == 'refs/heads/main' }},prefix=,suffix=,value=cuda126
          flavor: |
            latest=${{ github.ref == 'refs/heads/main' }}
            suffix=-cuda126,onlatest=true

      - name: Create manifest list and push
        working-directory: /tmp/digests
        run: |
          docker buildx imagetools create $(jq -cr '.tags | map(""-t "" + .) | join("" "")' <<< ""$DOCKER_METADATA_OUTPUT_JSON"") \
            $(printf '${{ env.FULL_IMAGE_NAME }}@sha256:%s ' *)

      - name: Inspect image
        run: |
          docker buildx imagetools inspect ${{ env.FULL_IMAGE_NAME }}:${{ steps.meta.outputs.version }}

  merge-ollama-images:
    runs-on: ubuntu-latest
    needs: [build-ollama-image]
    steps:
      # GitHub Packages requires the entire repository name to be in lowercase
      # although the repository owner has a lowercase username, this prevents some people from running actions after forking
      - name: Set repository and image name to lowercase
        run: |
          echo ""IMAGE_NAME=${IMAGE_NAME,,}"" >>${GITHUB_ENV}
          echo ""FULL_IMAGE_NAME=ghcr.io/${IMAGE_NAME,,}"" >>${GITHUB_ENV}
        env:
          IMAGE_NAME: '${{ github.repository }}'

      - name: Download digests
        uses: actions/download-artifact@v4
        with:
          pattern: digests-ollama-*
          path: /tmp/digests
          merge-multiple: true

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to the Container registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata for Docker images (default ollama tag)
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.FULL_IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=tag
            type=sha,prefix=git-
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=raw,enable=${{ github.ref == 'refs/heads/main' }},prefix=,suffix=,value=ollama
          flavor: |
            latest=${{ github.ref == 'refs/heads/main' }}
            suffix=-ollama,onlatest=true

      - name: Create manifest list and push
        working-directory: /tmp/digests
        run: |
          docker buildx imagetools create $(jq -cr '.tags | map(""-t "" + .) | join("" "")' <<< ""$DOCKER_METADATA_OUTPUT_JSON"") \
            $(printf '${{ env.FULL_IMAGE_NAME }}@sha256:%s ' *)

      - name: Inspect image
        run: |
          docker buildx imagetools inspect ${{ env.FULL_IMAGE_NAME }}:${{ steps.meta.outputs.version }}
",642,8,2,"workflow_dispatch, push",48
open-webui/open-webui,format-backend.yaml,"name: Python CI

on:
  push:
    branches:
      - main
      - dev
    paths:
      - 'backend/**'
      - 'pyproject.toml'
      - 'uv.lock'
  pull_request:
    branches:
      - main
      - dev
    paths:
      - 'backend/**'
      - 'pyproject.toml'
      - 'uv.lock'

jobs:
  build:
    name: 'Format Backend'
    runs-on: ubuntu-latest

    strategy:
      matrix:
        python-version:
          - 3.11.x
          - 3.12.x

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '${{ matrix.python-version }}'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install black

      - name: Format backend
        run: npm run format:backend

      - name: Check for changes after format
        run: git diff --exit-code
",49,1,2,"push, pull_request",2
open-webui/open-webui,format-build-frontend.yaml,"name: Frontend Build

on:
  push:
    branches:
      - main
      - dev
    paths-ignore:
      - 'backend/**'
      - 'pyproject.toml'
      - 'uv.lock'
  pull_request:
    branches:
      - main
      - dev
    paths-ignore:
      - 'backend/**'
      - 'pyproject.toml'
      - 'uv.lock'

jobs:
  build:
    name: 'Format & Build Frontend'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22'

      - name: Install Dependencies
        run: npm install

      - name: Format Frontend
        run: npm run format

      - name: Run i18next
        run: npm run i18n:parse

      - name: Check for Changes After Format
        run: git diff --exit-code

      - name: Build Frontend
        run: npm run build

  test-frontend:
    name: 'Frontend Unit Tests'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22'

      - name: Install Dependencies
        run: npm ci

      - name: Run vitest
        run: npm run test:frontend
",65,2,2,"push, pull_request",4
open-webui/open-webui,release-pypi.yml,"name: Release to PyPI

on:
  push:
    branches:
      - main # or whatever branch you want to use
      - pypi-release

jobs:
  release:
    runs-on: ubuntu-latest
    environment:
      name: pypi
      url: https://pypi.org/p/open-webui
    permissions:
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Install Git
        run: sudo apt-get update && sudo apt-get install -y git
      - uses: actions/setup-node@v4
        with:
          node-version: 22
      - uses: actions/setup-python@v5
        with:
          python-version: 3.11
      - name: Build
        run: |
          python -m pip install --upgrade pip
          pip install build
          python -m build .
      - name: Publish package distributions to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
",36,1,1,push,4
microsoft/terminal,addToProject.yml,"name: Add triaged bugs & tasks to project board
# https://github.com/actions/add-to-project

on:
  issues:
    types:
      - labeled
      - unlabeled

permissions: {}
jobs:
  add-to-project:
    name: Add issue to project
    runs-on: ubuntu-latest
    steps:
      - uses: actions/add-to-project@v1.0.2
        with:
          project-url: https://github.com/orgs/microsoft/projects/159
          github-token: ${{ secrets.ADD_TO_PROJECT_PAT }}
          labeled: Issue-Feature, Needs-Triage, Needs-Author-Feedback, Issue-Scenario
          label-operator: NOT
",21,1,1,issues,1
microsoft/terminal,spelling2.yml,"# spelling.yml is blocked per https://github.com/check-spelling/check-spelling/security/advisories/GHSA-g86g-chm8-7r2p
name: Spell checking

# Comment management is handled through a secondary job, for details see:
# https://github.com/check-spelling/check-spelling/wiki/Feature%3A-Restricted-Permissions
#
# `jobs.comment-push` runs when a push is made to a repository and the `jobs.spelling` job needs to make a comment
#   (in odd cases, it might actually run just to collapse a comment, but that's fairly rare)
#   it needs `contents: write` in order to add a comment.
#
# `jobs.comment-pr` runs when a pull_request is made to a repository and the `jobs.spelling` job needs to make a comment
#   or collapse a comment (in the case where it had previously made a comment and now no longer needs to show a comment)
#   it needs `pull-requests: write` in order to manipulate those comments.

# Updating pull request branches is managed via comment handling.
# For details, see: https://github.com/check-spelling/check-spelling/wiki/Feature:-Update-expect-list
#
# These elements work together to make it happen:
#
# `on.issue_comment`
#   This event listens to comments by users asking to update the metadata.
#
# `jobs.update`
#   This job runs in response to an issue_comment and will push a new commit
#   to update the spelling metadata.
#
# `with.experimental_apply_changes_via_bot`
#   Tells the action to support and generate messages that enable it
#   to make a commit to update the spelling metadata.
#
# `with.ssh_key`
#   In order to trigger workflows when the commit is made, you can provide a
#   secret (typically, a write-enabled github deploy key).
#
#   For background, see: https://github.com/check-spelling/check-spelling/wiki/Feature:-Update-with-deploy-key

# SARIF reporting
#
# Access to SARIF reports is generally restricted (by GitHub) to members of the repository.
#
# Requires enabling `security-events: write`
# and configuring the action with `use_sarif: 1`
#
#   For information on the feature, see: https://github.com/check-spelling/check-spelling/wiki/Feature:-SARIF-output

# Minimal workflow structure:
#
# on:
#   push:
#     ...
#   pull_request_target:
#     ...
# jobs:
#   # you only want the spelling job, all others should be omitted
#   spelling:
#     # remove `security-events: write` and `use_sarif: 1`
#     # remove `experimental_apply_changes_via_bot: 1`
#     ... otherwise adjust the `with:` as you wish

on:
  push:
    branches:
      - ""**""
    tags-ignore:
      - ""**""
  pull_request_target:
    branches:
      - ""**""
    types:
      - ""opened""
      - ""reopened""
      - ""synchronize""
  issue_comment:
    types:
      - ""created""

jobs:
  spelling:
    name: Check Spelling
    permissions:
      contents: read
      pull-requests: read
      actions: read
      security-events: write
    outputs:
      followup: ${{ steps.spelling.outputs.followup }}
    runs-on: ubuntu-latest
    if: ${{ contains(github.event_name, 'pull_request') || github.event_name == 'push' }}
    concurrency:
      group: spelling-${{ github.event.pull_request.number || github.ref }}
      # note: If you use only_check_changed_files, you do not want cancel-in-progress
      cancel-in-progress: true
    steps:
      - name: check-spelling
        id: spelling
        uses: check-spelling/check-spelling@v0.0.25
        with:
          suppress_push_for_open_pull_request: ${{ github.actor != 'dependabot[bot]' && 1 }}
          checkout: true
          check_file_names: 1
          spell_check_this: microsoft/terminal@main
          post_comment: 0
          use_magic_file: 1
          report-timing: 1
          warnings: bad-regex,binary-file,deprecated-feature,ignored-expect-variant,large-file,limited-references,no-newline-at-eof,noisy-file,non-alpha-in-dictionary,token-is-substring,unexpected-line-ending,whitespace-in-dictionary,minified-file,unsupported-configuration,no-files-to-check,unclosed-block-ignore-begin,unclosed-block-ignore-end
          experimental_apply_changes_via_bot: ${{ github.repository_owner != 'microsoft' && 1 }}
          use_sarif: ${{ (!github.event.pull_request || (github.event.pull_request.head.repo.full_name == github.repository)) && 1 }}
          check_extra_dictionaries: """"
          dictionary_source_prefixes: >
            {
            ""cspell"": ""https://raw.githubusercontent.com/check-spelling/cspell-dicts/v20241114/dictionaries/""
            }
          extra_dictionaries: |
            cspell:software-terms/softwareTerms.txt
            cspell:cpp/stdlib-cpp.txt
            cspell:cpp/stdlib-c.txt
            cspell:python/python/python-lib.txt
            cspell:php/php.txt
            cspell:node/node.txt
            cspell:dart/dart.txt
            cspell:filetypes/filetypes.txt
            cspell:java/java.txt
            cspell:css/css.txt
            cspell:dotnet/dotnet.txt
            cspell:npm/npm.txt
            cspell:fullstack/fullstack.txt
            cspell:java/java-terms.txt
            cspell:r/r.txt
            cspell:golang/go.txt
            cspell:cpp/stdlib-cmath.txt
            cspell:typescript/typescript.txt
            cspell:html/html.txt
            cspell:cpp/compiler-msvc.txt
            cspell:django/django.txt
            cspell:aws/aws.txt
            cspell:python/common/extra.txt
            cspell:cpp/ecosystem.txt
            cspell:cpp/lang-keywords.txt
            cspell:csharp/csharp.txt
            cspell:cpp/compiler-clang-attributes.txt
            cspell:python/python/python.txt
            cspell:mnemonics/mnemonics.txt
            cspell:powershell/powershell.txt

  comment-push:
    name: Report (Push)
    # If your workflow isn't running on push, you can remove this job
    runs-on: ubuntu-latest
    needs: spelling
    permissions:
      actions: read
      contents: write
    if: (success() || failure()) && needs.spelling.outputs.followup && github.event_name == 'push'
    steps:
      - name: comment
        uses: check-spelling/check-spelling@v0.0.25
        with:
          checkout: true
          spell_check_this: microsoft/terminal@main
          task: ${{ needs.spelling.outputs.followup }}

  comment-pr:
    name: Report (PR)
    # If you workflow isn't running on pull_request*, you can remove this job
    runs-on: ubuntu-latest
    needs: spelling
    permissions:
      actions: read
      contents: read
      pull-requests: write
    if: (success() || failure()) && needs.spelling.outputs.followup && contains(github.event_name, 'pull_request')
    steps:
      - name: comment
        uses: check-spelling/check-spelling@v0.0.25
        with:
          checkout: true
          spell_check_this: microsoft/terminal@main
          task: ${{ needs.spelling.outputs.followup }}
          experimental_apply_changes_via_bot: ${{ github.repository_owner != 'microsoft' && 1 }}

  update:
    name: Update PR
    permissions:
      contents: write
      pull-requests: write
      actions: read
    runs-on: ubuntu-latest
    if: ${{
      github.repository_owner != 'microsoft' &&
      github.event_name == 'issue_comment' &&
      github.event.issue.pull_request &&
      contains(github.event.comment.body, '@check-spelling-bot apply') &&
      contains(github.event.comment.body, 'https://')
      }}
    concurrency:
      group: spelling-update-${{ github.event.issue.number }}
      cancel-in-progress: false
    steps:
      - name: apply spelling updates
        uses: check-spelling/check-spelling@v0.0.25
        with:
          experimental_apply_changes_via_bot: ${{ github.repository_owner != 'microsoft' && 1 }}
          checkout: true
          ssh_key: ""${{ secrets.CHECK_SPELLING }}""
",204,4,3,"push, pull_request_target, issue_comment",4
microsoft/terminal,winget.yml,"name: Publish to WinGet

on:
  release:
    types: [published]

env:
  REGEX: 'Microsoft\.WindowsTerminal(?:Preview)?_([\d.]+)_8wekyb3d8bbwe\.msixbundle$'
  # winget-create will read the following environment variable to access the GitHub token needed for submitting a PR
  # See https://aka.ms/winget-create-token
  WINGET_CREATE_GITHUB_TOKEN: ${{ secrets.WINGET_TOKEN }}

jobs:
  publish:
    runs-on: windows-latest # Action can only run on Windows
    steps:
      - name: Publish Windows Terminal ${{ github.event.release.prerelease && 'Preview' || 'Stable' }}
        run: |
          $assets = '${{ toJSON(github.event.release.assets) }}' | ConvertFrom-Json
          $wingetRelevantAsset = $assets | Where-Object { $_.name -like '*.msixbundle' } | Select-Object -First 1
          $regex = [Regex]::New($env:REGEX)
          $version = $regex.Match($wingetRelevantAsset.name).Groups[1].Value

          $wingetPackage = ""Microsoft.WindowsTerminal${{ github.event.release.prerelease && '.Preview' || '' }}""

          & curl.exe -JLO https://aka.ms/wingetcreate/latest
          & .\wingetcreate.exe update $wingetPackage -s -v $version -u $wingetRelevantAsset.browser_download_url
",27,1,1,release,0
