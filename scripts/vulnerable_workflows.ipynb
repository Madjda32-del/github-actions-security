{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac5f8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/1-GHSA-4mgv-m5cm-f9h7/terraform.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/1-GHSA-4mgv-m5cm-f9h7__terraform.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/4-GHSA-4xqx-pqpj-9fqw/known-vulnerable-actions.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/4-GHSA-4xqx-pqpj-9fqw__known-vulnerable-actions.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/10-GHSA-h3qr-39j9-4r5v/gradle-build.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/10-GHSA-h3qr-39j9-4r5v__gradle-build.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/22-GHSA-vqf5-2xx6-9wfm/codeql-analysis.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/22-GHSA-vqf5-2xx6-9wfm__codeql-analysis.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/27-GHSA-2487-9f55-2vg9/action.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/27-GHSA-2487-9f55-2vg9__action.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/28-GHSA-m32f-fjw2-37v3/bullfrog.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/28-GHSA-m32f-fjw2-37v3__bullfrog.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/17-GHSA-xj87-mqvh-88w2/test.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/17-GHSA-xj87-mqvh-88w2__test.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/10-GHSA-h3qr-39j9-4r5v__gradle-build.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/collected-workflows__10-GHSA-h3qr-39j9-4r5v__gradle-build.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/22-GHSA-vqf5-2xx6-9wfm__codeql-analysis.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/collected-workflows__22-GHSA-vqf5-2xx6-9wfm__codeql-analysis.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/4-GHSA-4xqx-pqpj-9fqw__known-vulnerable-actions.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/collected-workflows__4-GHSA-4xqx-pqpj-9fqw__known-vulnerable-actions.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/1-GHSA-4mgv-m5cm-f9h7__terraform.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/collected-workflows__1-GHSA-4mgv-m5cm-f9h7__terraform.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/28-GHSA-m32f-fjw2-37v3__bullfrog.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/collected-workflows__28-GHSA-m32f-fjw2-37v3__bullfrog.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/27-GHSA-2487-9f55-2vg9__action.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/collected-workflows__27-GHSA-2487-9f55-2vg9__action.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/17-GHSA-xj87-mqvh-88w2__test.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/collected-workflows__17-GHSA-xj87-mqvh-88w2__test.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/18-GHSA-7x29-qqmq-v6qc/main.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/18-GHSA-7x29-qqmq-v6qc__main.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/8-GHSA-6q4m-7476-932w/build-and-upload.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/8-GHSA-6q4m-7476-932w__build-and-upload.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/19-GHSA-cxww-7g56-2vh6/pipeline.yaml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/19-GHSA-cxww-7g56-2vh6__pipeline.yaml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/5-GHSA-f9qj-7gh3-mhj4/run-terraform.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/5-GHSA-f9qj-7gh3-mhj4__run-terraform.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/29-GHSA-phf6-hm3h-x8qp/scalafmt-fix.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/29-GHSA-phf6-hm3h-x8qp__scalafmt-fix.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/2-GHSA-g86g-chm8-7r2p/spelling.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/2-GHSA-g86g-chm8-7r2p__spelling.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/12-GHSA-8v8w-v8xg-79rf/fix-style.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/12-GHSA-8v8w-v8xg-79rf__fix-style.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/21-GHSA-5xr6-xhww-33m4/PublishRelease.yaml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/21-GHSA-5xr6-xhww-33m4__PublishRelease.yaml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/26-GHSA-mxr3-8whj-j74r/code-review.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/26-GHSA-mxr3-8whj-j74r__code-review.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/26-GHSA-mxr3-8whj-j74r/test.yml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/26-GHSA-mxr3-8whj-j74r__test.yml\n",
      "Copied: ../ground-truth-vulnerabilities/Vul_Database/9-GHSA-rg3q-prf8-qxmp/check-wip.yaml -> ../ground-truth-vulnerabilities/Vul_Database/collected-workflows/9-GHSA-rg3q-prf8-qxmp__check-wip.yaml\n",
      "âœ… YAML files collected in 'collected-workflows'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# === Paths ===\n",
    "input_dir = \"../ground-truth-vulnerabilities/Vul_Database\"\n",
    "output_dir = os.path.join(input_dir, \"collected-workflows\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Collect YAML workflow files ===\n",
    "for subdir, _, files in os.walk(input_dir):\n",
    "    for file in files:\n",
    "        if file.endswith((\".yml\", \".yaml\")):\n",
    "            src_path = os.path.join(subdir, file)\n",
    "            rel_dir = os.path.relpath(subdir, input_dir).replace(os.sep, \"-\")\n",
    "            dst_filename = f\"{rel_dir}__{file}\"\n",
    "            dst_path = os.path.join(output_dir, dst_filename)\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "            print(f\"Copied: {src_path} -> {dst_path}\")\n",
    "\n",
    "print(\"YAML files collected in 'collected-workflows'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8239b09",
   "metadata": {},
   "source": [
    "RUN THE TOOLS : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff3d2aa",
   "metadata": {},
   "source": [
    "poutine :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a89bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 13 workflow files to ../ground-truth-vulnerabilities/tools_output/poutine/workflow_with_issues\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Input and output paths\n",
    "input_file = Path(\"../ground-truth-vulnerabilities/tools_output/poutine/findings.json\")\n",
    "output_dir = Path(\"../ground-truth-vulnerabilities/tools_output/poutine/workflow_with_issues\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load raw findings\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "all_findings = data.get(\"findings\", [])\n",
    "\n",
    "# Group findings by workflow file\n",
    "grouped = defaultdict(list)\n",
    "for finding in all_findings:\n",
    "    path = finding.get(\"meta\", {}).get(\"path\")\n",
    "    if path:\n",
    "        workflow_file = Path(path).name\n",
    "        grouped[workflow_file].append(finding)\n",
    "\n",
    "# Save per-workflow JSON files for workflows with issues\n",
    "for workflow_file, findings in grouped.items():\n",
    "    if not findings:\n",
    "        continue\n",
    "\n",
    "    rule_summary = {}\n",
    "    for f in findings:\n",
    "        rule_id = f.get(\"rule_id\")\n",
    "        if rule_id:\n",
    "            rule_summary[rule_id] = rule_summary.get(rule_id, 0) + 1\n",
    "\n",
    "    output_data = {\n",
    "        \"workflow\": workflow_file,\n",
    "        \"tool\": \"poutine\",\n",
    "        \"summary\": {\n",
    "            \"total_findings\": len(findings),\n",
    "            \"by_rule\": rule_summary\n",
    "        },\n",
    "        \"findings\": findings\n",
    "    }\n",
    "\n",
    "    output_path = output_dir / f\"{workflow_file}.json\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(output_data, f_out, indent=2)\n",
    "\n",
    "print(f\"Saved {len(grouped)} workflow files to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e75f011",
   "metadata": {},
   "source": [
    "actionlint :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a25387fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 9 workflow files to ../ground-truth-vulnerabilities/tools_output/actionlint/workflow_with_issues\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Input and output paths\n",
    "input_file = Path(\"../ground-truth-vulnerabilities/tools_output/actionlint/findings.txt\")\n",
    "output_dir = Path(\"../ground-truth-vulnerabilities/tools_output/actionlint/workflow_with_issues\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Parse findings.txt and group by workflow filename\n",
    "grouped = defaultdict(list)\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        # Example format: .github/workflows/deploy.yml:8:1: some issue description\n",
    "        match = re.match(r\"(.+?):(\\d+):(\\d+):\\s+(.*)\", line)\n",
    "        if match:\n",
    "            path, line_num, col_num, message = match.groups()\n",
    "            workflow_file = Path(path).name\n",
    "            grouped[workflow_file].append({\n",
    "                \"line\": int(line_num),\n",
    "                \"column\": int(col_num),\n",
    "                \"message\": message.strip()\n",
    "            })\n",
    "\n",
    "# Save JSON output for workflows with findings\n",
    "for workflow_file, findings in grouped.items():\n",
    "    if not findings:\n",
    "        continue\n",
    "\n",
    "    output_data = {\n",
    "        \"workflow\": workflow_file,\n",
    "        \"tool\": \"actionlint\",\n",
    "        \"summary\": {\n",
    "            \"total_findings\": len(findings)\n",
    "        },\n",
    "        \"findings\": findings\n",
    "    }\n",
    "\n",
    "    output_path = output_dir / f\"{workflow_file}.json\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(output_data, f_out, indent=2)\n",
    "\n",
    "print(f\"Saved {len(grouped)} workflow files to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7923a7c4",
   "metadata": {},
   "source": [
    "frizbee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "060f2a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 14 normalized workflow result files to ../ground-truth-vulnerabilities/tools_output/frizbee/workflow_with_issues\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "original_dir = Path(\"../ground-truth-vulnerabilities/.github/workflows\")\n",
    "modified_dir = Path(\"../ground-truth-vulnerabilities/tools_output/frizbee/modified_workflows\")\n",
    "output_dir = Path(\"../ground-truth-vulnerabilities/tools_output/frizbee/workflow_with_issues\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Pattern to detect pinned actions with hash and version comment\n",
    "pattern = re.compile(r\"uses:\\s+([\\w\\-./]+)@([a-f0-9]{10,})\\s+#\\s*(\\S+)\", re.IGNORECASE)\n",
    "\n",
    "# Gather workflow names directly from modified_workflows directory\n",
    "modified_files = [p.name for p in modified_dir.glob(\"*.yml\")] + [p.name for p in modified_dir.glob(\"*.yaml\")]\n",
    "\n",
    "saved = 0\n",
    "\n",
    "for wf_name in modified_files:\n",
    "    original_path = original_dir / wf_name\n",
    "    modified_path = modified_dir / wf_name\n",
    "\n",
    "    if not original_path.exists() or not modified_path.exists():\n",
    "        print(f\"[!] Skipped: {wf_name} (missing original or modified)\")\n",
    "        continue\n",
    "\n",
    "    original_lines = original_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "    modified_lines = modified_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "    findings = []\n",
    "\n",
    "    # Compare line by line\n",
    "    for i, (orig, mod) in enumerate(zip(original_lines, modified_lines)):\n",
    "        match = pattern.search(mod)\n",
    "        if match:\n",
    "            action, sha, tag = match.groups()\n",
    "            expected_unpinned = f\"{action}@{tag}\"\n",
    "            actual_pinned = f\"{action}@{sha}\"\n",
    "            if expected_unpinned in orig:\n",
    "                findings.append({\n",
    "                    \"rule\": \"unpinned-github-actions\",\n",
    "                    \"original\": expected_unpinned,\n",
    "                    \"pinned\": actual_pinned,\n",
    "                    \"line\": i + 1\n",
    "                })\n",
    "\n",
    "    if not findings:\n",
    "        continue\n",
    "\n",
    "    summary = {\n",
    "        \"total_findings\": len(findings),\n",
    "        \"by_rule\": {\"unpinned-github-actions\": len(findings)}\n",
    "    }\n",
    "\n",
    "    result = {\n",
    "        \"workflow\": wf_name,\n",
    "        \"tool\": \"frizbee\",\n",
    "        \"summary\": summary,\n",
    "        \"findings\": findings\n",
    "    }\n",
    "\n",
    "    with open(output_dir / f\"{wf_name}.json\", \"w\", encoding=\"utf-8\") as out_f:\n",
    "        json.dump(result, out_f, indent=2)\n",
    "        saved += 1\n",
    "\n",
    "print(f\"Saved {saved} normalized workflow result files to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d718d3",
   "metadata": {},
   "source": [
    "scharf :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62b1ca5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 16 workflow result files to ../ground-truth-vulnerabilities/tools_output/scharf/workflow_with_issues\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Input/output paths\n",
    "input_file = Path(\"../ground-truth-vulnerabilities/tools_output/scharf/findings.txt\")\n",
    "output_dir = Path(\"../ground-truth-vulnerabilities/tools_output/scharf/workflow_with_issues\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Regex to remove ANSI escape codes\n",
    "ansi_escape = re.compile(r\"\\x1B\\[[0-?]*[ -/]*[@-~]\")\n",
    "\n",
    "# Read and clean lines\n",
    "lines = []\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for raw in f:\n",
    "        clean = ansi_escape.sub(\"\", raw).strip()\n",
    "        if clean:\n",
    "            lines.append(clean)\n",
    "\n",
    "# Initialize parsing\n",
    "grouped = defaultdict(list)\n",
    "current_workflow = None\n",
    "\n",
    "# Go through each line\n",
    "for line in lines:\n",
    "    # Detect workflow line\n",
    "    if line.endswith(\".yml\") or line.endswith(\".yaml\"):\n",
    "        current_workflow = Path(line).name\n",
    "        continue\n",
    "\n",
    "    # Detect and parse issue line (new format)\n",
    "    if current_workflow and \"[Line\" in line:\n",
    "        match = re.search(r\"\\[Line (\\d+), Col (\\d+)\\] (.+)\", line)\n",
    "        if match:\n",
    "            line_num, col_num, message = match.groups()\n",
    "            grouped[current_workflow].append({\n",
    "                \"line\": int(line_num),\n",
    "                \"column\": int(col_num),\n",
    "                \"message\": message\n",
    "            })\n",
    "        else:\n",
    "            print(f\"[NO MATCH] {line}\")\n",
    "\n",
    "# Save per-workflow JSONs\n",
    "for workflow_file, findings in grouped.items():\n",
    "    output_data = {\n",
    "        \"workflow\": workflow_file,\n",
    "        \"tool\": \"scharf\",\n",
    "        \"summary\": {\n",
    "            \"total_findings\": len(findings),\n",
    "            \"by_rule\": {\n",
    "                \"unpinned-github-actions\": len(findings)\n",
    "            }\n",
    "        },\n",
    "        \"findings\": findings\n",
    "    }\n",
    "\n",
    "    out_path = output_dir / f\"{workflow_file}.json\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(output_data, f_out, indent=2)\n",
    "\n",
    "print(f\"Saved {len(grouped)} workflow result files to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a45f6",
   "metadata": {},
   "source": [
    "pinny "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae9ee108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 unpinned actions in findings.txt\n",
      "Saved 2 workflow result files to ../ground-truth-vulnerabilities/tools_output/pinny/workflow_with_issues\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Paths\n",
    "FINDINGS_FILE = Path(\"../ground-truth-vulnerabilities/tools_output/pinny/findings.txt\")\n",
    "WORKFLOWS_DIR = Path(\"../ground-truth-vulnerabilities/.github/workflows\")\n",
    "OUTPUT_DIR = Path(\"../ground-truth-vulnerabilities/tools_output/pinny/workflow_with_issues\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Step 1: Extract all unpinned actions from findings.txt\n",
    "unresolved_refs = set()\n",
    "with open(FINDINGS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if \"Branch references are being used\" in line or \"No exact match found for ref\" in line:\n",
    "            parts = line.strip().split(\":\")\n",
    "            if parts:\n",
    "                action = parts[-1].strip().strip(\"`\")\n",
    "                if action:\n",
    "                    unresolved_refs.add(action)\n",
    "\n",
    "print(f\"Found {len(unresolved_refs)} unpinned actions in findings.txt\")\n",
    "\n",
    "# Step 2: Search these refs in all workflow files\n",
    "findings_by_file = defaultdict(list)\n",
    "\n",
    "yml_files = list(WORKFLOWS_DIR.glob(\"*.yml\")) + list(WORKFLOWS_DIR.glob(\"*.yaml\"))\n",
    "for yml_file in yml_files:\n",
    "    with open(yml_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for idx, line in enumerate(lines, 1):\n",
    "        for ref in unresolved_refs:\n",
    "            if ref in line:\n",
    "                findings_by_file[yml_file.name].append({\n",
    "                    \"rule_id\": \"unpinned-github-actions\",\n",
    "                    \"meta\": {\n",
    "                        \"action\": ref,\n",
    "                        \"line_snippet\": line.strip(),\n",
    "                        \"line_number\": idx\n",
    "                    }\n",
    "                })\n",
    "\n",
    "# Step 3: Save per-workflow normalized output\n",
    "for wf_file, findings in findings_by_file.items():\n",
    "    summary = {\n",
    "        \"total_findings\": len(findings),\n",
    "        \"by_rule\": {\"unpinned-github-actions\": len(findings)}\n",
    "    }\n",
    "\n",
    "    result = {\n",
    "        \"workflow\": wf_file,\n",
    "        \"tool\": \"pinny\",\n",
    "        \"summary\": summary,\n",
    "        \"findings\": findings\n",
    "    }\n",
    "\n",
    "    with open(OUTPUT_DIR / f\"{wf_file}.json\", \"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(result, f_out, indent=2)\n",
    "\n",
    "print(f\"Saved {len(findings_by_file)} workflow result files to {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed21a89",
   "metadata": {},
   "source": [
    "zizmor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13478090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 22 workflow result files to ../ground-truth-vulnerabilities/tools_output/zizmor/workflow_with_issues\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Paths\n",
    "input_file = Path(\"../ground-truth-vulnerabilities/tools_output/zizmor/findings.txt\")\n",
    "output_dir = Path(\"../ground-truth-vulnerabilities/tools_output/zizmor/workflow_with_issues\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Patterns\n",
    "warning_pattern = re.compile(r\"warning\\[(.+?)\\]: (.+)\")\n",
    "location_pattern = re.compile(r\"-->\\s+\\.github/workflows/(.+?):(\\d+):(\\d+)\")\n",
    "note_pattern = re.compile(r\"= note: (.+)\", re.IGNORECASE)\n",
    "\n",
    "# Storage\n",
    "grouped = defaultdict(list)\n",
    "current = {}\n",
    "\n",
    "# Read and parse\n",
    "with input_file.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # New finding block\n",
    "        m = warning_pattern.match(line)\n",
    "        if m:\n",
    "            # Save previous\n",
    "            if current.get(\"workflow\"):\n",
    "                grouped[current[\"workflow\"]].append(current)\n",
    "            # Start new\n",
    "            rule_id, message = m.groups()\n",
    "            current = {\n",
    "                \"rule_id\": rule_id,\n",
    "                \"message\": message,\n",
    "                \"workflow\": None,\n",
    "                \"line\": None,\n",
    "                \"column\": None,\n",
    "                \"note\": None\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        # Detect workflow file location\n",
    "        m = location_pattern.match(line)\n",
    "        if m:\n",
    "            workflow, line_num, col_num = m.groups()\n",
    "            current[\"workflow\"] = workflow.strip()\n",
    "            current[\"line\"] = int(line_num)\n",
    "            current[\"column\"] = int(col_num)\n",
    "            continue\n",
    "\n",
    "        # Detect notes\n",
    "        m = note_pattern.match(line)\n",
    "        if m:\n",
    "            current[\"note\"] = m.group(1)\n",
    "            continue\n",
    "\n",
    "# Save last pending finding\n",
    "if current.get(\"workflow\"):\n",
    "    grouped[current[\"workflow\"]].append(current)\n",
    "\n",
    "# Write outputs\n",
    "for workflow_path, findings in grouped.items():\n",
    "    workflow_file = Path(workflow_path).name\n",
    "\n",
    "    rule_summary = {}\n",
    "    for f in findings:\n",
    "        rule = f[\"rule_id\"]\n",
    "        rule_summary[rule] = rule_summary.get(rule, 0) + 1\n",
    "\n",
    "    result = {\n",
    "        \"workflow\": workflow_file,\n",
    "        \"tool\": \"zizmor\",\n",
    "        \"summary\": {\n",
    "            \"total_findings\": len(findings),\n",
    "            \"by_rule\": rule_summary\n",
    "        },\n",
    "        \"findings\": findings\n",
    "    }\n",
    "\n",
    "    out_path = output_dir / f\"{workflow_file}.json\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        json.dump(result, out, indent=2)\n",
    "\n",
    "print(f\"Saved {len(grouped)} workflow result files to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbaf421",
   "metadata": {},
   "source": [
    "scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8876beae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Scorecard simulation on 18 workflows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating:   0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:06<00:00,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ“] All scorecard workflow results saved to: ../ground-truth-vulnerabilities/tools_output/scorecard/findings_all.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "WORKFLOW_DIR = Path(\"../ground-truth-vulnerabilities/.github/workflows\")\n",
    "TEMP_REPO_DIR = Path(\"scorecard_tmp_repo\")\n",
    "FINDINGS_PATH = Path(\"../ground-truth-vulnerabilities/tools_output/scorecard/findings_all.json\")\n",
    "FINDINGS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Clean previous run\n",
    "if TEMP_REPO_DIR.exists():\n",
    "    shutil.rmtree(TEMP_REPO_DIR)\n",
    "TEMP_REPO_DIR.mkdir(parents=True)\n",
    "(TEMP_REPO_DIR / \".github\" / \"workflows\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "all_workflow_results = []\n",
    "\n",
    "yml_files = list(WORKFLOW_DIR.glob(\"*.yml\")) + list(WORKFLOW_DIR.glob(\"*.yaml\"))\n",
    "print(f\"Running Scorecard simulation on {len(yml_files)} workflows...\")\n",
    "\n",
    "for wf in tqdm(yml_files, desc=\"Simulating\"):\n",
    "    # Clean temp workflows folder\n",
    "    temp_wf_dir = TEMP_REPO_DIR / \".github\" / \"workflows\"\n",
    "    for f in temp_wf_dir.glob(\"*\"):\n",
    "        f.unlink()\n",
    "\n",
    "    # Copy workflow into temp repo\n",
    "    temp_wf_path = temp_wf_dir / wf.name\n",
    "    shutil.copy(wf, temp_wf_path)\n",
    "\n",
    "    # Run Scorecard\n",
    "    result = subprocess.run(\n",
    "        [\"../../tools/scorecard/scorecard\", f\"--local={TEMP_REPO_DIR}\", \"--format=json\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        json_data = json.loads(result.stdout)\n",
    "    except json.JSONDecodeError:\n",
    "        continue\n",
    "\n",
    "    json_data[\"workflow\"] = wf.name\n",
    "    all_workflow_results.append(json_data)\n",
    "\n",
    "# Save all results to a single JSON file\n",
    "with open(FINDINGS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_workflow_results, f, indent=2)\n",
    "\n",
    "print(f\"[âœ“] All scorecard workflow results saved to: {FINDINGS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8fd51144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 17 workflow files to ../ground-truth-vulnerabilities/tools_output/scorecard/workflow_with_issues\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Paths\n",
    "input_file = Path(\"../ground-truth-vulnerabilities/tools_output/scorecard/findings_all.json\")\n",
    "output_dir = Path(\"../ground-truth-vulnerabilities/tools_output/scorecard/workflow_with_issues\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Rules we care about\n",
    "relevant_rules = {\n",
    "    \"Dangerous-Workflow\",\n",
    "    \"Pinned-Dependencies\",\n",
    "    \"Token-Permissions\",\n",
    "    \"SAST\",\n",
    "}\n",
    "\n",
    "# Load all scorecard JSON results\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    workflows_data = json.load(f)\n",
    "\n",
    "saved = 0\n",
    "for wf in workflows_data:\n",
    "    workflow_name = wf.get(\"workflow\")\n",
    "    checks = wf.get(\"checks\", [])\n",
    "    findings = []\n",
    "\n",
    "    for check in checks:\n",
    "        name = check.get(\"name\")\n",
    "        score = check.get(\"score\")\n",
    "\n",
    "        if name in relevant_rules and score is not None and score < 10 and score != -1:\n",
    "            findings.append({\n",
    "                \"name\": name,\n",
    "                \"score\": score,\n",
    "                \"reason\": check.get(\"reason\"),\n",
    "                \"details\": check.get(\"details\"),\n",
    "                \"documentation\": check.get(\"documentation\", {})\n",
    "            })\n",
    "\n",
    "    if not findings:\n",
    "        continue\n",
    "\n",
    "    # Group by rule name\n",
    "    by_rule = defaultdict(int)\n",
    "    for f in findings:\n",
    "        by_rule[f[\"name\"]] += 1\n",
    "\n",
    "    result = {\n",
    "        \"workflow\": workflow_name,\n",
    "        \"tool\": \"scorecard\",\n",
    "        \"summary\": {\n",
    "            \"total_findings\": len(findings),\n",
    "            \"by_rule\": dict(by_rule)\n",
    "        },\n",
    "        \"findings\": findings\n",
    "    }\n",
    "\n",
    "    out_path = output_dir / f\"{workflow_name}.json\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(result, f_out, indent=2)\n",
    "        saved += 1\n",
    "\n",
    "print(f\"Saved {saved} workflow files to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4bfb2",
   "metadata": {},
   "source": [
    "semgrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d44a033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ“] Saved 5 normalized workflow result files to ../ground-truth-vulnerabilities/tools_output/semgrep/workflow_with_issues\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Input and output paths\n",
    "input_file = Path(\"../ground-truth-vulnerabilities/tools_output/semgrep/findings.json\")\n",
    "workflow_output_dir = Path(\"../ground-truth-vulnerabilities/tools_output/semgrep/workflow_with_issues\")\n",
    "workflow_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load Semgrep findings JSON\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Group findings by file (workflow)\n",
    "grouped = defaultdict(list)\n",
    "for finding in data.get(\"results\", []):\n",
    "    path = Path(finding.get(\"path\", \"\"))\n",
    "    if not path.name:\n",
    "        continue\n",
    "\n",
    "    workflow_name = path.name\n",
    "    grouped[workflow_name].append(finding)\n",
    "\n",
    "# Normalize and save\n",
    "for wf_name, findings in grouped.items():\n",
    "    structured = []\n",
    "    rule_counts = defaultdict(int)\n",
    "\n",
    "    for f in findings:\n",
    "        rule_id = f.get(\"check_id\", \"unknown\").split('.')[-1]\n",
    "        rule_counts[rule_id] += 1\n",
    "\n",
    "        structured.append({\n",
    "            \"rule\": rule_id,\n",
    "            \"line\": f.get(\"start\", {}).get(\"line\"),\n",
    "            \"code\": f.get(\"extra\", {}).get(\"lines\"),\n",
    "            \"note\": f.get(\"extra\", {}).get(\"message\"),\n",
    "            \"documentation\": f.get(\"extra\", {}).get(\"shortlink\")\n",
    "        })\n",
    "\n",
    "    output_data = {\n",
    "        \"workflow\": wf_name,\n",
    "        \"tool\": \"semgrep\",\n",
    "        \"summary\": {\n",
    "            \"total_findings\": len(structured),\n",
    "            \"by_rule\": dict(rule_counts)\n",
    "        },\n",
    "        \"findings\": structured\n",
    "    }\n",
    "\n",
    "    with open(workflow_output_dir / f\"{wf_name}.json\", \"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(output_data, f_out, indent=2)\n",
    "\n",
    "print(f\"[âœ“] Saved {len(list(workflow_output_dir.glob('*.json')))} normalized workflow result files to {workflow_output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02a8feb",
   "metadata": {},
   "source": [
    "ggshield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65a3a8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ“] Saved 0 normalized workflow result files to ../ground-truth-vulnerabilities/tools_output/ggshield/workflow_with_issues\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "input_path = Path(\"../ground-truth-vulnerabilities/tools_output/ggshield/findings.json\")\n",
    "output_dir = Path(\"../ground-truth-vulnerabilities/tools_output/ggshield/workflow_with_issues\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load JSON data\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "saved = 0\n",
    "\n",
    "# Extract and normalize\n",
    "for entity in data.get(\"entities_with_incidents\", []):\n",
    "    incidents = entity.get(\"incidents\", [])\n",
    "    if not incidents:\n",
    "        continue\n",
    "\n",
    "    filename = Path(entity[\"filename\"]).name\n",
    "    findings = []\n",
    "\n",
    "    for inc in incidents:\n",
    "        rule = inc.get(\"policy\", \"secret-detected\")\n",
    "        documentation = inc.get(\"detector_documentation\")\n",
    "        note = inc.get(\"type\")\n",
    "\n",
    "        for occ in inc.get(\"occurrences\", []):\n",
    "            findings.append({\n",
    "                \"rule\": rule,\n",
    "                \"line\": occ.get(\"line_start\"),\n",
    "                \"code\": occ.get(\"match\"),\n",
    "                \"note\": note,\n",
    "                \"documentation\": documentation\n",
    "            })\n",
    "\n",
    "    if not findings:\n",
    "        continue\n",
    "\n",
    "    # Build summary by rule\n",
    "    by_rule = {}\n",
    "    for finding in findings:\n",
    "        r = finding[\"rule\"]\n",
    "        by_rule[r] = by_rule.get(r, 0) + 1\n",
    "\n",
    "    result = {\n",
    "        \"workflow\": filename,\n",
    "        \"tool\": \"ggshield\",\n",
    "        \"summary\": {\n",
    "            \"total_findings\": len(findings),\n",
    "            \"by_rule\": by_rule\n",
    "        },\n",
    "        \"findings\": findings\n",
    "    }\n",
    "\n",
    "    with open(output_dir / f\"{filename}.json\", \"w\", encoding=\"utf-8\") as out_f:\n",
    "        json.dump(result, out_f, indent=2)\n",
    "        saved += 1\n",
    "\n",
    "print(f\"[âœ“] Saved {saved} normalized workflow result files to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1c6ef0",
   "metadata": {},
   "source": [
    "EXTRACT RESULTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c993cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ“] Saved 18 merged workflow files to ../ground-truth-vulnerabilities/results/merged\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Directory where each tool's normalized results live\n",
    "TOOLS = [\"frizbee\", \"pinny\", \"semgrep\", \"scorecard\", \"poutine\", \"scharf\", \"zizmor\", \"actionlint\", \"ggshield\"]\n",
    "base_dir = Path(\"../ground-truth-vulnerabilities/tools_output\")\n",
    "output_dir = Path(\"../ground-truth-vulnerabilities/results/merged\") \n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Collect all results by workflow\n",
    "workflow_results = defaultdict(dict)\n",
    "\n",
    "for tool in TOOLS:\n",
    "    tool_dir = base_dir / tool / \"workflow_with_issues\"\n",
    "    if not tool_dir.exists():\n",
    "        continue\n",
    "\n",
    "    for wf_file in tool_dir.glob(\"*.json\"):\n",
    "        wf_name = wf_file.name\n",
    "        with open(wf_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            tool_result = json.load(f)\n",
    "            workflow_results[wf_name][tool] = tool_result\n",
    "\n",
    "# Save merged files\n",
    "for wf_name, tool_dict in workflow_results.items():\n",
    "    merged = {\n",
    "        \"workflow\": wf_name,\n",
    "        \"tools\": tool_dict\n",
    "    }\n",
    "    with open(output_dir / wf_name, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        json.dump(merged, out_f, indent=2)\n",
    "\n",
    "print(f\"[âœ“] Saved {len(workflow_results)} merged workflow files to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a17f556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ“] Normalized and saved files to: ../ground-truth-vulnerabilities/results/normalized\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Directories\n",
    "input_dir = Path(\"../ground-truth-vulnerabilities/results/merged\")\n",
    "output_dir = Path(\"../ground-truth-vulnerabilities/results/normalized\")\n",
    "mapping_file = Path(\"../capabilities/rules_mapping.csv\")\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Ordered: Most specific -> Least specific\n",
    "# ----------------------------\n",
    "ACTIONLINT_PATTERNS = [\n",
    "    ([\"[action]\", \"action is too old to run on GitHub Actions\"], \"pinning-check\"),\n",
    "    ([\"[expression]\", \"undefined variable\"], \"workflow_structural_check\"),\n",
    "    ([\"[expression]\", \"undefined function\"], \"workflow_structural_check\"),\n",
    "    ([\"[expression]\", \"is not defined in object type\"], \"workflow_structural_check\"),\n",
    "    ([\"[expression]\", \"value cannot be compared to\"], \"control-flow-check\"),\n",
    "    ([\"[expression]\", \"untrusted\"], \"injection-check\"),\n",
    "    ([\"[expression]\", \"by reusable workflow\"], \"workflow_structural_check\"),\n",
    "    ([\"[expression]\", \"availability for more details\"], \"workflow_structural_check\"),\n",
    "    ([\"[workflow-call]\"], \"workflow_structural_check\"),\n",
    "    ([\"[syntax-check]\"], \"workflow_structural_check\"),\n",
    "    ([\"[expression]\"], \"control-flow-check\"),\n",
    "    ([\"[shellcheck]\"], \"workflow_structural_check\"),\n",
    "    ([\"[pyflakes]\"], \"workflow_structural_check\"),\n",
    "    ([\"[job-needs]\"], \"workflow_structural_check\"),\n",
    "    ([\"[matrix]\"], \"workflow_structural_check\"),\n",
    "    ([\"[events]\"], \"workflow_structural_check\"),\n",
    "    ([\"[glob]\"], \"workflow_structural_check\"),\n",
    "    ([\"[runner-label]\"], \"workflow_structural_check\"),\n",
    "    ([\"[action]\"], \"workflow_structural_check\"),\n",
    "    ([\"[shell-name]\"], \"workflow_structural_check\"),\n",
    "    ([\"[id]\"], \"workflow_structural_check\"),\n",
    "    ([\"[credentials]\"], \"secrets-check\"),\n",
    "    ([\"[env-var]\"], \"workflow_structural_check\"),\n",
    "    ([\"[permissions]\"], \"permissions-check\"),\n",
    "    ([\"[deprecated-commands]\"], \"injection-check\"),\n",
    "    ([\"[if-cond]\"], \"control-flow-check\"),\n",
    "]\n",
    "\n",
    "# ----------------------------\n",
    "# Load all other tools' rules\n",
    "# ----------------------------\n",
    "rule_map = defaultdict(list)\n",
    "with open(mapping_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        tool = row[\"tool_name\"].strip()\n",
    "        if tool == \"actionlint\":\n",
    "            continue  # Skip, handled manually\n",
    "        rule = row[\"rule\"].strip()\n",
    "        cap = row[\"capability\"].strip()\n",
    "        rule_map[(tool, rule)].append(cap)\n",
    "\n",
    "# ----------------------------\n",
    "# Match actionlint: first matching pattern wins\n",
    "# ----------------------------\n",
    "def match_actionlint_capability(message):\n",
    "    msg = message.lower()\n",
    "    for keywords, capability in ACTIONLINT_PATTERNS:\n",
    "        if all(kw.lower() in msg for kw in keywords):\n",
    "            return capability\n",
    "    return None\n",
    "\n",
    "# ----------------------------\n",
    "# Generic capability assignment\n",
    "# ----------------------------\n",
    "def get_capability(tool, finding, rule_hint=None):\n",
    "    for key in [\"rule\", \"rule_id\", \"name\"]:\n",
    "        if key in finding:\n",
    "            rule_val = finding[key]\n",
    "            if (tool, rule_val) in rule_map:\n",
    "                return rule_map[(tool, rule_val)][0]\n",
    "\n",
    "    if rule_hint and (tool, rule_hint) in rule_map:\n",
    "        return rule_map[(tool, rule_hint)][0]\n",
    "\n",
    "    if tool == \"actionlint\" and \"message\" in finding:\n",
    "        return match_actionlint_capability(finding[\"message\"])\n",
    "\n",
    "    return None\n",
    "\n",
    "# ----------------------------\n",
    "# Main Processing Loop\n",
    "# ----------------------------\n",
    "for file_path in input_dir.glob(\"*.json\"):\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for tool, result in data.get(\"tools\", {}).items():\n",
    "        findings = result.get(\"findings\", [])\n",
    "\n",
    "        # For tools like scharf where rule is only in summary\n",
    "        by_rule = result.get(\"summary\", {}).get(\"by_rule\", {})\n",
    "        if tool == \"scharf\" and by_rule:\n",
    "            for rule_name in by_rule:\n",
    "                cap = rule_map.get((tool, rule_name), [None])[0]\n",
    "                if cap:\n",
    "                    for finding in findings:\n",
    "                        finding[\"capability\"] = cap\n",
    "            continue\n",
    "\n",
    "        # Normal tools and actionlint\n",
    "        for finding in findings:\n",
    "            cap = get_capability(tool, finding)\n",
    "            if cap:\n",
    "                finding[\"capability\"] = cap\n",
    "\n",
    "    # Save final output\n",
    "    output_file = output_dir / file_path.name\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        json.dump(data, out_f, indent=2)\n",
    "\n",
    "print(f\"[âœ“] Normalized and saved files to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a8cc800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "findings summary saved to: ../ground-truth-vulnerabilities/results/tools_findings_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Directories\n",
    "input_dir = Path(\"../ground-truth-vulnerabilities/results/normalized\")\n",
    "output_csv = Path(\"../ground-truth-vulnerabilities/results/tools_findings_summary.csv\")\n",
    "\n",
    "# Collect all tool names and findings\n",
    "all_tools = set()\n",
    "all_workflows = []\n",
    "\n",
    "for file_path in input_dir.glob(\"*.json\"):\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    workflow_name = data.get(\"workflow\", file_path.stem)\n",
    "    tool_counts = {}\n",
    "\n",
    "    for tool_name, tool_data in data.get(\"tools\", {}).items():\n",
    "        findings = tool_data.get(\"findings\", [])\n",
    "        count = len(findings)\n",
    "        tool_counts[tool_name] = count\n",
    "        all_tools.add(tool_name)\n",
    "\n",
    "    all_workflows.append((workflow_name, tool_counts))\n",
    "\n",
    "# Sort workflows alphabetically\n",
    "all_workflows.sort(key=lambda x: x[0].lower())\n",
    "\n",
    "# Prepare CSV rows\n",
    "all_tools = sorted(all_tools)\n",
    "header = [\"workflow\"] + all_tools\n",
    "\n",
    "with open(output_csv, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for wf_name, tool_counts in all_workflows:\n",
    "        row = [wf_name] + [tool_counts.get(tool, 0) for tool in all_tools]\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"findings summary saved to: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b79b26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ“] Extracted 18 workflow names.\n",
      "[âœ“] Saved to: ../ground-truth-vulnerabilities/results/manual_review/selected_workflows.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === Paths ===\n",
    "input_csv = Path(\"../ground-truth-vulnerabilities/results/tools_findings_summary.csv\")\n",
    "output_txt = Path(\"../ground-truth-vulnerabilities/results/manual_review/selected_workflows.txt\")\n",
    "output_txt.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === Load CSV ===\n",
    "df = pd.read_csv(input_csv, index_col=0)\n",
    "\n",
    "# === Extract all index entries (workflow file names) ===\n",
    "workflow_names = df.index.tolist()\n",
    "\n",
    "# === Save to file ===\n",
    "with open(output_txt, \"w\") as f:\n",
    "    for wf in workflow_names:\n",
    "        f.write(wf + \"\\n\")\n",
    "\n",
    "print(f\"[âœ“] Extracted {len(workflow_names)} workflow names.\")\n",
    "print(f\"[âœ“] Saved to: {output_txt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "033be64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ“] Created summary for: 1-GHSA-4mgv-m5cm-f9h7__terraform.yml.json\n",
      "[âœ“] Created summary for: 10-GHSA-h3qr-39j9-4r5v__gradle-build.yml.json\n",
      "[âœ“] Created summary for: 12-GHSA-8v8w-v8xg-79rf__fix-style.yml.json\n",
      "[âœ“] Created summary for: 17-GHSA-xj87-mqvh-88w2__test.yml.json\n",
      "[âœ“] Created summary for: 18-GHSA-7x29-qqmq-v6qc__main.yml.json\n",
      "[âœ“] Created summary for: 19-GHSA-cxww-7g56-2vh6__pipeline.yaml.json\n",
      "[âœ“] Created summary for: 2-GHSA-g86g-chm8-7r2p__spelling.yml.json\n",
      "[âœ“] Created summary for: 21-GHSA-5xr6-xhww-33m4__PublishRelease.yaml.json\n",
      "[âœ“] Created summary for: 22-GHSA-vqf5-2xx6-9wfm__codeql-analysis.yml.json\n",
      "[âœ“] Created summary for: 26-GHSA-mxr3-8whj-j74r__code-review.yml.json\n",
      "[âœ“] Created summary for: 26-GHSA-mxr3-8whj-j74r__test.yml.json\n",
      "[âœ“] Created summary for: 27-GHSA-2487-9f55-2vg9__action.yml.json\n",
      "[âœ“] Created summary for: 28-GHSA-m32f-fjw2-37v3__bullfrog.yml.json\n",
      "[âœ“] Created summary for: 29-GHSA-phf6-hm3h-x8qp__scalafmt-fix.yml.json\n",
      "[âœ“] Created summary for: 4-GHSA-4xqx-pqpj-9fqw__known-vulnerable-actions.yml.json\n",
      "[âœ“] Created summary for: 5-GHSA-f9qj-7gh3-mhj4__run-terraform.yml.json\n",
      "[âœ“] Created summary for: 8-GHSA-6q4m-7476-932w__build-and-upload.yml.json\n",
      "[âœ“] Created summary for: 9-GHSA-rg3q-prf8-qxmp__check-wip.yaml.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "# === Paths ===\n",
    "input_dir = Path(\"../ground-truth-vulnerabilities/results/normalized\")\n",
    "output_dir = Path(\"../ground-truth-vulnerabilities/results/manual_review\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# === Load workflow filenames ===\n",
    "with open(output_dir / \"selected_workflows.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    selected_workflows = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# === Generate summaries ===\n",
    "for wf_file in selected_workflows:\n",
    "    wf_path = input_dir / wf_file\n",
    "    if not wf_path.exists():\n",
    "        print(f\"[!] Workflow not found: {wf_file}\")\n",
    "        continue\n",
    "\n",
    "    with open(wf_path, encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    findings_data = []\n",
    "\n",
    "    for tool, tool_data in data.get(\"tools\", {}).items():\n",
    "        findings = tool_data.get(\"findings\", [])\n",
    "        for f in findings:\n",
    "            # Default values\n",
    "            line = f.get(\"line\", None)\n",
    "            column = f.get(\"column\", None)\n",
    "            message = f.get(\"message\", f.get(\"reason\", f.get(\"note\", None)))\n",
    "\n",
    "            # Fallbacks from meta\n",
    "            if not line and isinstance(f.get(\"meta\"), dict):\n",
    "                line = f[\"meta\"].get(\"line_number\") or f[\"meta\"].get(\"line\", \"N/A\")\n",
    "            if not column and isinstance(f.get(\"meta\"), dict):\n",
    "                column = f[\"meta\"].get(\"column\", \"N/A\")\n",
    "\n",
    "            # === Tool-specific message logic ===\n",
    "            if tool == \"frizbee\" and \"original\" in f:\n",
    "                message = f[\"original\"]\n",
    "            elif tool == \"pinny\" and \"meta\" in f:\n",
    "                message = f[\"meta\"].get(\"action\") or f[\"meta\"].get(\"line_snippet\")\n",
    "            elif not message and isinstance(f.get(\"meta\"), dict):\n",
    "                message = f[\"meta\"].get(\"details\", \"N/A\")\n",
    "\n",
    "            findings_data.append({\n",
    "                \"tool\": tool,\n",
    "                \"capability\": f.get(\"capability\", \"N/A\"),\n",
    "                \"line\": line if line is not None else \"N/A\",\n",
    "                \"column\": column if column is not None else \"N/A\",\n",
    "                \"message\": message if message is not None else \"N/A\"\n",
    "            })\n",
    "\n",
    "    # Save to CSV\n",
    "    out_file = output_dir / f\"{wf_file.replace('.json', '')}_summary.csv\"\n",
    "    with open(out_file, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=[\"tool\", \"capability\", \"line\", \"column\", \"message\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(findings_data)\n",
    "\n",
    "    print(f\"[âœ“] Created summary for: {wf_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
